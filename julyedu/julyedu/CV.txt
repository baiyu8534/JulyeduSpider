# CV
## 基于深度学习的目标检测技术演进：R-CNN、Fast R-CNN、Faster R-CNN、YOLO、SSD
一、目标检测常见算法

object detection，就是在给定的图片中精确找到物体所在位置，并标注出物体的类别。所以，object detection要解决的问题就是物体在哪里以及是什么的整个流程问题。

然而，这个问题可不是那么容易解决的，物体的尺寸变化范围很大，摆放物体的角度，姿态不定，而且可以出现在图片的任何地方，更何况物体还可以是多个类别。

目前学术和工业界出现的目标检测算法分成3类：
1. 传统的目标检测算法：Cascade + HOG/DPM + Haar/SVM以及上述方法的诸多改进、优化；

2. 候选区域/框 + 深度学习分类：通过提取候选区域，并对相应区域进行以深度学习方法为主的分类的方案，如：
R-CNN（Selective Search + CNN + SVM）
SPP-net（ROI Pooling）
Fast R-CNN（Selective Search + CNN + ROI）
Faster R-CNN（RPN + CNN + ROI）
R-FCN
等系列方法；

3. 基于深度学习的回归方法：YOLO/SSD/DenseBox 等方法；以及最近出现的结合RNN算法的RRC detection；结合DPM的Deformable CNN等
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525258820_318.jpg'/>

传统目标检测流程：
1）区域选择（穷举策略：采用滑动窗口，且设置不同的大小，不同的长宽比对图像进行遍历，时间复杂度高）
2）特征提取（SIFT、HOG等；形态多样性、光照变化多样性、背景多样性使得特征鲁棒性差）
3）分类器分类（主要有SVM、Adaboost等）

二、传统的目标检测算法
2.1 从图像识别的任务说起
这里有一个图像任务：既要把图中的物体识别出来，又要用方框框出它的位置。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517392987_360.jpg'/>

这个任务本质上就是这两个问题：一：图像识别，二：定位。

图像识别（classification）：
输入：图片
输出：物体的类别
评估方法：准确率
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393007_344.jpg'/>

定位（localization）：
输入：图片
输出：方框在图片中的位置（x,y,w,h）
评估方法：检测评价函数intersection-over-union（关于什么是IOU，请参看本深度学习分类下第55题：https://www.julyedu.com/question/big/kp_id/26/ques_id/2138）
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393013_253.png'/>
卷积神经网络CNN已经帮我们完成了图像识别（判定是猫还是狗）的任务了，我们只需要添加一些额外的功能来完成定位任务即可。

定位的问题的解决思路有哪些？
思路一：看做回归问题
看做回归问题，我们需要预测出（x,y,w,h）四个参数的值，从而得出方框的位置。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393027_336.png'/>

步骤1:
　　•	先解决简单问题， 搭一个识别图像的神经网络
　　•	在AlexNet VGG GoogleLenet上fine-tuning一下（关于什么是微调fine-tuning，请参看本深度学习分类下第54题：https://www.julyedu.com/question/big/kp_id/26/ques_id/2137）
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393041_799.jpg'/>
 
步骤2:
　　•	在上述神经网络的尾部展开（也就说CNN前面保持不变，我们对CNN的结尾处作出改进：加了两个头：“分类头”和“回归头”）
　　•	成为classification + regression模式
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393046_527.png'/>

步骤3:
　　•	Regression那个部分用欧氏距离损失
　　•	使用SGD训练

步骤4:
　　•	预测阶段把2个头部拼上
　　•	完成不同的功能

这里需要进行两次fine-tuning
第一次在ALexNet上做，第二次将头部改成regression head，前面不变，做一次fine-tuning

Regression的部分加在哪？

有两种处理方法：
　　•	加在最后一个卷积层后面（如VGG）
　　•	加在最后一个全连接层后面（如R-CNN）

regression太难做了，应想方设法转换为classification问题。
regression的训练参数收敛的时间要长得多，所以上面的网络采取了用classification的网络来计算出网络共同部分的连接权值。

思路二：取图像窗口
　　•	还是刚才的classification + regression思路
　　•	咱们取不同的大小的“框”
　　•	让框出现在不同的位置，得出这个框的判定得分
　　•	取得分最高的那个框

左上角的黑框：得分0.5
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393142_364.jpg'/>

右上角的黑框：得分0.75
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393149_762.jpg'/>

左下角的黑框：得分0.6
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393156_659.jpg'/>

右下角的黑框：得分0.8
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393161_204.jpg'/>

根据得分的高低，我们选择了右下角的黑框作为目标位置的预测。
注：有的时候也会选择得分最高的两个框，然后取两框的交集作为最终的位置预测。

疑惑：框要取多大？
取不同的框，依次从左上角扫到右下角。非常粗暴啊。

总结一下思路：
对一张图片，用各种大小的框（遍历整张图片）将图片截取出来，输入到CNN，然后CNN会输出这个框的得分（classification）以及这个框图片对应的x,y,h,w（regression）。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393174_730.jpg'/>

这方法实在太耗时间了，做个优化。
原来网络是这样的：
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393184_685.jpg'/>

优化成这样：把全连接层改为卷积层，这样可以提提速。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393192_919.jpg'/>


2.2 物体检测（Object Detection）
当图像有很多物体怎么办的？难度可是一下暴增啊。

那任务就变成了：多物体识别+定位多个物体
那把这个任务看做分类问题？
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393210_554.jpg'/>

看成分类问题有何不妥？
　　•	你需要找很多位置， 给很多个不同大小的框
　　•	你还需要对框内的图像分类
　　•	当然， 如果你的GPU很强大， 恩， 那加油做吧…

所以，传统目标检测的主要问题是：
1）基于滑动窗口的区域选择策略没有针对性，时间复杂度高，窗口冗余
2）手工设计的特征对于多样性的变化没有很好的鲁棒性

看做classification， 有没有办法优化下？我可不想试那么多框那么多位置啊！

三、候选区域/窗 + 深度学习分类
3.1 R-CNN横空出世
有人想到一个好方法：预先找出图中目标可能出现的位置，即候选区域（Region Proposal）。利用图像中的纹理、边缘、颜色等信息，可以保证在选取较少窗口(几千甚至几百）的情况下保持较高的召回率（Recall）。

所以，问题就转变成找出可能含有物体的区域/框（也就是候选区域/框，比如选2000个候选框），这些框之间是可以互相重叠互相包含的，这样我们就可以避免暴力枚举所有框了。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393217_390.jpg'/>

大牛们发明好多选定候选框Region Proposal的方法，比如Selective Search和EdgeBoxes。那提取候选框用到的算法“选择性搜索”到底怎么选出这些候选框的呢？具体可以看一下PAMI2015的“What makes for effective detection proposals？”

以下是各种选定候选框的方法的性能对比。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393227_723.jpg'/>

有了候选区域，剩下的工作实际就是对候选区域进行图像分类的工作（特征提取+分类）。

对于图像分类，不得不提的是2012年ImageNet大规模视觉识别挑战赛（ILSVRC）上，机器学习泰斗Geoffrey Hinton教授带领学生Krizhevsky使用卷积神经网络将ILSVRC分类任务的Top-5 error降低到了15.3%，而使用传统方法的第二名top-5 error高达 26.2%。此后，卷积神经网络CNN占据了图像分类任务的绝对统治地位。

2014年，RBG（Ross B. Girshick）使用Region Proposal + CNN代替传统目标检测使用的滑动窗口+手工设计特征，设计了R-CNN框架，使得目标检测取得巨大突破，并开启了基于深度学习目标检测的热潮。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393238_127.png'/>

R-CNN的简要步骤如下
(1) 输入测试图像
(2) 利用选择性搜索Selective Search算法在图像中从下到上提取2000个左右的可能包含物体的候选区域Region Proposal
(3) 因为取出的区域大小各自不同，所以需要将每个Region Proposal缩放（warp）成统一的227x227的大小并输入到CNN，将CNN的fc7层的输出作为特征
(4) 将每个Region Proposal提取到的CNN特征输入到SVM进行分类

具体步骤则如下
步骤一：训练（或者下载）一个分类模型（比如AlexNet）
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393249_806.jpg'/>

步骤二：对该模型做fine-tuning
　　•	将分类数从1000改为21，比如20个物体类别 + 1个背景
　　•	去掉最后一个全连接层
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393259_895.png'/>

步骤三：特征提取
　　•	提取图像的所有候选框（选择性搜索Selective Search）
　　•	对于每一个区域：修正区域大小以适合CNN的输入，做一次前向运算，将第五个池化层的输出（就是对候选框提取到的特征）存到硬盘
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393264_688.png'/>

步骤四：训练一个SVM分类器（二分类）来判断这个候选框里物体的类别
每个类别对应一个SVM，判断是不是属于这个类别，是就是positive，反之nagative。

比如下图，就是狗分类的SVM
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393277_461.png'/>

步骤五：使用回归器精细修正候选框位置：对于每一个类，训练一个线性回归模型去判定这个框是否框得完美。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393285_829.png'/>

细心的同学可能看出来了问题，R-CNN虽然不再像传统方法那样穷举，但R-CNN流程的第一步中对原始图片通过Selective Search提取的候选框region proposal多达2000个左右，而这2000个候选框每个框都需要进行CNN提特征+SVM分类，计算量很大，导致R-CNN检测速度很慢，一张图都需要47s。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525103713_502.png'/>

有没有方法提速呢？答案是有的，这2000个region proposal不都是图像的一部分吗，那么我们完全可以对图像提一次卷积层特征，然后只需要将region proposal在原图的位置映射到卷积层特征图上，这样对于一张图像我们只需要提一次卷积层特征，然后将每个region proposal的卷积层特征输入到全连接层做后续操作。

但现在的问题是每个region proposal的尺度不一样，而全连接层输入必须是固定的长度，所以直接这样输入全连接层肯定是不行的。SPP Net恰好可以解决这个问题。

3.2 SPP Net
SPP：Spatial Pyramid Pooling（空间金字塔池化）

SPP-Net是出自2015年发表在IEEE上的论文-《Spatial Pyramid Pooling in Deep ConvolutionalNetworks for Visual Recognition》。

众所周知，CNN一般都含有卷积部分和全连接部分，其中，卷积层不需要固定尺寸的图像，而全连接层是需要固定大小的输入。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525243658_939.png'/>

所以当全连接层面对各种尺寸的输入数据时，就需要对输入数据进行crop（crop就是从一个大图扣出网络输入大小的patch，比如227×227），或warp（把一个边界框bounding box的内容resize成227×227）等一系列操作以统一图片的尺寸大小，比如224*224（ImageNet）、32*32(LenNet)、96*96等。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525249316_603.png'/>

所以才如你在上文中看到的，在R-CNN中，“因为取出的区域大小各自不同，所以需要将每个Region Proposal缩放（warp）成统一的227x227的大小并输入到CNN”。

但warp/crop这种预处理，导致的问题要么被拉伸变形、要么物体不全，限制了识别精确度。没太明白？说句人话就是，一张16:9比例的图片你硬是要Resize成1:1的图片，你说图片失真不？

SPP Net的作者Kaiming He等人逆向思考，既然由于全连接FC层的存在，普通的CNN需要通过固定输入图片的大小来使得全连接层的输入固定。那借鉴卷积层可以适应任何尺寸，为何不能在卷积层的最后加入某种结构，使得后面全连接层得到的输入变成固定的呢？

这个“化腐朽为神奇”的结构就是spatial pyramid pooling layer。

下图便是R-CNN和SPP Net检测流程的比较：

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525249330_874.png'/>

它的特点有两个:

1.结合空间金字塔方法实现CNNs的多尺度输入。

SPP Net的第一个贡献就是在最后一个卷积层后，接入了金字塔池化层，保证传到下一层全连接层的输入固定。

换句话说，在普通的CNN机构中，输入图像的尺寸往往是固定的（比如224*224像素），输出则是一个固定维数的向量。SPP Net在普通的CNN结构中加入了ROI池化层（ROI Pooling），使得网络的输入图像可以是任意尺寸的，输出则不变，同样是一个固定维数的向量。

简言之，CNN原本只能固定输入、固定输出，CNN加上SSP之后，便能任意输入、固定输出。神奇吧？

ROI池化层一般跟在卷积层后面，此时网络的输入可以是任意尺度的，在SPP layer中每一个pooling的filter会根据输入调整大小，而SPP的输出则是固定维数的向量，然后给到全连接FC层。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393299_167.jpg'/>

2.只对原图提取一次卷积特征
在R-CNN中，每个候选框先resize到统一大小，然后分别作为CNN的输入，这样是很低效的。
而SPP Net根据这个缺点做了优化：只对原图进行一次卷积计算，便得到整张图的卷积特征feature map，然后找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层，完成特征提取工作。

如此这般，R-CNN要对每个区域计算卷积，而SPPNet只需要计算一次卷积，从而节省了大量的计算时间，比R-CNN有一百倍左右的提速。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393307_311.jpg'/>


3.3 Fast R-CNN
SPP Net真是个好方法，R-CNN的进阶版Fast R-CNN就是在R-CNN的基础上采纳了SPP Net方法，对R-CNN作了改进，使得性能进一步提高。

R-CNN与Fast R-CNN的区别有哪些呢？
先说R-CNN的缺点：即使使用了Selective Search等预处理步骤来提取潜在的边界框bounding box作为输入，但是R-CNN仍会有严重的速度瓶颈，原因也很明显，就是计算机对所有region进行特征提取时会有重复计算，Fast-RCNN正是为了解决这个问题诞生的。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393313_544.png'/>

与R-CNN框架图对比，可以发现主要有两处不同：一是最后一个卷积层后加了一个ROI pooling layer，二是损失函数使用了多任务损失函数(multi-task loss)，将边框回归Bounding Box Regression直接加入到CNN网络中训练（关于什么是边框回归，请参看本深度学习分类下第56题：https://www.julyedu.com/question/big/kp_id/26/ques_id/2139）。

(1) ROI pooling layer实际上是SPP-NET的一个精简版，SPP-NET对每个proposal使用了不同大小的金字塔映射，而ROI pooling layer只需要下采样到一个7x7的特征图。对于VGG16网络conv5_3有512个特征图，这样所有region proposal对应了一个7*7*512维度的特征向量作为全连接层的输入。

换言之，这个网络层可以把不同大小的输入映射到一个固定尺度的特征向量，而我们知道，conv、pooling、relu等操作都不需要固定size的输入，因此，在原始图片上执行这些操作后，虽然输入图片size不同导致得到的feature map尺寸也不同，不能直接接到一个全连接层进行分类，但是可以加入这个神奇的ROI Pooling层，对每个region都提取一个固定维度的特征表示，再通过正常的softmax进行类型识别。

(2) R-CNN训练过程分为了三个阶段，而Fast R-CNN直接使用softmax替代SVM分类，同时利用多任务损失函数边框回归也加入到了网络中，这样整个的训练过程是端到端的(除去region proposal提取阶段)。

也就是说，之前R-CNN的处理流程是先提proposal，然后CNN提取特征，之后用SVM分类器，最后再做box regression，而在Fast R-CNN中，作者巧妙的把box regression放进了神经网络内部，与region分类和并成为了一个multi-task模型，实际实验也证明，这两个任务能够共享卷积特征，并相互促进。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525104022_285.png'/>
所以，Fast-RCNN很重要的一个贡献是成功的让人们看到了Region Proposal + CNN这一框架实时检测的希望，原来多类检测真的可以在保证准确率的同时提升处理速度，也为后来的Faster R-CNN做下了铺垫。

画一画重点：
R-CNN有一些相当大的缺点（把这些缺点都改掉了，就成了Fast R-CNN）。
大缺点：由于每一个候选框都要独自经过CNN，这使得花费的时间非常多。
解决：共享卷积层，现在不是每一个候选框都当做输入进入CNN了，而是输入一张完整的图片，在第五个卷积层再得到每个候选框的特征

原来的方法：许多候选框（比如两千个）-->CNN-->得到每个候选框的特征-->分类+回归
现在的方法：一张完整图片-->CNN-->得到每张候选框的特征-->分类+回归

所以容易看见，Fast R-CNN相对于R-CNN的提速原因就在于：不过不像R-CNN把每个候选区域给深度网络提特征，而是整张图提一次特征，再把候选框映射到conv5上，而SPP只需要计算一次特征，剩下的只需要在conv5层上操作就可以了。

在性能上提升也是相当明显的：
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393334_157.png'/>

3.4 Faster R-CNN
Fast R-CNN存在的问题：存在瓶颈：选择性搜索，找出所有的候选框，这个也非常耗时。那我们能不能找出一个更加高效的方法来求出这些候选框呢？

解决：加入一个提取边缘的神经网络，也就说找到候选框的工作也交给神经网络来做了。

所以，rgbd在Fast R-CNN中引入Region Proposal Network(RPN)替代Selective Search，同时引入anchor box应对目标形状的变化问题（anchor就是位置和大小固定的box，可以理解成事先设置好的固定的proposal）。

具体做法：
　　•	将RPN放在最后一个卷积层的后面
　　•	RPN直接训练得到候选区域
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393348_780.png'/>

RPN简介：
　　•	在feature map上滑动窗口
　　•	建一个神经网络用于物体分类+框位置的回归
　　•	滑动窗口的位置提供了物体的大体位置信息
　　•	框的回归提供了框更精确的位置
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393355_598.png'/>

一种网络，四个损失函数;
　　•	RPN calssification(anchor good.bad)
　　•	RPN regression(anchor->propoasal)
　　•	Fast R-CNN classification(over classes)
　　•	Fast R-CNN regression(proposal ->box)
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393380_922.png'/>

速度对比
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393385_752.png'/>

Faster R-CNN的主要贡献就是设计了提取候选区域的网络RPN，代替了费时的选择性搜索Selective Search，使得检测速度大幅提高。

最后总结一下各大算法的步骤：
RCNN
1.在图像中确定约1000-2000个候选框 (使用选择性搜索Selective Search)
2.每个候选框内图像块缩放至相同大小，并输入到CNN内进行特征提取 
3.对候选框中提取出的特征，使用分类器判别是否属于一个特定类 
4.对于属于某一类别的候选框，用回归器进一步调整其位置

Fast R-CNN
1.在图像中确定约1000-2000个候选框 (使用选择性搜索)
2.对整张图片输进CNN，得到feature map
3.找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层
4.对候选框中提取出的特征，使用分类器判别是否属于一个特定类 
5.对于属于某一类别的候选框，用回归器进一步调整其位置

Faster R-CNN
1.对整张图片输进CNN，得到feature map
2.卷积特征输入到RPN，得到候选框的特征信息
3.对候选框中提取出的特征，使用分类器判别是否属于一个特定类 
4.对于属于某一类别的候选框，用回归器进一步调整其位置
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525664484_495.jpg'/>

简言之，即如本文开头所列
R-CNN（Selective Search + CNN + SVM）
SPP-net（ROI Pooling）
Fast R-CNN（Selective Search + CNN + ROI）
Faster R-CNN（RPN + CNN + ROI）

总的来说，从R-CNN, SPP-NET, Fast R-CNN, Faster R-CNN一路走来，基于深度学习目标检测的流程变得越来越精简，精度越来越高，速度也越来越快。可以说基于region proposal的R-CNN系列目标检测方法是当前目标检测技术领域最主要的一个分支。

四、基于深度学习的回归方法
4.1 YOLO (CVPR2016, oral)
(You Only Look Once: Unified, Real-Time Object Detection)

Faster R-CNN的方法目前是主流的目标检测方法，但是速度上并不能满足实时的要求。YOLO一类的方法慢慢显现出其重要性，这类方法使用了回归的思想，利用整张图作为网络的输入，直接在图像的多个位置上回归出这个位置的目标边框，以及目标所属的类别。

我们直接看上面YOLO的目标检测的流程图：

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525171091_647.jpg'/>

(1) 给个一个输入图像，首先将图像划分成7*7的网格
(2) 对于每个网格，我们都预测2个边框（包括每个边框是目标的置信度以及每个边框区域在多个类别上的概率）
(3) 根据上一步可以预测出7*7*2个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后NMS去除冗余窗口即可（关于什么是非极大值抑制NMS，请参看本深度学习分类下第58题：https://www.julyedu.com/question/big/kp_id/26/ques_id/2141）。

可以看到整个过程非常简单，不再需要中间的region proposal找目标，直接回归便完成了位置和类别的判定。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525171142_149.jpg'/>

小结：YOLO将目标检测任务转换成一个回归问题，大大加快了检测的速度，使得YOLO可以每秒处理45张图像。而且由于每个网络预测目标窗口时使用的是全图信息，使得false positive比例大幅降低（充分的上下文信息）。

但是YOLO也存在问题：没有了Region Proposal机制，只使用7*7的网格回归会使得目标不能非常精准的定位，这也导致了YOLO的检测精度并不是很高。

4.2 SSD
(SSD: Single Shot MultiBox Detector)

上面分析了YOLO存在的问题，使用整图特征在7*7的粗糙网格内回归对目标的定位并不是很精准。那是不是可以结合region proposal的思想实现精准一些的定位？SSD结合YOLO的回归思想以及Faster R-CNN的anchor机制做到了这点。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525171268_230.jpg'/>

上图是SSD的一个框架图，首先SSD获取目标位置和类别的方法跟YOLO一样，都是使用回归，但是YOLO预测某个位置使用的是全图的特征，SSD预测某个位置使用的是这个位置周围的特征（感觉更合理一些）。

那么如何建立某个位置和其特征的对应关系呢？可能你已经想到了，使用Faster R-CNN的anchor机制。如SSD的框架图所示，假如某一层特征图(图b)大小是8*8，那么就使用3*3的滑窗提取每个位置的特征，然后这个特征回归得到目标的坐标信息和类别信息(图c)。

不同于Faster R-CNN，这个anchor是在多个feature map上，这样可以利用多层的特征并且自然的达到多尺度（不同层的feature map 3*3滑窗感受野不同）。

小结：SSD结合了YOLO中的回归思想和Faster R-CNN中的anchor机制，使用全图各个位置的多尺度区域特征进行回归，既保持了YOLO速度快的特性，也保证了窗口预测的跟Faster R-CNN一样比较精准。SSD在VOC2007上mAP可以达到72.1%，速度在GPU上达到58帧每秒。

主要参考及扩展阅读
1 https://www.cnblogs.com/skyfsm/p/6806246.html，by @Madcola
2 https://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&mid=502841131&idx=1&sn=bb3e8e6aeee2ee1f4d3f22459062b814#rd
3 https://zhuanlan.zhihu.com/p/27546796
4 https://blog.csdn.net/v1_vivian/article/details/73275259
5 https://blog.csdn.net/tinyzhao/article/details/53717136
6 Spatial Pyramid Pooling in Deep Convolutional
Networks for Visual Recognition，by Kaiming He等人
7 https://zhuanlan.zhihu.com/p/24774302
8 知乎专栏作者何之源新书《21个项目玩转深度学习——基于TensorFlow的实践详解》
9 YOLO，https://blog.csdn.net/u011534057/article/details/51244354，https://zhuanlan.zhihu.com/p/24916786

后记
咱公司七月在线开设的深度学习等一系列课程经常会讲目标检测，包括R-CNN、Fast R-CNN、Faster R-CNN，但一直没有比较好的机会深入（但当你对目标检测有个基本的了解之后，再看这些课程你会收益很大）。但目标检测这个领域实在是太火了，经常会看到一些写的不错的通俗易懂的资料，加之之前在京东上掏了一本书看了看，就这样耳濡目染中，还是开始研究了。

今年五一，从保定回京，怕高速路上堵 没坐大巴，高铁又没抢上，只好选择哐当哐当好几年没坐过的绿皮车，关键还不断晚点。在车站，用手机做个热点，修改题库，顺便终于搞清R-CNN、fast R-CNN、faster R-CNN的核心区别。有心中热爱 何惧任何啥。

为纪念这心中热爱，故成此文。
七月在线July、二零一八年五月三日
## 请简单解释下目标检测中的这个IOU评价函数（intersection-over-union）
在目标检测的评价体系中，有一个参数叫做 IoU ，简单来讲就是模型产生的目标窗口和原来标记窗口的交叠率。

具体我们可以简单的理解为：即检测结果DetectionResult与真实值Ground Truth的交集比上它们的并集，即为检测的准确率 IoU :

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525494665_290.png'/>

举个例子，下面是一张原图

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525495290_937.png'/>

然后我们对其做下目标检测，其DR = DetectionResult，GT = GroundTruth。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525494780_911.png'/>

黄色边框框起来的是：

DR⋂GT

绿色框框起来的是：
DR⋃GT

不难看出，最理想的情况就是DR与GT完全重合，即IoU = 1。

本题解析来源：https://blog.csdn.net/Eddy_zheng/article/details/52126641
## 计算图片相似度的方法有哪些?
直方图距离
平均哈希算法
感知哈希算法
差异哈希算法
## KNN与K-means区别？
有监督和无监督学习
KNN是寻找最相近的K个点,中心点是其本身,已知,求解周围点的标签众数; K-means是寻找K个聚类中心,聚类中心未知
## K-means选择初始点的方法有哪些,优缺点是什么?(列出两种以上)
随机选取初始点
选择批次距离尽可能远的K个点
选用层次聚类或者Canopy算法进行初始聚类，然后利用这些类簇的中心点作为KMeans算法初始类簇中心点
## 像素值的读写方式有哪些？(列出两种以上)
at方式循环遍历
Mat迭代器循环遍历
for循环读取
## 图像边缘检测的原理？
图像的边缘是指其周围像素灰度急剧变化的那些像素的集合，它是图像最基本的特征，而图像的边缘检测即先检测图像的边缘点，再按照某种策略将边缘点连接成轮廓，从而构成分割区域
## 图像中的角点(Harris角点)是什么？为什么用角点作为特征点？
Harris角点:在任意两个相互垂直的方向上，都有较大变化的点。
角点在保留图像图形重要特征的同时,可以有效地减少信息的数据量,使其信息的含量很高,有效地提高了计算的速度,有利于图像的可靠匹配,使得实时处理成为可能。
## 简述图像特征的SIFT描述
SIFT在不同的尺度空间上查找关键点(特征点)，并计算出关键点的方向。SIFT所查找到的关键点是一些十分突出，不会因光照，仿射变换和噪音等因素而变化的点，如角点、边缘点、暗区的亮点及亮区的暗点等。
## 简述RANSAC离群点操作方法
RANSAC算法是采用迭代的算法从一组包含离群点的被观测数据中估计一个数学模型的参数。RANSAC算法只会产生一个在一定概率下合理的结果，而更多次的迭代会使得这一概率增加。
## 简述线性分类器的原理(要求对权重矩阵进行剖析)
线性分类器就是将每个被分类样本与权重矩阵进行对比,找到相似度最大的权重行,给出响应的标签.
## 简述Cross-Entropy Loss(交叉熵损失函数)与Hinge Loss(折页损失函数)
softmax(柔性最大值)函数，一般在神经网络中， softmax可以作为分类任务的输出层。其实可以认为softmax输出的是几个类别选择的概率，比如我有一个分类任务，要分为三个类，softmax函数可以根据它们相对的大小，输出三个类别选取的概率，并且概率和为1。

Hinge Loss 是机器学习领域中的一种损失函数，可用于“最大间隔(max-margin)”分类，其最著名的应用是作为SVM的目标函数。
## 简述正则化与奥卡姆剃刀原则
正则化是为了防止过拟合

奥卡姆剃刀是一种思维方式，可用于指导我们的工作，比如我们可以用A和B达到同样的效果，但B更简单，于是我们选择B。同时，也有人说可能因为能力不足，于是我们选择更简单的方式来处理问题，这也是剃刀的原则的一种应用吧。

举个贴切的例子，做决策树分析的时候，采用9个属性的预测性能和5个属性的预测性能是相似的，那么我们就会选择5个属性来预测。
## 写出神经网络中常见的激励函数(至少三个)
Sigmoid
Tanh
ReLU
Leaky RelU
## 典型的卷积神经网络有哪些层?
INPUT(输入层)
CONV(卷积层)
RELU(非线性层)
POOL(池化层)
FC(全连接层)
## 计算题(提示:参数共享)
Example1: 200*200 image(灰度图), 40K hidden units 有多少个参数?
Example2: 200*200 image(灰度图), 40K hidden units, Filter size: 10*10 有多少个参数?
Example3: 200*200 image(灰度图), 100 Filter, Filter size:10*10 有多少参数?
Example4: 图像尺寸为[32*32*3], 卷积窗口大小为5*5, 卷积模板个数为1, 有多少参数?
Example5: 图像尺寸为[16*16*20], 卷积窗口大小为3*3, 卷积模板个数为10, 有多少参数?
Example1: 40000 * 40000 = 16 0000 0000个
Example2: 40000 * 10 * 10 = 4 00 0000个
Example3: 100 * 10 * 10 = 1 00 00个
Example4: 5 * 5 * 3 = 75个
Example5: 3 * 3 * 20 * 10 = 1800个
## 图像尺寸为 7*7, 卷积窗口大小为3*3, 步长为3, 是否能输出图像?如果能,输出图像大小为多少?如果不能,说明原因?
不能,因为 (7-3)/3 + 1 不为整数!滑动窗口会丢失
## 计算题: 输入图像尺寸为 32*32*3 , 卷积模板尺寸为 5*5 , 步长stride为1, 卷积神经元个数为 5
问题1: 输出图像尺寸为?
问题2: 每个卷积层神经元的参数个数为?
(32 - 5)/1 +1 = 28,结果就是: 28 * 28 * 5
每个神经元的参数个数为 : 5 * 5 * 3
## 如果最后一个卷积层和第一个全连接层参数量太大怎么办?
加入全局池化层
## 为什么说神经网络是端到端的网络?
特征提取包含在神经网络内部
## 当参数量 >> 样本量时候, 神经网络是如何预防过拟合?
增加数据量
加入全局池化减少参数量
减少卷积层数
迁移学习
## 什么是感受野？
某一层特征图中的一个cell,对应到原始输入的响应的大小区域。
## 简述你对CBIR(Content-based Image Retrieval基于内容的图像检索)的理解
通过对比特征点\\u7279征值的相似度,判断两个图片是否相近
## 工业界中遇到上亿的图像检索任务,如何提高图像对比效率?(假设原图像输出的特征维度为2048维)
通过哈希的索引技术,将原图的2048维度映射到128维度的0/1值中,再进行特征维度对比
## 什么是计算机视觉单词模型？
将图片中的特征提取出来，汇总到一个大的容器中，然后取其中核心点，组成一个模型
## 简述什么是Local Feature(局部特征算子)？
视觉单词就是Local Feature

由位置信息x,y坐标组成.
利用描述符来进行描述,量化.
## KD-Tree相比KNN来进行快速图像特征比对的好处在哪里?
极大的节约了时间成本．
点线距离如果 >　最小点，无需回溯上一层 　　　　　
如果<,则再上一层寻找
## Locality Sensitive Hashing（基于位置敏感的哈希值）相比KD-Tree优点？（高纬度）
当维度更高时候（5000维以上），利用LSH能够更快进行查询,sklearn有API可以调用
## 如何将相关图像投影到bucket中？
利用128条直线对图像集进行切分，每个图像都有128维的（0,1）向量，就可以将相关性强的图像分类到一个bucket中
## 简述encode和decode思想

将一个input信息编码到一个压缩空间中
将一个压缩空间向量解码到一个原始空间中
## 输入图片尺寸不匹配CNN网络input时候的解决方式？（三种以上）
two-fixed
one-fixed
free（去掉FC层，加入全局池化层）FCN（全卷积神经网络）
## FCN与CNN最大的区别？
卷积层不再与FC层相连,而是加入一个全局池化层
## 遇到class-imbalanced data（数据类目不平衡）问题怎么办？
设置不同类的权重weighted loss
batch-wise balanced sampling(循环平衡采样)　
## 简述hard negatives模式采样
对难以区分的样本进行采样，而非随机采样
## 简述孪生随机网络（Siamese Network）
ｘ1,x2如果一致，输出１；否则输出０
## 物体检测方法列举

    Deformable Parts Model
    RCNN
    Fast-RCNN
    Faster-RCNN
    RFCN
    Mask-RCNN

## 计算机视觉中，有哪几种基本任务？

    图像分类
    图像定位
    物体检测
    物体分割

## DPM（Deformable Parts Model）算法流程

    将原图与已经准备好的每个类别的“模板”做卷积操作，生成一中类似热力图（hot map）的图像，将不同尺度上的图合成一张，图中较量点就是与最相关“模板”相似的点。

    拓展：

    * SGD(stochastic gradient descent)到training里
    * NMS(non-maximum suppression)对后期testing的处理非常重要
    * Data mining hard examples这些概念至今仍在使用
## 什么是NMS（Non-maximum suppression 非极大值抑制）?

    NMS是一种Post-Procession（后处理）方式，跟算法无关的方式。
    NMS应用在所有物体检测的方法里。
    NMS物体检测的指标里，不允许出现多个重复的检测。
    NMS把所有检测结果按照分值(conf. score)从高到底排序,保留最高分数的 box,删除其余值。

## 列举出常见的损失函数(三个以上)?

    L1 Loss
    MES Loss
    Cross Entropy Loss (物体检测常用)
    NLL Loss
    Poisson NLL Loss
    KLDiv Loss
    BCE Loss
    BCE With Logits Loss
    Margin Ranking Loss
    Hinge Embedding Loss
    Multi Label Margin Loss
    Smooth L1 Loss
    Soft Margin Loss
    Multi Label Soft Margin Loss
    Cosine Embedding Loss
    Multi Margin Loss
    Triplet Margin Loss

## 常见的R-CNN'家族'成员有哪些?

    RCNN
    Fast-RCNN
    Faster-RCNN(这个工作是大部分流行方法的基石)
    FPN
    RetinaNet
    Mask-RCNN

## 做过目标检测项目么？比如Mask R-CNN和Python做一个抢车位神器
来源：Medium
编译：大数据文摘编译组，李雷、笪洁琼、Aileen、ZoeY

每逢春节过年，就要开始走亲访友了。这时候的商场、饭馆也都是“人声鼎沸”，毕竟走亲戚串门必不可少要带点礼品、聚餐喝茶。

热闹归热闹，这个时候最难的问题可能就是怎样从小区、商场、菜市场的人山人海里准确定位，找到一个“车位”。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728129239871909.png'/>
别慌！

一位名叫Adam Geitgey的软件工程师、AI软件工程博主也被“停车难”的问题困扰已久。为了让自己能给迅速定位空车位，他用实例分割模型Mask R-CNN和python写了一个抢占停车位的小程序。

以下是作者以第一人称给出的教程，enjoy。

一、如何找停车位
我住在一个大都市，但就像大多数城市一样，在这里很难找到停车位。停车场总是停得满满的，即使你自己有私人车位，朋友来访的时候也很麻烦，因为他们找不到停车位。

我的解决方法是：
用摄像头对着窗外拍摄，并利用深度学习算法让我的电脑在发现新的停车位时给我发短信。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728129938186278.png'/>
这可能听起来相当复杂，但是用深度学习来构建这个应用，实际上非常快速和简单。有各种现有的实用工具 - 我们只需找到这些工具并且将它们组合在一起。

现在，让我们花几分钟时间用Python和深度学习建立一个高精度的停车位通知系统吧！

分解问题
当我们想要通过机器学习解决一个复杂的问题时，第一步是将问题分解为简单任务的列表。然后，对于每个简单任务，我们可以从机器学习工具箱中找寻不同的工具来解决。

通过将这些简单问题的解决方案串起来形成框架（例如下面的思维导图），我们将实现一个可以执行复杂操作的系统。

以下就是我如何将检测公共停车位的问题分解并形成流程：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641557281385138852.png'/>
机器学习模型流程的输入是来自对着窗外的普通网络摄像头的视频：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728139198764729.png'/>
我们将每一帧视频送入模型里，一次一帧。

流程的第一步是检测视频帧中所有可能的停车位。显然，我们需要知道图像的哪些部分是停车位才能检测到哪些停车位是空的。

第二步是识别每帧视频中所有的汽车，这样我们可以跟踪每辆车在帧与帧之间的位移。

第三步是确定哪些停车位上目前有汽车，哪些没有。这需要综合第一步和第二步的结果。

最后一步是在停车位空出来的时候发送通知。这是基于视频帧之间的汽车位置的变化。

我们可以使用各种技术以多种不同方式完成这些步骤。构建流程的方法不是唯一的，不同的方法没有对错，但有不同的优点和缺点。现在让我们来看看每一步吧！

二、检测图像中的停车位
以下是相机拍到的图像：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728139948197422.png'/>
我们需要能够扫描该图像并返回可以停车的区域列表，如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728140768990441.png'/>
街区上可用的停车位

有一种偷懒的方法是手动将每个停车位的位置编入到程序中，而不是自动检测停车位。但是如果我们移动相机或想要检测不同街道上的停车位时，我们必须再次手动输入停车位的位置。这太不爽了，所以让我们找到一种自动检测停车位的方法。

一个想法是寻找停车计费表并假设每个计费表旁边都有一个停车位：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728141537930503.png'/>
在图片中检测的停车计费表

但这种方法存在一定的问题。首先，并非每个停车位都有停车咪表 – 实际上，我们最感兴趣的是找到免费停车位！其次，停车咪表的位置并不能确切地告诉我们停车位的具体位置，只能让我们离车位更接近一点。
另一个想法是建立一个物体检测模型，寻找在道路上绘制的停车位斜线标记，如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728142371724813.png'/>
留意那些微小的黄色标记，这些是在道路上绘制每个停车位的边界。

但这种做法也很痛苦。首先，我所在城市的停车位斜线标记非常小，从远处很难看清，电脑也难以察觉。第二，街道上到处都是各种不相关的线条和标记，所以要区分哪些线条是停车位，哪些线条是车道分隔线或人行横道是很困难的。

每当遇到一个看似困难的问题时，请先花几分钟时间看看是否能够采用不同的方式来避免某些技术难点并解决问题。到底什么是停车位呢？停车位就是车辆停留很长一段时间的地方。因此也许我们根本不需要检测停车位。为什么我们不能只检测那些长时间不动的车并假设它们停在停车位上？

换句话说，真正的停车位只是容纳了非移动中的车辆的区域：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415572814332979899.png'/>
这里每辆车的边框实际上都是一个停车位！如果我们能够检测到静止的汽车，就不需要真的去检测停车位。

因此，如果我们能够检测到汽车并找出哪些汽车在视频的每帧之间没有移动，我们就可以推断停车位的位置。这就变得很容易了！

三、检测图像中的汽车
检测视频每帧中的汽车是一个标准的对象检测问题。我们可以使用许多种机器学习方法来检测图像中的对象。以下是一些从过去到现在最常见的对象检测算法：
    a)训练一个HOG（梯度方向直方图）物体探测器滑过（扫描）我们的图像以找到所有的汽车。这种比较古老的非深度学习方法运行起来相对较快，但它对于朝向不同方向的汽车不能很好地处理。
    b)训练CNN（卷积神经网络）物体探测器阅览（扫描）我们的图像，直到我们找到所有的汽车。这种方法虽然准确，但效率不高，因为我们必须使用CNN算法多次扫描同一图像才能找到其中的所有汽车。虽然它可以很容易地找到朝向不同方向的汽车，但它需要比基于HOG的物体探测器更多的训练数据。
    c)使用更新的深度学习方法，如Mask R-CNN，快速R-CNN或YOLO，将CNN的准确性与巧妙设计和效率技巧相结合，可以大大加快检测过程。即使有大量训练数据来训练模型，这种方法的速度也相对较快（在GPU上）。

一般来说，我们希望选择最简单的解决方案，以最少的训练数据完成工作，而不是最新、最花哨的算法。但在这种特殊情况下，Mask R-CNN对我们来说是一个比较合理的选择，尽管它相当花哨新潮。

Mask R-CNN架构的设计理念是在不使用滑动窗口方法的情况下以高计算效率的方式检测整幅图像上的对象。换句话说，它运行得相当快。使用最新GPU，我们可以以每秒几帧的速度检测高分辨率视频中的对象。那对于这个项目来说应该没问题。

此外，Mask R-CNN对每个检测到的对象给出了大量信息。大多数对象检测算法仅返回每个对象的边界。但Mask R-CNN不仅会给我们每个对象的位置，还会给我们一个对象轮廓（或概述），如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728144328752002.png'/>
为了训练Mask R-CNN，我们需要大量的包含我们想要检测的对象的图片。我们可以去拍摄汽车照片并检测这些照片中的所有汽车，但这需要几天的工作。幸运的是，汽车是许多人想要检测的常见物体，因此已经有不少汽车图像的公共数据集。

其中一个名为COCO (Common Objects in Context）的流行数据集，其中包含标注了对象轮廓的图像。在此数据集中，有超过12,000张做了标注的汽车图像。以下是COCO数据集中的其中一张：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728145353832914.png'/>
这个数据集非常适合训练Mask R-CNN模型。

等等，还有更好的事！由于太多人使用COCO数据集构建对象检测模型，很多人已经完成并共享了他们的结果。因此，我们可以从预先训练好的模型开始，而无需训练我们自己的模型，这种模型可以即插即用。对于这个项目，我们将使用来自Matterport的大型开源Mask R-CNN实现项目，它自带预先训练的模型。
 
旁注：不要害怕训练一个定制的Mask R-CNN目标探测器！注释数据是很费时，但并不难。

如果我们在摄像头拍摄的图像上运行预先培训过的模型，就会得到如下的结果：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728146364810369.png'/>
在我们的图像上，识别出了COCO数据集中的默认对象-汽车、人、交通灯和一棵树。
 
我们不仅能识别汽车，还能识别交通灯和人。幽默的是，其中一棵树被识别成一个“盆栽植物”。

对于图像中检测到的每一个物体，我们从Mask R-CNN模型中都会得到以下四个数据：
1.检测到的对象类型（以整数形式表示）。经过预先训练的COCO模型知道如何检测80种不同的常见物体，如汽车和卡车。这是80种的常见物体的完整清单https://gist.github.com/ageitgey/b143ee809bf08e4927dd59bace44db0d。
2.目标检测的置信度得分。数值越高，模型就越确定它正确地识别了对象。
3.图像中对象的边界框，以X/Y像素位置表示。
4.位图图层告诉我们边界框中的哪些像素是对象的一部分，哪些不是。通过图层数据，我们还可以计算出对象的轮廓。

下面是使用Matterport’s Mask R-CNN中的预培训模型和OpenCV共同实现汽车边界框检测的Python代码：
当您运行该代码时，会看到图像上每辆被检测到的汽车周围都有一个边框，如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728147347073096.png'/>
被检测到的每辆汽车周围都有一个绿色的边框。

您还可以在控制台中查看每个被检测到的汽车的像素坐标，如下所示：
Cars found in frame of video:
Car:  [492 871 551 961]
Car:  [450 819 509 913]
Car:  [411 774 470 856]

有了这些数据，我们已经成功地在图像中检测到了汽车。可以进行下一步了

四、检测空车位                         
我们知道图像中每辆车的像素位置。通过连续查看多帧视频，我们可以很容易地确定哪些车辆没有移动，并假设这些区域是停车位。但我们如何检测汽车何时离开停车位呢？

主要问题是，我们的图像中汽车的边界框有部分重叠：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728148087469556.png'/>
即使对于不同停车位的汽车，每辆车的边界框也有一点重叠。

因此，如果我们假设每一个边界框中的都代表一个停车位，那么即使停车位是空的，这个边界框也可能有一部分被汽车占据。我们需要一种方法来测量两个对象重叠的程度，以便检查“大部分是空的”的边框。

我们将使用交并比（Intersection Over Union ，IoU）方法。用两个对象重叠的像素数量除以两个对象覆盖的像素总数量，如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728148764563760.png'/>
IoU可以告诉我们汽车边界框与停车位边界框的重叠程度。有了这个指标，我们就可以很容易地确定一辆车是否在停车位。如果IoU测量值很低，比如0.15，这意味着这辆车并没有占用太多的停车位。但是如果测量值很高，比如0.6，这意味着汽车占据了大部分的停车位，那么我们可以确定停车位被占用了。

由于IoU在计算机视觉中是一种常见的度量方法，所以通常您使用的库已经有相关实践。事实上，Matterport Mask R-CNN库中就有这样的函数mrcnn.utils.compute_overlaps()，我们可以直接调用这个函数。

假设在图像中有一个表示停车区域的边界框列表，那么检查被检测到的车辆是否在这些边界框中，就如同添加一行或两行代码一样简单。

结果如下：
[
 [1.         0.07040032 0.         0.]
 [0.07040032 1.         0.07673165 0.]
 [0.         0.         0.02332112 0.]
]

在这个二维数组中，每一行表示一个停车位的边界框。相应的，每列表示该停车位与被检测到的汽车有多少重叠。1.0分意味着汽车完全占据了停车位，而0.02分这样的低分意味着汽车只是接触了停车位边界框，但并没有占据很多区域。

为了找到空置的停车位，我们只需要检查这个数组中的每一行。如果所有的数字都是零或者非常小的数字，就意味着没有东西占据了这个空间，因此它就是空闲的！

但请记住，物体检测并不总是与实时视频完美结合。尽管Mask R-CNN非常准确，但偶尔它会在一帧视频中错过一两辆车。因此，在将停车位标记为空闲之前，我们应该确保它在一段时间内都是空闲的，可能是5或10帧连续视频。这将防止仅仅在一帧视频上出现暂时性的物体检测问题而误导系统将停车位判定为空闲。但当我们看到至少有一个停车位在连续几帧视频图像中都被判定为空闲，我们就可以发送短信了！

五、发送短信
最后一步是当我们注意到一个停车位在连续几帧视频图像中都被判定为空闲时，就发送一条短信提醒。

使用 Twilio从Python发送短消息非常简单。Twilio是一个流行的接口，它可以让您用几行代码从任何编程语言发送短消息。当然，如果您喜欢使用其他的短信服务提供者，也是可以的。我和Twilio没有利害关系。只是第一个就想到了它。

Twilio：
https://www.twilio.com

要使用Twilio，需要注册试用帐户，创建Twilio电话号码并获取您的帐户凭据。然后，您需要安装Twilio Python客户端的库：

pip3 install twilio

安装后，使用下面的代码（需要将关键信息替换成您的账户信息），就可以从Python发送短信了：

from twilio.rest import Client
  
  # Twilio account details
  twilio_account_sid = &#39;Your Twilio SID here&#39;
  twilio_auth_token = &#39;Your Twilio Auth Token here&#39;
  twilio_source_phone_number = &#39;Your Twilio phone number here&#39;
  
  # Create a Twilio client object instance
  client = Client(twilio_account_sid, twilio_auth_token)
  
  # Send an SMS
  message = client.messages.create(
      body="This is my SMS message!",
      from_=twilio_source_phone_number,
      to="Destination phone number here"
  )


直接将代码插入到我们的脚本里，就可以添加发送短信的功能了。但我们需要注意的是，如果一个停车位一直是空闲的，就不需要在每一帧视频都给自己发送短信了。因此，我们需要有一个标志来标记我们是否已经发送了一条短信，并确保在经过一定时间或检测到其他停车位空闲之前，我们不会再发送另一条短信息。

打包组装
我们将流程中的每个步骤写成了一个单独的Python脚本。以下是完整的代码：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728149633520660.png'/>
此部分代码太长，感兴趣的同学可以在下面的网址中找到：
https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnn-and-python-955f2231c400

要运行此代码，您需要首先安装Python 3.6+, Matterport Mask R-CNN和OpenCV。

Matterport Mask R-CNN：
https://github.com/matterport/Mask_RCNN
OpenCV：
https://pypi.org/project/opencv-python/

我有意让代码尽可能简明扼要。例如，它只是假设在第一帧视频中出现的任何汽车都是已停放的汽车。试试修改代码，看看您能不能提高它的可靠性。

不用担心修改此代码就不能适应不同的场景。只需更改模型搜寻的对象ID，就可以将代码完全转换为其他内容。例如，假设您在滑雪场工作。通过一些调整，您可以将此脚本转换为一个自动检测滑雪板从斜坡上跳下的系统，并记录炫酷的滑雪板跳跃轨迹。或者，如果您在一个游戏保护区工作，您可以把这个脚本变成一个用来计数在野外能看到多少斑马的系统。

唯一的限制就是你的想象力。

一起来试试吧!
 
相关报道：
https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnn-and-python-955f2231c400
## 如何理解Faster RCNN
目前学术和工业界出现的目标检测算法分成3类：
1. 传统的目标检测算法：Cascade + HOG/DPM + Haar/SVM以及上述方法的诸多改进、优化；

2. 候选区域/框 + 深度学习分类：通过提取候选区域，并对相应区域进行以深度学习方法为主的分类的方案，如：
R-CNN（Selective Search + CNN + SVM）
SPP-net（ROI Pooling）
Fast R-CNN（Selective Search + CNN + ROI）
Faster R-CNN（RPN + CNN + ROI）
R-FCN
等系列方法；

3. 基于深度学习的回归方法：YOLO/SSD/DenseBox 等方法；以及最近出现的结合RNN算法的RRC detection；结合DPM的Deformable CNN等

经过R-CNN和Fast RCNN的积淀，Ross B. Girshick在2016年提出了新的Faster RCNN，在结构上，Faster RCNN已经将特征抽取(feature extraction)，proposal提取，bounding box regression(rect refine)，classification都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421007015280035.jpg'/>
图1 Faster RCNN基本结构（来自原论文）

依作者看来，如图1，Faster RCNN其实可以分为4个主要内容：

①Conv layers。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的

②feature maps。该feature maps被共享用于后续RPN层和全连接层。

③Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于foreground或者background，再利用bounding box regression修正anchors获得精确的proposals。
Roi Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。

④Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。

所以本文以上述4个内容作为切入点介绍Faster R-CNN网络。

下图图2展示了python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421014336794242.jpg'/>
图2 faster_rcnn_test.pt网络结构 （pascal_voc/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt）

可以清晰的看到该网络对于一副任意大小PxQ的图像，首先缩放至固定大小MxN，然后将MxN图像送入网络；
而Conv layers中包含了13个conv层+13个relu层+4个pooling层；RPN网络首先经过3x3卷积，再分别生成foreground anchors与bounding box regression偏移量，然后计算出proposals；
而Roi Pooling层则利用proposals从feature maps中提取proposal feature送入后续全连接和softmax网络作classification（即分类proposal到底是什么object）。

关于R-CNN家族的历史，请参见此文：https://www.julyedu.com/question/big/kp_id/32/ques_id/2103。


一 Conv layers
Conv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图2，Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv layers中：

所有的conv层都是： <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421023867826680.svg'/> ， <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421024489104164.svg'/>， <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421024979956340.svg'/>
所有的pooling层都是： <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421025692099613.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421026266538870.svg'/>， <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542102685635210.svg'/>

为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（ pad=1，即填充一圈0），导致原图变为 (M+2)x(N+2)大小，再做3x3卷积后输出MxN 。正是这种设置，导致Conv layers中的conv层不改变输入和输出矩阵大小。如图3：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421029364916114.jpg'/>
图3 卷积示意图

类似的是，Conv layers中的pooling层kernel_size=2，stride=2。这样每个经过pooling层的MxN矩阵，都会变为(M/2)x(N/2)大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。

那么，一个MxN大小的矩阵经过Conv layers固定变为(M/16)x(N/16)！这样Conv layers生成的featuure map中都可以和原图对应起来。


二 Region Proposal Networks(RPN)
经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如R-CNN使用SS(Selective Search)方法生成检测框。

而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster R-CNN的巨大优势，能极大提升检测框的生成速度。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421034131889529.jpg'/>
图4 RPN网络结构

上图4展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和background（检测目标是foreground），下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。

而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。

2.1 多通道图像卷积基础知识介绍
在介绍RPN前，还要多解释几句基础知识，已经懂的看官老爷跳过就好。

对于单通道图像+单卷积核做卷积，第一章中的图3已经展示了；
对于多通道图像+多卷积核做卷积，计算方式如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421036178738034.jpg'/>
图5 多通道卷积计算方式

如图5，输入有3个通道，同时有2个卷积核。对于每个卷积核，先在输入3个通道分别作卷积，再将3个通道结果加起来得到卷积输出。所以对于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量！
对多通道图像做1x1卷积，其实就是将输入图像于每个通道乘以卷积系数后加在一起，即相当于把原图像中本来各个独立的通道“联通”在了一起。

2.2 anchors
提到RPN网络，就不能不说anchors。所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。直接运行Faster RCNN的作者在其论文中给的demo中的generate_anchors.py可以得到以下输出：

[[ -84.  -40.   99.   55.]
 [-176.  -88.  191.  103.]
 [-360. -184.  375.  199.]
 [ -56.  -56.   71.   71.]
 [-120. -120.  135.  135.]
 [-248. -248.  263.  263.]
 [ -36.  -80.   51.   95.]
 [ -80. -168.   95.  183.]
 [-168. -344.  183.  359.]]

其中每行的4个值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420879293384076.svg'/>表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420880792797017.svg'/>三种，如下图。实际上通过anchors就引入了检测中常用到的多尺度方法。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542088266229831.jpg'/>

注：关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成800x600（即下图中的M=800，N=600）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420887897013780.jpg'/>

再回头来看anchors的大小，anchors中长宽1:2中最大为352x704，长宽2:1中最大736x384，基本是cover了800x600的各个尺度和形状。

那么这9个anchors是做什么的呢？借用Faster RCNN论文中的原图，如下图，遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420890012978963.jpg'/>

解释一下上面这张图的数字。

a）在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions

b）在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息（猜测这样做也许更鲁棒？反正我没测试），同时256-d不变（如上图中的红框）

c)假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分foreground和background，所以每个点由256d feature转化为cls=2k scores；而每个anchor都有(x, y, w, h)对应4个偏移量，所以reg=4k coordinates

d)补充一点，全部anchors拿去训练太多了，训练程序会在合适的anchors中随机选取128个postive anchors+128个negative anchors进行训练

注意，在本文讲解中使用的VGG conv5 num_output=512，所以是512d，其他类似。

其实RPN最终就是在原图尺度上，设置了密密麻麻的候选Anchor。然后用cnn去判断哪些Anchor是里面有目标的foreground anchor，哪些是没目标的backgroud。所以，仅仅是个二分类而已！

那么Anchor一共有多少个？原图800x600，VGG下采样16倍，feature map每个点设置9个Anchor，所以：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420896774070786.svg'/>

其中ceil()表示向上取整，是因为VGG输出的feature map size= 50*38。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420898095049747.jpg'/>

2.3 softmax判定foreground与background
一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16，H=N/16。在进入reshape与softmax之前，先做了1x1卷积，如图9：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421038657233871.jpg'/>
图9 RPN中判定fg/bg网络结构

该1x1卷积的caffe prototxt定义如下：

layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn/output"
  top: "rpn_cls_score"
  convolution_param {
    num_output: 18   # 2(bg/fg) * 9(anchors)
    kernel_size: 1 pad: 0 stride: 1
  }
}
可以看到其num_output=18，也就是经过该卷积的输出图像为WxHx18大小（注意第二章开头提到的卷积计算方式）。这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是foreground和background，所有这些信息都保存WxHx(9*2)大小的矩阵。为何这样做？后面接softmax分类获得foreground anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在foreground anchors中）。

那么为何要在softmax前后都接一个reshape layer？其实只是为了便于softmax分类，至于具体原因这就要从caffe的实现形式说起了。在caffe基本数据结构blob中以如下形式保存数据：

blob=[batch_size, channel，height，width]
对应至上面的保存bg/fg anchors的矩阵，其在caffe blob中的存储形式为[1, 2x9, H, W]。而在softmax分类时需要进行fg/bg二分类，所以reshape layer会将其变为[1, 2, 9xH, W]大小，即单独“腾空”出来一个维度以便softmax分类，之后再reshape回复原状。贴一段caffe softmax_loss_layer.cpp的reshape函数的解释，非常精辟：

"Number of labels must match number of predictions; "
"e.g., if softmax axis == 1 and prediction shape is (N, C, H, W), "
"label count (number of labels) must be N*H*W, "
"with integer values in {0, 1, ..., C-1}.";

综上所述，RPN网络中利用anchors和softmax初步提取出foreground anchors作为候选区域。

2.4 bounding box regression原理
如图9所示绿色框为飞机的Ground Truth(GT)，红色为提取的foreground anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得foreground anchors和GT更加接近。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542104848988017.jpg'/>
图10

对于窗口一般使用四维向量  (x, y, w, h) 表示，分别表示窗口的中心点坐标和宽高。对于图 11，红色的框A代表原始的Foreground Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G'，即：

给定：anchor <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421052937197877.svg'/>,和 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421054464832961.svg'/>
寻找一种变换F，使得：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421055595429698.svg'/>，其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421057069663415.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421058029605774.jpg'/>
图11

那么经过何种变换F才能从图10中的anchor A变为G'呢？ 比较简单的思路就是:

先做平移
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421062065900717.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421062791843737.svg'/>

再做缩放
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421063397533378.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421063911969984.svg'/>

观察上面4个公式发现，需要学习的是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421069079358060.svg'/>这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当anchors A和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。

接下来的问题就是如何通过线性回归获得<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421070067453287.svg'/>了。线性回归就是给定输入的特征向量X, 学习一组参数W, 使得经过线性回归后的值跟真实值Y非常接近，即Y=WX。对于该问题，输入X是cnn feature map，定义为Φ；同时还有训练传入A与GT之间的变换量，即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421071455390992.svg'/>。输出是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421072182371619.svg'/>四个变换。那么目标函数可以表示为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421072836233581.svg'/>

其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421076093661500.svg'/>是对应anchor的feature map组成的特征向量，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421076822660740.svg'/>是需要学习的参数，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421077552865835.svg'/>是得到的预测值（*表示 x，y，w，h，也就是每一个变换对应一个上述目标函数）。为了让预测值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421078771200222.svg'/>与真实值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421079513852603.svg'/>差距最小，设计损失函数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542108447962919.svg'/>

函数优化目标为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421085373307319.svg'/>

需要说明，只有在GT与需要回归框位置比较接近时，才可近似认为上述线性变换成立。
说完原理，对应于Faster RCNN原文，foreground anchor与ground truth之间的平移量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421086624709449.svg'/>与尺度因子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421087366484514.svg'/>如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421087959428027.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421088571443870.svg'/>

对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421091221974580.svg'/>，即训练目标是：输入 Φ的情况下使网络输出与监督信号尽可能接近。
那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421092084865231.svg'/>，显然即可用来修正Anchor位置了。

2.5 对proposals进行bounding box regression
在了解bounding box regression后，再回头来看RPN网络第二条线路，如图12。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421094031196768.jpg'/>
图12 RPN中的bbox reg

先来看一看上图11中1x1卷积的caffe prototxt定义：

layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn/output"
  top: "rpn_bbox_pred"
  convolution_param {
    num_output: 36   # 4 * 9(anchors)
    kernel_size: 1 pad: 0 stride: 1
  }
}
可以看到其 num_output=36，即经过该卷积输出图像为WxHx36，在caffe blob存储为[1, 4x9, H, W]，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542109609785247.svg'/>
变换量。

2.6 Proposal Layer
Proposal Layer负责综合所有
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421097849884869.svg'/>
变换量和foreground anchors，计算出精准的proposal，送入后续RoI Pooling Layer。还是先来看看Proposal Layer的caffe prototxt定义：
layer {
  name: 'proposal'
  type: 'Python'
  bottom: 'rpn_cls_prob_reshape'
  bottom: 'rpn_bbox_pred'
  bottom: 'im_info'
  top: 'rois'
  python_param {
    module: 'rpn.proposal_layer'
    layer: 'ProposalLayer'
    param_str: "'feat_stride': 16"
  }
}
Proposal Layer有3个输入：fg/bg anchors分类器结果rpn_cls_prob_reshape，对应的bbox reg的
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421100117598083.svg'/>

变换量rpn_bbox_pred，以及im_info；另外还有参数feat_stride=16，这和图4是对应的。
首先解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定MxN，im_info=[M, N, scale_factor]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为WxH=(M/16)x(N/16)大小，其中feature_stride=16则保存了该信息，用于计算anchor偏移量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421102423249417.jpg'/>
图13

Proposal Layer forward（caffe layer的前传函数）按照以下顺序依次处理：

1)生成anchors，利用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421104665475598.svg'/>对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）
2)按照输入的foreground softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的foreground anchors。
3)限定超出图像边界的foreground anchors为图像边界（防止后续roi pooling时proposal超出图像边界）

4)剔除非常小（width<threshold or height<threshold）的foreground anchors
5)进行nonmaximum suppression
6)再次按照nms后的foreground softmax scores由大到小排序fg anchors，提取前post_nms_topN(e.g. 300)结果作为proposal输出。

之后输出proposal=[x1, y1, x2, y2]，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的，这点在后续网络中有用。另外我认为，严格意义上的检测应该到此就结束了，后续部分应该属于识别了~
RPN网络结构就介绍到这里，总结起来就是：
生成anchors -> softmax分类器提取fg anchors -> bbox reg回归fg anchors -> Proposal Layer生成proposals

3 RoI pooling
而RoI Pooling层则负责收集proposal，并计算出proposal feature maps，送入后续网络。从图2中可以看到Rol pooling层有2个输入：

原始的feature maps
RPN输出的proposal boxes（大小各不相同）

3.1 为何需要RoI Pooling
先来看一个问题：对于传统的CNN（如AlexNet，VGG），当网络训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的vector or matrix。如果输入图像大小不定，这个问题就变得比较麻烦。有2种解决办法：
i)从图像中crop一部分传入网络
ii)将图像warp成需要的大小后传入网络
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421116017452060.jpg'/>
图14 crop与warp破坏图像原有结构信息

两种办法的示意图如图14，可以看到无论采取那种办法都不好，要么crop后破坏了图像的完整结构，要么warp破坏了图像原始形状信息。

回忆RPN网络生成的proposals的方法：对foreground anchors进行bounding box regression，那么这样获得的proposals也是大小形状各不相同，即也存在上述问题。所以Faster R-CNN中提出了RoI Pooling解决这个问题。不过RoI Pooling确实是从Spatial Pyramid Pooling发展而来，但是限于篇幅这里略去不讲，有兴趣的读者可以自行查阅相关论文。

3.2 RoI Pooling原理
分析之前先来看看RoI Pooling Layer的caffe prototxt的定义：

layer {
  name: "roi_pool5"
  type: "ROIPooling"
  bottom: "conv5_3"
  bottom: "rois"
  top: "pool5"
  roi_pooling_param {
    pooled_w: 7
    pooled_h: 7
    spatial_scale: 0.0625 # 1/16
  }
}
其中有新参数 ，另外一个参数 认真阅读的读者肯定已经知道知道用途。
RoI Pooling layer forward过程：

由于proposal是对应<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421122987881625.svg'/>尺度的，所以首先使用spatial_scale参数将其映射回 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421124173288985.svg'/>大小的feature map尺度；

再将每个proposal对应的feature map区域水平分为 \text{pooled_w}\times \text{pooled_h} 的网格；
对网格的每一份都进行max pooling处理。

这样处理后，即使大小不同的proposal输出结果都是 \text{pooled_w}\times \text{pooled_h} 固定大小，实现了固定长度输出。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421127531054389.jpg'/>
图15 proposal示意图

四 Classification
Classification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框。Classification部分网络结构如图16。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421130541667433.jpg'/>
图16 Classification部分网络结构图

从PoI Pooling获取到7x7=49大小的proposal feature maps后，送入后续网络，可以看到做了如下2件事：

a)通过全连接和softmax对proposals进行分类，这实际上已经是识别的范畴了
b)再次对proposals进行bounding box regression，获取更高精度的rect box

这里来看看全连接层InnerProduct layers，简单的示意图如图17，
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421133733092777.jpg'/>
图17 全连接层示意图

其计算公式如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421135245894870.jpg'/>

其中W和bias B都是预先训练好的，即大小是固定的，当然输入X和输出Y也就是固定大小。所以，这也就印证了之前Roi Pooling的必要性。到这里，我想其他内容已经很容易理解，不在赘述了。

五 Faster R-CNN训练
Faster R-CNN的训练，是在已经训练好的model（如VGG_CNN_M_1024，VGG，ZF）的基础上继续进行训练。实际中训练过程分为6个步骤：

①在已经训练好的model上，训练RPN网络，对应stage1_rpn_train.pt
②利用步骤1中训练好的RPN网络，收集proposals，对应rpn_test.pt
③第一次训练Fast RCNN网络，对应stage1_fast_rcnn_train.pt
④第二训练RPN网络，对应stage2_rpn_train.pt
⑤再次利用步骤4中训练好的RPN网络，收集proposals，对应rpn_test.pt
⑥第二次训练Fast RCNN网络，对应stage2_fast_rcnn_train.pt

可以看到训练过程类似于一种“迭代”的过程，不过只循环了2次。至于只循环了2次的原因是应为作者提到："A similar alternating training can be run for more iterations, but we have observed negligible improvements"，即循环更多次没有提升了。接下来本章以上述6个步骤讲解训练过程。

下面是一张训练过程流程图，应该更加清晰。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421145351772320.jpg'/>
图18 Faster RCNN训练步骤（引用自参考文章[1]）

5.1 训练RPN网络
在该步骤中，首先读取RBG提供的预训练好的model（本文使用VGG），开始迭代训练。来看看stage1_rpn_train.pt网络结构，如图19。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421147531698889.jpg'/>
图19 stage1_rpn_train.pt（考虑图片大小，Conv Layers中所有的层都画在一起了，如红圈所示，后续图都如此处理）
与检测网络类似的是，依然使用Conv Layers提取feature maps。整个网络使用的Loss如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542115069611824.svg'/>

上述公式中 i 表示anchors index， <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421155080356088.svg'/>表示foreground softmax probability，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421155950731054.svg'/>代表对应的GT predict概率（即当第i个anchor与GT间IoU>0.7，认为是该anchor是foreground，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421156756380556.svg'/>；

反之IoU<0.3时，认为是该anchor是background，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421160545058096.svg'/>；至于那些0.3<IoU<0.7的anchor则不参与训练）；t代表predict bounding box，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421161390061770.svg'/>代表对应foreground anchor对应的GT box。可以看到，整个Loss分为2部分：

1.cls loss，即rpn_cls_loss层计算的softmax loss，用于分类anchors为forground与background的网络训练
2.reg loss，即rpn_loss_bbox层计算的soomth L1 loss，用于bounding box regression网络训练。注意在该loss中乘了 p_{i}^{*} ，相当于只关心foreground anchors的回归（其实在回归中也完全没必要去关心background）。

由于在实际过程中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421169442525502.svg'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421170160839746.svg'/>差距过大，用参数λ平衡二者（如<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421171348594720.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542117205261512.svg'/>时设置 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421172922618453.svg'/>，使总的网络Loss计算过程中能够均匀考虑2种Loss。这里比较重要是 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421173733199110.svg'/>使用的soomth L1 loss，计算公式如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421176149880653.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421176712172567.svg'/>

了解数学原理后，反过来看图18：

1)在RPN训练阶段，rpn-data（python AnchorTargetLayer）层会按照和test阶段Proposal层完全一样的方式生成Anchors用于训练
2)对于rpn_loss_cls，输入的rpn_cls_scors_reshape和rpn_labels分别对应 p 与<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421182939956677.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421183672817331.svg'/>参数隐含在p与<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421184349917223.svg'/>的caffe blob的大小中
3)对于rpn_loss_bbox，输入的rpn_bbox_pred和rpn_bbox_targets分别对应 t 与<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421186593562075.svg'/>，rpn_bbox_inside_weigths对应<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542118936420863.svg'/>，rpn_bbox_outside_weigths未用到（从soomth_L1_Loss layer代码中可以看到），而<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421195326282408.svg'/>同样隐含在caffe blob大小中

这样，公式与代码就完全对应了。特别需要注意的是，在训练和检测阶段生成和存储anchors的顺序完全一样，这样训练结果才能被用于检测！

5.2 通过训练好的RPN网络收集proposals
在该步骤中，利用之前的RPN网络，获取proposal rois，同时获取foreground softmax probability，如图20，然后将获取的信息保存在python pickle文件中。该网络本质上和检测中的RPN网络一样，没有什么区别。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421198012597782.jpg'/>
图20 rpn_test.pt

5.3 训练Faster RCNN网络
读取之前保存的pickle文件，获取proposals与foreground probability。从data层输入网络。然后：

将提取的proposals作为rois传入网络，如图19蓝框
计算bbox_inside_weights+bbox_outside_weights，作用与RPN一样，传入soomth_L1_loss layer，如图20绿框
这样就可以训练最后的识别softmax与最终的bounding box regression了，如图21。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542120013124204.jpg'/>
图21 stage1_fast_rcnn_train.pt

之后的stage2训练都是大同小异，不再赘述了。Faster R-CNN还有一种end-to-end的训练方式，可以一次完成train，有兴趣请自己看作者GitHub吧：https://github.com/rbgirshick/py-faster-rcnn


参考文献：
Object Detection and Classification using R-CNNs
http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/

拓展
如何将Faster RCNN应用于文字检测？CTPN:场景文字检测—CTPN原理与实现
https://zhuanlan.zhihu.com/p/34757009

本题解析来源：https://zhuanlan.zhihu.com/p/31426458
## one-stage和two-stage目标检测方法的区别和优缺点？
众所周知，物体检测的任务是找出图像或视频中的感兴趣物体，同时检测出它们的位置和大小。

当然，物体检测过程中有很多不确定因素，如图像中物体数量不确定，物体有不同的外观、形状、姿态，加之物体成像时会有光照、遮挡等因素的干扰，导致检测算法有一定的难度。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726659599200997.jpg'/>
由于目标检测的应用场景广泛，所以在CV面试中经常出现，比如七月在线有一CV就业班的学员出去面试时，便被问到“one-stage和two-stage目标检测方法的区别和优缺点？”（详见此文：https://ask.julyedu.com/question/88747） 

虽然我们在本文中详细介绍了各个目标检测的方法：https://www.julyedu.com/question/big/kp_id/32/ques_id/2103 ，但如果你是第一次听到one-stage和two-stage，你会不会瞬间一脸懵逼，这是啥？

其实很简单，顾名思义，区别在于是一步到位还是两步到位。
具体说来，进入深度学习时代以来，物体检测发展主要集中在两个方向：
two stage算法，如R-CNN系列；
ones-tage算法，如YOLO、SSD等。
两者的主要区别在于two stage算法需要先生成proposal（一个有可能包含待检物体的预选框），然后进行细粒度的物体检测，而one stage算法会直接在网络中提取特征来预测物体分类和位置。

所以说，目标检测算法two-stage，如Faster R-CNN算法会先生成候选框（region proposals，可能包含物体的区域），然后再对每个候选框进行分类（也会修正位置）。这类算法相对就慢，因为它需要多次运行检测和分类流程。
而另外一类one-stage目标检测算法（也称one-shot object detectors），其特点是一步到位，仅仅需要送入网络一次就可以预测出所有的边界框，速度相对较快，非常适合移动端，最典型的one-stage检测算法包括YOLO，SSD，SqueezeDet以及DetectNet。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726967446574479.jpg'/>


简单吧，恍然大悟，原来如此！而且one-stage看起来更高级。

当然，目标检测还包括其他很多方法，如下图所示：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726642393420130.jpg'/>

由于two-stage方法在此文中已经详细介绍：https://www.julyedu.com/question/big/kp_id/32/ques_id/2103  ， 故下面重点介绍看起来更高级的one-stage方法。

为什么目标检测问题更难
图像分类是生成单个输出，即类别概率分布。但是这只能给出图像整体内容的摘要，当图像有多个感兴趣的物体时，它就不行了。在下面的图像中，分类器可能会识别出图像即包含猫，也包含狗，这是它最擅长的。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726835338718424.jpg'/>

而目标检测模型将通过预测每个物体的边界框来给出各个物体的位置：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726837618803820.jpg'/>
因为可以专注于对边界框内的物体进行分类并忽略外部背景，因此模型能够为各个物体提供更加准确的预测。如果数据集带有边界框标注，则可以非常轻松地在模型添加一个定位预测：只需预测额外4个数字，分别用于边界框的每个角落。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726841055868983.jpg'/>
现在该模型有两部分输出：类别概率分布和边界框回归。模型的损失函数只是将边界框的回归损失与分类的交叉熵损失相加，通常使用均方误差（MSE）：
outputs = model.forward_pass(image)class_pred = outputs[0]
bbox_pred = outputs[1]
class_loss = cross_entropy_loss(class_pred, class_true)
bbox_loss = mse_loss(bbox_pred, bbox_true)
loss = class_loss + bbox_lossoptimize(loss)

然后采用SGD方法对模型优化训练，这是一个预测实例：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726849862660942.jpg'/>

模型已正确对图像中物体（狗）分类，并给出它在图像中的位置。红色框是真实框，而青色框是预测框，虽然有偏差，但非常接近。为了评估预测框与真实框的匹配程度，我们可以计算两个边界框之间的IOU（intersection-over-union，也称为Jaccard index）。IOU在0到1之间，越大越好。理想情况下，预测框和真框的IOU为100％，但实际上任何超过50％的预测通常都被认为是正确的。对于上面的示例，IOU为74.9％，因而预测框比较精确。使用回归方法预测单个边界框可以获得较好的结果。

然而，当图像中存在多个感兴趣的物体时，就会出现问题：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726852570511138.jpg'/>

由于模型只能预测一个边界框，因而它必须要选择一个物体，这会最终落在中间位置。实际上这很容易理解：图像里有两个物体，但是模型只能给出一个边界框，因而选择了折中，预测框位于两者中间，也许大小也是介于两个物体大小之间。

注意：也许你可能认为模型应该给出一个包含两个物体的边界框，但是这不太会发生，因为训练不是这样的，真实框都是各个物体分开标注的，而不是一组物体进行标注。你也许认为，上述问题很好解决，对于模型的回归部分增加更多的边界框预测不就好了。

毕竟，如果模型可以预测N个边界框，那么就应该可以正确定位N个物体。听起来不错，但是并没有效。就算模型有多个检测器（这里一个边界框回归称为一个检测器），我们得到的边界框依然会落在图像中间：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415672685453722376.jpg'/>
为什么会这样？问题是模型不知道应该将哪个边界框分配给哪个物体，为了安全起见，它将它们放在中间的某个位置。该模型无法决定：“我可以在左边的马周围放置边界框1，并在右边的马周围放置框2。”

相反，每个检测器仍然试图预测所有物体，而不是一个检测器预测一个物体。尽管该模型具有N个检测器，但它们无法协同工作（对，无法协同工作）。具有多个边界框检测器的模型的效果与仅预测一个边界框的模型完全相同。

我们需要的是使边界框检测器更专一化的一些方法，以便每个检测器将尝试仅预测单个物体，并且不同的探测器将找到不同的物体。在不专一的模型中，每个检测器应该能够处理图像中任何可能位置的各类物体。

这太简单了，模型学会的是预测位于图像中心的方框，因为这样整个训练集实际上会最小化损失函数。从SGD的角度来看，这样做平均得到了相当不错的结果，但在实践中它却不是真正有用的结果，所以我们需要更加有效地训练模型。

通过将每个边界框检测器分配到图像中的特定位置，one-stage目标检测算法（例如YOLO，SSD和DetectNet）都是这样来解决这个问题。因为，检测器学会专注于某些位置的物体。为了获得更好的效果，我们还可以让检测器专注于物体的形状和大小。

继续深入请见此文：https://zhuanlan.zhihu.com/p/61485202
## 请画下YOLOv3的网络结构
本题解析来源：https://blog.csdn.net/qq_37541097/article/details/81214953 ，https://blog.csdn.net/dz4543/article/details/90049377

本人是小白，看后表示有点蒙。于是在Github上搜了大牛们基于Tensorflow搭建的YOLOv3模型进行分析（本人只接触过TF，所以就不去看caffe的源码了）。接下来我会根据我阅读的代码来进一步分析网络的结构。Github YOLOv3大牛代码链接。

1.Darknet-53 模型结构
在论文中虽然有给网络的图，但我还是简单说一下。这个网络主要是由一系列的1x1和3x3的卷积层组成（每个卷积层后都会跟一个BN层和一个LeakyReLU)层，作者说因为网络中有53个convolutional layers，所以叫做Darknet-53（2 + 1*2 + 1 + 2*2 + 1 + 8*2 + 1 + 8*2 + 1 + 4*2 + 1 = 53 按照顺序数，不包括Residual中的卷积层，最后的Connected是全连接层也算卷积层，一共53个）。
下图就是Darknet-53的结构图，在右侧标注了一些信息方便理解（卷积的strides默认为（1，1），padding默认为same，当strides为（2，2）时padding为valid）
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313746962362993.png'/>
看完上图应该就能自己搭建出Darknet-53的网络结构了，上图是以输入图像256 x 256进行预训练来进行介绍的，常用的尺寸是416 x 416，都是32的倍数。下面我们再来分析下YOLOv3的特征提取器，看看究竟是在哪几层Features上做的预测。

2.YOLOv3 模型结构
作者在论文中提到利用三个特征层进行边框的预测，具体在哪三层我感觉作者在论文中表述的并不清楚（例如文中有“添加几个卷积层”这样的表述），同样根据代码我将这部分更加详细的分析展示在下图中。
注意：原Darknet53中的尺寸是在图片分类训练集上训练的，所以输入的图像尺寸是256x256，下图是以YOLO v3 416模型进行绘制的，所以输入的尺寸是416x416，预测的三个特征层大小分别是52，26，13。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313761741474063.jpg'/>
在上图中我们能够很清晰的看到三个预测层分别来自的什么地方，以及Concatenate层与哪个层进行拼接。注意Convolutional是指Conv2d+BN+LeakyReLU，和Darknet53图中的一样，而生成预测结果的最后三层都只是Conv2d。通过上图小伙伴们就能更加容易地搭建出YOLOv3的网络框架了。

3.目标边界框的预测
YOLOv3网络在三个特征图中分别通过(4+1+c) k个大小为11的卷积核进行卷积预测，k为预设边界框（bounding box prior）的个数（k默认取3），c为预测目标的类别数，其中4k个参数负责预测目标边界框的偏移量，k个参数负责预测目标边界框内包含目标的概率，ck个参数负责预测这k个预设边界框对应c个目标类别的概率。
下图展示了目标边界框的预测过程（该图是本人重新绘制的，与论文中的示意图有些不同，个人感觉自己绘制的更便于理解）。
图中虚线矩形框为预设边界框，实线矩形框为通过网络预测的偏移量计算得到的预测边界框。
其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313842624654549.gif'/>为预设边界框在特征图上的中心坐标，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313843314124415.gif'/>为预设边界框在特征图上的宽和高，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313844275776899.gif'/>分别为网络预测的边界框中心偏移量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415831384567094883.gif'/>以及宽高缩放比<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313846478916318.gif'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313847889733775.gif'/>为最终预测的目标边界框，从预设边界框到最终预测边界框的转换过程如图右侧公式所示，其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313849260307297.gif'/>函数是sigmoid函数其目的是将预测偏移量缩放到0到1之间（这样能够将预设边界框的中心坐标固定在一个cell当中，作者说这样能够加快网络收敛）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313769499865825.png'/>

下图给出了三个预测层的特征图大小以及每个特征图上预设边界框的尺寸（这些预设边界框尺寸都是作者根据COCO数据集聚类得到的）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313770786807442.png'/>

4.损失函数的计算
关于YOLOv3的损失函数文章中写的很粗略，比如坐标损失采用的是误差的平方和，类别损失采用的是二值交叉熵，本人在github上也找了很多YOLO v3的公开代码，有的采用的是YOLOv1或者YOLOv2的损失函数，下面给出本人认为正确的损失函数（这里偷个懒，公式都是从本人之前写的论文中截图的）。

YOLOv3的损失函数主要分为三个部分：目标定位偏移量损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313855878379421.gif'/>，目标置信度损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313856464682616.gif'/>以及目标分类损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313857856262876.gif'/>，其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415831385869675310.gif'/>是平衡系数。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313775959399980.png'/>

4.1目标置信度损失
目标置信度可以理解为预测目标矩形框内存在目标的概率，目标置信度损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313863437234915.gif'/>采用的是二值交叉熵损失(Binary Cross Entropy)，其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313864334829694.gif'/>，表示预测目标边界框i中是否真实存在目标，0表示不存在，1表示存在。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313865890679659.gif'/>表示预测目标矩形框i内是否存在目标的Sigmoid概率（将预测值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313866983237258.gif'/>通过sigmoid函数得到）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313778186063850.png'/>

4.2目标类别损失
目标类别损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313877170493442.gif'/>同样采用的是二值交叉熵损失（采用二值交叉熵损失的原因是，作者认为同一目标可同时归为多类，比如猫可归为猫类以及动物类，这样能够应对更加复杂的场景。但在本人实践过程中发现使用原始的多类别交叉熵损失函数效果会更好一点，原因是本人针对识别的目标都是固定归于哪一类的，并没有可同时归于多类的情况），其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313878843925498.gif'/>，表示预测目标边界框i中是否真实存在第j类目标，0表示不存在，1表示存在。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313880852694620.gif'/>表示网络预测目标边界框i内存在第j类目标的Sigmoid概率（将预测值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313883323504239.gif'/>通过sigmoid函数得到）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313779974821830.png'/>

4.3目标定位损失
目标定位损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313892310075204.gif'/>采用的是真实偏差值与预测偏差值差的平方和，其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313892999728054.gif'/>表示预测矩形框坐标偏移量（注意网络预测的是偏移量，不是直接预测坐标），<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313893945451242.gif'/>表示与之匹配的GTbox与默认框之间的坐标偏移量，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313895559801460.gif'/>为预测的目标矩形框参数，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313896589205355.gif'/>为默认矩形框参数，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313897325809160.gif'/>为与之匹配的真实目标矩形框参数，这些参数都是映射在预测特征图上的。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415831378587826363.png'/>

5 YOLOv3的几个输出实例
来一个YOLO输出时的显示：
layer     filters    size              input                output
   0 conv     32  3 x 3 / 1   416 x 416 x   3   ->   416 x 416 x  32 0.299 BF
   1 conv     64  3 x 3 / 2   416 x 416 x  32   ->   208 x 208 x  64 1.595 BF
   2 conv     32  1 x 1 / 1   208 x 208 x  64   ->   208 x 208 x  32 0.177 BF
   3 conv     64  3 x 3 / 1   208 x 208 x  32   ->   208 x 208 x  64 1.595 BF
   4 Shortcut Layer: 1
   5 conv    128  3 x 3 / 2   208 x 208 x  64   ->   104 x 104 x 128 1.595 BF
   6 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64 0.177 BF
   7 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128 1.595 BF
   8 Shortcut Layer: 5
   9 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64 0.177 BF
  10 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128 1.595 BF
  11 Shortcut Layer: 8
  12 conv    256  3 x 3 / 2   104 x 104 x 128   ->    52 x  52 x 256 1.595 BF
  13 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  14 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  15 Shortcut Layer: 12
  16 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  17 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  18 Shortcut Layer: 15
  19 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  20 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  21 Shortcut Layer: 18
  22 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  23 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  24 Shortcut Layer: 21
  25 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  26 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  27 Shortcut Layer: 24
  28 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  29 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  30 Shortcut Layer: 27
  31 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  32 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  33 Shortcut Layer: 30
  34 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  35 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  36 Shortcut Layer: 33
  37 conv    512  3 x 3 / 2    52 x  52 x 256   ->    26 x  26 x 512 1.595 BF
  38 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  39 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  40 Shortcut Layer: 37
  41 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  42 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  43 Shortcut Layer: 40
  44 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  45 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  46 Shortcut Layer: 43
  47 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  48 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  49 Shortcut Layer: 46
  50 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  51 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  52 Shortcut Layer: 49
  53 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  54 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  55 Shortcut Layer: 52
  56 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  57 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  58 Shortcut Layer: 55
  59 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  60 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  61 Shortcut Layer: 58
  62 conv   1024  3 x 3 / 2    26 x  26 x 512   ->    13 x  13 x1024 1.595 BF
  63 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  64 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  65 Shortcut Layer: 62
  66 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  67 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  68 Shortcut Layer: 65
  69 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  70 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  71 Shortcut Layer: 68
  72 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  73 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  74 Shortcut Layer: 71
  75 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  76 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  77 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  78 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  79 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  80 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  81 conv     18  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x  18 0.006 BF
  82 yolo
  83 route  79
  84 conv    256  1 x 1 / 1    13 x  13 x 512   ->    13 x  13 x 256 0.044 BF
  85 upsample            2x    13 x  13 x 256   ->    26 x  26 x 256
  86 route  85 61
  87 conv    256  1 x 1 / 1    26 x  26 x 768   ->    26 x  26 x 256 0.266 BF
  88 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  89 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  90 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  91 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  92 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  93 conv     18  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x  18 0.012 BF
  94 yolo
  95 route  91
  96 conv    128  1 x 1 / 1    26 x  26 x 256   ->    26 x  26 x 128 0.044 BF
  97 upsample            2x    26 x  26 x 128   ->    52 x  52 x 128
  98 route  97 36
  99 conv    128  1 x 1 / 1    52 x  52 x 384   ->    52 x  52 x 128 0.266 BF
 100 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
 101 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
 102 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
 103 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
 104 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
 105 conv     18  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x  18 0.025 BF
 106 yolo

实际，这个已经告诉了我们每层的输出情况。每层特征图的大小情况：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313961038895239.png'/>

在前文网络的基础上，用红色做了注释。residual使用残差结构。什么是残差结构？举个例子在第一层残差结构（其输出为208208128），其输入为20820864，经过3211和6433的卷积后，其生成的特征图与输入叠加起来。其结构如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313963074357755.png'/>

其叠加后的特征图作为新的输入输入下一层。YOLO主体是由许多这种残差模块组成，减小了梯度爆炸的风险，加强了网络的学习能力。

可以看到YOLO有3个尺度的输出，分别在52×52，26×26，13×13。嗯，都是奇数，使得网格会有个中心位置。同时YOLO输出为3个尺度，每个尺度之间还有联系。比如说，13×13这个尺度输出用于检测大型目标，对应的26×26为中型的，52×52用于检测小型目标。
上一张图，我觉得很详细看得懂。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313964860182548.png'/>

这个检测COCO（80个类的），所以其输出需要构造为：S×S×3×（5+class_number）。解释下为什么是这样。
YOLO将图像划分为S×S的网格，当目标中心落在某个网格中，就用这个网格去检测它，这是S×S的由来。
为什么是3，是因为每个网格需要检测3个anchorbox（注意有3个尺度），所以对于每个尺度，其输出为S×S×3×？？？
对于一个anchor box，它包含坐标信息（x , y , w , h ）以及置信度，而这有5个信息；同时还会包含是否所有类别的信息，使用one-hot编码。

比如说有3个类:person、car、dog。检测的结果是人，那么就编码为[1,0,0]。可见所有类别信息都会被编码，COCO有80个类别的话，便是5+80。所以，对于每个维度的输出，其结果为：S×S×3×（5+80）=S×S×255 S×S×3×（5+80） = S×S×255S×S×3×（5+80）=S×S×255。
同时从上图可以看到，其结果便是通过一些卷积操作，将输出构造成这样。并且将不同尺度的特征图叠加到一起，增加输出的信息。这个图可以好好看看。


# CV
## 基于深度学习的目标检测技术演进：R-CNN、Fast R-CNN、Faster R-CNN、YOLO、SSD
一、目标检测常见算法

object detection，就是在给定的图片中精确找到物体所在位置，并标注出物体的类别。所以，object detection要解决的问题就是物体在哪里以及是什么的整个流程问题。

然而，这个问题可不是那么容易解决的，物体的尺寸变化范围很大，摆放物体的角度，姿态不定，而且可以出现在图片的任何地方，更何况物体还可以是多个类别。

目前学术和工业界出现的目标检测算法分成3类：
1. 传统的目标检测算法：Cascade + HOG/DPM + Haar/SVM以及上述方法的诸多改进、优化；

2. 候选区域/框 + 深度学习分类：通过提取候选区域，并对相应区域进行以深度学习方法为主的分类的方案，如：
R-CNN（Selective Search + CNN + SVM）
SPP-net（ROI Pooling）
Fast R-CNN（Selective Search + CNN + ROI）
Faster R-CNN（RPN + CNN + ROI）
R-FCN
等系列方法；

3. 基于深度学习的回归方法：YOLO/SSD/DenseBox 等方法；以及最近出现的结合RNN算法的RRC detection；结合DPM的Deformable CNN等
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525258820_318.jpg'/>

传统目标检测流程：
1）区域选择（穷举策略：采用滑动窗口，且设置不同的大小，不同的长宽比对图像进行遍历，时间复杂度高）
2）特征提取（SIFT、HOG等；形态多样性、光照变化多样性、背景多样性使得特征鲁棒性差）
3）分类器分类（主要有SVM、Adaboost等）

二、传统的目标检测算法
2.1 从图像识别的任务说起
这里有一个图像任务：既要把图中的物体识别出来，又要用方框框出它的位置。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517392987_360.jpg'/>

这个任务本质上就是这两个问题：一：图像识别，二：定位。

图像识别（classification）：
输入：图片
输出：物体的类别
评估方法：准确率
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393007_344.jpg'/>

定位（localization）：
输入：图片
输出：方框在图片中的位置（x,y,w,h）
评估方法：检测评价函数intersection-over-union（关于什么是IOU，请参看本深度学习分类下第55题：https://www.julyedu.com/question/big/kp_id/26/ques_id/2138）
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393013_253.png'/>
卷积神经网络CNN已经帮我们完成了图像识别（判定是猫还是狗）的任务了，我们只需要添加一些额外的功能来完成定位任务即可。

定位的问题的解决思路有哪些？
思路一：看做回归问题
看做回归问题，我们需要预测出（x,y,w,h）四个参数的值，从而得出方框的位置。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393027_336.png'/>

步骤1:
　　•	先解决简单问题， 搭一个识别图像的神经网络
　　•	在AlexNet VGG GoogleLenet上fine-tuning一下（关于什么是微调fine-tuning，请参看本深度学习分类下第54题：https://www.julyedu.com/question/big/kp_id/26/ques_id/2137）
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393041_799.jpg'/>
 
步骤2:
　　•	在上述神经网络的尾部展开（也就说CNN前面保持不变，我们对CNN的结尾处作出改进：加了两个头：“分类头”和“回归头”）
　　•	成为classification + regression模式
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393046_527.png'/>

步骤3:
　　•	Regression那个部分用欧氏距离损失
　　•	使用SGD训练

步骤4:
　　•	预测阶段把2个头部拼上
　　•	完成不同的功能

这里需要进行两次fine-tuning
第一次在ALexNet上做，第二次将头部改成regression head，前面不变，做一次fine-tuning

Regression的部分加在哪？

有两种处理方法：
　　•	加在最后一个卷积层后面（如VGG）
　　•	加在最后一个全连接层后面（如R-CNN）

regression太难做了，应想方设法转换为classification问题。
regression的训练参数收敛的时间要长得多，所以上面的网络采取了用classification的网络来计算出网络共同部分的连接权值。

思路二：取图像窗口
　　•	还是刚才的classification + regression思路
　　•	咱们取不同的大小的“框”
　　•	让框出现在不同的位置，得出这个框的判定得分
　　•	取得分最高的那个框

左上角的黑框：得分0.5
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393142_364.jpg'/>

右上角的黑框：得分0.75
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393149_762.jpg'/>

左下角的黑框：得分0.6
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393156_659.jpg'/>

右下角的黑框：得分0.8
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393161_204.jpg'/>

根据得分的高低，我们选择了右下角的黑框作为目标位置的预测。
注：有的时候也会选择得分最高的两个框，然后取两框的交集作为最终的位置预测。

疑惑：框要取多大？
取不同的框，依次从左上角扫到右下角。非常粗暴啊。

总结一下思路：
对一张图片，用各种大小的框（遍历整张图片）将图片截取出来，输入到CNN，然后CNN会输出这个框的得分（classification）以及这个框图片对应的x,y,h,w（regression）。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393174_730.jpg'/>

这方法实在太耗时间了，做个优化。
原来网络是这样的：
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393184_685.jpg'/>

优化成这样：把全连接层改为卷积层，这样可以提提速。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393192_919.jpg'/>


2.2 物体检测（Object Detection）
当图像有很多物体怎么办的？难度可是一下暴增啊。

那任务就变成了：多物体识别+定位多个物体
那把这个任务看做分类问题？
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393210_554.jpg'/>

看成分类问题有何不妥？
　　•	你需要找很多位置， 给很多个不同大小的框
　　•	你还需要对框内的图像分类
　　•	当然， 如果你的GPU很强大， 恩， 那加油做吧…

所以，传统目标检测的主要问题是：
1）基于滑动窗口的区域选择策略没有针对性，时间复杂度高，窗口冗余
2）手工设计的特征对于多样性的变化没有很好的鲁棒性

看做classification， 有没有办法优化下？我可不想试那么多框那么多位置啊！

三、候选区域/窗 + 深度学习分类
3.1 R-CNN横空出世
有人想到一个好方法：预先找出图中目标可能出现的位置，即候选区域（Region Proposal）。利用图像中的纹理、边缘、颜色等信息，可以保证在选取较少窗口(几千甚至几百）的情况下保持较高的召回率（Recall）。

所以，问题就转变成找出可能含有物体的区域/框（也就是候选区域/框，比如选2000个候选框），这些框之间是可以互相重叠互相包含的，这样我们就可以避免暴力枚举所有框了。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393217_390.jpg'/>

大牛们发明好多选定候选框Region Proposal的方法，比如Selective Search和EdgeBoxes。那提取候选框用到的算法“选择性搜索”到底怎么选出这些候选框的呢？具体可以看一下PAMI2015的“What makes for effective detection proposals？”

以下是各种选定候选框的方法的性能对比。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393227_723.jpg'/>

有了候选区域，剩下的工作实际就是对候选区域进行图像分类的工作（特征提取+分类）。

对于图像分类，不得不提的是2012年ImageNet大规模视觉识别挑战赛（ILSVRC）上，机器学习泰斗Geoffrey Hinton教授带领学生Krizhevsky使用卷积神经网络将ILSVRC分类任务的Top-5 error降低到了15.3%，而使用传统方法的第二名top-5 error高达 26.2%。此后，卷积神经网络CNN占据了图像分类任务的绝对统治地位。

2014年，RBG（Ross B. Girshick）使用Region Proposal + CNN代替传统目标检测使用的滑动窗口+手工设计特征，设计了R-CNN框架，使得目标检测取得巨大突破，并开启了基于深度学习目标检测的热潮。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393238_127.png'/>

R-CNN的简要步骤如下
(1) 输入测试图像
(2) 利用选择性搜索Selective Search算法在图像中从下到上提取2000个左右的可能包含物体的候选区域Region Proposal
(3) 因为取出的区域大小各自不同，所以需要将每个Region Proposal缩放（warp）成统一的227x227的大小并输入到CNN，将CNN的fc7层的输出作为特征
(4) 将每个Region Proposal提取到的CNN特征输入到SVM进行分类

具体步骤则如下
步骤一：训练（或者下载）一个分类模型（比如AlexNet）
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393249_806.jpg'/>

步骤二：对该模型做fine-tuning
　　•	将分类数从1000改为21，比如20个物体类别 + 1个背景
　　•	去掉最后一个全连接层
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393259_895.png'/>

步骤三：特征提取
　　•	提取图像的所有候选框（选择性搜索Selective Search）
　　•	对于每一个区域：修正区域大小以适合CNN的输入，做一次前向运算，将第五个池化层的输出（就是对候选框提取到的特征）存到硬盘
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393264_688.png'/>

步骤四：训练一个SVM分类器（二分类）来判断这个候选框里物体的类别
每个类别对应一个SVM，判断是不是属于这个类别，是就是positive，反之nagative。

比如下图，就是狗分类的SVM
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393277_461.png'/>

步骤五：使用回归器精细修正候选框位置：对于每一个类，训练一个线性回归模型去判定这个框是否框得完美。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393285_829.png'/>

细心的同学可能看出来了问题，R-CNN虽然不再像传统方法那样穷举，但R-CNN流程的第一步中对原始图片通过Selective Search提取的候选框region proposal多达2000个左右，而这2000个候选框每个框都需要进行CNN提特征+SVM分类，计算量很大，导致R-CNN检测速度很慢，一张图都需要47s。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525103713_502.png'/>

有没有方法提速呢？答案是有的，这2000个region proposal不都是图像的一部分吗，那么我们完全可以对图像提一次卷积层特征，然后只需要将region proposal在原图的位置映射到卷积层特征图上，这样对于一张图像我们只需要提一次卷积层特征，然后将每个region proposal的卷积层特征输入到全连接层做后续操作。

但现在的问题是每个region proposal的尺度不一样，而全连接层输入必须是固定的长度，所以直接这样输入全连接层肯定是不行的。SPP Net恰好可以解决这个问题。

3.2 SPP Net
SPP：Spatial Pyramid Pooling（空间金字塔池化）

SPP-Net是出自2015年发表在IEEE上的论文-《Spatial Pyramid Pooling in Deep ConvolutionalNetworks for Visual Recognition》。

众所周知，CNN一般都含有卷积部分和全连接部分，其中，卷积层不需要固定尺寸的图像，而全连接层是需要固定大小的输入。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525243658_939.png'/>

所以当全连接层面对各种尺寸的输入数据时，就需要对输入数据进行crop（crop就是从一个大图扣出网络输入大小的patch，比如227×227），或warp（把一个边界框bounding box的内容resize成227×227）等一系列操作以统一图片的尺寸大小，比如224*224（ImageNet）、32*32(LenNet)、96*96等。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525249316_603.png'/>

所以才如你在上文中看到的，在R-CNN中，“因为取出的区域大小各自不同，所以需要将每个Region Proposal缩放（warp）成统一的227x227的大小并输入到CNN”。

但warp/crop这种预处理，导致的问题要么被拉伸变形、要么物体不全，限制了识别精确度。没太明白？说句人话就是，一张16:9比例的图片你硬是要Resize成1:1的图片，你说图片失真不？

SPP Net的作者Kaiming He等人逆向思考，既然由于全连接FC层的存在，普通的CNN需要通过固定输入图片的大小来使得全连接层的输入固定。那借鉴卷积层可以适应任何尺寸，为何不能在卷积层的最后加入某种结构，使得后面全连接层得到的输入变成固定的呢？

这个“化腐朽为神奇”的结构就是spatial pyramid pooling layer。

下图便是R-CNN和SPP Net检测流程的比较：

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525249330_874.png'/>

它的特点有两个:

1.结合空间金字塔方法实现CNNs的多尺度输入。

SPP Net的第一个贡献就是在最后一个卷积层后，接入了金字塔池化层，保证传到下一层全连接层的输入固定。

换句话说，在普通的CNN机构中，输入图像的尺寸往往是固定的（比如224*224像素），输出则是一个固定维数的向量。SPP Net在普通的CNN结构中加入了ROI池化层（ROI Pooling），使得网络的输入图像可以是任意尺寸的，输出则不变，同样是一个固定维数的向量。

简言之，CNN原本只能固定输入、固定输出，CNN加上SSP之后，便能任意输入、固定输出。神奇吧？

ROI池化层一般跟在卷积层后面，此时网络的输入可以是任意尺度的，在SPP layer中每一个pooling的filter会根据输入调整大小，而SPP的输出则是固定维数的向量，然后给到全连接FC层。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393299_167.jpg'/>

2.只对原图提取一次卷积特征
在R-CNN中，每个候选框先resize到统一大小，然后分别作为CNN的输入，这样是很低效的。
而SPP Net根据这个缺点做了优化：只对原图进行一次卷积计算，便得到整张图的卷积特征feature map，然后找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层，完成特征提取工作。

如此这般，R-CNN要对每个区域计算卷积，而SPPNet只需要计算一次卷积，从而节省了大量的计算时间，比R-CNN有一百倍左右的提速。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393307_311.jpg'/>


3.3 Fast R-CNN
SPP Net真是个好方法，R-CNN的进阶版Fast R-CNN就是在R-CNN的基础上采纳了SPP Net方法，对R-CNN作了改进，使得性能进一步提高。

R-CNN与Fast R-CNN的区别有哪些呢？
先说R-CNN的缺点：即使使用了Selective Search等预处理步骤来提取潜在的边界框bounding box作为输入，但是R-CNN仍会有严重的速度瓶颈，原因也很明显，就是计算机对所有region进行特征提取时会有重复计算，Fast-RCNN正是为了解决这个问题诞生的。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393313_544.png'/>

与R-CNN框架图对比，可以发现主要有两处不同：一是最后一个卷积层后加了一个ROI pooling layer，二是损失函数使用了多任务损失函数(multi-task loss)，将边框回归Bounding Box Regression直接加入到CNN网络中训练（关于什么是边框回归，请参看本深度学习分类下第56题：https://www.julyedu.com/question/big/kp_id/26/ques_id/2139）。

(1) ROI pooling layer实际上是SPP-NET的一个精简版，SPP-NET对每个proposal使用了不同大小的金字塔映射，而ROI pooling layer只需要下采样到一个7x7的特征图。对于VGG16网络conv5_3有512个特征图，这样所有region proposal对应了一个7*7*512维度的特征向量作为全连接层的输入。

换言之，这个网络层可以把不同大小的输入映射到一个固定尺度的特征向量，而我们知道，conv、pooling、relu等操作都不需要固定size的输入，因此，在原始图片上执行这些操作后，虽然输入图片size不同导致得到的feature map尺寸也不同，不能直接接到一个全连接层进行分类，但是可以加入这个神奇的ROI Pooling层，对每个region都提取一个固定维度的特征表示，再通过正常的softmax进行类型识别。

(2) R-CNN训练过程分为了三个阶段，而Fast R-CNN直接使用softmax替代SVM分类，同时利用多任务损失函数边框回归也加入到了网络中，这样整个的训练过程是端到端的(除去region proposal提取阶段)。

也就是说，之前R-CNN的处理流程是先提proposal，然后CNN提取特征，之后用SVM分类器，最后再做box regression，而在Fast R-CNN中，作者巧妙的把box regression放进了神经网络内部，与region分类和并成为了一个multi-task模型，实际实验也证明，这两个任务能够共享卷积特征，并相互促进。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525104022_285.png'/>
所以，Fast-RCNN很重要的一个贡献是成功的让人们看到了Region Proposal + CNN这一框架实时检测的希望，原来多类检测真的可以在保证准确率的同时提升处理速度，也为后来的Faster R-CNN做下了铺垫。

画一画重点：
R-CNN有一些相当大的缺点（把这些缺点都改掉了，就成了Fast R-CNN）。
大缺点：由于每一个候选框都要独自经过CNN，这使得花费的时间非常多。
解决：共享卷积层，现在不是每一个候选框都当做输入进入CNN了，而是输入一张完整的图片，在第五个卷积层再得到每个候选框的特征

原来的方法：许多候选框（比如两千个）-->CNN-->得到每个候选框的特征-->分类+回归
现在的方法：一张完整图片-->CNN-->得到每张候选框的特征-->分类+回归

所以容易看见，Fast R-CNN相对于R-CNN的提速原因就在于：不过不像R-CNN把每个候选区域给深度网络提特征，而是整张图提一次特征，再把候选框映射到conv5上，而SPP只需要计算一次特征，剩下的只需要在conv5层上操作就可以了。

在性能上提升也是相当明显的：
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393334_157.png'/>

3.4 Faster R-CNN
Fast R-CNN存在的问题：存在瓶颈：选择性搜索，找出所有的候选框，这个也非常耗时。那我们能不能找出一个更加高效的方法来求出这些候选框呢？

解决：加入一个提取边缘的神经网络，也就说找到候选框的工作也交给神经网络来做了。

所以，rgbd在Fast R-CNN中引入Region Proposal Network(RPN)替代Selective Search，同时引入anchor box应对目标形状的变化问题（anchor就是位置和大小固定的box，可以理解成事先设置好的固定的proposal）。

具体做法：
　　•	将RPN放在最后一个卷积层的后面
　　•	RPN直接训练得到候选区域
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393348_780.png'/>

RPN简介：
　　•	在feature map上滑动窗口
　　•	建一个神经网络用于物体分类+框位置的回归
　　•	滑动窗口的位置提供了物体的大体位置信息
　　•	框的回归提供了框更精确的位置
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393355_598.png'/>

一种网络，四个损失函数;
　　•	RPN calssification(anchor good.bad)
　　•	RPN regression(anchor->propoasal)
　　•	Fast R-CNN classification(over classes)
　　•	Fast R-CNN regression(proposal ->box)
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393380_922.png'/>

速度对比
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393385_752.png'/>

Faster R-CNN的主要贡献就是设计了提取候选区域的网络RPN，代替了费时的选择性搜索Selective Search，使得检测速度大幅提高。

最后总结一下各大算法的步骤：
RCNN
1.在图像中确定约1000-2000个候选框 (使用选择性搜索Selective Search)
2.每个候选框内图像块缩放至相同大小，并输入到CNN内进行特征提取 
3.对候选框中提取出的特征，使用分类器判别是否属于一个特定类 
4.对于属于某一类别的候选框，用回归器进一步调整其位置

Fast R-CNN
1.在图像中确定约1000-2000个候选框 (使用选择性搜索)
2.对整张图片输进CNN，得到feature map
3.找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层
4.对候选框中提取出的特征，使用分类器判别是否属于一个特定类 
5.对于属于某一类别的候选框，用回归器进一步调整其位置

Faster R-CNN
1.对整张图片输进CNN，得到feature map
2.卷积特征输入到RPN，得到候选框的特征信息
3.对候选框中提取出的特征，使用分类器判别是否属于一个特定类 
4.对于属于某一类别的候选框，用回归器进一步调整其位置
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525664484_495.jpg'/>

简言之，即如本文开头所列
R-CNN（Selective Search + CNN + SVM）
SPP-net（ROI Pooling）
Fast R-CNN（Selective Search + CNN + ROI）
Faster R-CNN（RPN + CNN + ROI）

总的来说，从R-CNN, SPP-NET, Fast R-CNN, Faster R-CNN一路走来，基于深度学习目标检测的流程变得越来越精简，精度越来越高，速度也越来越快。可以说基于region proposal的R-CNN系列目标检测方法是当前目标检测技术领域最主要的一个分支。

四、基于深度学习的回归方法
4.1 YOLO (CVPR2016, oral)
(You Only Look Once: Unified, Real-Time Object Detection)

Faster R-CNN的方法目前是主流的目标检测方法，但是速度上并不能满足实时的要求。YOLO一类的方法慢慢显现出其重要性，这类方法使用了回归的思想，利用整张图作为网络的输入，直接在图像的多个位置上回归出这个位置的目标边框，以及目标所属的类别。

我们直接看上面YOLO的目标检测的流程图：

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525171091_647.jpg'/>

(1) 给个一个输入图像，首先将图像划分成7*7的网格
(2) 对于每个网格，我们都预测2个边框（包括每个边框是目标的置信度以及每个边框区域在多个类别上的概率）
(3) 根据上一步可以预测出7*7*2个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后NMS去除冗余窗口即可（关于什么是非极大值抑制NMS，请参看本深度学习分类下第58题：https://www.julyedu.com/question/big/kp_id/26/ques_id/2141）。

可以看到整个过程非常简单，不再需要中间的region proposal找目标，直接回归便完成了位置和类别的判定。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525171142_149.jpg'/>

小结：YOLO将目标检测任务转换成一个回归问题，大大加快了检测的速度，使得YOLO可以每秒处理45张图像。而且由于每个网络预测目标窗口时使用的是全图信息，使得false positive比例大幅降低（充分的上下文信息）。

但是YOLO也存在问题：没有了Region Proposal机制，只使用7*7的网格回归会使得目标不能非常精准的定位，这也导致了YOLO的检测精度并不是很高。

4.2 SSD
(SSD: Single Shot MultiBox Detector)

上面分析了YOLO存在的问题，使用整图特征在7*7的粗糙网格内回归对目标的定位并不是很精准。那是不是可以结合region proposal的思想实现精准一些的定位？SSD结合YOLO的回归思想以及Faster R-CNN的anchor机制做到了这点。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525171268_230.jpg'/>

上图是SSD的一个框架图，首先SSD获取目标位置和类别的方法跟YOLO一样，都是使用回归，但是YOLO预测某个位置使用的是全图的特征，SSD预测某个位置使用的是这个位置周围的特征（感觉更合理一些）。

那么如何建立某个位置和其特征的对应关系呢？可能你已经想到了，使用Faster R-CNN的anchor机制。如SSD的框架图所示，假如某一层特征图(图b)大小是8*8，那么就使用3*3的滑窗提取每个位置的特征，然后这个特征回归得到目标的坐标信息和类别信息(图c)。

不同于Faster R-CNN，这个anchor是在多个feature map上，这样可以利用多层的特征并且自然的达到多尺度（不同层的feature map 3*3滑窗感受野不同）。

小结：SSD结合了YOLO中的回归思想和Faster R-CNN中的anchor机制，使用全图各个位置的多尺度区域特征进行回归，既保持了YOLO速度快的特性，也保证了窗口预测的跟Faster R-CNN一样比较精准。SSD在VOC2007上mAP可以达到72.1%，速度在GPU上达到58帧每秒。

主要参考及扩展阅读
1 https://www.cnblogs.com/skyfsm/p/6806246.html，by @Madcola
2 https://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&mid=502841131&idx=1&sn=bb3e8e6aeee2ee1f4d3f22459062b814#rd
3 https://zhuanlan.zhihu.com/p/27546796
4 https://blog.csdn.net/v1_vivian/article/details/73275259
5 https://blog.csdn.net/tinyzhao/article/details/53717136
6 Spatial Pyramid Pooling in Deep Convolutional
Networks for Visual Recognition，by Kaiming He等人
7 https://zhuanlan.zhihu.com/p/24774302
8 知乎专栏作者何之源新书《21个项目玩转深度学习——基于TensorFlow的实践详解》
9 YOLO，https://blog.csdn.net/u011534057/article/details/51244354，https://zhuanlan.zhihu.com/p/24916786

后记
咱公司七月在线开设的深度学习等一系列课程经常会讲目标检测，包括R-CNN、Fast R-CNN、Faster R-CNN，但一直没有比较好的机会深入（但当你对目标检测有个基本的了解之后，再看这些课程你会收益很大）。但目标检测这个领域实在是太火了，经常会看到一些写的不错的通俗易懂的资料，加之之前在京东上掏了一本书看了看，就这样耳濡目染中，还是开始研究了。

今年五一，从保定回京，怕高速路上堵 没坐大巴，高铁又没抢上，只好选择哐当哐当好几年没坐过的绿皮车，关键还不断晚点。在车站，用手机做个热点，修改题库，顺便终于搞清R-CNN、fast R-CNN、faster R-CNN的核心区别。有心中热爱 何惧任何啥。

为纪念这心中热爱，故成此文。
七月在线July、二零一八年五月三日
## 请简单解释下目标检测中的这个IOU评价函数（intersection-over-union）
在目标检测的评价体系中，有一个参数叫做 IoU ，简单来讲就是模型产生的目标窗口和原来标记窗口的交叠率。

具体我们可以简单的理解为：即检测结果DetectionResult与真实值Ground Truth的交集比上它们的并集，即为检测的准确率 IoU :

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525494665_290.png'/>

举个例子，下面是一张原图

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525495290_937.png'/>

然后我们对其做下目标检测，其DR = DetectionResult，GT = GroundTruth。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525494780_911.png'/>

黄色边框框起来的是：

DR⋂GT

绿色框框起来的是：
DR⋃GT

不难看出，最理想的情况就是DR与GT完全重合，即IoU = 1。

本题解析来源：https://blog.csdn.net/Eddy_zheng/article/details/52126641
## 计算图片相似度的方法有哪些?
直方图距离
平均哈希算法
感知哈希算法
差异哈希算法
## KNN与K-means区别？
有监督和无监督学习
KNN是寻找最相近的K个点,中心点是其本身,已知,求解周围点的标签众数; K-means是寻找K个聚类中心,聚类中心未知
## K-means选择初始点的方法有哪些,优缺点是什么?(列出两种以上)
随机选取初始点
选择批次距离尽可能远的K个点
选用层次聚类或者Canopy算法进行初始聚类，然后利用这些类簇的中心点作为KMeans算法初始类簇中心点
## 像素值的读写方式有哪些？(列出两种以上)
at方式循环遍历
Mat迭代器循环遍历
for循环读取
## 图像边缘检测的原理？
图像的边缘是指其周围像素灰度急剧变化的那些像素的集合，它是图像最基本的特征，而图像的边缘检测即先检测图像的边缘点，再按照某种策略将边缘点连接成轮廓，从而构成分割区域
## 图像中的角点(Harris角点)是什么？为什么用角点作为特征点？
Harris角点:在任意两个相互垂直的方向上，都有较大变化的点。
角点在保留图像图形重要特征的同时,可以有效地减少信息的数据量,使其信息的含量很高,有效地提高了计算的速度,有利于图像的可靠匹配,使得实时处理成为可能。
## 简述图像特征的SIFT描述
SIFT在不同的尺度空间上查找关键点(特征点)，并计算出关键点的方向。SIFT所查找到的关键点是一些十分突出，不会因光照，仿射变换和噪音等因素而变化的点，如角点、边缘点、暗区的亮点及亮区的暗点等。
## 简述RANSAC离群点操作方法
RANSAC算法是采用迭代的算法从一组包含离群点的被观测数据中估计一个数学模型的参数。RANSAC算法只会产生一个在一定概率下合理的结果，而更多次的迭代会使得这一概率增加。
## 简述线性分类器的原理(要求对权重矩阵进行剖析)
线性分类器就是将每个被分类样本与权重矩阵进行对比,找到相似度最大的权重行,给出响应的标签.
## 简述Cross-Entropy Loss(交叉熵损失函数)与Hinge Loss(折页损失函数)
softmax(柔性最大值)函数，一般在神经网络中， softmax可以作为分类任务的输出层。其实可以认为softmax输出的是几个类别选择的概率，比如我有一个分类任务，要分为三个类，softmax函数可以根据它们相对的大小，输出三个类别选取的概率，并且概率和为1。

Hinge Loss 是机器学习领域中的一种损失函数，可用于“最大间隔(max-margin)”分类，其最著名的应用是作为SVM的目标函数。
## 简述正则化与奥卡姆剃刀原则
正则化是为了防止过拟合

奥卡姆剃刀是一种思维方式，可用于指导我们的工作，比如我们可以用A和B达到同样的效果，但B更简单，于是我们选择B。同时，也有人说可能因为能力不足，于是我们选择更简单的方式来处理问题，这也是剃刀的原则的一种应用吧。

举个贴切的例子，做决策树分析的时候，采用9个属性的预测性能和5个属性的预测性能是相似的，那么我们就会选择5个属性来预测。
## 写出神经网络中常见的激励函数(至少三个)
Sigmoid
Tanh
ReLU
Leaky RelU
## 典型的卷积神经网络有哪些层?
INPUT(输入层)
CONV(卷积层)
RELU(非线性层)
POOL(池化层)
FC(全连接层)
## 计算题(提示:参数共享)
Example1: 200*200 image(灰度图), 40K hidden units 有多少个参数?
Example2: 200*200 image(灰度图), 40K hidden units, Filter size: 10*10 有多少个参数?
Example3: 200*200 image(灰度图), 100 Filter, Filter size:10*10 有多少参数?
Example4: 图像尺寸为[32*32*3], 卷积窗口大小为5*5, 卷积模板个数为1, 有多少参数?
Example5: 图像尺寸为[16*16*20], 卷积窗口大小为3*3, 卷积模板个数为10, 有多少参数?
Example1: 40000 * 40000 = 16 0000 0000个
Example2: 40000 * 10 * 10 = 4 00 0000个
Example3: 100 * 10 * 10 = 1 00 00个
Example4: 5 * 5 * 3 = 75个
Example5: 3 * 3 * 20 * 10 = 1800个
## 图像尺寸为 7*7, 卷积窗口大小为3*3, 步长为3, 是否能输出图像?如果能,输出图像大小为多少?如果不能,说明原因?
不能,因为 (7-3)/3 + 1 不为整数!滑动窗口会丢失
## 计算题: 输入图像尺寸为 32*32*3 , 卷积模板尺寸为 5*5 , 步长stride为1, 卷积神经元个数为 5
问题1: 输出图像尺寸为?
问题2: 每个卷积层神经元的参数个数为?
(32 - 5)/1 +1 = 28,结果就是: 28 * 28 * 5
每个神经元的参数个数为 : 5 * 5 * 3
## 如果最后一个卷积层和第一个全连接层参数量太大怎么办?
加入全局池化层
## 为什么说神经网络是端到端的网络?
特征提取包含在神经网络内部
## 当参数量 >> 样本量时候, 神经网络是如何预防过拟合?
增加数据量
加入全局池化减少参数量
减少卷积层数
迁移学习
## 什么是感受野？
某一层特征图中的一个cell,对应到原始输入的响应的大小区域。
## 简述你对CBIR(Content-based Image Retrieval基于内容的图像检索)的理解
通过对比特征点\\u7279征值的相似度,判断两个图片是否相近
## 工业界中遇到上亿的图像检索任务,如何提高图像对比效率?(假设原图像输出的特征维度为2048维)
通过哈希的索引技术,将原图的2048维度映射到128维度的0/1值中,再进行特征维度对比
## 什么是计算机视觉单词模型？
将图片中的特征提取出来，汇总到一个大的容器中，然后取其中核心点，组成一个模型
## 简述什么是Local Feature(局部特征算子)？
视觉单词就是Local Feature

由位置信息x,y坐标组成.
利用描述符来进行描述,量化.
## KD-Tree相比KNN来进行快速图像特征比对的好处在哪里?
极大的节约了时间成本．
点线距离如果 >　最小点，无需回溯上一层 　　　　　
如果<,则再上一层寻找
## Locality Sensitive Hashing（基于位置敏感的哈希值）相比KD-Tree优点？（高纬度）
当维度更高时候（5000维以上），利用LSH能够更快进行查询,sklearn有API可以调用
## 如何将相关图像投影到bucket中？
利用128条直线对图像集进行切分，每个图像都有128维的（0,1）向量，就可以将相关性强的图像分类到一个bucket中
## 简述encode和decode思想

将一个input信息编码到一个压缩空间中
将一个压缩空间向量解码到一个原始空间中
## 输入图片尺寸不匹配CNN网络input时候的解决方式？（三种以上）
two-fixed
one-fixed
free（去掉FC层，加入全局池化层）FCN（全卷积神经网络）
## FCN与CNN最大的区别？
卷积层不再与FC层相连,而是加入一个全局池化层
## 遇到class-imbalanced data（数据类目不平衡）问题怎么办？
设置不同类的权重weighted loss
batch-wise balanced sampling(循环平衡采样)　
## 简述hard negatives模式采样
对难以区分的样本进行采样，而非随机采样
## 简述孪生随机网络（Siamese Network）
ｘ1,x2如果一致，输出１；否则输出０
## 物体检测方法列举

    Deformable Parts Model
    RCNN
    Fast-RCNN
    Faster-RCNN
    RFCN
    Mask-RCNN

## 计算机视觉中，有哪几种基本任务？

    图像分类
    图像定位
    物体检测
    物体分割

## DPM（Deformable Parts Model）算法流程

    将原图与已经准备好的每个类别的“模板”做卷积操作，生成一中类似热力图（hot map）的图像，将不同尺度上的图合成一张，图中较量点就是与最相关“模板”相似的点。

    拓展：

    * SGD(stochastic gradient descent)到training里
    * NMS(non-maximum suppression)对后期testing的处理非常重要
    * Data mining hard examples这些概念至今仍在使用
## 什么是NMS（Non-maximum suppression 非极大值抑制）?

    NMS是一种Post-Procession（后处理）方式，跟算法无关的方式。
    NMS应用在所有物体检测的方法里。
    NMS物体检测的指标里，不允许出现多个重复的检测。
    NMS把所有检测结果按照分值(conf. score)从高到底排序,保留最高分数的 box,删除其余值。

## 列举出常见的损失函数(三个以上)?

    L1 Loss
    MES Loss
    Cross Entropy Loss (物体检测常用)
    NLL Loss
    Poisson NLL Loss
    KLDiv Loss
    BCE Loss
    BCE With Logits Loss
    Margin Ranking Loss
    Hinge Embedding Loss
    Multi Label Margin Loss
    Smooth L1 Loss
    Soft Margin Loss
    Multi Label Soft Margin Loss
    Cosine Embedding Loss
    Multi Margin Loss
    Triplet Margin Loss

## 常见的R-CNN'家族'成员有哪些?

    RCNN
    Fast-RCNN
    Faster-RCNN(这个工作是大部分流行方法的基石)
    FPN
    RetinaNet
    Mask-RCNN

## 做过目标检测项目么？比如Mask R-CNN和Python做一个抢车位神器
来源：Medium
编译：大数据文摘编译组，李雷、笪洁琼、Aileen、ZoeY

每逢春节过年，就要开始走亲访友了。这时候的商场、饭馆也都是“人声鼎沸”，毕竟走亲戚串门必不可少要带点礼品、聚餐喝茶。

热闹归热闹，这个时候最难的问题可能就是怎样从小区、商场、菜市场的人山人海里准确定位，找到一个“车位”。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728129239871909.png'/>
别慌！

一位名叫Adam Geitgey的软件工程师、AI软件工程博主也被“停车难”的问题困扰已久。为了让自己能给迅速定位空车位，他用实例分割模型Mask R-CNN和python写了一个抢占停车位的小程序。

以下是作者以第一人称给出的教程，enjoy。

一、如何找停车位
我住在一个大都市，但就像大多数城市一样，在这里很难找到停车位。停车场总是停得满满的，即使你自己有私人车位，朋友来访的时候也很麻烦，因为他们找不到停车位。

我的解决方法是：
用摄像头对着窗外拍摄，并利用深度学习算法让我的电脑在发现新的停车位时给我发短信。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728129938186278.png'/>
这可能听起来相当复杂，但是用深度学习来构建这个应用，实际上非常快速和简单。有各种现有的实用工具 - 我们只需找到这些工具并且将它们组合在一起。

现在，让我们花几分钟时间用Python和深度学习建立一个高精度的停车位通知系统吧！

分解问题
当我们想要通过机器学习解决一个复杂的问题时，第一步是将问题分解为简单任务的列表。然后，对于每个简单任务，我们可以从机器学习工具箱中找寻不同的工具来解决。

通过将这些简单问题的解决方案串起来形成框架（例如下面的思维导图），我们将实现一个可以执行复杂操作的系统。

以下就是我如何将检测公共停车位的问题分解并形成流程：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641557281385138852.png'/>
机器学习模型流程的输入是来自对着窗外的普通网络摄像头的视频：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728139198764729.png'/>
我们将每一帧视频送入模型里，一次一帧。

流程的第一步是检测视频帧中所有可能的停车位。显然，我们需要知道图像的哪些部分是停车位才能检测到哪些停车位是空的。

第二步是识别每帧视频中所有的汽车，这样我们可以跟踪每辆车在帧与帧之间的位移。

第三步是确定哪些停车位上目前有汽车，哪些没有。这需要综合第一步和第二步的结果。

最后一步是在停车位空出来的时候发送通知。这是基于视频帧之间的汽车位置的变化。

我们可以使用各种技术以多种不同方式完成这些步骤。构建流程的方法不是唯一的，不同的方法没有对错，但有不同的优点和缺点。现在让我们来看看每一步吧！

二、检测图像中的停车位
以下是相机拍到的图像：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728139948197422.png'/>
我们需要能够扫描该图像并返回可以停车的区域列表，如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728140768990441.png'/>
街区上可用的停车位

有一种偷懒的方法是手动将每个停车位的位置编入到程序中，而不是自动检测停车位。但是如果我们移动相机或想要检测不同街道上的停车位时，我们必须再次手动输入停车位的位置。这太不爽了，所以让我们找到一种自动检测停车位的方法。

一个想法是寻找停车计费表并假设每个计费表旁边都有一个停车位：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728141537930503.png'/>
在图片中检测的停车计费表

但这种方法存在一定的问题。首先，并非每个停车位都有停车咪表 – 实际上，我们最感兴趣的是找到免费停车位！其次，停车咪表的位置并不能确切地告诉我们停车位的具体位置，只能让我们离车位更接近一点。
另一个想法是建立一个物体检测模型，寻找在道路上绘制的停车位斜线标记，如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728142371724813.png'/>
留意那些微小的黄色标记，这些是在道路上绘制每个停车位的边界。

但这种做法也很痛苦。首先，我所在城市的停车位斜线标记非常小，从远处很难看清，电脑也难以察觉。第二，街道上到处都是各种不相关的线条和标记，所以要区分哪些线条是停车位，哪些线条是车道分隔线或人行横道是很困难的。

每当遇到一个看似困难的问题时，请先花几分钟时间看看是否能够采用不同的方式来避免某些技术难点并解决问题。到底什么是停车位呢？停车位就是车辆停留很长一段时间的地方。因此也许我们根本不需要检测停车位。为什么我们不能只检测那些长时间不动的车并假设它们停在停车位上？

换句话说，真正的停车位只是容纳了非移动中的车辆的区域：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415572814332979899.png'/>
这里每辆车的边框实际上都是一个停车位！如果我们能够检测到静止的汽车，就不需要真的去检测停车位。

因此，如果我们能够检测到汽车并找出哪些汽车在视频的每帧之间没有移动，我们就可以推断停车位的位置。这就变得很容易了！

三、检测图像中的汽车
检测视频每帧中的汽车是一个标准的对象检测问题。我们可以使用许多种机器学习方法来检测图像中的对象。以下是一些从过去到现在最常见的对象检测算法：
    a)训练一个HOG（梯度方向直方图）物体探测器滑过（扫描）我们的图像以找到所有的汽车。这种比较古老的非深度学习方法运行起来相对较快，但它对于朝向不同方向的汽车不能很好地处理。
    b)训练CNN（卷积神经网络）物体探测器阅览（扫描）我们的图像，直到我们找到所有的汽车。这种方法虽然准确，但效率不高，因为我们必须使用CNN算法多次扫描同一图像才能找到其中的所有汽车。虽然它可以很容易地找到朝向不同方向的汽车，但它需要比基于HOG的物体探测器更多的训练数据。
    c)使用更新的深度学习方法，如Mask R-CNN，快速R-CNN或YOLO，将CNN的准确性与巧妙设计和效率技巧相结合，可以大大加快检测过程。即使有大量训练数据来训练模型，这种方法的速度也相对较快（在GPU上）。

一般来说，我们希望选择最简单的解决方案，以最少的训练数据完成工作，而不是最新、最花哨的算法。但在这种特殊情况下，Mask R-CNN对我们来说是一个比较合理的选择，尽管它相当花哨新潮。

Mask R-CNN架构的设计理念是在不使用滑动窗口方法的情况下以高计算效率的方式检测整幅图像上的对象。换句话说，它运行得相当快。使用最新GPU，我们可以以每秒几帧的速度检测高分辨率视频中的对象。那对于这个项目来说应该没问题。

此外，Mask R-CNN对每个检测到的对象给出了大量信息。大多数对象检测算法仅返回每个对象的边界。但Mask R-CNN不仅会给我们每个对象的位置，还会给我们一个对象轮廓（或概述），如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728144328752002.png'/>
为了训练Mask R-CNN，我们需要大量的包含我们想要检测的对象的图片。我们可以去拍摄汽车照片并检测这些照片中的所有汽车，但这需要几天的工作。幸运的是，汽车是许多人想要检测的常见物体，因此已经有不少汽车图像的公共数据集。

其中一个名为COCO (Common Objects in Context）的流行数据集，其中包含标注了对象轮廓的图像。在此数据集中，有超过12,000张做了标注的汽车图像。以下是COCO数据集中的其中一张：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728145353832914.png'/>
这个数据集非常适合训练Mask R-CNN模型。

等等，还有更好的事！由于太多人使用COCO数据集构建对象检测模型，很多人已经完成并共享了他们的结果。因此，我们可以从预先训练好的模型开始，而无需训练我们自己的模型，这种模型可以即插即用。对于这个项目，我们将使用来自Matterport的大型开源Mask R-CNN实现项目，它自带预先训练的模型。
 
旁注：不要害怕训练一个定制的Mask R-CNN目标探测器！注释数据是很费时，但并不难。

如果我们在摄像头拍摄的图像上运行预先培训过的模型，就会得到如下的结果：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728146364810369.png'/>
在我们的图像上，识别出了COCO数据集中的默认对象-汽车、人、交通灯和一棵树。
 
我们不仅能识别汽车，还能识别交通灯和人。幽默的是，其中一棵树被识别成一个“盆栽植物”。

对于图像中检测到的每一个物体，我们从Mask R-CNN模型中都会得到以下四个数据：
1.检测到的对象类型（以整数形式表示）。经过预先训练的COCO模型知道如何检测80种不同的常见物体，如汽车和卡车。这是80种的常见物体的完整清单https://gist.github.com/ageitgey/b143ee809bf08e4927dd59bace44db0d。
2.目标检测的置信度得分。数值越高，模型就越确定它正确地识别了对象。
3.图像中对象的边界框，以X/Y像素位置表示。
4.位图图层告诉我们边界框中的哪些像素是对象的一部分，哪些不是。通过图层数据，我们还可以计算出对象的轮廓。

下面是使用Matterport’s Mask R-CNN中的预培训模型和OpenCV共同实现汽车边界框检测的Python代码：
当您运行该代码时，会看到图像上每辆被检测到的汽车周围都有一个边框，如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728147347073096.png'/>
被检测到的每辆汽车周围都有一个绿色的边框。

您还可以在控制台中查看每个被检测到的汽车的像素坐标，如下所示：
Cars found in frame of video:
Car:  [492 871 551 961]
Car:  [450 819 509 913]
Car:  [411 774 470 856]

有了这些数据，我们已经成功地在图像中检测到了汽车。可以进行下一步了

四、检测空车位                         
我们知道图像中每辆车的像素位置。通过连续查看多帧视频，我们可以很容易地确定哪些车辆没有移动，并假设这些区域是停车位。但我们如何检测汽车何时离开停车位呢？

主要问题是，我们的图像中汽车的边界框有部分重叠：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728148087469556.png'/>
即使对于不同停车位的汽车，每辆车的边界框也有一点重叠。

因此，如果我们假设每一个边界框中的都代表一个停车位，那么即使停车位是空的，这个边界框也可能有一部分被汽车占据。我们需要一种方法来测量两个对象重叠的程度，以便检查“大部分是空的”的边框。

我们将使用交并比（Intersection Over Union ，IoU）方法。用两个对象重叠的像素数量除以两个对象覆盖的像素总数量，如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728148764563760.png'/>
IoU可以告诉我们汽车边界框与停车位边界框的重叠程度。有了这个指标，我们就可以很容易地确定一辆车是否在停车位。如果IoU测量值很低，比如0.15，这意味着这辆车并没有占用太多的停车位。但是如果测量值很高，比如0.6，这意味着汽车占据了大部分的停车位，那么我们可以确定停车位被占用了。

由于IoU在计算机视觉中是一种常见的度量方法，所以通常您使用的库已经有相关实践。事实上，Matterport Mask R-CNN库中就有这样的函数mrcnn.utils.compute_overlaps()，我们可以直接调用这个函数。

假设在图像中有一个表示停车区域的边界框列表，那么检查被检测到的车辆是否在这些边界框中，就如同添加一行或两行代码一样简单。

结果如下：
[
 [1.         0.07040032 0.         0.]
 [0.07040032 1.         0.07673165 0.]
 [0.         0.         0.02332112 0.]
]

在这个二维数组中，每一行表示一个停车位的边界框。相应的，每列表示该停车位与被检测到的汽车有多少重叠。1.0分意味着汽车完全占据了停车位，而0.02分这样的低分意味着汽车只是接触了停车位边界框，但并没有占据很多区域。

为了找到空置的停车位，我们只需要检查这个数组中的每一行。如果所有的数字都是零或者非常小的数字，就意味着没有东西占据了这个空间，因此它就是空闲的！

但请记住，物体检测并不总是与实时视频完美结合。尽管Mask R-CNN非常准确，但偶尔它会在一帧视频中错过一两辆车。因此，在将停车位标记为空闲之前，我们应该确保它在一段时间内都是空闲的，可能是5或10帧连续视频。这将防止仅仅在一帧视频上出现暂时性的物体检测问题而误导系统将停车位判定为空闲。但当我们看到至少有一个停车位在连续几帧视频图像中都被判定为空闲，我们就可以发送短信了！

五、发送短信
最后一步是当我们注意到一个停车位在连续几帧视频图像中都被判定为空闲时，就发送一条短信提醒。

使用 Twilio从Python发送短消息非常简单。Twilio是一个流行的接口，它可以让您用几行代码从任何编程语言发送短消息。当然，如果您喜欢使用其他的短信服务提供者，也是可以的。我和Twilio没有利害关系。只是第一个就想到了它。

Twilio：
https://www.twilio.com

要使用Twilio，需要注册试用帐户，创建Twilio电话号码并获取您的帐户凭据。然后，您需要安装Twilio Python客户端的库：

pip3 install twilio

安装后，使用下面的代码（需要将关键信息替换成您的账户信息），就可以从Python发送短信了：

from twilio.rest import Client
  
  # Twilio account details
  twilio_account_sid = &#39;Your Twilio SID here&#39;
  twilio_auth_token = &#39;Your Twilio Auth Token here&#39;
  twilio_source_phone_number = &#39;Your Twilio phone number here&#39;
  
  # Create a Twilio client object instance
  client = Client(twilio_account_sid, twilio_auth_token)
  
  # Send an SMS
  message = client.messages.create(
      body="This is my SMS message!",
      from_=twilio_source_phone_number,
      to="Destination phone number here"
  )


直接将代码插入到我们的脚本里，就可以添加发送短信的功能了。但我们需要注意的是，如果一个停车位一直是空闲的，就不需要在每一帧视频都给自己发送短信了。因此，我们需要有一个标志来标记我们是否已经发送了一条短信，并确保在经过一定时间或检测到其他停车位空闲之前，我们不会再发送另一条短信息。

打包组装
我们将流程中的每个步骤写成了一个单独的Python脚本。以下是完整的代码：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728149633520660.png'/>
此部分代码太长，感兴趣的同学可以在下面的网址中找到：
https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnn-and-python-955f2231c400

要运行此代码，您需要首先安装Python 3.6+, Matterport Mask R-CNN和OpenCV。

Matterport Mask R-CNN：
https://github.com/matterport/Mask_RCNN
OpenCV：
https://pypi.org/project/opencv-python/

我有意让代码尽可能简明扼要。例如，它只是假设在第一帧视频中出现的任何汽车都是已停放的汽车。试试修改代码，看看您能不能提高它的可靠性。

不用担心修改此代码就不能适应不同的场景。只需更改模型搜寻的对象ID，就可以将代码完全转换为其他内容。例如，假设您在滑雪场工作。通过一些调整，您可以将此脚本转换为一个自动检测滑雪板从斜坡上跳下的系统，并记录炫酷的滑雪板跳跃轨迹。或者，如果您在一个游戏保护区工作，您可以把这个脚本变成一个用来计数在野外能看到多少斑马的系统。

唯一的限制就是你的想象力。

一起来试试吧!
 
相关报道：
https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnn-and-python-955f2231c400
## 如何理解Faster RCNN
目前学术和工业界出现的目标检测算法分成3类：
1. 传统的目标检测算法：Cascade + HOG/DPM + Haar/SVM以及上述方法的诸多改进、优化；

2. 候选区域/框 + 深度学习分类：通过提取候选区域，并对相应区域进行以深度学习方法为主的分类的方案，如：
R-CNN（Selective Search + CNN + SVM）
SPP-net（ROI Pooling）
Fast R-CNN（Selective Search + CNN + ROI）
Faster R-CNN（RPN + CNN + ROI）
R-FCN
等系列方法；

3. 基于深度学习的回归方法：YOLO/SSD/DenseBox 等方法；以及最近出现的结合RNN算法的RRC detection；结合DPM的Deformable CNN等

经过R-CNN和Fast RCNN的积淀，Ross B. Girshick在2016年提出了新的Faster RCNN，在结构上，Faster RCNN已经将特征抽取(feature extraction)，proposal提取，bounding box regression(rect refine)，classification都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421007015280035.jpg'/>
图1 Faster RCNN基本结构（来自原论文）

依作者看来，如图1，Faster RCNN其实可以分为4个主要内容：

①Conv layers。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的

②feature maps。该feature maps被共享用于后续RPN层和全连接层。

③Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于foreground或者background，再利用bounding box regression修正anchors获得精确的proposals。
Roi Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。

④Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。

所以本文以上述4个内容作为切入点介绍Faster R-CNN网络。

下图图2展示了python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421014336794242.jpg'/>
图2 faster_rcnn_test.pt网络结构 （pascal_voc/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt）

可以清晰的看到该网络对于一副任意大小PxQ的图像，首先缩放至固定大小MxN，然后将MxN图像送入网络；
而Conv layers中包含了13个conv层+13个relu层+4个pooling层；RPN网络首先经过3x3卷积，再分别生成foreground anchors与bounding box regression偏移量，然后计算出proposals；
而Roi Pooling层则利用proposals从feature maps中提取proposal feature送入后续全连接和softmax网络作classification（即分类proposal到底是什么object）。

关于R-CNN家族的历史，请参见此文：https://www.julyedu.com/question/big/kp_id/32/ques_id/2103。


一 Conv layers
Conv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图2，Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv layers中：

所有的conv层都是： <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421023867826680.svg'/> ， <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421024489104164.svg'/>， <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421024979956340.svg'/>
所有的pooling层都是： <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421025692099613.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421026266538870.svg'/>， <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542102685635210.svg'/>

为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（ pad=1，即填充一圈0），导致原图变为 (M+2)x(N+2)大小，再做3x3卷积后输出MxN 。正是这种设置，导致Conv layers中的conv层不改变输入和输出矩阵大小。如图3：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421029364916114.jpg'/>
图3 卷积示意图

类似的是，Conv layers中的pooling层kernel_size=2，stride=2。这样每个经过pooling层的MxN矩阵，都会变为(M/2)x(N/2)大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。

那么，一个MxN大小的矩阵经过Conv layers固定变为(M/16)x(N/16)！这样Conv layers生成的featuure map中都可以和原图对应起来。


二 Region Proposal Networks(RPN)
经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如R-CNN使用SS(Selective Search)方法生成检测框。

而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster R-CNN的巨大优势，能极大提升检测框的生成速度。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421034131889529.jpg'/>
图4 RPN网络结构

上图4展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和background（检测目标是foreground），下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。

而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。

2.1 多通道图像卷积基础知识介绍
在介绍RPN前，还要多解释几句基础知识，已经懂的看官老爷跳过就好。

对于单通道图像+单卷积核做卷积，第一章中的图3已经展示了；
对于多通道图像+多卷积核做卷积，计算方式如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421036178738034.jpg'/>
图5 多通道卷积计算方式

如图5，输入有3个通道，同时有2个卷积核。对于每个卷积核，先在输入3个通道分别作卷积，再将3个通道结果加起来得到卷积输出。所以对于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量！
对多通道图像做1x1卷积，其实就是将输入图像于每个通道乘以卷积系数后加在一起，即相当于把原图像中本来各个独立的通道“联通”在了一起。

2.2 anchors
提到RPN网络，就不能不说anchors。所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。直接运行Faster RCNN的作者在其论文中给的demo中的generate_anchors.py可以得到以下输出：

[[ -84.  -40.   99.   55.]
 [-176.  -88.  191.  103.]
 [-360. -184.  375.  199.]
 [ -56.  -56.   71.   71.]
 [-120. -120.  135.  135.]
 [-248. -248.  263.  263.]
 [ -36.  -80.   51.   95.]
 [ -80. -168.   95.  183.]
 [-168. -344.  183.  359.]]

其中每行的4个值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420879293384076.svg'/>表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420880792797017.svg'/>三种，如下图。实际上通过anchors就引入了检测中常用到的多尺度方法。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542088266229831.jpg'/>

注：关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成800x600（即下图中的M=800，N=600）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420887897013780.jpg'/>

再回头来看anchors的大小，anchors中长宽1:2中最大为352x704，长宽2:1中最大736x384，基本是cover了800x600的各个尺度和形状。

那么这9个anchors是做什么的呢？借用Faster RCNN论文中的原图，如下图，遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420890012978963.jpg'/>

解释一下上面这张图的数字。

a）在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions

b）在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息（猜测这样做也许更鲁棒？反正我没测试），同时256-d不变（如上图中的红框）

c)假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分foreground和background，所以每个点由256d feature转化为cls=2k scores；而每个anchor都有(x, y, w, h)对应4个偏移量，所以reg=4k coordinates

d)补充一点，全部anchors拿去训练太多了，训练程序会在合适的anchors中随机选取128个postive anchors+128个negative anchors进行训练

注意，在本文讲解中使用的VGG conv5 num_output=512，所以是512d，其他类似。

其实RPN最终就是在原图尺度上，设置了密密麻麻的候选Anchor。然后用cnn去判断哪些Anchor是里面有目标的foreground anchor，哪些是没目标的backgroud。所以，仅仅是个二分类而已！

那么Anchor一共有多少个？原图800x600，VGG下采样16倍，feature map每个点设置9个Anchor，所以：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420896774070786.svg'/>

其中ceil()表示向上取整，是因为VGG输出的feature map size= 50*38。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420898095049747.jpg'/>

2.3 softmax判定foreground与background
一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16，H=N/16。在进入reshape与softmax之前，先做了1x1卷积，如图9：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421038657233871.jpg'/>
图9 RPN中判定fg/bg网络结构

该1x1卷积的caffe prototxt定义如下：

layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn/output"
  top: "rpn_cls_score"
  convolution_param {
    num_output: 18   # 2(bg/fg) * 9(anchors)
    kernel_size: 1 pad: 0 stride: 1
  }
}
可以看到其num_output=18，也就是经过该卷积的输出图像为WxHx18大小（注意第二章开头提到的卷积计算方式）。这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是foreground和background，所有这些信息都保存WxHx(9*2)大小的矩阵。为何这样做？后面接softmax分类获得foreground anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在foreground anchors中）。

那么为何要在softmax前后都接一个reshape layer？其实只是为了便于softmax分类，至于具体原因这就要从caffe的实现形式说起了。在caffe基本数据结构blob中以如下形式保存数据：

blob=[batch_size, channel，height，width]
对应至上面的保存bg/fg anchors的矩阵，其在caffe blob中的存储形式为[1, 2x9, H, W]。而在softmax分类时需要进行fg/bg二分类，所以reshape layer会将其变为[1, 2, 9xH, W]大小，即单独“腾空”出来一个维度以便softmax分类，之后再reshape回复原状。贴一段caffe softmax_loss_layer.cpp的reshape函数的解释，非常精辟：

"Number of labels must match number of predictions; "
"e.g., if softmax axis == 1 and prediction shape is (N, C, H, W), "
"label count (number of labels) must be N*H*W, "
"with integer values in {0, 1, ..., C-1}.";

综上所述，RPN网络中利用anchors和softmax初步提取出foreground anchors作为候选区域。

2.4 bounding box regression原理
如图9所示绿色框为飞机的Ground Truth(GT)，红色为提取的foreground anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得foreground anchors和GT更加接近。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542104848988017.jpg'/>
图10

对于窗口一般使用四维向量  (x, y, w, h) 表示，分别表示窗口的中心点坐标和宽高。对于图 11，红色的框A代表原始的Foreground Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G'，即：

给定：anchor <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421052937197877.svg'/>,和 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421054464832961.svg'/>
寻找一种变换F，使得：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421055595429698.svg'/>，其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421057069663415.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421058029605774.jpg'/>
图11

那么经过何种变换F才能从图10中的anchor A变为G'呢？ 比较简单的思路就是:

先做平移
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421062065900717.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421062791843737.svg'/>

再做缩放
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421063397533378.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421063911969984.svg'/>

观察上面4个公式发现，需要学习的是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421069079358060.svg'/>这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当anchors A和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。

接下来的问题就是如何通过线性回归获得<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421070067453287.svg'/>了。线性回归就是给定输入的特征向量X, 学习一组参数W, 使得经过线性回归后的值跟真实值Y非常接近，即Y=WX。对于该问题，输入X是cnn feature map，定义为Φ；同时还有训练传入A与GT之间的变换量，即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421071455390992.svg'/>。输出是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421072182371619.svg'/>四个变换。那么目标函数可以表示为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421072836233581.svg'/>

其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421076093661500.svg'/>是对应anchor的feature map组成的特征向量，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421076822660740.svg'/>是需要学习的参数，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421077552865835.svg'/>是得到的预测值（*表示 x，y，w，h，也就是每一个变换对应一个上述目标函数）。为了让预测值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421078771200222.svg'/>与真实值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421079513852603.svg'/>差距最小，设计损失函数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542108447962919.svg'/>

函数优化目标为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421085373307319.svg'/>

需要说明，只有在GT与需要回归框位置比较接近时，才可近似认为上述线性变换成立。
说完原理，对应于Faster RCNN原文，foreground anchor与ground truth之间的平移量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421086624709449.svg'/>与尺度因子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421087366484514.svg'/>如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421087959428027.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421088571443870.svg'/>

对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421091221974580.svg'/>，即训练目标是：输入 Φ的情况下使网络输出与监督信号尽可能接近。
那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421092084865231.svg'/>，显然即可用来修正Anchor位置了。

2.5 对proposals进行bounding box regression
在了解bounding box regression后，再回头来看RPN网络第二条线路，如图12。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421094031196768.jpg'/>
图12 RPN中的bbox reg

先来看一看上图11中1x1卷积的caffe prototxt定义：

layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn/output"
  top: "rpn_bbox_pred"
  convolution_param {
    num_output: 36   # 4 * 9(anchors)
    kernel_size: 1 pad: 0 stride: 1
  }
}
可以看到其 num_output=36，即经过该卷积输出图像为WxHx36，在caffe blob存储为[1, 4x9, H, W]，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542109609785247.svg'/>
变换量。

2.6 Proposal Layer
Proposal Layer负责综合所有
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421097849884869.svg'/>
变换量和foreground anchors，计算出精准的proposal，送入后续RoI Pooling Layer。还是先来看看Proposal Layer的caffe prototxt定义：
layer {
  name: 'proposal'
  type: 'Python'
  bottom: 'rpn_cls_prob_reshape'
  bottom: 'rpn_bbox_pred'
  bottom: 'im_info'
  top: 'rois'
  python_param {
    module: 'rpn.proposal_layer'
    layer: 'ProposalLayer'
    param_str: "'feat_stride': 16"
  }
}
Proposal Layer有3个输入：fg/bg anchors分类器结果rpn_cls_prob_reshape，对应的bbox reg的
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421100117598083.svg'/>

变换量rpn_bbox_pred，以及im_info；另外还有参数feat_stride=16，这和图4是对应的。
首先解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定MxN，im_info=[M, N, scale_factor]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为WxH=(M/16)x(N/16)大小，其中feature_stride=16则保存了该信息，用于计算anchor偏移量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421102423249417.jpg'/>
图13

Proposal Layer forward（caffe layer的前传函数）按照以下顺序依次处理：

1)生成anchors，利用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421104665475598.svg'/>对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）
2)按照输入的foreground softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的foreground anchors。
3)限定超出图像边界的foreground anchors为图像边界（防止后续roi pooling时proposal超出图像边界）

4)剔除非常小（width<threshold or height<threshold）的foreground anchors
5)进行nonmaximum suppression
6)再次按照nms后的foreground softmax scores由大到小排序fg anchors，提取前post_nms_topN(e.g. 300)结果作为proposal输出。

之后输出proposal=[x1, y1, x2, y2]，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的，这点在后续网络中有用。另外我认为，严格意义上的检测应该到此就结束了，后续部分应该属于识别了~
RPN网络结构就介绍到这里，总结起来就是：
生成anchors -> softmax分类器提取fg anchors -> bbox reg回归fg anchors -> Proposal Layer生成proposals

3 RoI pooling
而RoI Pooling层则负责收集proposal，并计算出proposal feature maps，送入后续网络。从图2中可以看到Rol pooling层有2个输入：

原始的feature maps
RPN输出的proposal boxes（大小各不相同）

3.1 为何需要RoI Pooling
先来看一个问题：对于传统的CNN（如AlexNet，VGG），当网络训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的vector or matrix。如果输入图像大小不定，这个问题就变得比较麻烦。有2种解决办法：
i)从图像中crop一部分传入网络
ii)将图像warp成需要的大小后传入网络
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421116017452060.jpg'/>
图14 crop与warp破坏图像原有结构信息

两种办法的示意图如图14，可以看到无论采取那种办法都不好，要么crop后破坏了图像的完整结构，要么warp破坏了图像原始形状信息。

回忆RPN网络生成的proposals的方法：对foreground anchors进行bounding box regression，那么这样获得的proposals也是大小形状各不相同，即也存在上述问题。所以Faster R-CNN中提出了RoI Pooling解决这个问题。不过RoI Pooling确实是从Spatial Pyramid Pooling发展而来，但是限于篇幅这里略去不讲，有兴趣的读者可以自行查阅相关论文。

3.2 RoI Pooling原理
分析之前先来看看RoI Pooling Layer的caffe prototxt的定义：

layer {
  name: "roi_pool5"
  type: "ROIPooling"
  bottom: "conv5_3"
  bottom: "rois"
  top: "pool5"
  roi_pooling_param {
    pooled_w: 7
    pooled_h: 7
    spatial_scale: 0.0625 # 1/16
  }
}
其中有新参数 ，另外一个参数 认真阅读的读者肯定已经知道知道用途。
RoI Pooling layer forward过程：

由于proposal是对应<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421122987881625.svg'/>尺度的，所以首先使用spatial_scale参数将其映射回 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421124173288985.svg'/>大小的feature map尺度；

再将每个proposal对应的feature map区域水平分为 \text{pooled_w}\times \text{pooled_h} 的网格；
对网格的每一份都进行max pooling处理。

这样处理后，即使大小不同的proposal输出结果都是 \text{pooled_w}\times \text{pooled_h} 固定大小，实现了固定长度输出。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421127531054389.jpg'/>
图15 proposal示意图

四 Classification
Classification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框。Classification部分网络结构如图16。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421130541667433.jpg'/>
图16 Classification部分网络结构图

从PoI Pooling获取到7x7=49大小的proposal feature maps后，送入后续网络，可以看到做了如下2件事：

a)通过全连接和softmax对proposals进行分类，这实际上已经是识别的范畴了
b)再次对proposals进行bounding box regression，获取更高精度的rect box

这里来看看全连接层InnerProduct layers，简单的示意图如图17，
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421133733092777.jpg'/>
图17 全连接层示意图

其计算公式如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421135245894870.jpg'/>

其中W和bias B都是预先训练好的，即大小是固定的，当然输入X和输出Y也就是固定大小。所以，这也就印证了之前Roi Pooling的必要性。到这里，我想其他内容已经很容易理解，不在赘述了。

五 Faster R-CNN训练
Faster R-CNN的训练，是在已经训练好的model（如VGG_CNN_M_1024，VGG，ZF）的基础上继续进行训练。实际中训练过程分为6个步骤：

①在已经训练好的model上，训练RPN网络，对应stage1_rpn_train.pt
②利用步骤1中训练好的RPN网络，收集proposals，对应rpn_test.pt
③第一次训练Fast RCNN网络，对应stage1_fast_rcnn_train.pt
④第二训练RPN网络，对应stage2_rpn_train.pt
⑤再次利用步骤4中训练好的RPN网络，收集proposals，对应rpn_test.pt
⑥第二次训练Fast RCNN网络，对应stage2_fast_rcnn_train.pt

可以看到训练过程类似于一种“迭代”的过程，不过只循环了2次。至于只循环了2次的原因是应为作者提到："A similar alternating training can be run for more iterations, but we have observed negligible improvements"，即循环更多次没有提升了。接下来本章以上述6个步骤讲解训练过程。

下面是一张训练过程流程图，应该更加清晰。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421145351772320.jpg'/>
图18 Faster RCNN训练步骤（引用自参考文章[1]）

5.1 训练RPN网络
在该步骤中，首先读取RBG提供的预训练好的model（本文使用VGG），开始迭代训练。来看看stage1_rpn_train.pt网络结构，如图19。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421147531698889.jpg'/>
图19 stage1_rpn_train.pt（考虑图片大小，Conv Layers中所有的层都画在一起了，如红圈所示，后续图都如此处理）
与检测网络类似的是，依然使用Conv Layers提取feature maps。整个网络使用的Loss如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542115069611824.svg'/>

上述公式中 i 表示anchors index， <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421155080356088.svg'/>表示foreground softmax probability，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421155950731054.svg'/>代表对应的GT predict概率（即当第i个anchor与GT间IoU>0.7，认为是该anchor是foreground，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421156756380556.svg'/>；

反之IoU<0.3时，认为是该anchor是background，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421160545058096.svg'/>；至于那些0.3<IoU<0.7的anchor则不参与训练）；t代表predict bounding box，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421161390061770.svg'/>代表对应foreground anchor对应的GT box。可以看到，整个Loss分为2部分：

1.cls loss，即rpn_cls_loss层计算的softmax loss，用于分类anchors为forground与background的网络训练
2.reg loss，即rpn_loss_bbox层计算的soomth L1 loss，用于bounding box regression网络训练。注意在该loss中乘了 p_{i}^{*} ，相当于只关心foreground anchors的回归（其实在回归中也完全没必要去关心background）。

由于在实际过程中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421169442525502.svg'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421170160839746.svg'/>差距过大，用参数λ平衡二者（如<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421171348594720.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542117205261512.svg'/>时设置 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421172922618453.svg'/>，使总的网络Loss计算过程中能够均匀考虑2种Loss。这里比较重要是 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421173733199110.svg'/>使用的soomth L1 loss，计算公式如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421176149880653.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421176712172567.svg'/>

了解数学原理后，反过来看图18：

1)在RPN训练阶段，rpn-data（python AnchorTargetLayer）层会按照和test阶段Proposal层完全一样的方式生成Anchors用于训练
2)对于rpn_loss_cls，输入的rpn_cls_scors_reshape和rpn_labels分别对应 p 与<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421182939956677.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421183672817331.svg'/>参数隐含在p与<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421184349917223.svg'/>的caffe blob的大小中
3)对于rpn_loss_bbox，输入的rpn_bbox_pred和rpn_bbox_targets分别对应 t 与<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421186593562075.svg'/>，rpn_bbox_inside_weigths对应<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542118936420863.svg'/>，rpn_bbox_outside_weigths未用到（从soomth_L1_Loss layer代码中可以看到），而<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421195326282408.svg'/>同样隐含在caffe blob大小中

这样，公式与代码就完全对应了。特别需要注意的是，在训练和检测阶段生成和存储anchors的顺序完全一样，这样训练结果才能被用于检测！

5.2 通过训练好的RPN网络收集proposals
在该步骤中，利用之前的RPN网络，获取proposal rois，同时获取foreground softmax probability，如图20，然后将获取的信息保存在python pickle文件中。该网络本质上和检测中的RPN网络一样，没有什么区别。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421198012597782.jpg'/>
图20 rpn_test.pt

5.3 训练Faster RCNN网络
读取之前保存的pickle文件，获取proposals与foreground probability。从data层输入网络。然后：

将提取的proposals作为rois传入网络，如图19蓝框
计算bbox_inside_weights+bbox_outside_weights，作用与RPN一样，传入soomth_L1_loss layer，如图20绿框
这样就可以训练最后的识别softmax与最终的bounding box regression了，如图21。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542120013124204.jpg'/>
图21 stage1_fast_rcnn_train.pt

之后的stage2训练都是大同小异，不再赘述了。Faster R-CNN还有一种end-to-end的训练方式，可以一次完成train，有兴趣请自己看作者GitHub吧：https://github.com/rbgirshick/py-faster-rcnn


参考文献：
Object Detection and Classification using R-CNNs
http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/

拓展
如何将Faster RCNN应用于文字检测？CTPN:场景文字检测—CTPN原理与实现
https://zhuanlan.zhihu.com/p/34757009

本题解析来源：https://zhuanlan.zhihu.com/p/31426458
## one-stage和two-stage目标检测方法的区别和优缺点？
众所周知，物体检测的任务是找出图像或视频中的感兴趣物体，同时检测出它们的位置和大小。

当然，物体检测过程中有很多不确定因素，如图像中物体数量不确定，物体有不同的外观、形状、姿态，加之物体成像时会有光照、遮挡等因素的干扰，导致检测算法有一定的难度。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726659599200997.jpg'/>
由于目标检测的应用场景广泛，所以在CV面试中经常出现，比如七月在线有一CV就业班的学员出去面试时，便被问到“one-stage和two-stage目标检测方法的区别和优缺点？”（详见此文：https://ask.julyedu.com/question/88747） 

虽然我们在本文中详细介绍了各个目标检测的方法：https://www.julyedu.com/question/big/kp_id/32/ques_id/2103 ，但如果你是第一次听到one-stage和two-stage，你会不会瞬间一脸懵逼，这是啥？

其实很简单，顾名思义，区别在于是一步到位还是两步到位。
具体说来，进入深度学习时代以来，物体检测发展主要集中在两个方向：
two stage算法，如R-CNN系列；
ones-tage算法，如YOLO、SSD等。
两者的主要区别在于two stage算法需要先生成proposal（一个有可能包含待检物体的预选框），然后进行细粒度的物体检测，而one stage算法会直接在网络中提取特征来预测物体分类和位置。

所以说，目标检测算法two-stage，如Faster R-CNN算法会先生成候选框（region proposals，可能包含物体的区域），然后再对每个候选框进行分类（也会修正位置）。这类算法相对就慢，因为它需要多次运行检测和分类流程。
而另外一类one-stage目标检测算法（也称one-shot object detectors），其特点是一步到位，仅仅需要送入网络一次就可以预测出所有的边界框，速度相对较快，非常适合移动端，最典型的one-stage检测算法包括YOLO，SSD，SqueezeDet以及DetectNet。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726967446574479.jpg'/>


简单吧，恍然大悟，原来如此！而且one-stage看起来更高级。

当然，目标检测还包括其他很多方法，如下图所示：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726642393420130.jpg'/>

由于two-stage方法在此文中已经详细介绍：https://www.julyedu.com/question/big/kp_id/32/ques_id/2103  ， 故下面重点介绍看起来更高级的one-stage方法。

为什么目标检测问题更难
图像分类是生成单个输出，即类别概率分布。但是这只能给出图像整体内容的摘要，当图像有多个感兴趣的物体时，它就不行了。在下面的图像中，分类器可能会识别出图像即包含猫，也包含狗，这是它最擅长的。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726835338718424.jpg'/>

而目标检测模型将通过预测每个物体的边界框来给出各个物体的位置：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726837618803820.jpg'/>
因为可以专注于对边界框内的物体进行分类并忽略外部背景，因此模型能够为各个物体提供更加准确的预测。如果数据集带有边界框标注，则可以非常轻松地在模型添加一个定位预测：只需预测额外4个数字，分别用于边界框的每个角落。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726841055868983.jpg'/>
现在该模型有两部分输出：类别概率分布和边界框回归。模型的损失函数只是将边界框的回归损失与分类的交叉熵损失相加，通常使用均方误差（MSE）：
outputs = model.forward_pass(image)class_pred = outputs[0]
bbox_pred = outputs[1]
class_loss = cross_entropy_loss(class_pred, class_true)
bbox_loss = mse_loss(bbox_pred, bbox_true)
loss = class_loss + bbox_lossoptimize(loss)

然后采用SGD方法对模型优化训练，这是一个预测实例：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726849862660942.jpg'/>

模型已正确对图像中物体（狗）分类，并给出它在图像中的位置。红色框是真实框，而青色框是预测框，虽然有偏差，但非常接近。为了评估预测框与真实框的匹配程度，我们可以计算两个边界框之间的IOU（intersection-over-union，也称为Jaccard index）。IOU在0到1之间，越大越好。理想情况下，预测框和真框的IOU为100％，但实际上任何超过50％的预测通常都被认为是正确的。对于上面的示例，IOU为74.9％，因而预测框比较精确。使用回归方法预测单个边界框可以获得较好的结果。

然而，当图像中存在多个感兴趣的物体时，就会出现问题：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726852570511138.jpg'/>

由于模型只能预测一个边界框，因而它必须要选择一个物体，这会最终落在中间位置。实际上这很容易理解：图像里有两个物体，但是模型只能给出一个边界框，因而选择了折中，预测框位于两者中间，也许大小也是介于两个物体大小之间。

注意：也许你可能认为模型应该给出一个包含两个物体的边界框，但是这不太会发生，因为训练不是这样的，真实框都是各个物体分开标注的，而不是一组物体进行标注。你也许认为，上述问题很好解决，对于模型的回归部分增加更多的边界框预测不就好了。

毕竟，如果模型可以预测N个边界框，那么就应该可以正确定位N个物体。听起来不错，但是并没有效。就算模型有多个检测器（这里一个边界框回归称为一个检测器），我们得到的边界框依然会落在图像中间：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415672685453722376.jpg'/>
为什么会这样？问题是模型不知道应该将哪个边界框分配给哪个物体，为了安全起见，它将它们放在中间的某个位置。该模型无法决定：“我可以在左边的马周围放置边界框1，并在右边的马周围放置框2。”

相反，每个检测器仍然试图预测所有物体，而不是一个检测器预测一个物体。尽管该模型具有N个检测器，但它们无法协同工作（对，无法协同工作）。具有多个边界框检测器的模型的效果与仅预测一个边界框的模型完全相同。

我们需要的是使边界框检测器更专一化的一些方法，以便每个检测器将尝试仅预测单个物体，并且不同的探测器将找到不同的物体。在不专一的模型中，每个检测器应该能够处理图像中任何可能位置的各类物体。

这太简单了，模型学会的是预测位于图像中心的方框，因为这样整个训练集实际上会最小化损失函数。从SGD的角度来看，这样做平均得到了相当不错的结果，但在实践中它却不是真正有用的结果，所以我们需要更加有效地训练模型。

通过将每个边界框检测器分配到图像中的特定位置，one-stage目标检测算法（例如YOLO，SSD和DetectNet）都是这样来解决这个问题。因为，检测器学会专注于某些位置的物体。为了获得更好的效果，我们还可以让检测器专注于物体的形状和大小。

继续深入请见此文：https://zhuanlan.zhihu.com/p/61485202
## 请画下YOLOv3的网络结构
本题解析来源：https://blog.csdn.net/qq_37541097/article/details/81214953 ，https://blog.csdn.net/dz4543/article/details/90049377

本人是小白，看后表示有点蒙。于是在Github上搜了大牛们基于Tensorflow搭建的YOLOv3模型进行分析（本人只接触过TF，所以就不去看caffe的源码了）。接下来我会根据我阅读的代码来进一步分析网络的结构。Github YOLOv3大牛代码链接。

1.Darknet-53 模型结构
在论文中虽然有给网络的图，但我还是简单说一下。这个网络主要是由一系列的1x1和3x3的卷积层组成（每个卷积层后都会跟一个BN层和一个LeakyReLU)层，作者说因为网络中有53个convolutional layers，所以叫做Darknet-53（2 + 1*2 + 1 + 2*2 + 1 + 8*2 + 1 + 8*2 + 1 + 4*2 + 1 = 53 按照顺序数，不包括Residual中的卷积层，最后的Connected是全连接层也算卷积层，一共53个）。
下图就是Darknet-53的结构图，在右侧标注了一些信息方便理解（卷积的strides默认为（1，1），padding默认为same，当strides为（2，2）时padding为valid）
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313746962362993.png'/>
看完上图应该就能自己搭建出Darknet-53的网络结构了，上图是以输入图像256 x 256进行预训练来进行介绍的，常用的尺寸是416 x 416，都是32的倍数。下面我们再来分析下YOLOv3的特征提取器，看看究竟是在哪几层Features上做的预测。

2.YOLOv3 模型结构
作者在论文中提到利用三个特征层进行边框的预测，具体在哪三层我感觉作者在论文中表述的并不清楚（例如文中有“添加几个卷积层”这样的表述），同样根据代码我将这部分更加详细的分析展示在下图中。
注意：原Darknet53中的尺寸是在图片分类训练集上训练的，所以输入的图像尺寸是256x256，下图是以YOLO v3 416模型进行绘制的，所以输入的尺寸是416x416，预测的三个特征层大小分别是52，26，13。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313761741474063.jpg'/>
在上图中我们能够很清晰的看到三个预测层分别来自的什么地方，以及Concatenate层与哪个层进行拼接。注意Convolutional是指Conv2d+BN+LeakyReLU，和Darknet53图中的一样，而生成预测结果的最后三层都只是Conv2d。通过上图小伙伴们就能更加容易地搭建出YOLOv3的网络框架了。

3.目标边界框的预测
YOLOv3网络在三个特征图中分别通过(4+1+c) k个大小为11的卷积核进行卷积预测，k为预设边界框（bounding box prior）的个数（k默认取3），c为预测目标的类别数，其中4k个参数负责预测目标边界框的偏移量，k个参数负责预测目标边界框内包含目标的概率，ck个参数负责预测这k个预设边界框对应c个目标类别的概率。
下图展示了目标边界框的预测过程（该图是本人重新绘制的，与论文中的示意图有些不同，个人感觉自己绘制的更便于理解）。
图中虚线矩形框为预设边界框，实线矩形框为通过网络预测的偏移量计算得到的预测边界框。
其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313842624654549.gif'/>为预设边界框在特征图上的中心坐标，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313843314124415.gif'/>为预设边界框在特征图上的宽和高，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313844275776899.gif'/>分别为网络预测的边界框中心偏移量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415831384567094883.gif'/>以及宽高缩放比<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313846478916318.gif'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313847889733775.gif'/>为最终预测的目标边界框，从预设边界框到最终预测边界框的转换过程如图右侧公式所示，其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313849260307297.gif'/>函数是sigmoid函数其目的是将预测偏移量缩放到0到1之间（这样能够将预设边界框的中心坐标固定在一个cell当中，作者说这样能够加快网络收敛）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313769499865825.png'/>

下图给出了三个预测层的特征图大小以及每个特征图上预设边界框的尺寸（这些预设边界框尺寸都是作者根据COCO数据集聚类得到的）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313770786807442.png'/>

4.损失函数的计算
关于YOLOv3的损失函数文章中写的很粗略，比如坐标损失采用的是误差的平方和，类别损失采用的是二值交叉熵，本人在github上也找了很多YOLO v3的公开代码，有的采用的是YOLOv1或者YOLOv2的损失函数，下面给出本人认为正确的损失函数（这里偷个懒，公式都是从本人之前写的论文中截图的）。

YOLOv3的损失函数主要分为三个部分：目标定位偏移量损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313855878379421.gif'/>，目标置信度损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313856464682616.gif'/>以及目标分类损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313857856262876.gif'/>，其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415831385869675310.gif'/>是平衡系数。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313775959399980.png'/>

4.1目标置信度损失
目标置信度可以理解为预测目标矩形框内存在目标的概率，目标置信度损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313863437234915.gif'/>采用的是二值交叉熵损失(Binary Cross Entropy)，其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313864334829694.gif'/>，表示预测目标边界框i中是否真实存在目标，0表示不存在，1表示存在。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313865890679659.gif'/>表示预测目标矩形框i内是否存在目标的Sigmoid概率（将预测值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313866983237258.gif'/>通过sigmoid函数得到）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313778186063850.png'/>

4.2目标类别损失
目标类别损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313877170493442.gif'/>同样采用的是二值交叉熵损失（采用二值交叉熵损失的原因是，作者认为同一目标可同时归为多类，比如猫可归为猫类以及动物类，这样能够应对更加复杂的场景。但在本人实践过程中发现使用原始的多类别交叉熵损失函数效果会更好一点，原因是本人针对识别的目标都是固定归于哪一类的，并没有可同时归于多类的情况），其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313878843925498.gif'/>，表示预测目标边界框i中是否真实存在第j类目标，0表示不存在，1表示存在。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313880852694620.gif'/>表示网络预测目标边界框i内存在第j类目标的Sigmoid概率（将预测值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313883323504239.gif'/>通过sigmoid函数得到）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313779974821830.png'/>

4.3目标定位损失
目标定位损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313892310075204.gif'/>采用的是真实偏差值与预测偏差值差的平方和，其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313892999728054.gif'/>表示预测矩形框坐标偏移量（注意网络预测的是偏移量，不是直接预测坐标），<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313893945451242.gif'/>表示与之匹配的GTbox与默认框之间的坐标偏移量，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313895559801460.gif'/>为预测的目标矩形框参数，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313896589205355.gif'/>为默认矩形框参数，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313897325809160.gif'/>为与之匹配的真实目标矩形框参数，这些参数都是映射在预测特征图上的。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415831378587826363.png'/>

5 YOLOv3的几个输出实例
来一个YOLO输出时的显示：
layer     filters    size              input                output
   0 conv     32  3 x 3 / 1   416 x 416 x   3   ->   416 x 416 x  32 0.299 BF
   1 conv     64  3 x 3 / 2   416 x 416 x  32   ->   208 x 208 x  64 1.595 BF
   2 conv     32  1 x 1 / 1   208 x 208 x  64   ->   208 x 208 x  32 0.177 BF
   3 conv     64  3 x 3 / 1   208 x 208 x  32   ->   208 x 208 x  64 1.595 BF
   4 Shortcut Layer: 1
   5 conv    128  3 x 3 / 2   208 x 208 x  64   ->   104 x 104 x 128 1.595 BF
   6 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64 0.177 BF
   7 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128 1.595 BF
   8 Shortcut Layer: 5
   9 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64 0.177 BF
  10 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128 1.595 BF
  11 Shortcut Layer: 8
  12 conv    256  3 x 3 / 2   104 x 104 x 128   ->    52 x  52 x 256 1.595 BF
  13 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  14 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  15 Shortcut Layer: 12
  16 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  17 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  18 Shortcut Layer: 15
  19 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  20 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  21 Shortcut Layer: 18
  22 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  23 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  24 Shortcut Layer: 21
  25 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  26 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  27 Shortcut Layer: 24
  28 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  29 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  30 Shortcut Layer: 27
  31 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  32 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  33 Shortcut Layer: 30
  34 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  35 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  36 Shortcut Layer: 33
  37 conv    512  3 x 3 / 2    52 x  52 x 256   ->    26 x  26 x 512 1.595 BF
  38 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  39 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  40 Shortcut Layer: 37
  41 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  42 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  43 Shortcut Layer: 40
  44 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  45 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  46 Shortcut Layer: 43
  47 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  48 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  49 Shortcut Layer: 46
  50 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  51 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  52 Shortcut Layer: 49
  53 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  54 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  55 Shortcut Layer: 52
  56 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  57 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  58 Shortcut Layer: 55
  59 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  60 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  61 Shortcut Layer: 58
  62 conv   1024  3 x 3 / 2    26 x  26 x 512   ->    13 x  13 x1024 1.595 BF
  63 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  64 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  65 Shortcut Layer: 62
  66 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  67 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  68 Shortcut Layer: 65
  69 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  70 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  71 Shortcut Layer: 68
  72 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  73 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  74 Shortcut Layer: 71
  75 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  76 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  77 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  78 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  79 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  80 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  81 conv     18  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x  18 0.006 BF
  82 yolo
  83 route  79
  84 conv    256  1 x 1 / 1    13 x  13 x 512   ->    13 x  13 x 256 0.044 BF
  85 upsample            2x    13 x  13 x 256   ->    26 x  26 x 256
  86 route  85 61
  87 conv    256  1 x 1 / 1    26 x  26 x 768   ->    26 x  26 x 256 0.266 BF
  88 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  89 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  90 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  91 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  92 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  93 conv     18  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x  18 0.012 BF
  94 yolo
  95 route  91
  96 conv    128  1 x 1 / 1    26 x  26 x 256   ->    26 x  26 x 128 0.044 BF
  97 upsample            2x    26 x  26 x 128   ->    52 x  52 x 128
  98 route  97 36
  99 conv    128  1 x 1 / 1    52 x  52 x 384   ->    52 x  52 x 128 0.266 BF
 100 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
 101 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
 102 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
 103 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
 104 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
 105 conv     18  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x  18 0.025 BF
 106 yolo

实际，这个已经告诉了我们每层的输出情况。每层特征图的大小情况：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313961038895239.png'/>

在前文网络的基础上，用红色做了注释。residual使用残差结构。什么是残差结构？举个例子在第一层残差结构（其输出为208208128），其输入为20820864，经过3211和6433的卷积后，其生成的特征图与输入叠加起来。其结构如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313963074357755.png'/>

其叠加后的特征图作为新的输入输入下一层。YOLO主体是由许多这种残差模块组成，减小了梯度爆炸的风险，加强了网络的学习能力。

可以看到YOLO有3个尺度的输出，分别在52×52，26×26，13×13。嗯，都是奇数，使得网格会有个中心位置。同时YOLO输出为3个尺度，每个尺度之间还有联系。比如说，13×13这个尺度输出用于检测大型目标，对应的26×26为中型的，52×52用于检测小型目标。
上一张图，我觉得很详细看得懂。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313964860182548.png'/>

这个检测COCO（80个类的），所以其输出需要构造为：S×S×3×（5+class_number）。解释下为什么是这样。
YOLO将图像划分为S×S的网格，当目标中心落在某个网格中，就用这个网格去检测它，这是S×S的由来。
为什么是3，是因为每个网格需要检测3个anchorbox（注意有3个尺度），所以对于每个尺度，其输出为S×S×3×？？？
对于一个anchor box，它包含坐标信息（x , y , w , h ）以及置信度，而这有5个信息；同时还会包含是否所有类别的信息，使用one-hot编码。

比如说有3个类:person、car、dog。检测的结果是人，那么就编码为[1,0,0]。可见所有类别信息都会被编码，COCO有80个类别的话，便是5+80。所以，对于每个维度的输出，其结果为：S×S×3×（5+80）=S×S×255 S×S×3×（5+80） = S×S×255S×S×3×（5+80）=S×S×255。
同时从上图可以看到，其结果便是通过一些卷积操作，将输出构造成这样。并且将不同尺度的特征图叠加到一起，增加输出的信息。这个图可以好好看看。


# CV
## 1.基于深度学习的目标检测技术演进：R-CNN、Fast R-CNN、Faster R-CNN、YOLO、SSD
一、目标检测常见算法

object detection，就是在给定的图片中精确找到物体所在位置，并标注出物体的类别。所以，object detection要解决的问题就是物体在哪里以及是什么的整个流程问题。

然而，这个问题可不是那么容易解决的，物体的尺寸变化范围很大，摆放物体的角度，姿态不定，而且可以出现在图片的任何地方，更何况物体还可以是多个类别。

目前学术和工业界出现的目标检测算法分成3类：
1. 传统的目标检测算法：Cascade + HOG/DPM + Haar/SVM以及上述方法的诸多改进、优化；

2. 候选区域/框 + 深度学习分类：通过提取候选区域，并对相应区域进行以深度学习方法为主的分类的方案，如：
R-CNN（Selective Search + CNN + SVM）
SPP-net（ROI Pooling）
Fast R-CNN（Selective Search + CNN + ROI）
Faster R-CNN（RPN + CNN + ROI）
R-FCN
等系列方法；

3. 基于深度学习的回归方法：YOLO/SSD/DenseBox 等方法；以及最近出现的结合RNN算法的RRC detection；结合DPM的Deformable CNN等
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525258820_318.jpg'/>

传统目标检测流程：
1）区域选择（穷举策略：采用滑动窗口，且设置不同的大小，不同的长宽比对图像进行遍历，时间复杂度高）
2）特征提取（SIFT、HOG等；形态多样性、光照变化多样性、背景多样性使得特征鲁棒性差）
3）分类器分类（主要有SVM、Adaboost等）

二、传统的目标检测算法
2.1 从图像识别的任务说起
这里有一个图像任务：既要把图中的物体识别出来，又要用方框框出它的位置。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517392987_360.jpg'/>

这个任务本质上就是这两个问题：一：图像识别，二：定位。

图像识别（classification）：
输入：图片
输出：物体的类别
评估方法：准确率
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393007_344.jpg'/>

定位（localization）：
输入：图片
输出：方框在图片中的位置（x,y,w,h）
评估方法：检测评价函数intersection-over-union（关于什么是IOU，请参看本深度学习分类下第55题：https://www.julyedu.com/question/big/kp_id/26/ques_id/2138）
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393013_253.png'/>
卷积神经网络CNN已经帮我们完成了图像识别（判定是猫还是狗）的任务了，我们只需要添加一些额外的功能来完成定位任务即可。

定位的问题的解决思路有哪些？
思路一：看做回归问题
看做回归问题，我们需要预测出（x,y,w,h）四个参数的值，从而得出方框的位置。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393027_336.png'/>

步骤1:
　　•	先解决简单问题， 搭一个识别图像的神经网络
　　•	在AlexNet VGG GoogleLenet上fine-tuning一下（关于什么是微调fine-tuning，请参看本深度学习分类下第54题：https://www.julyedu.com/question/big/kp_id/26/ques_id/2137）
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393041_799.jpg'/>
 
步骤2:
　　•	在上述神经网络的尾部展开（也就说CNN前面保持不变，我们对CNN的结尾处作出改进：加了两个头：“分类头”和“回归头”）
　　•	成为classification + regression模式
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393046_527.png'/>

步骤3:
　　•	Regression那个部分用欧氏距离损失
　　•	使用SGD训练

步骤4:
　　•	预测阶段把2个头部拼上
　　•	完成不同的功能

这里需要进行两次fine-tuning
第一次在ALexNet上做，第二次将头部改成regression head，前面不变，做一次fine-tuning

Regression的部分加在哪？

有两种处理方法：
　　•	加在最后一个卷积层后面（如VGG）
　　•	加在最后一个全连接层后面（如R-CNN）

regression太难做了，应想方设法转换为classification问题。
regression的训练参数收敛的时间要长得多，所以上面的网络采取了用classification的网络来计算出网络共同部分的连接权值。

思路二：取图像窗口
　　•	还是刚才的classification + regression思路
　　•	咱们取不同的大小的“框”
　　•	让框出现在不同的位置，得出这个框的判定得分
　　•	取得分最高的那个框

左上角的黑框：得分0.5
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393142_364.jpg'/>

右上角的黑框：得分0.75
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393149_762.jpg'/>

左下角的黑框：得分0.6
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393156_659.jpg'/>

右下角的黑框：得分0.8
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393161_204.jpg'/>

根据得分的高低，我们选择了右下角的黑框作为目标位置的预测。
注：有的时候也会选择得分最高的两个框，然后取两框的交集作为最终的位置预测。

疑惑：框要取多大？
取不同的框，依次从左上角扫到右下角。非常粗暴啊。

总结一下思路：
对一张图片，用各种大小的框（遍历整张图片）将图片截取出来，输入到CNN，然后CNN会输出这个框的得分（classification）以及这个框图片对应的x,y,h,w（regression）。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393174_730.jpg'/>

这方法实在太耗时间了，做个优化。
原来网络是这样的：
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393184_685.jpg'/>

优化成这样：把全连接层改为卷积层，这样可以提提速。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393192_919.jpg'/>


2.2 物体检测（Object Detection）
当图像有很多物体怎么办的？难度可是一下暴增啊。

那任务就变成了：多物体识别+定位多个物体
那把这个任务看做分类问题？
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393210_554.jpg'/>

看成分类问题有何不妥？
　　•	你需要找很多位置， 给很多个不同大小的框
　　•	你还需要对框内的图像分类
　　•	当然， 如果你的GPU很强大， 恩， 那加油做吧…

所以，传统目标检测的主要问题是：
1）基于滑动窗口的区域选择策略没有针对性，时间复杂度高，窗口冗余
2）手工设计的特征对于多样性的变化没有很好的鲁棒性

看做classification， 有没有办法优化下？我可不想试那么多框那么多位置啊！

三、候选区域/窗 + 深度学习分类
3.1 R-CNN横空出世
有人想到一个好方法：预先找出图中目标可能出现的位置，即候选区域（Region Proposal）。利用图像中的纹理、边缘、颜色等信息，可以保证在选取较少窗口(几千甚至几百）的情况下保持较高的召回率（Recall）。

所以，问题就转变成找出可能含有物体的区域/框（也就是候选区域/框，比如选2000个候选框），这些框之间是可以互相重叠互相包含的，这样我们就可以避免暴力枚举所有框了。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393217_390.jpg'/>

大牛们发明好多选定候选框Region Proposal的方法，比如Selective Search和EdgeBoxes。那提取候选框用到的算法“选择性搜索”到底怎么选出这些候选框的呢？具体可以看一下PAMI2015的“What makes for effective detection proposals？”

以下是各种选定候选框的方法的性能对比。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393227_723.jpg'/>

有了候选区域，剩下的工作实际就是对候选区域进行图像分类的工作（特征提取+分类）。

对于图像分类，不得不提的是2012年ImageNet大规模视觉识别挑战赛（ILSVRC）上，机器学习泰斗Geoffrey Hinton教授带领学生Krizhevsky使用卷积神经网络将ILSVRC分类任务的Top-5 error降低到了15.3%，而使用传统方法的第二名top-5 error高达 26.2%。此后，卷积神经网络CNN占据了图像分类任务的绝对统治地位。

2014年，RBG（Ross B. Girshick）使用Region Proposal + CNN代替传统目标检测使用的滑动窗口+手工设计特征，设计了R-CNN框架，使得目标检测取得巨大突破，并开启了基于深度学习目标检测的热潮。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393238_127.png'/>

R-CNN的简要步骤如下
(1) 输入测试图像
(2) 利用选择性搜索Selective Search算法在图像中从下到上提取2000个左右的可能包含物体的候选区域Region Proposal
(3) 因为取出的区域大小各自不同，所以需要将每个Region Proposal缩放（warp）成统一的227x227的大小并输入到CNN，将CNN的fc7层的输出作为特征
(4) 将每个Region Proposal提取到的CNN特征输入到SVM进行分类

具体步骤则如下
步骤一：训练（或者下载）一个分类模型（比如AlexNet）
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393249_806.jpg'/>

步骤二：对该模型做fine-tuning
　　•	将分类数从1000改为21，比如20个物体类别 + 1个背景
　　•	去掉最后一个全连接层
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393259_895.png'/>

步骤三：特征提取
　　•	提取图像的所有候选框（选择性搜索Selective Search）
　　•	对于每一个区域：修正区域大小以适合CNN的输入，做一次前向运算，将第五个池化层的输出（就是对候选框提取到的特征）存到硬盘
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393264_688.png'/>

步骤四：训练一个SVM分类器（二分类）来判断这个候选框里物体的类别
每个类别对应一个SVM，判断是不是属于这个类别，是就是positive，反之nagative。

比如下图，就是狗分类的SVM
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393277_461.png'/>

步骤五：使用回归器精细修正候选框位置：对于每一个类，训练一个线性回归模型去判定这个框是否框得完美。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393285_829.png'/>

细心的同学可能看出来了问题，R-CNN虽然不再像传统方法那样穷举，但R-CNN流程的第一步中对原始图片通过Selective Search提取的候选框region proposal多达2000个左右，而这2000个候选框每个框都需要进行CNN提特征+SVM分类，计算量很大，导致R-CNN检测速度很慢，一张图都需要47s。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525103713_502.png'/>

有没有方法提速呢？答案是有的，这2000个region proposal不都是图像的一部分吗，那么我们完全可以对图像提一次卷积层特征，然后只需要将region proposal在原图的位置映射到卷积层特征图上，这样对于一张图像我们只需要提一次卷积层特征，然后将每个region proposal的卷积层特征输入到全连接层做后续操作。

但现在的问题是每个region proposal的尺度不一样，而全连接层输入必须是固定的长度，所以直接这样输入全连接层肯定是不行的。SPP Net恰好可以解决这个问题。

3.2 SPP Net
SPP：Spatial Pyramid Pooling（空间金字塔池化）

SPP-Net是出自2015年发表在IEEE上的论文-《Spatial Pyramid Pooling in Deep ConvolutionalNetworks for Visual Recognition》。

众所周知，CNN一般都含有卷积部分和全连接部分，其中，卷积层不需要固定尺寸的图像，而全连接层是需要固定大小的输入。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525243658_939.png'/>

所以当全连接层面对各种尺寸的输入数据时，就需要对输入数据进行crop（crop就是从一个大图扣出网络输入大小的patch，比如227×227），或warp（把一个边界框bounding box的内容resize成227×227）等一系列操作以统一图片的尺寸大小，比如224*224（ImageNet）、32*32(LenNet)、96*96等。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525249316_603.png'/>

所以才如你在上文中看到的，在R-CNN中，“因为取出的区域大小各自不同，所以需要将每个Region Proposal缩放（warp）成统一的227x227的大小并输入到CNN”。

但warp/crop这种预处理，导致的问题要么被拉伸变形、要么物体不全，限制了识别精确度。没太明白？说句人话就是，一张16:9比例的图片你硬是要Resize成1:1的图片，你说图片失真不？

SPP Net的作者Kaiming He等人逆向思考，既然由于全连接FC层的存在，普通的CNN需要通过固定输入图片的大小来使得全连接层的输入固定。那借鉴卷积层可以适应任何尺寸，为何不能在卷积层的最后加入某种结构，使得后面全连接层得到的输入变成固定的呢？

这个“化腐朽为神奇”的结构就是spatial pyramid pooling layer。

下图便是R-CNN和SPP Net检测流程的比较：

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525249330_874.png'/>

它的特点有两个:

1.结合空间金字塔方法实现CNNs的多尺度输入。

SPP Net的第一个贡献就是在最后一个卷积层后，接入了金字塔池化层，保证传到下一层全连接层的输入固定。

换句话说，在普通的CNN机构中，输入图像的尺寸往往是固定的（比如224*224像素），输出则是一个固定维数的向量。SPP Net在普通的CNN结构中加入了ROI池化层（ROI Pooling），使得网络的输入图像可以是任意尺寸的，输出则不变，同样是一个固定维数的向量。

简言之，CNN原本只能固定输入、固定输出，CNN加上SSP之后，便能任意输入、固定输出。神奇吧？

ROI池化层一般跟在卷积层后面，此时网络的输入可以是任意尺度的，在SPP layer中每一个pooling的filter会根据输入调整大小，而SPP的输出则是固定维数的向量，然后给到全连接FC层。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393299_167.jpg'/>

2.只对原图提取一次卷积特征
在R-CNN中，每个候选框先resize到统一大小，然后分别作为CNN的输入，这样是很低效的。
而SPP Net根据这个缺点做了优化：只对原图进行一次卷积计算，便得到整张图的卷积特征feature map，然后找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层，完成特征提取工作。

如此这般，R-CNN要对每个区域计算卷积，而SPPNet只需要计算一次卷积，从而节省了大量的计算时间，比R-CNN有一百倍左右的提速。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393307_311.jpg'/>


3.3 Fast R-CNN
SPP Net真是个好方法，R-CNN的进阶版Fast R-CNN就是在R-CNN的基础上采纳了SPP Net方法，对R-CNN作了改进，使得性能进一步提高。

R-CNN与Fast R-CNN的区别有哪些呢？
先说R-CNN的缺点：即使使用了Selective Search等预处理步骤来提取潜在的边界框bounding box作为输入，但是R-CNN仍会有严重的速度瓶颈，原因也很明显，就是计算机对所有region进行特征提取时会有重复计算，Fast-RCNN正是为了解决这个问题诞生的。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393313_544.png'/>

与R-CNN框架图对比，可以发现主要有两处不同：一是最后一个卷积层后加了一个ROI pooling layer，二是损失函数使用了多任务损失函数(multi-task loss)，将边框回归Bounding Box Regression直接加入到CNN网络中训练（关于什么是边框回归，请参看本深度学习分类下第56题：https://www.julyedu.com/question/big/kp_id/26/ques_id/2139）。

(1) ROI pooling layer实际上是SPP-NET的一个精简版，SPP-NET对每个proposal使用了不同大小的金字塔映射，而ROI pooling layer只需要下采样到一个7x7的特征图。对于VGG16网络conv5_3有512个特征图，这样所有region proposal对应了一个7*7*512维度的特征向量作为全连接层的输入。

换言之，这个网络层可以把不同大小的输入映射到一个固定尺度的特征向量，而我们知道，conv、pooling、relu等操作都不需要固定size的输入，因此，在原始图片上执行这些操作后，虽然输入图片size不同导致得到的feature map尺寸也不同，不能直接接到一个全连接层进行分类，但是可以加入这个神奇的ROI Pooling层，对每个region都提取一个固定维度的特征表示，再通过正常的softmax进行类型识别。

(2) R-CNN训练过程分为了三个阶段，而Fast R-CNN直接使用softmax替代SVM分类，同时利用多任务损失函数边框回归也加入到了网络中，这样整个的训练过程是端到端的(除去region proposal提取阶段)。

也就是说，之前R-CNN的处理流程是先提proposal，然后CNN提取特征，之后用SVM分类器，最后再做box regression，而在Fast R-CNN中，作者巧妙的把box regression放进了神经网络内部，与region分类和并成为了一个multi-task模型，实际实验也证明，这两个任务能够共享卷积特征，并相互促进。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525104022_285.png'/>
所以，Fast-RCNN很重要的一个贡献是成功的让人们看到了Region Proposal + CNN这一框架实时检测的希望，原来多类检测真的可以在保证准确率的同时提升处理速度，也为后来的Faster R-CNN做下了铺垫。

画一画重点：
R-CNN有一些相当大的缺点（把这些缺点都改掉了，就成了Fast R-CNN）。
大缺点：由于每一个候选框都要独自经过CNN，这使得花费的时间非常多。
解决：共享卷积层，现在不是每一个候选框都当做输入进入CNN了，而是输入一张完整的图片，在第五个卷积层再得到每个候选框的特征

原来的方法：许多候选框（比如两千个）-->CNN-->得到每个候选框的特征-->分类+回归
现在的方法：一张完整图片-->CNN-->得到每张候选框的特征-->分类+回归

所以容易看见，Fast R-CNN相对于R-CNN的提速原因就在于：不过不像R-CNN把每个候选区域给深度网络提特征，而是整张图提一次特征，再把候选框映射到conv5上，而SPP只需要计算一次特征，剩下的只需要在conv5层上操作就可以了。

在性能上提升也是相当明显的：
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393334_157.png'/>

3.4 Faster R-CNN
Fast R-CNN存在的问题：存在瓶颈：选择性搜索，找出所有的候选框，这个也非常耗时。那我们能不能找出一个更加高效的方法来求出这些候选框呢？

解决：加入一个提取边缘的神经网络，也就说找到候选框的工作也交给神经网络来做了。

所以，rgbd在Fast R-CNN中引入Region Proposal Network(RPN)替代Selective Search，同时引入anchor box应对目标形状的变化问题（anchor就是位置和大小固定的box，可以理解成事先设置好的固定的proposal）。

具体做法：
　　•	将RPN放在最后一个卷积层的后面
　　•	RPN直接训练得到候选区域
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393348_780.png'/>

RPN简介：
　　•	在feature map上滑动窗口
　　•	建一个神经网络用于物体分类+框位置的回归
　　•	滑动窗口的位置提供了物体的大体位置信息
　　•	框的回归提供了框更精确的位置
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393355_598.png'/>

一种网络，四个损失函数;
　　•	RPN calssification(anchor good.bad)
　　•	RPN regression(anchor->propoasal)
　　•	Fast R-CNN classification(over classes)
　　•	Fast R-CNN regression(proposal ->box)
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393380_922.png'/>

速度对比
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1517393385_752.png'/>

Faster R-CNN的主要贡献就是设计了提取候选区域的网络RPN，代替了费时的选择性搜索Selective Search，使得检测速度大幅提高。

最后总结一下各大算法的步骤：
RCNN
1.在图像中确定约1000-2000个候选框 (使用选择性搜索Selective Search)
2.每个候选框内图像块缩放至相同大小，并输入到CNN内进行特征提取 
3.对候选框中提取出的特征，使用分类器判别是否属于一个特定类 
4.对于属于某一类别的候选框，用回归器进一步调整其位置

Fast R-CNN
1.在图像中确定约1000-2000个候选框 (使用选择性搜索)
2.对整张图片输进CNN，得到feature map
3.找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层
4.对候选框中提取出的特征，使用分类器判别是否属于一个特定类 
5.对于属于某一类别的候选框，用回归器进一步调整其位置

Faster R-CNN
1.对整张图片输进CNN，得到feature map
2.卷积特征输入到RPN，得到候选框的特征信息
3.对候选框中提取出的特征，使用分类器判别是否属于一个特定类 
4.对于属于某一类别的候选框，用回归器进一步调整其位置
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525664484_495.jpg'/>

简言之，即如本文开头所列
R-CNN（Selective Search + CNN + SVM）
SPP-net（ROI Pooling）
Fast R-CNN（Selective Search + CNN + ROI）
Faster R-CNN（RPN + CNN + ROI）

总的来说，从R-CNN, SPP-NET, Fast R-CNN, Faster R-CNN一路走来，基于深度学习目标检测的流程变得越来越精简，精度越来越高，速度也越来越快。可以说基于region proposal的R-CNN系列目标检测方法是当前目标检测技术领域最主要的一个分支。

四、基于深度学习的回归方法
4.1 YOLO (CVPR2016, oral)
(You Only Look Once: Unified, Real-Time Object Detection)

Faster R-CNN的方法目前是主流的目标检测方法，但是速度上并不能满足实时的要求。YOLO一类的方法慢慢显现出其重要性，这类方法使用了回归的思想，利用整张图作为网络的输入，直接在图像的多个位置上回归出这个位置的目标边框，以及目标所属的类别。

我们直接看上面YOLO的目标检测的流程图：

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525171091_647.jpg'/>

(1) 给个一个输入图像，首先将图像划分成7*7的网格
(2) 对于每个网格，我们都预测2个边框（包括每个边框是目标的置信度以及每个边框区域在多个类别上的概率）
(3) 根据上一步可以预测出7*7*2个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后NMS去除冗余窗口即可（关于什么是非极大值抑制NMS，请参看本深度学习分类下第58题：https://www.julyedu.com/question/big/kp_id/26/ques_id/2141）。

可以看到整个过程非常简单，不再需要中间的region proposal找目标，直接回归便完成了位置和类别的判定。
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525171142_149.jpg'/>

小结：YOLO将目标检测任务转换成一个回归问题，大大加快了检测的速度，使得YOLO可以每秒处理45张图像。而且由于每个网络预测目标窗口时使用的是全图信息，使得false positive比例大幅降低（充分的上下文信息）。

但是YOLO也存在问题：没有了Region Proposal机制，只使用7*7的网格回归会使得目标不能非常精准的定位，这也导致了YOLO的检测精度并不是很高。

4.2 SSD
(SSD: Single Shot MultiBox Detector)

上面分析了YOLO存在的问题，使用整图特征在7*7的粗糙网格内回归对目标的定位并不是很精准。那是不是可以结合region proposal的思想实现精准一些的定位？SSD结合YOLO的回归思想以及Faster R-CNN的anchor机制做到了这点。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525171268_230.jpg'/>

上图是SSD的一个框架图，首先SSD获取目标位置和类别的方法跟YOLO一样，都是使用回归，但是YOLO预测某个位置使用的是全图的特征，SSD预测某个位置使用的是这个位置周围的特征（感觉更合理一些）。

那么如何建立某个位置和其特征的对应关系呢？可能你已经想到了，使用Faster R-CNN的anchor机制。如SSD的框架图所示，假如某一层特征图(图b)大小是8*8，那么就使用3*3的滑窗提取每个位置的特征，然后这个特征回归得到目标的坐标信息和类别信息(图c)。

不同于Faster R-CNN，这个anchor是在多个feature map上，这样可以利用多层的特征并且自然的达到多尺度（不同层的feature map 3*3滑窗感受野不同）。

小结：SSD结合了YOLO中的回归思想和Faster R-CNN中的anchor机制，使用全图各个位置的多尺度区域特征进行回归，既保持了YOLO速度快的特性，也保证了窗口预测的跟Faster R-CNN一样比较精准。SSD在VOC2007上mAP可以达到72.1%，速度在GPU上达到58帧每秒。

主要参考及扩展阅读
1 https://www.cnblogs.com/skyfsm/p/6806246.html，by @Madcola
2 https://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&mid=502841131&idx=1&sn=bb3e8e6aeee2ee1f4d3f22459062b814#rd
3 https://zhuanlan.zhihu.com/p/27546796
4 https://blog.csdn.net/v1_vivian/article/details/73275259
5 https://blog.csdn.net/tinyzhao/article/details/53717136
6 Spatial Pyramid Pooling in Deep Convolutional
Networks for Visual Recognition，by Kaiming He等人
7 https://zhuanlan.zhihu.com/p/24774302
8 知乎专栏作者何之源新书《21个项目玩转深度学习——基于TensorFlow的实践详解》
9 YOLO，https://blog.csdn.net/u011534057/article/details/51244354，https://zhuanlan.zhihu.com/p/24916786

后记
咱公司七月在线开设的深度学习等一系列课程经常会讲目标检测，包括R-CNN、Fast R-CNN、Faster R-CNN，但一直没有比较好的机会深入（但当你对目标检测有个基本的了解之后，再看这些课程你会收益很大）。但目标检测这个领域实在是太火了，经常会看到一些写的不错的通俗易懂的资料，加之之前在京东上掏了一本书看了看，就这样耳濡目染中，还是开始研究了。

今年五一，从保定回京，怕高速路上堵 没坐大巴，高铁又没抢上，只好选择哐当哐当好几年没坐过的绿皮车，关键还不断晚点。在车站，用手机做个热点，修改题库，顺便终于搞清R-CNN、fast R-CNN、faster R-CNN的核心区别。有心中热爱 何惧任何啥。

为纪念这心中热爱，故成此文。
七月在线July、二零一八年五月三日
## 2.请简单解释下目标检测中的这个IOU评价函数（intersection-over-union）
在目标检测的评价体系中，有一个参数叫做 IoU ，简单来讲就是模型产生的目标窗口和原来标记窗口的交叠率。

具体我们可以简单的理解为：即检测结果DetectionResult与真实值Ground Truth的交集比上它们的并集，即为检测的准确率 IoU :

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525494665_290.png'/>

举个例子，下面是一张原图

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525495290_937.png'/>

然后我们对其做下目标检测，其DR = DetectionResult，GT = GroundTruth。

<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525494780_911.png'/>

黄色边框框起来的是：

DR⋂GT

绿色框框起来的是：
DR⋃GT

不难看出，最理想的情况就是DR与GT完全重合，即IoU = 1。

本题解析来源：https://blog.csdn.net/Eddy_zheng/article/details/52126641
## 3.计算图片相似度的方法有哪些?
直方图距离
平均哈希算法
感知哈希算法
差异哈希算法
## 4.KNN与K-means区别？
有监督和无监督学习
KNN是寻找最相近的K个点,中心点是其本身,已知,求解周围点的标签众数; K-means是寻找K个聚类中心,聚类中心未知
## 5.K-means选择初始点的方法有哪些,优缺点是什么?(列出两种以上)
随机选取初始点
选择批次距离尽可能远的K个点
选用层次聚类或者Canopy算法进行初始聚类，然后利用这些类簇的中心点作为KMeans算法初始类簇中心点
## 6.像素值的读写方式有哪些？(列出两种以上)
at方式循环遍历
Mat迭代器循环遍历
for循环读取
## 7.图像边缘检测的原理？
图像的边缘是指其周围像素灰度急剧变化的那些像素的集合，它是图像最基本的特征，而图像的边缘检测即先检测图像的边缘点，再按照某种策略将边缘点连接成轮廓，从而构成分割区域
## 8.图像中的角点(Harris角点)是什么？为什么用角点作为特征点？
Harris角点:在任意两个相互垂直的方向上，都有较大变化的点。
角点在保留图像图形重要特征的同时,可以有效地减少信息的数据量,使其信息的含量很高,有效地提高了计算的速度,有利于图像的可靠匹配,使得实时处理成为可能。
## 9.简述图像特征的SIFT描述
SIFT在不同的尺度空间上查找关键点(特征点)，并计算出关键点的方向。SIFT所查找到的关键点是一些十分突出，不会因光照，仿射变换和噪音等因素而变化的点，如角点、边缘点、暗区的亮点及亮区的暗点等。
## 10.简述RANSAC离群点操作方法
RANSAC算法是采用迭代的算法从一组包含离群点的被观测数据中估计一个数学模型的参数。RANSAC算法只会产生一个在一定概率下合理的结果，而更多次的迭代会使得这一概率增加。
## 11.简述线性分类器的原理(要求对权重矩阵进行剖析)
线性分类器就是将每个被分类样本与权重矩阵进行对比,找到相似度最大的权重行,给出响应的标签.
## 12.简述Cross-Entropy Loss(交叉熵损失函数)与Hinge Loss(折页损失函数)
softmax(柔性最大值)函数，一般在神经网络中， softmax可以作为分类任务的输出层。其实可以认为softmax输出的是几个类别选择的概率，比如我有一个分类任务，要分为三个类，softmax函数可以根据它们相对的大小，输出三个类别选取的概率，并且概率和为1。

Hinge Loss 是机器学习领域中的一种损失函数，可用于“最大间隔(max-margin)”分类，其最著名的应用是作为SVM的目标函数。
## 13.简述正则化与奥卡姆剃刀原则
正则化是为了防止过拟合

奥卡姆剃刀是一种思维方式，可用于指导我们的工作，比如我们可以用A和B达到同样的效果，但B更简单，于是我们选择B。同时，也有人说可能因为能力不足，于是我们选择更简单的方式来处理问题，这也是剃刀的原则的一种应用吧。

举个贴切的例子，做决策树分析的时候，采用9个属性的预测性能和5个属性的预测性能是相似的，那么我们就会选择5个属性来预测。
## 14.写出神经网络中常见的激励函数(至少三个)
Sigmoid
Tanh
ReLU
Leaky RelU
## 15.典型的卷积神经网络有哪些层?
INPUT(输入层)
CONV(卷积层)
RELU(非线性层)
POOL(池化层)
FC(全连接层)
## 16.计算题(提示:参数共享)
Example1: 200*200 image(灰度图), 40K hidden units 有多少个参数?
Example2: 200*200 image(灰度图), 40K hidden units, Filter size: 10*10 有多少个参数?
Example3: 200*200 image(灰度图), 100 Filter, Filter size:10*10 有多少参数?
Example4: 图像尺寸为[32*32*3], 卷积窗口大小为5*5, 卷积模板个数为1, 有多少参数?
Example5: 图像尺寸为[16*16*20], 卷积窗口大小为3*3, 卷积模板个数为10, 有多少参数?
Example1: 40000 * 40000 = 16 0000 0000个
Example2: 40000 * 10 * 10 = 4 00 0000个
Example3: 100 * 10 * 10 = 1 00 00个
Example4: 5 * 5 * 3 = 75个
Example5: 3 * 3 * 20 * 10 = 1800个
## 17.图像尺寸为 7*7, 卷积窗口大小为3*3, 步长为3, 是否能输出图像?如果能,输出图像大小为多少?如果不能,说明原因?
不能,因为 (7-3)/3 + 1 不为整数!滑动窗口会丢失
## 18.计算题: 输入图像尺寸为 32*32*3 , 卷积模板尺寸为 5*5 , 步长stride为1, 卷积神经元个数为 5
问题1: 输出图像尺寸为?
问题2: 每个卷积层神经元的参数个数为?
(32 - 5)/1 +1 = 28,结果就是: 28 * 28 * 5
每个神经元的参数个数为 : 5 * 5 * 3
## 19.如果最后一个卷积层和第一个全连接层参数量太大怎么办?
加入全局池化层
## 20.为什么说神经网络是端到端的网络?
特征提取包含在神经网络内部
## 21.当参数量 >> 样本量时候, 神经网络是如何预防过拟合?
增加数据量
加入全局池化减少参数量
减少卷积层数
迁移学习
## 22.什么是感受野？
某一层特征图中的一个cell,对应到原始输入的响应的大小区域。
## 23.简述你对CBIR(Content-based Image Retrieval基于内容的图像检索)的理解
通过对比特征点\\u7279征值的相似度,判断两个图片是否相近
## 24.工业界中遇到上亿的图像检索任务,如何提高图像对比效率?(假设原图像输出的特征维度为2048维)
通过哈希的索引技术,将原图的2048维度映射到128维度的0/1值中,再进行特征维度对比
## 25.什么是计算机视觉单词模型？
将图片中的特征提取出来，汇总到一个大的容器中，然后取其中核心点，组成一个模型
## 26.简述什么是Local Feature(局部特征算子)？
视觉单词就是Local Feature

由位置信息x,y坐标组成.
利用描述符来进行描述,量化.
## 27.KD-Tree相比KNN来进行快速图像特征比对的好处在哪里?
极大的节约了时间成本．
点线距离如果 >　最小点，无需回溯上一层 　　　　　
如果<,则再上一层寻找
## 28.Locality Sensitive Hashing（基于位置敏感的哈希值）相比KD-Tree优点？（高纬度）
当维度更高时候（5000维以上），利用LSH能够更快进行查询,sklearn有API可以调用
## 29.如何将相关图像投影到bucket中？
利用128条直线对图像集进行切分，每个图像都有128维的（0,1）向量，就可以将相关性强的图像分类到一个bucket中
## 30.简述encode和decode思想

将一个input信息编码到一个压缩空间中
将一个压缩空间向量解码到一个原始空间中
## 31.输入图片尺寸不匹配CNN网络input时候的解决方式？（三种以上）
two-fixed
one-fixed
free（去掉FC层，加入全局池化层）FCN（全卷积神经网络）
## 32.FCN与CNN最大的区别？
卷积层不再与FC层相连,而是加入一个全局池化层
## 33.遇到class-imbalanced data（数据类目不平衡）问题怎么办？
设置不同类的权重weighted loss
batch-wise balanced sampling(循环平衡采样)　
## 34.简述hard negatives模式采样
对难以区分的样本进行采样，而非随机采样
## 35.简述孪生随机网络（Siamese Network）
ｘ1,x2如果一致，输出１；否则输出０
## 36.物体检测方法列举

    Deformable Parts Model
    RCNN
    Fast-RCNN
    Faster-RCNN
    RFCN
    Mask-RCNN

## 37.计算机视觉中，有哪几种基本任务？

    图像分类
    图像定位
    物体检测
    物体分割

## 38.DPM（Deformable Parts Model）算法流程

    将原图与已经准备好的每个类别的“模板”做卷积操作，生成一中类似热力图（hot map）的图像，将不同尺度上的图合成一张，图中较量点就是与最相关“模板”相似的点。

    拓展：

    * SGD(stochastic gradient descent)到training里
    * NMS(non-maximum suppression)对后期testing的处理非常重要
    * Data mining hard examples这些概念至今仍在使用
## 39.什么是NMS（Non-maximum suppression 非极大值抑制）?

    NMS是一种Post-Procession（后处理）方式，跟算法无关的方式。
    NMS应用在所有物体检测的方法里。
    NMS物体检测的指标里，不允许出现多个重复的检测。
    NMS把所有检测结果按照分值(conf. score)从高到底排序,保留最高分数的 box,删除其余值。

## 40.列举出常见的损失函数(三个以上)?

    L1 Loss
    MES Loss
    Cross Entropy Loss (物体检测常用)
    NLL Loss
    Poisson NLL Loss
    KLDiv Loss
    BCE Loss
    BCE With Logits Loss
    Margin Ranking Loss
    Hinge Embedding Loss
    Multi Label Margin Loss
    Smooth L1 Loss
    Soft Margin Loss
    Multi Label Soft Margin Loss
    Cosine Embedding Loss
    Multi Margin Loss
    Triplet Margin Loss

## 41.常见的R-CNN'家族'成员有哪些?

    RCNN
    Fast-RCNN
    Faster-RCNN(这个工作是大部分流行方法的基石)
    FPN
    RetinaNet
    Mask-RCNN

## 42.做过目标检测项目么？比如Mask R-CNN和Python做一个抢车位神器
来源：Medium
编译：大数据文摘编译组，李雷、笪洁琼、Aileen、ZoeY

每逢春节过年，就要开始走亲访友了。这时候的商场、饭馆也都是“人声鼎沸”，毕竟走亲戚串门必不可少要带点礼品、聚餐喝茶。

热闹归热闹，这个时候最难的问题可能就是怎样从小区、商场、菜市场的人山人海里准确定位，找到一个“车位”。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728129239871909.png'/>
别慌！

一位名叫Adam Geitgey的软件工程师、AI软件工程博主也被“停车难”的问题困扰已久。为了让自己能给迅速定位空车位，他用实例分割模型Mask R-CNN和python写了一个抢占停车位的小程序。

以下是作者以第一人称给出的教程，enjoy。

一、如何找停车位
我住在一个大都市，但就像大多数城市一样，在这里很难找到停车位。停车场总是停得满满的，即使你自己有私人车位，朋友来访的时候也很麻烦，因为他们找不到停车位。

我的解决方法是：
用摄像头对着窗外拍摄，并利用深度学习算法让我的电脑在发现新的停车位时给我发短信。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728129938186278.png'/>
这可能听起来相当复杂，但是用深度学习来构建这个应用，实际上非常快速和简单。有各种现有的实用工具 - 我们只需找到这些工具并且将它们组合在一起。

现在，让我们花几分钟时间用Python和深度学习建立一个高精度的停车位通知系统吧！

分解问题
当我们想要通过机器学习解决一个复杂的问题时，第一步是将问题分解为简单任务的列表。然后，对于每个简单任务，我们可以从机器学习工具箱中找寻不同的工具来解决。

通过将这些简单问题的解决方案串起来形成框架（例如下面的思维导图），我们将实现一个可以执行复杂操作的系统。

以下就是我如何将检测公共停车位的问题分解并形成流程：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641557281385138852.png'/>
机器学习模型流程的输入是来自对着窗外的普通网络摄像头的视频：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728139198764729.png'/>
我们将每一帧视频送入模型里，一次一帧。

流程的第一步是检测视频帧中所有可能的停车位。显然，我们需要知道图像的哪些部分是停车位才能检测到哪些停车位是空的。

第二步是识别每帧视频中所有的汽车，这样我们可以跟踪每辆车在帧与帧之间的位移。

第三步是确定哪些停车位上目前有汽车，哪些没有。这需要综合第一步和第二步的结果。

最后一步是在停车位空出来的时候发送通知。这是基于视频帧之间的汽车位置的变化。

我们可以使用各种技术以多种不同方式完成这些步骤。构建流程的方法不是唯一的，不同的方法没有对错，但有不同的优点和缺点。现在让我们来看看每一步吧！

二、检测图像中的停车位
以下是相机拍到的图像：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728139948197422.png'/>
我们需要能够扫描该图像并返回可以停车的区域列表，如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728140768990441.png'/>
街区上可用的停车位

有一种偷懒的方法是手动将每个停车位的位置编入到程序中，而不是自动检测停车位。但是如果我们移动相机或想要检测不同街道上的停车位时，我们必须再次手动输入停车位的位置。这太不爽了，所以让我们找到一种自动检测停车位的方法。

一个想法是寻找停车计费表并假设每个计费表旁边都有一个停车位：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728141537930503.png'/>
在图片中检测的停车计费表

但这种方法存在一定的问题。首先，并非每个停车位都有停车咪表 – 实际上，我们最感兴趣的是找到免费停车位！其次，停车咪表的位置并不能确切地告诉我们停车位的具体位置，只能让我们离车位更接近一点。
另一个想法是建立一个物体检测模型，寻找在道路上绘制的停车位斜线标记，如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728142371724813.png'/>
留意那些微小的黄色标记，这些是在道路上绘制每个停车位的边界。

但这种做法也很痛苦。首先，我所在城市的停车位斜线标记非常小，从远处很难看清，电脑也难以察觉。第二，街道上到处都是各种不相关的线条和标记，所以要区分哪些线条是停车位，哪些线条是车道分隔线或人行横道是很困难的。

每当遇到一个看似困难的问题时，请先花几分钟时间看看是否能够采用不同的方式来避免某些技术难点并解决问题。到底什么是停车位呢？停车位就是车辆停留很长一段时间的地方。因此也许我们根本不需要检测停车位。为什么我们不能只检测那些长时间不动的车并假设它们停在停车位上？

换句话说，真正的停车位只是容纳了非移动中的车辆的区域：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415572814332979899.png'/>
这里每辆车的边框实际上都是一个停车位！如果我们能够检测到静止的汽车，就不需要真的去检测停车位。

因此，如果我们能够检测到汽车并找出哪些汽车在视频的每帧之间没有移动，我们就可以推断停车位的位置。这就变得很容易了！

三、检测图像中的汽车
检测视频每帧中的汽车是一个标准的对象检测问题。我们可以使用许多种机器学习方法来检测图像中的对象。以下是一些从过去到现在最常见的对象检测算法：
    a)训练一个HOG（梯度方向直方图）物体探测器滑过（扫描）我们的图像以找到所有的汽车。这种比较古老的非深度学习方法运行起来相对较快，但它对于朝向不同方向的汽车不能很好地处理。
    b)训练CNN（卷积神经网络）物体探测器阅览（扫描）我们的图像，直到我们找到所有的汽车。这种方法虽然准确，但效率不高，因为我们必须使用CNN算法多次扫描同一图像才能找到其中的所有汽车。虽然它可以很容易地找到朝向不同方向的汽车，但它需要比基于HOG的物体探测器更多的训练数据。
    c)使用更新的深度学习方法，如Mask R-CNN，快速R-CNN或YOLO，将CNN的准确性与巧妙设计和效率技巧相结合，可以大大加快检测过程。即使有大量训练数据来训练模型，这种方法的速度也相对较快（在GPU上）。

一般来说，我们希望选择最简单的解决方案，以最少的训练数据完成工作，而不是最新、最花哨的算法。但在这种特殊情况下，Mask R-CNN对我们来说是一个比较合理的选择，尽管它相当花哨新潮。

Mask R-CNN架构的设计理念是在不使用滑动窗口方法的情况下以高计算效率的方式检测整幅图像上的对象。换句话说，它运行得相当快。使用最新GPU，我们可以以每秒几帧的速度检测高分辨率视频中的对象。那对于这个项目来说应该没问题。

此外，Mask R-CNN对每个检测到的对象给出了大量信息。大多数对象检测算法仅返回每个对象的边界。但Mask R-CNN不仅会给我们每个对象的位置，还会给我们一个对象轮廓（或概述），如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728144328752002.png'/>
为了训练Mask R-CNN，我们需要大量的包含我们想要检测的对象的图片。我们可以去拍摄汽车照片并检测这些照片中的所有汽车，但这需要几天的工作。幸运的是，汽车是许多人想要检测的常见物体，因此已经有不少汽车图像的公共数据集。

其中一个名为COCO (Common Objects in Context）的流行数据集，其中包含标注了对象轮廓的图像。在此数据集中，有超过12,000张做了标注的汽车图像。以下是COCO数据集中的其中一张：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728145353832914.png'/>
这个数据集非常适合训练Mask R-CNN模型。

等等，还有更好的事！由于太多人使用COCO数据集构建对象检测模型，很多人已经完成并共享了他们的结果。因此，我们可以从预先训练好的模型开始，而无需训练我们自己的模型，这种模型可以即插即用。对于这个项目，我们将使用来自Matterport的大型开源Mask R-CNN实现项目，它自带预先训练的模型。
 
旁注：不要害怕训练一个定制的Mask R-CNN目标探测器！注释数据是很费时，但并不难。

如果我们在摄像头拍摄的图像上运行预先培训过的模型，就会得到如下的结果：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728146364810369.png'/>
在我们的图像上，识别出了COCO数据集中的默认对象-汽车、人、交通灯和一棵树。
 
我们不仅能识别汽车，还能识别交通灯和人。幽默的是，其中一棵树被识别成一个“盆栽植物”。

对于图像中检测到的每一个物体，我们从Mask R-CNN模型中都会得到以下四个数据：
1.检测到的对象类型（以整数形式表示）。经过预先训练的COCO模型知道如何检测80种不同的常见物体，如汽车和卡车。这是80种的常见物体的完整清单https://gist.github.com/ageitgey/b143ee809bf08e4927dd59bace44db0d。
2.目标检测的置信度得分。数值越高，模型就越确定它正确地识别了对象。
3.图像中对象的边界框，以X/Y像素位置表示。
4.位图图层告诉我们边界框中的哪些像素是对象的一部分，哪些不是。通过图层数据，我们还可以计算出对象的轮廓。

下面是使用Matterport’s Mask R-CNN中的预培训模型和OpenCV共同实现汽车边界框检测的Python代码：
当您运行该代码时，会看到图像上每辆被检测到的汽车周围都有一个边框，如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728147347073096.png'/>
被检测到的每辆汽车周围都有一个绿色的边框。

您还可以在控制台中查看每个被检测到的汽车的像素坐标，如下所示：
Cars found in frame of video:
Car:  [492 871 551 961]
Car:  [450 819 509 913]
Car:  [411 774 470 856]

有了这些数据，我们已经成功地在图像中检测到了汽车。可以进行下一步了

四、检测空车位                         
我们知道图像中每辆车的像素位置。通过连续查看多帧视频，我们可以很容易地确定哪些车辆没有移动，并假设这些区域是停车位。但我们如何检测汽车何时离开停车位呢？

主要问题是，我们的图像中汽车的边界框有部分重叠：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728148087469556.png'/>
即使对于不同停车位的汽车，每辆车的边界框也有一点重叠。

因此，如果我们假设每一个边界框中的都代表一个停车位，那么即使停车位是空的，这个边界框也可能有一部分被汽车占据。我们需要一种方法来测量两个对象重叠的程度，以便检查“大部分是空的”的边框。

我们将使用交并比（Intersection Over Union ，IoU）方法。用两个对象重叠的像素数量除以两个对象覆盖的像素总数量，如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728148764563760.png'/>
IoU可以告诉我们汽车边界框与停车位边界框的重叠程度。有了这个指标，我们就可以很容易地确定一辆车是否在停车位。如果IoU测量值很低，比如0.15，这意味着这辆车并没有占用太多的停车位。但是如果测量值很高，比如0.6，这意味着汽车占据了大部分的停车位，那么我们可以确定停车位被占用了。

由于IoU在计算机视觉中是一种常见的度量方法，所以通常您使用的库已经有相关实践。事实上，Matterport Mask R-CNN库中就有这样的函数mrcnn.utils.compute_overlaps()，我们可以直接调用这个函数。

假设在图像中有一个表示停车区域的边界框列表，那么检查被检测到的车辆是否在这些边界框中，就如同添加一行或两行代码一样简单。

结果如下：
[
 [1.         0.07040032 0.         0.]
 [0.07040032 1.         0.07673165 0.]
 [0.         0.         0.02332112 0.]
]

在这个二维数组中，每一行表示一个停车位的边界框。相应的，每列表示该停车位与被检测到的汽车有多少重叠。1.0分意味着汽车完全占据了停车位，而0.02分这样的低分意味着汽车只是接触了停车位边界框，但并没有占据很多区域。

为了找到空置的停车位，我们只需要检查这个数组中的每一行。如果所有的数字都是零或者非常小的数字，就意味着没有东西占据了这个空间，因此它就是空闲的！

但请记住，物体检测并不总是与实时视频完美结合。尽管Mask R-CNN非常准确，但偶尔它会在一帧视频中错过一两辆车。因此，在将停车位标记为空闲之前，我们应该确保它在一段时间内都是空闲的，可能是5或10帧连续视频。这将防止仅仅在一帧视频上出现暂时性的物体检测问题而误导系统将停车位判定为空闲。但当我们看到至少有一个停车位在连续几帧视频图像中都被判定为空闲，我们就可以发送短信了！

五、发送短信
最后一步是当我们注意到一个停车位在连续几帧视频图像中都被判定为空闲时，就发送一条短信提醒。

使用 Twilio从Python发送短消息非常简单。Twilio是一个流行的接口，它可以让您用几行代码从任何编程语言发送短消息。当然，如果您喜欢使用其他的短信服务提供者，也是可以的。我和Twilio没有利害关系。只是第一个就想到了它。

Twilio：
https://www.twilio.com

要使用Twilio，需要注册试用帐户，创建Twilio电话号码并获取您的帐户凭据。然后，您需要安装Twilio Python客户端的库：

pip3 install twilio

安装后，使用下面的代码（需要将关键信息替换成您的账户信息），就可以从Python发送短信了：

from twilio.rest import Client
  
  # Twilio account details
  twilio_account_sid = &#39;Your Twilio SID here&#39;
  twilio_auth_token = &#39;Your Twilio Auth Token here&#39;
  twilio_source_phone_number = &#39;Your Twilio phone number here&#39;
  
  # Create a Twilio client object instance
  client = Client(twilio_account_sid, twilio_auth_token)
  
  # Send an SMS
  message = client.messages.create(
      body="This is my SMS message!",
      from_=twilio_source_phone_number,
      to="Destination phone number here"
  )


直接将代码插入到我们的脚本里，就可以添加发送短信的功能了。但我们需要注意的是，如果一个停车位一直是空闲的，就不需要在每一帧视频都给自己发送短信了。因此，我们需要有一个标志来标记我们是否已经发送了一条短信，并确保在经过一定时间或检测到其他停车位空闲之前，我们不会再发送另一条短信息。

打包组装
我们将流程中的每个步骤写成了一个单独的Python脚本。以下是完整的代码：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728149633520660.png'/>
此部分代码太长，感兴趣的同学可以在下面的网址中找到：
https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnn-and-python-955f2231c400

要运行此代码，您需要首先安装Python 3.6+, Matterport Mask R-CNN和OpenCV。

Matterport Mask R-CNN：
https://github.com/matterport/Mask_RCNN
OpenCV：
https://pypi.org/project/opencv-python/

我有意让代码尽可能简明扼要。例如，它只是假设在第一帧视频中出现的任何汽车都是已停放的汽车。试试修改代码，看看您能不能提高它的可靠性。

不用担心修改此代码就不能适应不同的场景。只需更改模型搜寻的对象ID，就可以将代码完全转换为其他内容。例如，假设您在滑雪场工作。通过一些调整，您可以将此脚本转换为一个自动检测滑雪板从斜坡上跳下的系统，并记录炫酷的滑雪板跳跃轨迹。或者，如果您在一个游戏保护区工作，您可以把这个脚本变成一个用来计数在野外能看到多少斑马的系统。

唯一的限制就是你的想象力。

一起来试试吧!
 
相关报道：
https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnn-and-python-955f2231c400
## 43.如何理解Faster RCNN
目前学术和工业界出现的目标检测算法分成3类：
1. 传统的目标检测算法：Cascade + HOG/DPM + Haar/SVM以及上述方法的诸多改进、优化；

2. 候选区域/框 + 深度学习分类：通过提取候选区域，并对相应区域进行以深度学习方法为主的分类的方案，如：
R-CNN（Selective Search + CNN + SVM）
SPP-net（ROI Pooling）
Fast R-CNN（Selective Search + CNN + ROI）
Faster R-CNN（RPN + CNN + ROI）
R-FCN
等系列方法；

3. 基于深度学习的回归方法：YOLO/SSD/DenseBox 等方法；以及最近出现的结合RNN算法的RRC detection；结合DPM的Deformable CNN等

经过R-CNN和Fast RCNN的积淀，Ross B. Girshick在2016年提出了新的Faster RCNN，在结构上，Faster RCNN已经将特征抽取(feature extraction)，proposal提取，bounding box regression(rect refine)，classification都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421007015280035.jpg'/>
图1 Faster RCNN基本结构（来自原论文）

依作者看来，如图1，Faster RCNN其实可以分为4个主要内容：

①Conv layers。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的

②feature maps。该feature maps被共享用于后续RPN层和全连接层。

③Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于foreground或者background，再利用bounding box regression修正anchors获得精确的proposals。
Roi Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。

④Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。

所以本文以上述4个内容作为切入点介绍Faster R-CNN网络。

下图图2展示了python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421014336794242.jpg'/>
图2 faster_rcnn_test.pt网络结构 （pascal_voc/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt）

可以清晰的看到该网络对于一副任意大小PxQ的图像，首先缩放至固定大小MxN，然后将MxN图像送入网络；
而Conv layers中包含了13个conv层+13个relu层+4个pooling层；RPN网络首先经过3x3卷积，再分别生成foreground anchors与bounding box regression偏移量，然后计算出proposals；
而Roi Pooling层则利用proposals从feature maps中提取proposal feature送入后续全连接和softmax网络作classification（即分类proposal到底是什么object）。

关于R-CNN家族的历史，请参见此文：https://www.julyedu.com/question/big/kp_id/32/ques_id/2103。


一 Conv layers
Conv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图2，Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv layers中：

所有的conv层都是： <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421023867826680.svg'/> ， <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421024489104164.svg'/>， <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421024979956340.svg'/>
所有的pooling层都是： <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421025692099613.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421026266538870.svg'/>， <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542102685635210.svg'/>

为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（ pad=1，即填充一圈0），导致原图变为 (M+2)x(N+2)大小，再做3x3卷积后输出MxN 。正是这种设置，导致Conv layers中的conv层不改变输入和输出矩阵大小。如图3：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421029364916114.jpg'/>
图3 卷积示意图

类似的是，Conv layers中的pooling层kernel_size=2，stride=2。这样每个经过pooling层的MxN矩阵，都会变为(M/2)x(N/2)大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。

那么，一个MxN大小的矩阵经过Conv layers固定变为(M/16)x(N/16)！这样Conv layers生成的featuure map中都可以和原图对应起来。


二 Region Proposal Networks(RPN)
经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如R-CNN使用SS(Selective Search)方法生成检测框。

而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster R-CNN的巨大优势，能极大提升检测框的生成速度。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421034131889529.jpg'/>
图4 RPN网络结构

上图4展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和background（检测目标是foreground），下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。

而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。

2.1 多通道图像卷积基础知识介绍
在介绍RPN前，还要多解释几句基础知识，已经懂的看官老爷跳过就好。

对于单通道图像+单卷积核做卷积，第一章中的图3已经展示了；
对于多通道图像+多卷积核做卷积，计算方式如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421036178738034.jpg'/>
图5 多通道卷积计算方式

如图5，输入有3个通道，同时有2个卷积核。对于每个卷积核，先在输入3个通道分别作卷积，再将3个通道结果加起来得到卷积输出。所以对于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量！
对多通道图像做1x1卷积，其实就是将输入图像于每个通道乘以卷积系数后加在一起，即相当于把原图像中本来各个独立的通道“联通”在了一起。

2.2 anchors
提到RPN网络，就不能不说anchors。所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。直接运行Faster RCNN的作者在其论文中给的demo中的generate_anchors.py可以得到以下输出：

[[ -84.  -40.   99.   55.]
 [-176.  -88.  191.  103.]
 [-360. -184.  375.  199.]
 [ -56.  -56.   71.   71.]
 [-120. -120.  135.  135.]
 [-248. -248.  263.  263.]
 [ -36.  -80.   51.   95.]
 [ -80. -168.   95.  183.]
 [-168. -344.  183.  359.]]

其中每行的4个值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420879293384076.svg'/>表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420880792797017.svg'/>三种，如下图。实际上通过anchors就引入了检测中常用到的多尺度方法。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542088266229831.jpg'/>

注：关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成800x600（即下图中的M=800，N=600）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420887897013780.jpg'/>

再回头来看anchors的大小，anchors中长宽1:2中最大为352x704，长宽2:1中最大736x384，基本是cover了800x600的各个尺度和形状。

那么这9个anchors是做什么的呢？借用Faster RCNN论文中的原图，如下图，遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420890012978963.jpg'/>

解释一下上面这张图的数字。

a）在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions

b）在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息（猜测这样做也许更鲁棒？反正我没测试），同时256-d不变（如上图中的红框）

c)假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分foreground和background，所以每个点由256d feature转化为cls=2k scores；而每个anchor都有(x, y, w, h)对应4个偏移量，所以reg=4k coordinates

d)补充一点，全部anchors拿去训练太多了，训练程序会在合适的anchors中随机选取128个postive anchors+128个negative anchors进行训练

注意，在本文讲解中使用的VGG conv5 num_output=512，所以是512d，其他类似。

其实RPN最终就是在原图尺度上，设置了密密麻麻的候选Anchor。然后用cnn去判断哪些Anchor是里面有目标的foreground anchor，哪些是没目标的backgroud。所以，仅仅是个二分类而已！

那么Anchor一共有多少个？原图800x600，VGG下采样16倍，feature map每个点设置9个Anchor，所以：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420896774070786.svg'/>

其中ceil()表示向上取整，是因为VGG输出的feature map size= 50*38。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155420898095049747.jpg'/>

2.3 softmax判定foreground与background
一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16，H=N/16。在进入reshape与softmax之前，先做了1x1卷积，如图9：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421038657233871.jpg'/>
图9 RPN中判定fg/bg网络结构

该1x1卷积的caffe prototxt定义如下：

layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn/output"
  top: "rpn_cls_score"
  convolution_param {
    num_output: 18   # 2(bg/fg) * 9(anchors)
    kernel_size: 1 pad: 0 stride: 1
  }
}
可以看到其num_output=18，也就是经过该卷积的输出图像为WxHx18大小（注意第二章开头提到的卷积计算方式）。这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是foreground和background，所有这些信息都保存WxHx(9*2)大小的矩阵。为何这样做？后面接softmax分类获得foreground anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在foreground anchors中）。

那么为何要在softmax前后都接一个reshape layer？其实只是为了便于softmax分类，至于具体原因这就要从caffe的实现形式说起了。在caffe基本数据结构blob中以如下形式保存数据：

blob=[batch_size, channel，height，width]
对应至上面的保存bg/fg anchors的矩阵，其在caffe blob中的存储形式为[1, 2x9, H, W]。而在softmax分类时需要进行fg/bg二分类，所以reshape layer会将其变为[1, 2, 9xH, W]大小，即单独“腾空”出来一个维度以便softmax分类，之后再reshape回复原状。贴一段caffe softmax_loss_layer.cpp的reshape函数的解释，非常精辟：

"Number of labels must match number of predictions; "
"e.g., if softmax axis == 1 and prediction shape is (N, C, H, W), "
"label count (number of labels) must be N*H*W, "
"with integer values in {0, 1, ..., C-1}.";

综上所述，RPN网络中利用anchors和softmax初步提取出foreground anchors作为候选区域。

2.4 bounding box regression原理
如图9所示绿色框为飞机的Ground Truth(GT)，红色为提取的foreground anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得foreground anchors和GT更加接近。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542104848988017.jpg'/>
图10

对于窗口一般使用四维向量  (x, y, w, h) 表示，分别表示窗口的中心点坐标和宽高。对于图 11，红色的框A代表原始的Foreground Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G'，即：

给定：anchor <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421052937197877.svg'/>,和 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421054464832961.svg'/>
寻找一种变换F，使得：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421055595429698.svg'/>，其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421057069663415.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421058029605774.jpg'/>
图11

那么经过何种变换F才能从图10中的anchor A变为G'呢？ 比较简单的思路就是:

先做平移
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421062065900717.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421062791843737.svg'/>

再做缩放
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421063397533378.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421063911969984.svg'/>

观察上面4个公式发现，需要学习的是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421069079358060.svg'/>这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当anchors A和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。

接下来的问题就是如何通过线性回归获得<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421070067453287.svg'/>了。线性回归就是给定输入的特征向量X, 学习一组参数W, 使得经过线性回归后的值跟真实值Y非常接近，即Y=WX。对于该问题，输入X是cnn feature map，定义为Φ；同时还有训练传入A与GT之间的变换量，即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421071455390992.svg'/>。输出是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421072182371619.svg'/>四个变换。那么目标函数可以表示为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421072836233581.svg'/>

其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421076093661500.svg'/>是对应anchor的feature map组成的特征向量，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421076822660740.svg'/>是需要学习的参数，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421077552865835.svg'/>是得到的预测值（*表示 x，y，w，h，也就是每一个变换对应一个上述目标函数）。为了让预测值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421078771200222.svg'/>与真实值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421079513852603.svg'/>差距最小，设计损失函数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542108447962919.svg'/>

函数优化目标为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421085373307319.svg'/>

需要说明，只有在GT与需要回归框位置比较接近时，才可近似认为上述线性变换成立。
说完原理，对应于Faster RCNN原文，foreground anchor与ground truth之间的平移量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421086624709449.svg'/>与尺度因子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421087366484514.svg'/>如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421087959428027.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421088571443870.svg'/>

对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421091221974580.svg'/>，即训练目标是：输入 Φ的情况下使网络输出与监督信号尽可能接近。
那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421092084865231.svg'/>，显然即可用来修正Anchor位置了。

2.5 对proposals进行bounding box regression
在了解bounding box regression后，再回头来看RPN网络第二条线路，如图12。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421094031196768.jpg'/>
图12 RPN中的bbox reg

先来看一看上图11中1x1卷积的caffe prototxt定义：

layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn/output"
  top: "rpn_bbox_pred"
  convolution_param {
    num_output: 36   # 4 * 9(anchors)
    kernel_size: 1 pad: 0 stride: 1
  }
}
可以看到其 num_output=36，即经过该卷积输出图像为WxHx36，在caffe blob存储为[1, 4x9, H, W]，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542109609785247.svg'/>
变换量。

2.6 Proposal Layer
Proposal Layer负责综合所有
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421097849884869.svg'/>
变换量和foreground anchors，计算出精准的proposal，送入后续RoI Pooling Layer。还是先来看看Proposal Layer的caffe prototxt定义：
layer {
  name: 'proposal'
  type: 'Python'
  bottom: 'rpn_cls_prob_reshape'
  bottom: 'rpn_bbox_pred'
  bottom: 'im_info'
  top: 'rois'
  python_param {
    module: 'rpn.proposal_layer'
    layer: 'ProposalLayer'
    param_str: "'feat_stride': 16"
  }
}
Proposal Layer有3个输入：fg/bg anchors分类器结果rpn_cls_prob_reshape，对应的bbox reg的
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421100117598083.svg'/>

变换量rpn_bbox_pred，以及im_info；另外还有参数feat_stride=16，这和图4是对应的。
首先解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定MxN，im_info=[M, N, scale_factor]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为WxH=(M/16)x(N/16)大小，其中feature_stride=16则保存了该信息，用于计算anchor偏移量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421102423249417.jpg'/>
图13

Proposal Layer forward（caffe layer的前传函数）按照以下顺序依次处理：

1)生成anchors，利用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421104665475598.svg'/>对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）
2)按照输入的foreground softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的foreground anchors。
3)限定超出图像边界的foreground anchors为图像边界（防止后续roi pooling时proposal超出图像边界）

4)剔除非常小（width<threshold or height<threshold）的foreground anchors
5)进行nonmaximum suppression
6)再次按照nms后的foreground softmax scores由大到小排序fg anchors，提取前post_nms_topN(e.g. 300)结果作为proposal输出。

之后输出proposal=[x1, y1, x2, y2]，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的，这点在后续网络中有用。另外我认为，严格意义上的检测应该到此就结束了，后续部分应该属于识别了~
RPN网络结构就介绍到这里，总结起来就是：
生成anchors -> softmax分类器提取fg anchors -> bbox reg回归fg anchors -> Proposal Layer生成proposals

3 RoI pooling
而RoI Pooling层则负责收集proposal，并计算出proposal feature maps，送入后续网络。从图2中可以看到Rol pooling层有2个输入：

原始的feature maps
RPN输出的proposal boxes（大小各不相同）

3.1 为何需要RoI Pooling
先来看一个问题：对于传统的CNN（如AlexNet，VGG），当网络训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的vector or matrix。如果输入图像大小不定，这个问题就变得比较麻烦。有2种解决办法：
i)从图像中crop一部分传入网络
ii)将图像warp成需要的大小后传入网络
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421116017452060.jpg'/>
图14 crop与warp破坏图像原有结构信息

两种办法的示意图如图14，可以看到无论采取那种办法都不好，要么crop后破坏了图像的完整结构，要么warp破坏了图像原始形状信息。

回忆RPN网络生成的proposals的方法：对foreground anchors进行bounding box regression，那么这样获得的proposals也是大小形状各不相同，即也存在上述问题。所以Faster R-CNN中提出了RoI Pooling解决这个问题。不过RoI Pooling确实是从Spatial Pyramid Pooling发展而来，但是限于篇幅这里略去不讲，有兴趣的读者可以自行查阅相关论文。

3.2 RoI Pooling原理
分析之前先来看看RoI Pooling Layer的caffe prototxt的定义：

layer {
  name: "roi_pool5"
  type: "ROIPooling"
  bottom: "conv5_3"
  bottom: "rois"
  top: "pool5"
  roi_pooling_param {
    pooled_w: 7
    pooled_h: 7
    spatial_scale: 0.0625 # 1/16
  }
}
其中有新参数 ，另外一个参数 认真阅读的读者肯定已经知道知道用途。
RoI Pooling layer forward过程：

由于proposal是对应<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421122987881625.svg'/>尺度的，所以首先使用spatial_scale参数将其映射回 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421124173288985.svg'/>大小的feature map尺度；

再将每个proposal对应的feature map区域水平分为 \text{pooled_w}\times \text{pooled_h} 的网格；
对网格的每一份都进行max pooling处理。

这样处理后，即使大小不同的proposal输出结果都是 \text{pooled_w}\times \text{pooled_h} 固定大小，实现了固定长度输出。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421127531054389.jpg'/>
图15 proposal示意图

四 Classification
Classification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框。Classification部分网络结构如图16。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421130541667433.jpg'/>
图16 Classification部分网络结构图

从PoI Pooling获取到7x7=49大小的proposal feature maps后，送入后续网络，可以看到做了如下2件事：

a)通过全连接和softmax对proposals进行分类，这实际上已经是识别的范畴了
b)再次对proposals进行bounding box regression，获取更高精度的rect box

这里来看看全连接层InnerProduct layers，简单的示意图如图17，
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421133733092777.jpg'/>
图17 全连接层示意图

其计算公式如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421135245894870.jpg'/>

其中W和bias B都是预先训练好的，即大小是固定的，当然输入X和输出Y也就是固定大小。所以，这也就印证了之前Roi Pooling的必要性。到这里，我想其他内容已经很容易理解，不在赘述了。

五 Faster R-CNN训练
Faster R-CNN的训练，是在已经训练好的model（如VGG_CNN_M_1024，VGG，ZF）的基础上继续进行训练。实际中训练过程分为6个步骤：

①在已经训练好的model上，训练RPN网络，对应stage1_rpn_train.pt
②利用步骤1中训练好的RPN网络，收集proposals，对应rpn_test.pt
③第一次训练Fast RCNN网络，对应stage1_fast_rcnn_train.pt
④第二训练RPN网络，对应stage2_rpn_train.pt
⑤再次利用步骤4中训练好的RPN网络，收集proposals，对应rpn_test.pt
⑥第二次训练Fast RCNN网络，对应stage2_fast_rcnn_train.pt

可以看到训练过程类似于一种“迭代”的过程，不过只循环了2次。至于只循环了2次的原因是应为作者提到："A similar alternating training can be run for more iterations, but we have observed negligible improvements"，即循环更多次没有提升了。接下来本章以上述6个步骤讲解训练过程。

下面是一张训练过程流程图，应该更加清晰。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421145351772320.jpg'/>
图18 Faster RCNN训练步骤（引用自参考文章[1]）

5.1 训练RPN网络
在该步骤中，首先读取RBG提供的预训练好的model（本文使用VGG），开始迭代训练。来看看stage1_rpn_train.pt网络结构，如图19。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421147531698889.jpg'/>
图19 stage1_rpn_train.pt（考虑图片大小，Conv Layers中所有的层都画在一起了，如红圈所示，后续图都如此处理）
与检测网络类似的是，依然使用Conv Layers提取feature maps。整个网络使用的Loss如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542115069611824.svg'/>

上述公式中 i 表示anchors index， <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421155080356088.svg'/>表示foreground softmax probability，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421155950731054.svg'/>代表对应的GT predict概率（即当第i个anchor与GT间IoU>0.7，认为是该anchor是foreground，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421156756380556.svg'/>；

反之IoU<0.3时，认为是该anchor是background，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421160545058096.svg'/>；至于那些0.3<IoU<0.7的anchor则不参与训练）；t代表predict bounding box，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421161390061770.svg'/>代表对应foreground anchor对应的GT box。可以看到，整个Loss分为2部分：

1.cls loss，即rpn_cls_loss层计算的softmax loss，用于分类anchors为forground与background的网络训练
2.reg loss，即rpn_loss_bbox层计算的soomth L1 loss，用于bounding box regression网络训练。注意在该loss中乘了 p_{i}^{*} ，相当于只关心foreground anchors的回归（其实在回归中也完全没必要去关心background）。

由于在实际过程中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421169442525502.svg'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421170160839746.svg'/>差距过大，用参数λ平衡二者（如<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421171348594720.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542117205261512.svg'/>时设置 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421172922618453.svg'/>，使总的网络Loss计算过程中能够均匀考虑2种Loss。这里比较重要是 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421173733199110.svg'/>使用的soomth L1 loss，计算公式如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421176149880653.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421176712172567.svg'/>

了解数学原理后，反过来看图18：

1)在RPN训练阶段，rpn-data（python AnchorTargetLayer）层会按照和test阶段Proposal层完全一样的方式生成Anchors用于训练
2)对于rpn_loss_cls，输入的rpn_cls_scors_reshape和rpn_labels分别对应 p 与<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421182939956677.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421183672817331.svg'/>参数隐含在p与<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421184349917223.svg'/>的caffe blob的大小中
3)对于rpn_loss_bbox，输入的rpn_bbox_pred和rpn_bbox_targets分别对应 t 与<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421186593562075.svg'/>，rpn_bbox_inside_weigths对应<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542118936420863.svg'/>，rpn_bbox_outside_weigths未用到（从soomth_L1_Loss layer代码中可以看到），而<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421195326282408.svg'/>同样隐含在caffe blob大小中

这样，公式与代码就完全对应了。特别需要注意的是，在训练和检测阶段生成和存储anchors的顺序完全一样，这样训练结果才能被用于检测！

5.2 通过训练好的RPN网络收集proposals
在该步骤中，利用之前的RPN网络，获取proposal rois，同时获取foreground softmax probability，如图20，然后将获取的信息保存在python pickle文件中。该网络本质上和检测中的RPN网络一样，没有什么区别。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155421198012597782.jpg'/>
图20 rpn_test.pt

5.3 训练Faster RCNN网络
读取之前保存的pickle文件，获取proposals与foreground probability。从data层输入网络。然后：

将提取的proposals作为rois传入网络，如图19蓝框
计算bbox_inside_weights+bbox_outside_weights，作用与RPN一样，传入soomth_L1_loss layer，如图20绿框
这样就可以训练最后的识别softmax与最终的bounding box regression了，如图21。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415542120013124204.jpg'/>
图21 stage1_fast_rcnn_train.pt

之后的stage2训练都是大同小异，不再赘述了。Faster R-CNN还有一种end-to-end的训练方式，可以一次完成train，有兴趣请自己看作者GitHub吧：https://github.com/rbgirshick/py-faster-rcnn


参考文献：
Object Detection and Classification using R-CNNs
http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/

拓展
如何将Faster RCNN应用于文字检测？CTPN:场景文字检测—CTPN原理与实现
https://zhuanlan.zhihu.com/p/34757009

本题解析来源：https://zhuanlan.zhihu.com/p/31426458
## 44.one-stage和two-stage目标检测方法的区别和优缺点？
众所周知，物体检测的任务是找出图像或视频中的感兴趣物体，同时检测出它们的位置和大小。

当然，物体检测过程中有很多不确定因素，如图像中物体数量不确定，物体有不同的外观、形状、姿态，加之物体成像时会有光照、遮挡等因素的干扰，导致检测算法有一定的难度。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726659599200997.jpg'/>
由于目标检测的应用场景广泛，所以在CV面试中经常出现，比如七月在线有一CV就业班的学员出去面试时，便被问到“one-stage和two-stage目标检测方法的区别和优缺点？”（详见此文：https://ask.julyedu.com/question/88747） 

虽然我们在本文中详细介绍了各个目标检测的方法：https://www.julyedu.com/question/big/kp_id/32/ques_id/2103 ，但如果你是第一次听到one-stage和two-stage，你会不会瞬间一脸懵逼，这是啥？

其实很简单，顾名思义，区别在于是一步到位还是两步到位。
具体说来，进入深度学习时代以来，物体检测发展主要集中在两个方向：
two stage算法，如R-CNN系列；
ones-tage算法，如YOLO、SSD等。
两者的主要区别在于two stage算法需要先生成proposal（一个有可能包含待检物体的预选框），然后进行细粒度的物体检测，而one stage算法会直接在网络中提取特征来预测物体分类和位置。

所以说，目标检测算法two-stage，如Faster R-CNN算法会先生成候选框（region proposals，可能包含物体的区域），然后再对每个候选框进行分类（也会修正位置）。这类算法相对就慢，因为它需要多次运行检测和分类流程。
而另外一类one-stage目标检测算法（也称one-shot object detectors），其特点是一步到位，仅仅需要送入网络一次就可以预测出所有的边界框，速度相对较快，非常适合移动端，最典型的one-stage检测算法包括YOLO，SSD，SqueezeDet以及DetectNet。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726967446574479.jpg'/>


简单吧，恍然大悟，原来如此！而且one-stage看起来更高级。

当然，目标检测还包括其他很多方法，如下图所示：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726642393420130.jpg'/>

由于two-stage方法在此文中已经详细介绍：https://www.julyedu.com/question/big/kp_id/32/ques_id/2103  ， 故下面重点介绍看起来更高级的one-stage方法。

为什么目标检测问题更难
图像分类是生成单个输出，即类别概率分布。但是这只能给出图像整体内容的摘要，当图像有多个感兴趣的物体时，它就不行了。在下面的图像中，分类器可能会识别出图像即包含猫，也包含狗，这是它最擅长的。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726835338718424.jpg'/>

而目标检测模型将通过预测每个物体的边界框来给出各个物体的位置：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726837618803820.jpg'/>
因为可以专注于对边界框内的物体进行分类并忽略外部背景，因此模型能够为各个物体提供更加准确的预测。如果数据集带有边界框标注，则可以非常轻松地在模型添加一个定位预测：只需预测额外4个数字，分别用于边界框的每个角落。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726841055868983.jpg'/>
现在该模型有两部分输出：类别概率分布和边界框回归。模型的损失函数只是将边界框的回归损失与分类的交叉熵损失相加，通常使用均方误差（MSE）：
outputs = model.forward_pass(image)class_pred = outputs[0]
bbox_pred = outputs[1]
class_loss = cross_entropy_loss(class_pred, class_true)
bbox_loss = mse_loss(bbox_pred, bbox_true)
loss = class_loss + bbox_lossoptimize(loss)

然后采用SGD方法对模型优化训练，这是一个预测实例：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726849862660942.jpg'/>

模型已正确对图像中物体（狗）分类，并给出它在图像中的位置。红色框是真实框，而青色框是预测框，虽然有偏差，但非常接近。为了评估预测框与真实框的匹配程度，我们可以计算两个边界框之间的IOU（intersection-over-union，也称为Jaccard index）。IOU在0到1之间，越大越好。理想情况下，预测框和真框的IOU为100％，但实际上任何超过50％的预测通常都被认为是正确的。对于上面的示例，IOU为74.9％，因而预测框比较精确。使用回归方法预测单个边界框可以获得较好的结果。

然而，当图像中存在多个感兴趣的物体时，就会出现问题：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156726852570511138.jpg'/>

由于模型只能预测一个边界框，因而它必须要选择一个物体，这会最终落在中间位置。实际上这很容易理解：图像里有两个物体，但是模型只能给出一个边界框，因而选择了折中，预测框位于两者中间，也许大小也是介于两个物体大小之间。

注意：也许你可能认为模型应该给出一个包含两个物体的边界框，但是这不太会发生，因为训练不是这样的，真实框都是各个物体分开标注的，而不是一组物体进行标注。你也许认为，上述问题很好解决，对于模型的回归部分增加更多的边界框预测不就好了。

毕竟，如果模型可以预测N个边界框，那么就应该可以正确定位N个物体。听起来不错，但是并没有效。就算模型有多个检测器（这里一个边界框回归称为一个检测器），我们得到的边界框依然会落在图像中间：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415672685453722376.jpg'/>
为什么会这样？问题是模型不知道应该将哪个边界框分配给哪个物体，为了安全起见，它将它们放在中间的某个位置。该模型无法决定：“我可以在左边的马周围放置边界框1，并在右边的马周围放置框2。”

相反，每个检测器仍然试图预测所有物体，而不是一个检测器预测一个物体。尽管该模型具有N个检测器，但它们无法协同工作（对，无法协同工作）。具有多个边界框检测器的模型的效果与仅预测一个边界框的模型完全相同。

我们需要的是使边界框检测器更专一化的一些方法，以便每个检测器将尝试仅预测单个物体，并且不同的探测器将找到不同的物体。在不专一的模型中，每个检测器应该能够处理图像中任何可能位置的各类物体。

这太简单了，模型学会的是预测位于图像中心的方框，因为这样整个训练集实际上会最小化损失函数。从SGD的角度来看，这样做平均得到了相当不错的结果，但在实践中它却不是真正有用的结果，所以我们需要更加有效地训练模型。

通过将每个边界框检测器分配到图像中的特定位置，one-stage目标检测算法（例如YOLO，SSD和DetectNet）都是这样来解决这个问题。因为，检测器学会专注于某些位置的物体。为了获得更好的效果，我们还可以让检测器专注于物体的形状和大小。

继续深入请见此文：https://zhuanlan.zhihu.com/p/61485202
## 45.请画下YOLOv3的网络结构
本题解析来源：https://blog.csdn.net/qq_37541097/article/details/81214953 ，https://blog.csdn.net/dz4543/article/details/90049377

本人是小白，看后表示有点蒙。于是在Github上搜了大牛们基于Tensorflow搭建的YOLOv3模型进行分析（本人只接触过TF，所以就不去看caffe的源码了）。接下来我会根据我阅读的代码来进一步分析网络的结构。Github YOLOv3大牛代码链接。

1.Darknet-53 模型结构
在论文中虽然有给网络的图，但我还是简单说一下。这个网络主要是由一系列的1x1和3x3的卷积层组成（每个卷积层后都会跟一个BN层和一个LeakyReLU)层，作者说因为网络中有53个convolutional layers，所以叫做Darknet-53（2 + 1*2 + 1 + 2*2 + 1 + 8*2 + 1 + 8*2 + 1 + 4*2 + 1 = 53 按照顺序数，不包括Residual中的卷积层，最后的Connected是全连接层也算卷积层，一共53个）。
下图就是Darknet-53的结构图，在右侧标注了一些信息方便理解（卷积的strides默认为（1，1），padding默认为same，当strides为（2，2）时padding为valid）
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313746962362993.png'/>
看完上图应该就能自己搭建出Darknet-53的网络结构了，上图是以输入图像256 x 256进行预训练来进行介绍的，常用的尺寸是416 x 416，都是32的倍数。下面我们再来分析下YOLOv3的特征提取器，看看究竟是在哪几层Features上做的预测。

2.YOLOv3 模型结构
作者在论文中提到利用三个特征层进行边框的预测，具体在哪三层我感觉作者在论文中表述的并不清楚（例如文中有“添加几个卷积层”这样的表述），同样根据代码我将这部分更加详细的分析展示在下图中。
注意：原Darknet53中的尺寸是在图片分类训练集上训练的，所以输入的图像尺寸是256x256，下图是以YOLO v3 416模型进行绘制的，所以输入的尺寸是416x416，预测的三个特征层大小分别是52，26，13。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313761741474063.jpg'/>
在上图中我们能够很清晰的看到三个预测层分别来自的什么地方，以及Concatenate层与哪个层进行拼接。注意Convolutional是指Conv2d+BN+LeakyReLU，和Darknet53图中的一样，而生成预测结果的最后三层都只是Conv2d。通过上图小伙伴们就能更加容易地搭建出YOLOv3的网络框架了。

3.目标边界框的预测
YOLOv3网络在三个特征图中分别通过(4+1+c) k个大小为11的卷积核进行卷积预测，k为预设边界框（bounding box prior）的个数（k默认取3），c为预测目标的类别数，其中4k个参数负责预测目标边界框的偏移量，k个参数负责预测目标边界框内包含目标的概率，ck个参数负责预测这k个预设边界框对应c个目标类别的概率。
下图展示了目标边界框的预测过程（该图是本人重新绘制的，与论文中的示意图有些不同，个人感觉自己绘制的更便于理解）。
图中虚线矩形框为预设边界框，实线矩形框为通过网络预测的偏移量计算得到的预测边界框。
其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313842624654549.gif'/>为预设边界框在特征图上的中心坐标，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313843314124415.gif'/>为预设边界框在特征图上的宽和高，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313844275776899.gif'/>分别为网络预测的边界框中心偏移量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415831384567094883.gif'/>以及宽高缩放比<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313846478916318.gif'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313847889733775.gif'/>为最终预测的目标边界框，从预设边界框到最终预测边界框的转换过程如图右侧公式所示，其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313849260307297.gif'/>函数是sigmoid函数其目的是将预测偏移量缩放到0到1之间（这样能够将预设边界框的中心坐标固定在一个cell当中，作者说这样能够加快网络收敛）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313769499865825.png'/>

下图给出了三个预测层的特征图大小以及每个特征图上预设边界框的尺寸（这些预设边界框尺寸都是作者根据COCO数据集聚类得到的）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313770786807442.png'/>

4.损失函数的计算
关于YOLOv3的损失函数文章中写的很粗略，比如坐标损失采用的是误差的平方和，类别损失采用的是二值交叉熵，本人在github上也找了很多YOLO v3的公开代码，有的采用的是YOLOv1或者YOLOv2的损失函数，下面给出本人认为正确的损失函数（这里偷个懒，公式都是从本人之前写的论文中截图的）。

YOLOv3的损失函数主要分为三个部分：目标定位偏移量损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313855878379421.gif'/>，目标置信度损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313856464682616.gif'/>以及目标分类损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313857856262876.gif'/>，其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415831385869675310.gif'/>是平衡系数。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313775959399980.png'/>

4.1目标置信度损失
目标置信度可以理解为预测目标矩形框内存在目标的概率，目标置信度损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313863437234915.gif'/>采用的是二值交叉熵损失(Binary Cross Entropy)，其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313864334829694.gif'/>，表示预测目标边界框i中是否真实存在目标，0表示不存在，1表示存在。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313865890679659.gif'/>表示预测目标矩形框i内是否存在目标的Sigmoid概率（将预测值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313866983237258.gif'/>通过sigmoid函数得到）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313778186063850.png'/>

4.2目标类别损失
目标类别损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313877170493442.gif'/>同样采用的是二值交叉熵损失（采用二值交叉熵损失的原因是，作者认为同一目标可同时归为多类，比如猫可归为猫类以及动物类，这样能够应对更加复杂的场景。但在本人实践过程中发现使用原始的多类别交叉熵损失函数效果会更好一点，原因是本人针对识别的目标都是固定归于哪一类的，并没有可同时归于多类的情况），其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313878843925498.gif'/>，表示预测目标边界框i中是否真实存在第j类目标，0表示不存在，1表示存在。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313880852694620.gif'/>表示网络预测目标边界框i内存在第j类目标的Sigmoid概率（将预测值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313883323504239.gif'/>通过sigmoid函数得到）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313779974821830.png'/>

4.3目标定位损失
目标定位损失<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313892310075204.gif'/>采用的是真实偏差值与预测偏差值差的平方和，其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313892999728054.gif'/>表示预测矩形框坐标偏移量（注意网络预测的是偏移量，不是直接预测坐标），<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313893945451242.gif'/>表示与之匹配的GTbox与默认框之间的坐标偏移量，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313895559801460.gif'/>为预测的目标矩形框参数，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313896589205355.gif'/>为默认矩形框参数，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313897325809160.gif'/>为与之匹配的真实目标矩形框参数，这些参数都是映射在预测特征图上的。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415831378587826363.png'/>

5 YOLOv3的几个输出实例
来一个YOLO输出时的显示：
layer     filters    size              input                output
   0 conv     32  3 x 3 / 1   416 x 416 x   3   ->   416 x 416 x  32 0.299 BF
   1 conv     64  3 x 3 / 2   416 x 416 x  32   ->   208 x 208 x  64 1.595 BF
   2 conv     32  1 x 1 / 1   208 x 208 x  64   ->   208 x 208 x  32 0.177 BF
   3 conv     64  3 x 3 / 1   208 x 208 x  32   ->   208 x 208 x  64 1.595 BF
   4 Shortcut Layer: 1
   5 conv    128  3 x 3 / 2   208 x 208 x  64   ->   104 x 104 x 128 1.595 BF
   6 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64 0.177 BF
   7 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128 1.595 BF
   8 Shortcut Layer: 5
   9 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64 0.177 BF
  10 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128 1.595 BF
  11 Shortcut Layer: 8
  12 conv    256  3 x 3 / 2   104 x 104 x 128   ->    52 x  52 x 256 1.595 BF
  13 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  14 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  15 Shortcut Layer: 12
  16 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  17 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  18 Shortcut Layer: 15
  19 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  20 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  21 Shortcut Layer: 18
  22 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  23 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  24 Shortcut Layer: 21
  25 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  26 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  27 Shortcut Layer: 24
  28 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  29 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  30 Shortcut Layer: 27
  31 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  32 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  33 Shortcut Layer: 30
  34 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
  35 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
  36 Shortcut Layer: 33
  37 conv    512  3 x 3 / 2    52 x  52 x 256   ->    26 x  26 x 512 1.595 BF
  38 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  39 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  40 Shortcut Layer: 37
  41 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  42 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  43 Shortcut Layer: 40
  44 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  45 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  46 Shortcut Layer: 43
  47 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  48 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  49 Shortcut Layer: 46
  50 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  51 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  52 Shortcut Layer: 49
  53 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  54 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  55 Shortcut Layer: 52
  56 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  57 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  58 Shortcut Layer: 55
  59 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  60 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  61 Shortcut Layer: 58
  62 conv   1024  3 x 3 / 2    26 x  26 x 512   ->    13 x  13 x1024 1.595 BF
  63 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  64 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  65 Shortcut Layer: 62
  66 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  67 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  68 Shortcut Layer: 65
  69 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  70 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  71 Shortcut Layer: 68
  72 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  73 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  74 Shortcut Layer: 71
  75 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  76 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  77 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  78 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  79 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF
  80 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF
  81 conv     18  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x  18 0.006 BF
  82 yolo
  83 route  79
  84 conv    256  1 x 1 / 1    13 x  13 x 512   ->    13 x  13 x 256 0.044 BF
  85 upsample            2x    13 x  13 x 256   ->    26 x  26 x 256
  86 route  85 61
  87 conv    256  1 x 1 / 1    26 x  26 x 768   ->    26 x  26 x 256 0.266 BF
  88 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  89 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  90 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  91 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF
  92 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF
  93 conv     18  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x  18 0.012 BF
  94 yolo
  95 route  91
  96 conv    128  1 x 1 / 1    26 x  26 x 256   ->    26 x  26 x 128 0.044 BF
  97 upsample            2x    26 x  26 x 128   ->    52 x  52 x 128
  98 route  97 36
  99 conv    128  1 x 1 / 1    52 x  52 x 384   ->    52 x  52 x 128 0.266 BF
 100 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
 101 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
 102 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
 103 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF
 104 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF
 105 conv     18  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x  18 0.025 BF
 106 yolo

实际，这个已经告诉了我们每层的输出情况。每层特征图的大小情况：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313961038895239.png'/>

在前文网络的基础上，用红色做了注释。residual使用残差结构。什么是残差结构？举个例子在第一层残差结构（其输出为208208128），其输入为20820864，经过3211和6433的卷积后，其生成的特征图与输入叠加起来。其结构如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313963074357755.png'/>

其叠加后的特征图作为新的输入输入下一层。YOLO主体是由许多这种残差模块组成，减小了梯度爆炸的风险，加强了网络的学习能力。

可以看到YOLO有3个尺度的输出，分别在52×52，26×26，13×13。嗯，都是奇数，使得网格会有个中心位置。同时YOLO输出为3个尺度，每个尺度之间还有联系。比如说，13×13这个尺度输出用于检测大型目标，对应的26×26为中型的，52×52用于检测小型目标。
上一张图，我觉得很详细看得懂。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64158313964860182548.png'/>

这个检测COCO（80个类的），所以其输出需要构造为：S×S×3×（5+class_number）。解释下为什么是这样。
YOLO将图像划分为S×S的网格，当目标中心落在某个网格中，就用这个网格去检测它，这是S×S的由来。
为什么是3，是因为每个网格需要检测3个anchorbox（注意有3个尺度），所以对于每个尺度，其输出为S×S×3×？？？
对于一个anchor box，它包含坐标信息（x , y , w , h ）以及置信度，而这有5个信息；同时还会包含是否所有类别的信息，使用one-hot编码。

比如说有3个类:person、car、dog。检测的结果是人，那么就编码为[1,0,0]。可见所有类别信息都会被编码，COCO有80个类别的话，便是5+80。所以，对于每个维度的输出，其结果为：S×S×3×（5+80）=S×S×255 S×S×3×（5+80） = S×S×255S×S×3×（5+80）=S×S×255。
同时从上图可以看到，其结果便是通过一些卷积操作，将输出构造成这样。并且将不同尺度的特征图叠加到一起，增加输出的信息。这个图可以好好看看。


