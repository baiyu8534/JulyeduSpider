# DM | 大数据
## 说说spark的性能调优。
本题解析来源：
https://tech.meituan.com/spark-tuning-basic.html
https://tech.meituan.com/spark-tuning-pro.html
## 数据挖掘对聚类的数据要求是什么？
（1）可伸缩性
（2）处理不同类型属性的能力
（3）发现任意形状的聚类
（4）使输入参数的领域知识最小化
（5）处理噪声数据的能力
（6）对于输入顺序不敏感
（7）高维性
（8）基于约束的聚类
（9）可解释性和可利用性
## 数据的逻辑存储结构（如数组，队列，树等）对于软件开发具有十分重要的影响，试对你所了解的各种存储结构从运行速度、存储效率和适用场合等方面进行简要地分析。 
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1514204103_293.png'/>
## 频繁项集分析有哪些指标？有什么意义？
1）支持度： 包含频繁项集F的集合的数目
2）可信度：频繁项F与某项j的并集 （即F U {j}）的支持度 与 频繁项集F的支持度的比值，
3）兴趣度：F U {j} 可信度 与 包含{j}的集合比率之间的差值。若兴趣度很高，则 频繁项集F会促进 j 的存在， 若兴趣度为负值，且频繁项集会抑制 j 的存在；若兴趣度为0，则频繁项集对 j 无太大影响。
频繁项集 与 某项 j 的关系就是 关联规则
## 什么是聚类分析？聚类算法有哪几种？请选择一种详细描述其计算原理和步骤。
聚类分析(clusteranalysis)是一组将研究对象分为相对同质的群组(clusters)的统计分析技术。 聚类分析也叫分类分析(classification analysis)或数值分类(numerical taxonomy)。聚类与分类的不同在于，聚类所要求划分的类是未知的。
聚类分析计算方法主要有： 层次的方法（hierarchical method）、划分方法（partitioning method）、基于密度的方法（density-based method）、基于网格的方法（grid-based method）、基于模型的方法（model-based method）等。其中，前两种算法是利用统计学定义的距离进行度量。
k-means 算法的工作过程说明如下：首先从n个数据对象任意选择 k 个对象作为初始聚类中心；而对于所剩下其它对象，则根据它们与这些聚类中心的相似度（距离），分别将它们分配给与其最相似的（聚类中心所代表的）聚类；然后再计算每个所获新聚类的聚类中心（该聚类中所有对象的均值）；不断重复这一过程直到标准测度函数开始收敛为止。一般都采用均方差作为标准测度函数. k个聚类具有以下特点：各聚类本身尽可能的紧凑，而各聚类之间尽可能的分开。
其流程如下：
（1）从 n个数据对象任意选择 k 个对象作为初始聚类中心； 　　　　
（2）根据每个聚类对象的均值（中心对象），计算每个对象与这些中心对象的距离；并根据最小距离重新对相应对象进行划分；　　
（3）重新计算每个（有变化）聚类的均值（中心对象）；
（4）循环（2）、（3）直到每个聚类不再发生变化为止（标准测量函数收敛）。
优 点：本算法确定的K 个划分到达平方误差最小。当聚类是密集的，且类与类之间区别明显时，效果较好。对于处理大数据集，这个算法是相对可伸缩和高效的，计算的复杂度为 O(NKt)，其中N是数据对象的数目，t是迭代的次数。一般来说，K<<N，t<<N 。
缺点：1. K 是事先给定的，但非常难以选定；2. 初始聚类中心的选择对聚类结果有较大的影响。
## 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。
　　方案：顺序读文件中，对于每个词x，取hash(x)P00，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。
　　如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。
　　对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用trie树/hash_map等），并取出出现频率最大的100个词（可以用含100个结点的最小堆），并把100个词及相应的频率存入文件，这样又得到了5000个文件。下一步就是把这5000个文件进行归并（类似与归并排序）的过程了。
## 有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。
　　还是典型的TOP K算法，解决方案如下：
　　方案1：
　　顺序读取10个文件，按照hash(query)的结果将query写入到另外10个文件（记为）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。
　　找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。利用快速/堆/归并排序按照出现次数进行排序。将排序好的query和对应的query_cout输出到文件中。这样得到了10个排好序的文件（记为）。
　　对这10个文件进行归并排序（内排序与外排序相结合）。
　　方案2：
　　一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。
　　方案3：
　　与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。
## 简答说一下hadoop的map-reduce编程模型
首先map task会从本地文件系统读取数据，转换成key-value形式的键值对集合

使用的是hadoop内置的数据类型，比如longwritable、text等

将键值对集合输入mapper进行业务处理过程，将其转换成需要的key-value在输出

之后会进行一个partition分区操作，默认使用的是hashpartitioner，可以通过重写hashpartitioner的getpartition方法来自定义分区规则

之后会对key进行进行sort排序，grouping分组操作将相同key的value合并分组输出，在这里可以使用自定义的数据类型，重写WritableComparator的Comparator方法来自定义排序规则，重写RawComparator的compara方法来自定义分组规则

之后进行一个combiner归约操作，其实就是一个本地段的reduce预处理，以减小后面shufle和reducer的工作量

reduce task会通过网络将各个数据收集进行reduce处理，最后将数据保存或者显示，结束整个job
## hadoop和spark的都是并行计算，那么他们有什么相同和区别
两者都是用mr模型来进行并行计算，hadoop的一个作业称为job，job里面分为map task和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束

spark用户提交的任务成为application，一个application对应一个sparkcontext，app中存在多个job，每触发一次action操作就会产生一个job

这些job可以并行或串行执行，每个job中有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行，executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算

hadoop的job只有map和reduce操作，表达能力比较欠缺而且在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系

spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作如join，groupby等，而且通过DAG图可以实现良好的容错
## 为什么要用flume导入hdfs，hdfs的构架是怎样的
flume可以实时的导入数据到hdfs中，当hdfs上的文件达到一个指定大小的时候会形成一个文件，或者超过指定时间的话也形成一个文件

文件都是存储在datanode上面的，namenode记录着datanode的元数据信息，而namenode的元数据信息是存在内存中的，所以当文件切片很小或者很多的时候会卡死
## spark的分区概念和hdfs、hive的分区概念是一样的吗？hive的分区就是个文件夹而已。

spark是计算引擎，它的分区partiton是针对RDD，和mapreduce的分区概念一致，分区目的提高并行计算速度。

hdfs是存储，它的分区是文件基于block存储，实现多副本分布式。

hive是hadoop的数据仓库工具，它的分区类似数据库的分库分表，解决数据提取速度。比如hive外表可以按天进行分区，每个分区指向一个hdfs路径。
# DM | 大数据
## 1.说说spark的性能调优。
本题解析来源：
https://tech.meituan.com/spark-tuning-basic.html
https://tech.meituan.com/spark-tuning-pro.html
## 2.数据挖掘对聚类的数据要求是什么？
（1）可伸缩性
（2）处理不同类型属性的能力
（3）发现任意形状的聚类
（4）使输入参数的领域知识最小化
（5）处理噪声数据的能力
（6）对于输入顺序不敏感
（7）高维性
（8）基于约束的聚类
（9）可解释性和可利用性
## 3.数据的逻辑存储结构（如数组，队列，树等）对于软件开发具有十分重要的影响，试对你所了解的各种存储结构从运行速度、存储效率和适用场合等方面进行简要地分析。 
<img  src='https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1514204103_293.png'/>
## 4.频繁项集分析有哪些指标？有什么意义？
1）支持度： 包含频繁项集F的集合的数目
2）可信度：频繁项F与某项j的并集 （即F U {j}）的支持度 与 频繁项集F的支持度的比值，
3）兴趣度：F U {j} 可信度 与 包含{j}的集合比率之间的差值。若兴趣度很高，则 频繁项集F会促进 j 的存在， 若兴趣度为负值，且频繁项集会抑制 j 的存在；若兴趣度为0，则频繁项集对 j 无太大影响。
频繁项集 与 某项 j 的关系就是 关联规则
## 5.什么是聚类分析？聚类算法有哪几种？请选择一种详细描述其计算原理和步骤。
聚类分析(clusteranalysis)是一组将研究对象分为相对同质的群组(clusters)的统计分析技术。 聚类分析也叫分类分析(classification analysis)或数值分类(numerical taxonomy)。聚类与分类的不同在于，聚类所要求划分的类是未知的。
聚类分析计算方法主要有： 层次的方法（hierarchical method）、划分方法（partitioning method）、基于密度的方法（density-based method）、基于网格的方法（grid-based method）、基于模型的方法（model-based method）等。其中，前两种算法是利用统计学定义的距离进行度量。
k-means 算法的工作过程说明如下：首先从n个数据对象任意选择 k 个对象作为初始聚类中心；而对于所剩下其它对象，则根据它们与这些聚类中心的相似度（距离），分别将它们分配给与其最相似的（聚类中心所代表的）聚类；然后再计算每个所获新聚类的聚类中心（该聚类中所有对象的均值）；不断重复这一过程直到标准测度函数开始收敛为止。一般都采用均方差作为标准测度函数. k个聚类具有以下特点：各聚类本身尽可能的紧凑，而各聚类之间尽可能的分开。
其流程如下：
（1）从 n个数据对象任意选择 k 个对象作为初始聚类中心； 　　　　
（2）根据每个聚类对象的均值（中心对象），计算每个对象与这些中心对象的距离；并根据最小距离重新对相应对象进行划分；　　
（3）重新计算每个（有变化）聚类的均值（中心对象）；
（4）循环（2）、（3）直到每个聚类不再发生变化为止（标准测量函数收敛）。
优 点：本算法确定的K 个划分到达平方误差最小。当聚类是密集的，且类与类之间区别明显时，效果较好。对于处理大数据集，这个算法是相对可伸缩和高效的，计算的复杂度为 O(NKt)，其中N是数据对象的数目，t是迭代的次数。一般来说，K<<N，t<<N 。
缺点：1. K 是事先给定的，但非常难以选定；2. 初始聚类中心的选择对聚类结果有较大的影响。
## 6.有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。
　　方案：顺序读文件中，对于每个词x，取hash(x)P00，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。
　　如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。
　　对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用trie树/hash_map等），并取出出现频率最大的100个词（可以用含100个结点的最小堆），并把100个词及相应的频率存入文件，这样又得到了5000个文件。下一步就是把这5000个文件进行归并（类似与归并排序）的过程了。
## 7.有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。
　　还是典型的TOP K算法，解决方案如下：
　　方案1：
　　顺序读取10个文件，按照hash(query)的结果将query写入到另外10个文件（记为）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。
　　找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。利用快速/堆/归并排序按照出现次数进行排序。将排序好的query和对应的query_cout输出到文件中。这样得到了10个排好序的文件（记为）。
　　对这10个文件进行归并排序（内排序与外排序相结合）。
　　方案2：
　　一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。
　　方案3：
　　与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。
## 8.简答说一下hadoop的map-reduce编程模型
首先map task会从本地文件系统读取数据，转换成key-value形式的键值对集合

使用的是hadoop内置的数据类型，比如longwritable、text等

将键值对集合输入mapper进行业务处理过程，将其转换成需要的key-value在输出

之后会进行一个partition分区操作，默认使用的是hashpartitioner，可以通过重写hashpartitioner的getpartition方法来自定义分区规则

之后会对key进行进行sort排序，grouping分组操作将相同key的value合并分组输出，在这里可以使用自定义的数据类型，重写WritableComparator的Comparator方法来自定义排序规则，重写RawComparator的compara方法来自定义分组规则

之后进行一个combiner归约操作，其实就是一个本地段的reduce预处理，以减小后面shufle和reducer的工作量

reduce task会通过网络将各个数据收集进行reduce处理，最后将数据保存或者显示，结束整个job
## 9.hadoop和spark的都是并行计算，那么他们有什么相同和区别
两者都是用mr模型来进行并行计算，hadoop的一个作业称为job，job里面分为map task和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束

spark用户提交的任务成为application，一个application对应一个sparkcontext，app中存在多个job，每触发一次action操作就会产生一个job

这些job可以并行或串行执行，每个job中有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行，executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算

hadoop的job只有map和reduce操作，表达能力比较欠缺而且在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系

spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作如join，groupby等，而且通过DAG图可以实现良好的容错
## 10.为什么要用flume导入hdfs，hdfs的构架是怎样的
flume可以实时的导入数据到hdfs中，当hdfs上的文件达到一个指定大小的时候会形成一个文件，或者超过指定时间的话也形成一个文件

文件都是存储在datanode上面的，namenode记录着datanode的元数据信息，而namenode的元数据信息是存在内存中的，所以当文件切片很小或者很多的时候会卡死
## 11.spark的分区概念和hdfs、hive的分区概念是一样的吗？hive的分区就是个文件夹而已。

spark是计算引擎，它的分区partiton是针对RDD，和mapreduce的分区概念一致，分区目的提高并行计算速度。

hdfs是存储，它的分区是文件基于block存储，实现多副本分布式。

hive是hadoop的数据仓库工具，它的分区类似数据库的分库分表，解决数据提取速度。比如hive外表可以按天进行分区，每个分区指向一个hdfs路径。
