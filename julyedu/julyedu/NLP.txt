# NLP
## 了解Google最新的模型bert么？
BERT (Bidirectional Encoder Representations from Transformers)

10月11日，Google AI Language 发布了论文
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding
提出的 BERT 模型在 11 个 NLP 任务上的表现刷新了记录，包括问答 Question Answering (SQuAD v1.1)，推理 Natural Language Inference (MNLI) 等：

GLUE ：General Language Understanding Evaluation
MNLI ：Multi-Genre Natural Language Inference
SQuAD v1.1 ：The Standford Question Answering Dataset
QQP ： Quora Question Pairs 
QNLI ： Question Natural Language Inference

SST-2 ：The Stanford Sentiment Treebank
CoLA ：The Corpus of Linguistic Acceptability 
STS-B ：The Semantic Textual Similarity Benchmark
MRPC ：Microsoft Research Paraphrase Corpus
RTE ：Recognizing Textual Entailment 
WNLI ：Winograd NLI
SWAG ：The Situations With Adversarial Generations

让我们先来看一下 BERT 在 Stanford Question Answering Dataset (SQuAD) 上面的排行榜吧：
https://rajpurkar.github.io/SQuAD-explorer/
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155721663860330387.png'/>
BERT 可以用来干什么？

BERT 可以用于问答系统，情感分析，垃圾邮件过滤，命名实体识别，文档聚类等任务中，作为这些任务的基础设施即语言模型，

BERT 的代码也已经开源：
https://github.com/google-research/bert
我们可以对其进行微调，将它应用于我们的目标任务中，BERT 的微调训练也是快而且简单的。

例如在 NER 问题上，BERT 语言模型已经经过 100 多种语言的预训练，这个是 top 100 语言的列表：
https://github.com/google-research/bert/blob/master/multilingual.md

只要在这 100 种语言中，如果有 NER 数据，就可以很快地训练 NER。

BERT 原理简述
BERT 的创新点在于它将双向 Transformer 用于语言模型，
之前的模型是从左向右输入一个文本序列，或者将 left-to-right 和 right-to-left 的训练结合起来。

实验的结果表明，双向训练的语言模型对语境的理解会比单向的语言模型更深刻，论文中介绍了一种新技术叫做 Masked LM（MLM），在这个技术出现之前是无法进行双向语言模型训练的。

BERT 利用了 Transformer 的 encoder 部分。
Transformer 是一种注意力机制，可以学习文本中单词之间的上下文关系的。
Transformer 的原型包括两个独立的机制，一个 encoder 负责接收文本作为输入，一个 decoder 负责预测任务的结果。
BERT 的目标是生成语言模型，所以只需要 encoder 机制。

Transformer 的 encoder 是一次性读取整个文本序列，而不是从左到右或从右到左地按顺序读取，
这个特征使得模型能够基于单词的两侧学习，相当于是一个双向的功能。

下图是 Transformer 的 encoder 部分，输入是一个 token 序列，先对其进行 embedding 称为向量，然后输入给神经网络，输出是大小为 H 的向量序列，每个向量对应着具有相同索引的 token。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155721672283337082.png'/>
图片 by Rani Horev

当我们在训练语言模型时，有一个挑战就是要定义一个预测目标，很多模型在一个序列中预测下一个单词，
“The child came home from ___”
双向的方法在这样的任务中是有限制的，为了克服这个问题，BERT 使用两个策略:

1. Masked LM (MLM)
在将单词序列输入给 BERT 之前，每个序列中有 15％ 的单词被 [MASK] token 替换。 然后模型尝试基于序列中其他未被 mask 的单词的上下文来预测被掩盖的原单词。

这样就需要：
i)在 encoder 的输出上添加一个分类层
ii)用嵌入矩阵乘以输出向量，将其转换为词汇的维度
iii)用 softmax 计算词汇表中每个单词的概率

BERT 的损失函数只考虑了 mask 的预测值，忽略了没有掩蔽的字的预测。这样的话，模型要比单向模型收敛得慢，不过结果的情境意识增加了。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155721677852106261.png'/>
图片 by Rani Horev

2. Next Sentence Prediction (NSP)
在 BERT 的训练过程中，模型接收成对的句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子。
在训练期间，50％ 的输入对在原始文档中是前后关系，另外 50％ 中是从语料库中随机组成的，并且是与第一句断开的。

为了帮助模型区分开训练中的两个句子，输入在进入模型之前要按以下方式进行处理：

在第一个句子的开头插入 [CLS] 标记，在每个句子的末尾插入 [SEP] 标记。
将表示句子 A 或句子 B 的一个句子 embedding 添加到每个 token 上。
给每个 token 添加一个位置 embedding，来表示它在序列中的位置。
为了预测第二个句子是否是第一个句子的后续句子，用下面几个步骤来预测：

整个输入序列输入给 Transformer 模型
用一个简单的分类层将 [CLS] 标记的输出变换为 2×1 形状的向量
用 softmax 计算 IsNextSequence 的概率
在训练 BERT 模型时，Masked LM 和 Next Sentence Prediction 是一起训练的，目标就是要最小化两种策略的组合损失函数。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155721681290293736.png'/>
如何使用 BERT?
BERT 可以用于各种NLP任务，只需在核心模型中添加一个层，例如：
a)在分类任务中，例如情感分析等，只需要在 Transformer 的输出之上加一个分类层
b)在问答任务（例如SQUAD v1.1）中，问答系统需要接收有关文本序列的 question，并且需要在序列中标记 answer。 可以使用 BERT 学习两个标记 answer 开始和结尾的向量来训练Q＆A模型。
c)在命名实体识别（NER）中，系统需要接收文本序列，标记文本中的各种类型的实体（人员，组织，日期等）。 可以用 BERT 将每个 token 的输出向量送到预测 NER 标签的分类层。

在 fine-tuning 中，大多数超参数可以保持与 BERT 相同，在论文中还给出了需要调整的超参数的具体指导（第3.5节）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155721683728608771.png'/>
学习资料：
https://arxiv.org/pdf/1810.04805.pdf
https://www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/
https://medium.com/syncedreview/best-nlp-model-ever-google-bert-sets-new-standards-in-11-language-tasks-4a2a189bc155
## 了解文本嵌入么？
什么是NLP？
自然语言处理（NLP） 是计算机科学，人工智能和语言学的交叉领域。目标是让计算机处理或“理解”自然语言，以执行语言翻译和问题回答等任务。

随着语音接口和聊天机器人的兴起，NLP正在成为信息时代最重要的技术之一，同时它也是人工智能的关键部分。充分理解和表达语言的含义是一个非常困难的目标。为什么？因为人类的语言很特别。

人类语言有什么特别之处？
1.人类语言是专门为传达说话人的意图而构建的系统。这不仅仅是一个环境信号，更是一个有意识的交流。
2.人类语言大多是离散/符号的/分类的信号系统，大概是因为信号可靠性更高。
3.一种语言的分类符号可以用几种方式编码为通信信号：声音，手势，写作，图像等。人类语言只是其中的一种。
4.人类语言是不明确的（与编程和其他正式语言不同）。 因此，在表达、学习和使用语言/情境/情境/文字/视觉知识对人类语言方面存在高度复杂性。

NLP应用到哪里？
从NLP研究领域衍生出了一批快速增长的应用程序。以下是其中几个：
1.拼写检查，关键字搜索，查找同义词；
2.从网站提取信息，例如：产品价格，日期，地点，人员或公司名称；
3.分类：长文档的积极/消极情绪；
4.机器翻译；
5.口语对话系统；
6.复杂的问答系统；

事实上，这些应用程序已经在现实中大量使用，从搜索到在线广告匹配 ; 从自动/辅助翻译到营销或财务/交易的情绪分析 ; 从语音识别到chatbots /对话代理（自动化客户支持，控制设备，订购商品）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946313784534369.jpg'/>

深度学习
大部分NLP技术都是由深度学习提供技术支持。近几年，深度学习才开始发挥作用，主要是因为：
·大量的训练数据；
·更快的机器和多核CPU / GPU；
·性能高的新模型和算法：有效的端到端联合系统学习、有效的使用上下文和任务间转换的学习方法，以及正则化优化方法。

在深度学习中，表示学习试图自动学习来自原始输入的良好特征或表示。而在机器学习中手动设计的特征通常过多且不完整，需要花费很长时间进行设计和验证。而且深度学习提供了一个非常灵活、通用且可学习的框架，用于呈现视觉和语言信息的世界。

最初，它在语音识别和计算机视觉等领域取得突破。最近，深度学习方法在许多不同的NLP任务中表现出了非常高的性能。这些模型通常可以通过单一的端到端模型进行训练，并且不需要传统的，特定于任务的特征工程。

下面简单介绍下文本嵌入（Text Embeddings）。

在传统的NLP中，我们将单词视为离散符号，然后可以用one-hot向量表示。向量的维度是整个词汇表中单词的数量。单词作为离散符号的问题在于，对于one-hot向量来说，没有自然的相似性概念。

因此，另一种方法是学习在向量本身中编码相似性。核心思想是一个词的含义是由经常出现在其旁边的单词给出的。

文本嵌入是字符串的实值向量表示。我们为每个单词建立一个密集的向量，选择它以便类似于类似上下文中出现的单词的向量。对于大多数NLP任务而言，词嵌入被认为是一个很好的起点。它们允许深度学习在较小的数据集上也是有效的，因为它们通常是深度学习体系的第一批输入，也是NLP中最流行的迁移学习方式。

在词嵌入中最流行的应该是Word2vec，它是由谷歌（Mikolov）开发的模型，另外一个是由斯坦福大学（彭宁顿，Socher和曼宁）开发的GloVe。

接着我们重点介绍这两种模型：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946332328697749.png'/>

在Word2vec中，我们有一个庞大的文本语料库，其中固定词汇表中的每个词都由一个向量表示。然后，我们通过文本中的每个位置t，其中有一个中心词c和上下文词o。

接下来，我们使用字向量的相似性Ç和Ò计算的概率ø给出Ç（或反之亦然）。我们不断调整单词向量来最大化这个概率。为了有效地训练Word2vec，我们可以从数据集中去除无意义的单词。这有助于提高模型的准确性。

Word2vec有两个变体值得一提：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946339451861219.png'/>

1.Skip-Gram：我们考虑一个包含k个连续项的上下文窗口。
然后，我们跳过其中一个单词，尝试学习一个神经网络，该网络可以获得除跳过的所有术语外的所有术语，并预测跳过的术语。
因此，如果两个单词在大语料库中反复共享相似的上下文，那么这些术语的嵌入向量将具有相似的向量。

2.Continuous Bag of Words：我们在一个大的语料库中获取大量的句子，每当我们看到一个词，我们就会联想到周围的词。
然后，我们将上下文单词输入到神经网络，并预测该上下文中心的单词。
当我们有数千个这样的上下文单词和中心单词时，我们就有了一个神经网络数据集的实例。我们训练神经网络，最后编码的隐藏层输出表示一个特定的词嵌入。

当我们通过大量的句子进行训练时，类似上下文中的单词会得到相似的向量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946350921665397.jpg'/>

对Skip-Gram和CBOW的一个吐槽就是它们都是基于窗口的模型，这意味着语料库的共现统计不能被有效使用，导致次优的嵌入（suboptimal embeddings）。

GloVe模型旨在通过捕捉一个字与整个观测语料库的结构嵌入的含义来解决这个问题。为此，该模型训练单词的全局共现次数，并通过最小化最小二乘误差来充分利用统计量，从而产生具有有意义子结构的单词向量空间。这样的做法足以保留单词与向量距离的相似性。

除了这两种文本嵌入外，还有许多最近开发的高级模型，包括FastText，Poincare嵌入，sense2vec，Skip-Thought，Adaptive Skip-Gram，我强烈建议你学习一下。

## 了解机器翻译中的NLP技术么？
机器翻译是语言理解的经典测试。它由语言分析和语言生成组成。大型机器翻译系统具有巨大的商业用途，给你一些值得注意的例子：

·谷歌翻译每天翻译1000亿字；
·Facebook使用机器翻译自动翻译帖子和评论中的文字，以打破语言障碍，让世界各地的人们相互交流；
·阿里巴巴使用机器翻译技术来实现跨境贸易，连接世界各地的买家和卖家；
·微软为Android、iOS和Amazon Fire上的最终用户和开发人员提供基于人工智能的翻译，无论他们是否可以访问互联网。

在传统的机器翻译系统中，我们必须使用平行语料库：一组文本，每个文本都被翻译成一种或多种不同于原文的其他语言。

例如，给定源语言f（例如法语）和目标语言e（例如英语），我们需要建立多个统计模型，包括使用贝叶斯规则的概率公式，训练的翻译模型p（f | e）平行语料库和语言模型p（e）在纯英文语料库上训练。这种方法跳过了数百个重要细节，需要大量的手工特征工程，整体而言它是一个非常复杂的系统。

神经机器翻译是通过一个称为递归神经网络（RNN）的大型人工神经网络对整个过程进行建模的方法。RNN是一个有状态的神经网络，它通过时间连接过去。神经元的信息不仅来自前一层，而且来自更前一层的信息。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946362363352968.png'/>

标准的神经机器翻译是一种端到端神经网络，其中，源语句由称为编码器的RNN 编码，目标词使用另一个称为解码器。RNN编码器一次读取一个源语句，然后在最后隐藏状态汇总整个源句子。RNN解码器使用反向传播学习这个汇总并返回翻译后的版本。

神经机器翻译从2014年的一项边缘研究领域发展到2016年广泛采用的领先机器翻译方式，那么，使用神经机器翻译的最大成功是什么？
1.端到端训练：NMT中的所有参数同时被优化，以最大限度地减少网络输出的损耗性能。
2.分布式表示的优势：NMT更好地利用单词和短语的相似性。
3.更好地探索上下文：NMT可以使用更多的上下文——源文本和部分目标文本以此进行更准确地翻译。
4.更流利的文本生成：深度学习文本生成质量高于平行语料库。

RNN的一个大问题是梯度消失（或爆炸）问题，其中取决于所使用的激活函数，随着时间的推移信息会迅速丢失。

直观地说，这不会成为一个很大问题，因为这些只是权重而不是神经元状态，但是时间的权重实际上是存储过去的信息的地方，如果权重达到0或1,000,000的值，那么以前的状态将不会提供很多信息。

因此，RNNs在记忆序列中的前几个单词时会表现的很困难，并且只能根据最近的单词进行预测。

长期/短期记忆（LSTM）网络试图通过引入门和明确定义的存储器单元来对抗梯度消失/爆炸问题。每个神经元都有一个存储单元和三个门：输入、输出和忘记。这些门的功能是通过停止或允许信息流来保护信息。
    ①输入门决定了来自上一层的多少信息存储在单元中；
    ②输出层在另一端获取任务，并确定下一层有多少单元知道该单元的状态。
    ③忘记门的作用起初看起来很奇怪，但有时候忘记门是个不错的设计：如果它正在学习一本书并开始新的一章，那么网络可能需要忘记前一章中的一些字符。

已经证明LSTM能够学习复杂的序列，例如像莎士比亚的写作或者创作原始音乐。请注意，这些门中的每一个都对前一个神经元中的一个单元具有权重，因此它们通常需要更多资源才能运行。

LSTM目前非常流行，并且在机器翻译中被广泛使用。除此之外，它是大多数序列标签任务的默认模型，其中有大量的数据。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946375621557650.jpg'/>

门控重复单元（GRU）是在LSTM的基础上变形得来的，也是神经机器翻译的扩展。它拥有更少的门，并且连接方式略有不同：它不是输入、输出和忘记门组成的，而是具有更新门。这个更新门决定了从最后一个状态开始保留多少信息以及从上一个层开始输入多少信息。

复位（reset）门的功能与LSTM的忘记（forget）门非常相似，但位置稍有不同。他们总是发出它们完整的状态因为他们没有输出门。在大多数情况下，它们的功能与LSTM非常相似，最大的不同之处在于GRUs稍快并且更容易运行（但表现力稍差）。

在实践中，这些往往会互相抵消，因为你需要一个更大的网络来重新获得一些表示能力，这反过来又抵消了性能的优势。在一些情况下，GRU可以胜过LSTM。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946381391394604.png'/>

除了这三大体系结构之外，过去几年神经​​机器翻译系统还有进一步的改进。以下是最显着的发展：
    a)用神经网络进行序列学习的序列证明了LSTM在神经机器翻译中的有效性。它提出了序列学习的一种通用的端到端方法，对序列结构进行了最少的假设。该方法使用多层Long Short Term Memory（LSTM）将输入序列映射为固定维度的向量，然后使用另一个深度LSTM从向量解码目标序列。
    b)通过联合学习对齐和翻译的神经机器翻译引入了NLP中的注意机制（将在下一篇文章中介绍）。认识到使用固定长度矢量是提高NMT性能的瓶颈，作者建议通过允许模型自动（软）搜索与预测目标相关的源句子部分来扩展，而不必将这些部分明确地形成为一个固定的长度。
    c)用于神经机器翻译的循环编码器上的卷积利用附加的卷积层增强NMT中的标准RNN编码器，以在编码器输出中捕捉更广泛的上下文。
谷歌的神经机器翻译，它解决了准确性和部署方便性的问题。该模型由一个深度LSTM网络组成，该网络包含8个编码器和8个解码器层，使用残余连接以及从解码器网络到编码器的注意力连接。
    d)Facebook AI研究人员不使用递归神经网络，而是使用卷积神经网络序列对NMT中的学习任务进行排序。
## 了解情感分析中的NLP技术么？
人际交往不仅仅是文字和其明确的含义，而且它还是微妙且复杂的。即使在完全基于文本的对话中，你也可以根据单词选择和标点符号判断客户是否感到愤怒。你可以阅读产品在天猫平台的评论，并了解评论者是否喜欢或不喜欢它，即使他们从未直接说过。

为了使计算机真正理解人类每天的交流方式，他们需要理解的不仅仅是客观意义上的词语定义、而且他们需要了解我们的情绪。

情绪分析是通过较小元素的语义组成来解释较大文本单元（实体、描述性术语、事实、论据、故事）的含义的过程。

传统情感分析的方法是将句子视为一个词袋，并查阅“积极”和“消极”单词的策划列表，以确定该句子的情绪。这需要手工设计的特征来捕捉情绪，所有这是非常耗时和不可扩展的。

用于情感分析的现代深度学习方法可用于形态学、语法和逻辑语义，其中最有效的是递归神经网络。顾名思义，递归神经网络开发的主要假设递归是描述语言的自然方式。递归在消歧方面很有用，有助于某些任务引用特定的短语，并且对于使用语法树结构的任务非常有效。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728521912439982.png'/>
递归神经网络非常适合具有嵌套层次结构和内部递归结构的设置。语法的句法规则是高度递归的，因此，我们利用递归神经网络！

使用RNN对句子进行建模的另一个好处是，我们现在可以输入任意长度的句子，这对于在NLP中使用神经网络来说是一个巨大的难题，使用非常聪明的技巧使句子的输入向量具有相同的大小，尽管句子的长度不相等。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728522658917987.png'/>
标准RNN是一种递归神经网络的最基本的版本。它具有最大边距结构预测架构，可以在复杂的场景图像和句子中成功地运用这种结构。它用于为自然语言句子提供有竞争力的语法分析器比如说Penn Treebank。

作为参考，Penn Treebank是第一个大型树形数据集，由华尔街日报三年（WSJ）收集的24,799个故事组成，它广泛用于句法注释。此外，它优于语义场景分割、注释和分类的替代方法。

然而，标准RNN并不能捕获语法短语的完整语法。在语法上解开RNN，也被称为成分矢量语法（CVG），这个方法是解决这个问题的一个重大升级。它使用语法解开的递归神经网络来学习句法语义和组合向量表示。该模型能够像标准RNN一样快速地进行训练和实施。

另一个演变是Matrix-Vector RNN，它能够捕获更长短语的组成含义。该模型为解析树中的每个节点分配一个向量和一个矩阵：向量用于捕获成分的固有含义，而矩阵捕获它如何改变相邻单词或短语的含义。而且该矩阵向量RNN可以在命题逻辑和自然语言中学习运算符的含义。

该模型在三个不同的实验中获得过不错的表示：
· 预测副词-形容词对的细粒度情感分布；
· 对电影评论的情感标签进行分类；
· 使用它们之间的句法路径对名词之间的语义关系（例如因果关系）进行分类。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728523343039565.png'/>
迄今为止用于情感分析的最强大的RNN模型是递归神经张量网络，其在每个节点处具有神经网络的树结构。该模型可用于边界分割，以确定哪些词组是积极的，哪些是消极的。

在Sentiment Treebank上接受训练时，该模型在几个指标上的表现优于所有以前的方法。
## 了解问答系统中涉及的NLP技术么？
问答（QA）系统的想法是直接从文档、对话、在线搜索和其他地方提取信息，以满足用户的信息需求。QA系统不是让用户阅读整个文档，而是更喜欢简短而简洁的答案。如今，QA系统可以非常容易地与其他NLP系统结合使用，并且一些QA系统甚至超越了对文本文档的搜索，并且可以从图片集合中提取信息。

事实上，大多数NLP问题都可以被视为一个问题回答问题。范例很简单：我们发出查询指令，机器提供响应。通过阅读文档或一组指令，智能系统应该能够回答各种各样的问题。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728507426066624.png'/>
强大的深度学习架构（称为动态内存网络（DMN））已针对QA问题进行了专门开发和优化。给定输入序列（知识）和问题的训练集，它可以形成情节记忆，并使用它们来产生相关答案。该体系结构具有以下组件：

· 语义内存模块（类似于知识库）被用来创建从输入句子的嵌入字序列预先训练手套载体。
· 输入模块处理与问题有关的输入矢量称为事实。该模块使用门控循环单元实现，GRU使网络能够了解当前正在考虑的句子是否相关或与答案无关。

· 问题模块逐字处理疑问词，并且使用输出相同权重的GRU输入模块的向量。事实和问题都被编码为嵌入。
· 情景记忆模块接收从输入中提取和编码的嵌入事实和问题载体。这使用了一个受大脑海马体启发的想法，它可以检索由某些反应触发的时间状态，如景点或声音。
· 答案生成模块，通过适当的响应，情景记忆应该包含回答问题所需的所有信息。该模块使用另一个GRU，使用正确序列的交叉熵错误分类进行训练，然后可以将其转换回自然语言。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728508087672647.png'/>
DMN不仅在质量保证方面做得非常好，而且在情感分析和词性标注方面也优于其他架构。自开发以来，动态内存网络已经有了重大改进，进一步提高其在问答环境中的准确性，包括：
· 用于视觉和文本问题的动态存储网络问答应用基本上是将DMN应用于图像，其内存和输入模块已升级，以便能够回答视觉问题。该模型改进了许多基准Visual Question Answering数据集的现有技术水平，而不支持事实监督。

· 用于问题应答的动态Coattention网络解决了从对应于不正确答案的局部最大值恢复的问题。它首先融合了问题和文件的共同依赖表示，以便集中于两 者的相关部分。然后，动态指向解码器迭代潜在的答案跨度，该迭代过程使模型能够从对应于不正确答案的初始局部最大值中恢复。
## 了解文本摘要中的NLP技术么？
人类很难手动汇总大型文本文档。文本摘要是NLP为源文档创建简短、准确和流畅的摘要问题。随着推送通知和文章摘要获得越来越多的注意力，为长文本生成智能且准确摘要的任务每天都在增长。

通过首先计算整个文本文档的单词频率来自动汇总文本。

然后，存储和排序100个最常用的单词。然后根据它包含的高频词数对每个句子进行评分，更高频率的词，价值更大。

最后，根据它们在原始文本中的位置来获取和排序前X个句子。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728501064833759.png'/>
文本摘要有两种基本方法：提取和抽象。前者从原始文本中提取单词和单词短语以创建摘要。后者是学习内部语言表示以生成更像人类的摘要，解释原始文本的意图。

提取摘要的方法是通过选择子集来工作。这是通过从实际文章中提取短语或句子以形成摘要来完成的，LexRank和TextRank是众所周知的摘要总结，它们都使用了Google PageRank算法的变体。
· LexRank是一种无监督的基于图形的算法，它使用IDF修改的余弦作为两个句子之间的相似性度量。该相似度用作两个句子之间的图形边缘的权重。LexRank还采用了智能后处理步骤，确保为摘要选择的顶级句子彼此不太相似。
· TextRank是一种类似于LexRank的算法，具有一些增强功能，例如使用词形化而不是词干，结合词性标注和命名实体分辨率，从文章中提取关键短语，以及根据这些短语提取摘要句子。除了文章摘要外，TextRank还从文章中提取了有意义的关键短语。

抽象概括的模型属于深度学习。使用深度学习的文本摘要已经取得了一定的突破。以下是一些NLP领域最大公司最显着的公布结果：
· Facebook的神经注意是一种神经网络架构，它利用基于本地注意力的模型，能够根据输入句子生成摘要中的每个单词。
· Google Brain的Sequence-to-Sequence模型遵循编码器-解码器架构。编码器负责读取源文档并将其编码为内部表示，解码器是一种语言模型，负责使用源文档的编码表示在输出摘要中生成每个单词。
· IBM Watson使用类似的序列到序列模型，但具有注意力和双向递归神经网络功能。
## 了解注意力机制么？
神经网络中的注意力机制是基于人类的视觉注意机制。人类的视觉注意力虽然存在不同的模型，但它们都基本上归结为能够以“高分辨率”聚焦于图像的某个区域，同时以“低分辨率”感知周围的图像，然后随着时间的推移调整焦点。

想象一下，你正在阅读一篇完整的文章：不是按顺序浏览每个单词或字符，而是潜意识地关注一些信息密度最高的句子并过滤掉其余部分。你的注意力有效地以分层方式捕获上下文信息，这样就可以在减少开销的同时做出决策。

那为什么这很重要？诸如LSTM和GRU之类的模型依赖于读取完整的句子并将所有信息压缩为固定长度的矢量。这需要基于文本统计属性的复杂特征工程，用几个单词表示的数百个单词的句子肯定会导致信息丢失，翻译不足等。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728485442341045.png'/>
通过注意力机制，我们不再尝试将全文编码为固定长度的矢量。相反，我们允许解码器在输出生成的每个步骤处理源语句的不同部分。我们让模型根据输入句子以及它到目前为止产生的内容来学习要注意的内容。

根据上面从基于注意力的神经机器翻译的有效方法的图像，蓝色表示编码器，红色表示解码器，因此我们可以看到上下文向量将所有单元格的输出作为输入来计算每个单元格的源语言单词的概率分布。解码器想要生成单个字，通过利用该机制，解码器可以捕获全局信息而不是仅基于一个隐藏状态进行推断。

除了机器翻译之外，注意力模型还可以处理各种其他NLP任务。在Show，Attend和Tell：使用视觉注意生成神经图像标题，作者将注意力机制应用于生成图像描述的问题。他们使用卷积神经网络对图像进行编码，使用具有注意力机制的递归神经网络来生成描述。通过可视化注意力，他们可以在生成单词时解释模型正在查看的内容：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728486099757508.png'/>
在语法作为外语中，作者使用具有注意力机制的递归神经网络来生成句子解析的树。可视化的注意力矩阵可以深入了解网络如何生成这些树：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728486624127618.png'/>
在阅读和理解的教学机器中，作者使用回归神经网络来阅读文本，阅读问题，然后产生答案。通过可视化关注矩阵，它们可以在尝试查找问题答案时显示网络的外观：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728487292365440.png'/>
然而，注意力机制需要付出代价。我们需要计算输入和输出字的每个组合的注意力值。如果你有一个100字的输入序列并生成一个100字的输出序列，那将是10,000个注意力值。如果你进行字符级计算并处理由数百个令牌组成的序列，上述机制可能变得非常昂贵。

自然语言处理已经解决的障碍
值得注意的是，研究人员不得不处理各种障碍：算法的局限性、模型的可扩展性、对人类语言的模糊理解。好消息是，这个领域的发展似乎是一个巨大的开源项目：研究人员不断构建更好的模型来解决现有问题并与社区分享他们的结果。

由于最近的学术研究进展，以下是NLP中已经解决的主要障碍：
· 没有单一的模型架构，跨任务具有一致的最新结果。例如，在Question Answering中，我们有强监督的端到端内存网络 ; 在情感分析中，我们有Tree-LSTM ; 在序列标记中，我们有双向LSTM-CRF。我之前在问题回答部分中提到的动态内存网络以某种方式解决了这一挑战，因为它可以在多个域中一致地运行。

· 机器学习中一种强大的方法是多任务学习，它共享相关任务之间的表示，以使模型能够更好地概括原始任务。然而，相关的多任务学习很难，因为它通常仅限于较低层，仅在任务相关时才有用，并且在所提出的模型中具有相同的解码器/分类器。在联合多任务模型中：为多个NLP任务增长，作者预先定义了一个由几个NLP任务组成的分层架构，作为多任务学习的联合模型。该模型包括字符n-gram和短路以及最先进的纯前馈解析器，能够执行依赖解析，多句子任务和联合训练。

· 另一个挑战是重复字表示的问题，其中模型中编码器和解码器的不同编码导致重复的参数/含义。对此最简单的解决方案是将单词向量联系在一起并联合训练单个权重，如“绑定单词向量” 和“单词分类器：语言建模的损失框架”中所示。

· 另一个障碍是，与诸如卷积神经网络或前馈神经网络相比，任何Deep NLP技术的基本构建块Recurrent Neural Networks相当慢。准递归神经网络采用RNN和CNN的最佳部分来提高训练速度，使用卷积跨越时间的并行性和跨越信道的并行性的元素级门控递归。这种方法比语言建模和情感分析中的任何其他模型更好，更快。

· 最后，在NLP中，架构搜索使用机器学习自动化人工神经网络设计的过程 非常缓慢，因为传统的手动过程需要大量的专业知识。如果我们可以使用AI为任何问题找到合适的架构怎么办？使用Google Brain进行强化学习的神经架构搜索是迄今为止开发的最可行的解决方案。作者使用循环网络生成神经网络的模型描述，并使用强化学习训练此RNN，以最大化验证集上生成的体系结构的预期准确性。
## 如何通俗理解Word2vec
小编注：考虑到下文（https://zhuanlan.zhihu.com/p/26306795）没有对比清楚2013年Mikolov原始论文与2014年Rong.X文章的区别，故建议初学者先看July在CSDN上写的Word2vec笔记，《如何通俗理解word2vec》：https://blog.csdn.net/v_JULY_v/article/details/102708459，更清晰、更易懂

1. 引子
大家好
我叫数据挖掘机
皇家布鲁斯特大学肄业
我喝最烈的果粒橙，钻最深的牛角尖
——执着如我

今天我要揭开Word2vec的神秘面纱
直窥其本质

相信我，这绝对是你看到的
最浅白易懂的 Word2vec 中文总结
（蛤？你问我为啥有这个底气？
且看下面，我的踩坑血泪史。。。）

2. Word2vec参考资料总结
(以下都是我踩过的坑，建议先跳过本节，阅读正文部分，读完全文回头再来看)

先大概说下我深挖 word2vec 的过程：先是按照惯例，看了 Mikolov 关于 Word2vec 的两篇原始论文，然而发现看完依然是一头雾水，似懂非懂，主要原因是这两篇文章省略了太多理论背景和推导细节；
然后翻出 Bengio 03年那篇JMLR和 Ronan 11年那篇JMLR，看完对语言模型、用CNN处理NLP任务有所了解，但依然无法完全吃透 word2vec；
这时候我开始大量阅读中英文博客，其中 北漂浪子 的一篇阅读量很多的博客吸引了我的注意，里面非常系统地讲解了 Word2vec 的前因后果，最难得的是深入剖析了代码的实现细节，看完之后细节方面了解了很多，不过还是觉得有些迷雾；
终于，我在 quora 上看到有人推荐 Xin Rong 的那篇英文paper，看完之后只觉醍醐灌顶，酣畅淋漓，相见恨晚，成为我首推的 Word2vec 参考资料。

下面我将详细列出我阅读过的所有 Word2vec 相关的参考资料，并给出评价

Mikolov 两篇原论文：
『Distributed Representations of Sentences and Documents』
在前人基础上提出更精简的语言模型（language model）框架并用于生成词向量，这个框架就是 Word2vec

『Efficient estimation of word representations in vector space』
专门讲训练 Word2vec 中的两个trick：hierarchical softmax 和 negative sampling
优点：Word2vec 开山之作，两篇论文均值得一读
缺点：只见树木，不见森林和树叶，读完不得要义。这里『森林』指 word2vec 模型的理论基础——即 以神经网络形式表示的语言模型，『树叶』指具体的神经网络形式、理论推导、hierarchical softmax 的实现细节等等

北漂浪子的博客：『深度学习word2vec 笔记之基础篇』
优点：非常系统，结合源码剖析，语言平实易懂
缺点：太啰嗦，有点抓不住精髓

Yoav Goldberg 的论文：『word2vec Explained- Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method』
优点：对 negative-sampling 的公式推导非常完备
缺点：不够全面，而且都是公式，没有图示，略显干枯

Xin Rong 的论文：『word2vec Parameter Learning Explained』：
！重点推荐！
理论完备由浅入深非常好懂，且直击要害，既有 high-level 的 intuition 的解释，也有细节的推导过程
一定要看这篇paper！一定要看这篇paper！一定要看这篇paper！
评论区 @huichan 告知了一条沉重的信息，Rong Xin 于2017年驾驶飞机失事，永远离开了我们。缅怀，R.I.P，愿他能在天堂继续开心地科研

来斯惟的博士论文『基于神经网络的词和文档语义向量表示方法研究』以及他的博客（网名：licstar）
可以作为更深入全面的扩展阅读，这里不仅仅有 word2vec，而是把词嵌入的所有主流方法通通梳理了一遍

几位大牛在知乎的回答：『word2vec 相比之前的 Word Embedding 方法好在什么地方？』
刘知远、邱锡鹏、李韶华等知名学者从不同角度发表对 Word2vec 的看法，非常值得一看

Sebastian 的博客：『On word embeddings - Part 2: Approximating the Softmax』
详细讲解了 softmax 的近似方法，Word2vec 的 hierarchical softmax 只是其中一种

3. 正文
你会在本文看到：
提纲挈领地讲解 word2vec 的理论精髓
学会用gensim训练词向量，并寻找相似词
你不会在本文看到

神经网络训练过程的推导
hierarchical softmax/negative sampling 等 trick 的理论和实现细节

3.1. 什么是 Word2vec?
在聊 Word2vec 之前，先聊聊 NLP (自然语言处理)。NLP 里面，最细粒度的是 词语，词语组成句子，句子再组成段落、篇章、文档。所以处理 NLP 的问题，首先就要拿词语开刀。

举个简单例子，判断一个词的词性，是动词还是名词。用机器学习的思路，我们有一系列样本(x,y)，这里 x 是词语，y 是它们的词性，我们要构建 f(x)->y 的映射，但这里的数学模型 f（比如神经网络、SVM）只接受数值型输入，而 NLP 里的词语，是人类的抽象总结，是符号形式的（比如中文、英文、拉丁文等等），所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec，就是词嵌入（ word embedding) 的一种。

我在前作『都是套路: 从上帝视角看透时间序列和数据挖掘』提到，大部分的有监督机器学习模型，都可以归结为：
f(x)->y

在 NLP 中，把 x 看做一个句子里的一个词语，y 是这个词语的上下文词语，那么这里的 f，便是 NLP 中经常出现的『语言模型』（language model），这个模型的目的，就是判断 (x,y) 这个样本，是否符合自然语言的法则，更通俗点说就是：词语x和词语y放在一起，是不是人话。

Word2vec 正是来源于这个思想，但它的最终目的，不是要把 f 训练得多么完美，而是只关心模型训练完后的副产物——模型参数（这里特指神经网络的权重），并将这些参数，作为输入 x 的某种向量化的表示，这个向量便叫做——词向量（这里看不懂没关系，下一节我们详细剖析）。

我们来看个例子，如何用 Word2vec 寻找相似词：

对于一句话：『她们 夸 吴彦祖 帅 到 没朋友』，如果输入 x 是『吴彦祖』，那么 y 可以是『她们』、『夸』、『帅』、『没朋友』这些词
现有另一句话：『她们 夸 我 帅 到 没朋友』，如果输入 x 是『我』，那么不难发现，这里的上下文 y 跟上面一句话一样
从而 f(吴彦祖) = f(我) = y，所以大数据告诉我们：我 = 吴彦祖（完美的结论）

3.2. Skip-gram 和 CBOW 模型
上面我们提到了语言模型

如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』
而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』

3.2.1 Skip-gram 和 CBOW 的简单情形
我们先来看个最简单的例子。上面说到， y 是 x 的上下文，所以 y 只取上下文里一个词语的时候，语言模型就变成：
用当前词 x 预测它的下一个词 y

但如上面所说，一般的数学模型只接受数值型输入，这里的 x 该怎么表示呢？ 显然不能用 Word2vec，因为这是我们训练完模型的产物，现在我们想要的是 x 的一个原始输入形式。

答案是：one-hot encoder

所谓 one-hot encoder，其思想跟特征工程里处理类别变量的 one-hot 一样（参考我的前作『数据挖掘比赛通用框架』、『深挖One-hot和Dummy背后的玄机』）。本质上是用一个只含一个 1、其他都是 0 的向量来唯一表示词语。

我举个例子，假设全世界所有的词语总共有 V 个，这 V 个词语有自己的先后顺序，假设『吴彦祖』这个词是第1个词，『我』这个单词是第2个词，那么『吴彦祖』就可以表示为一个 V 维全零向量、把第1个位置的0变成1，而『我』同样表示为 V 维全零向量、把第2个位置的0变成1。这样，每个词语都可以找到属于自己的唯一表示。

OK，那我们接下来就可以看看 Skip-gram 的网络结构了，x 就是上面提到的 one-hot encoder 形式的输入，y 是在这 V 个词上输出的概率，我们希望跟真实的 y 的 one-hot encoder 一样。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154955627342704968.jpg'/>

首先说明一点：隐层的激活函数其实是线性的，相当于没做任何处理（这也是 Word2vec 简化之前语言模型的独到之处），我们要训练这个神经网络，用反向传播算法，本质上是链式求导，在此不展开说明了，

当模型训练完后，最后得到的其实是神经网络的权重，比如现在输入一个 x 的 one-hot encoder: [1,0,0,…,0]，对应刚说的那个词语『吴彦祖』，则在输入层到隐含层的权重里，只有对应 1 这个位置的权重被激活，这些权重的个数，跟隐含层节点数是一致的，从而这些权重组成一个向量 vx 来表示x，而因为每个词语的 one-hot encoder 里面 1 的位置是不同的，所以，这个向量 vx 就可以用来唯一表示 x。

注意：上面这段话说的就是 Word2vec 的精髓！！

此外，我们刚说了，输出 y 也是用 V 个节点表示的，对应V个词语，所以其实，我们把输出节点置成 [1,0,0,…,0]，它也能表示『吴彦祖』这个单词，但是激活的是隐含层到输出层的权重，这些权重的个数，跟隐含层一样，也可以组成一个向量 vy，跟上面提到的 vx 维度一样，并且可以看做是词语『吴彦祖』的另一种词向量。而这两种词向量 vx 和 vy，正是 Mikolov 在论文里所提到的，『输入向量』和『输出向量』，一般我们用『输入向量』。

需要提到一点的是，这个词向量的维度（与隐含层节点数一致）一般情况下要远远小于词语总数 V 的大小，所以 Word2vec 本质上是一种降维操作——把词语从 one-hot encoder 形式的表示降维到 Word2vec 形式的表示。

3.2.2. Skip-gram 更一般的情形
上面讨论的是最简单情形，即 y 只有一个词，当 y 有多个词时，网络结构如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154955634355639125.jpg'/>

可以看成是 单个x->单个y 模型的并联，cost function 是单个 cost function 的累加（取log之后）
如果你想深入探究这些模型是如何并联、 cost function 的形式怎样，不妨仔细阅读参考资料4. 在此我们不展开。

3.2.3 CBOW 更一般的情形
跟 Skip-gram 相似，只不过:
Skip-gram 是预测一个词的上下文，而 CBOW 是用上下文预测这个词
网络结构如下
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154955642339133552.jpg'/>

更 Skip-gram 的模型并联不同，这里是输入变成了多个单词，所以要对输入处理下（一般是求和然后平均），输出的 cost function 不变，在此依然不展开，建议你阅读参考资料4.

3.3. Word2vec 的训练trick
相信很多初次踩坑的同学，会跟我一样陷入 Mikolov 那篇论文（参考资料1.）里提到的 hierarchical softmax 和 negative sampling 里不能自拔，但其实，它们并不是 Word2vec 的精髓，只是它的训练技巧，但也不是它独有的训练技巧。 Hierarchical softmax 只是 softmax 的一种近似形式（详见参考资料7.），而 negative sampling 也是从其他方法借鉴而来。

为什么要用训练技巧呢？ 如我们刚提到的，Word2vec 本质上是一个语言模型，它的输出节点数是 V 个，对应了 V 个词语，本质上是一个多分类问题，但实际当中，词语的个数非常非常多，会给计算造成很大困难，所以需要用技巧来加速训练。

这里我总结了一下这两个 trick 的本质，有助于大家更好地理解，在此也不做过多展开，有兴趣的同学可以深入阅读参考资料1.~7.

hierarchical softmax
本质是把 N 分类问题变成 log(N)次二分类

negative sampling
本质是预测总体类别的一个子集

3.4. 扩展
很多时候，当我们面对林林总总的模型、方法时，我们总希望总结出一些本质的、共性的东西，以构建我们的知识体系，比如我在前作『分类和回归的本质』里，原创性地梳理了分类模型和回归模型的本质联系，比如在词嵌入领域，除了 Word2vec之外，还有基于共现矩阵分解的 GloVe 等等词嵌入方法。

深入进去我们会发现，神经网络形式表示的模型（如 Word2vec），跟共现矩阵分解模型（如 GloVe），有理论上的相通性，这里我推荐大家阅读参考资料5. ——来斯惟博士在它的博士论文附录部分，证明了 Skip-gram 模型和 GloVe 的 cost fucntion 本质上是一样的。是不是一个很有意思的结论？ 所以在实际应用当中，这两者的差别并不算很大，尤其在很多 high-level 的 NLP 任务（如句子表示、命名体识别、文档表示）当中，经常把词向量作为原始输入，而到了 high-level 层面，差别就更小了。

鉴于词语是 NLP 里最细粒度的表达，所以词向量的应用很广泛，既可以执行词语层面的任务，也可以作为很多模型的输入，执行 high-level 如句子、文档层面的任务，包括但不限于：

计算相似度
寻找相似词
信息检索

作为 SVM/LSTM 等模型的输入
中文分词
命名体识别

句子表示
情感分析

文档表示
文档主题判别

4. 实战
上面讲了这么多理论细节，其实在真正应用的时候，只需要调用 Gensim （一个 Python 第三方库）的接口就可以。但对理论的探究仍然有必要，你能更好地知道参数的意义、模型结果受哪些因素影响，以及举一反三地应用到其他问题当中，甚至更改源码以实现自己定制化的需求。

这里我们将使用 Gensim 和 NLTK 这两个库，来完成对生物领域的相似词挖掘，将涉及：

解读 Gensim 里 Word2vec 模型的参数含义
基于相应语料训练 Word2vec 模型，并评估结果
对模型结果调优
语料我已经放出来了，可以关注我的公众号『数据挖掘机养成记』，并回复 Sherlocked 获取语料，包含5000行生物医学领域相关文献的摘要(英文)


我将在下一篇文章里详细讲解实战步骤，敬请关注本人公众号。友情建议：请先自行安装 Gensim 和 NLTK 两个库，并建议使用 jupyter notebook 作为代码运行环境
## 什么是词嵌入word embedding?
Embedding在数学上表示一个maping, f: X -> Y， 也就是一个function，其中该函数是injective（就是我们所说的单射函数，每个Y只有唯一的X对应，反之亦然）和structure-preserving (结构保存，比如在X所属的空间上X1 < X2,那么映射后在Y所属空间上同理 Y1 < Y2)。那么对于word embedding，就是将单词word映射到另外一个空间，其中这个映射具有injective和structure-preserving的特点。

通俗的翻译可以认为是单词嵌入，就是把X所属空间的单词映射为到Y空间的多维向量，那么该多维向量相当于嵌入到Y所属空间中，一个萝卜一个坑。

word embedding，就是找到一个映射或者函数，生成在一个新的空间上的表达，该表达就是word representation。
推广开来，还有image embedding, video embedding, 都是一种将源数据映射到另外一个空间。

更多解释见：https://www.zhihu.com/question/32275069#130283448
## 了解NLP神经网络的发展历史么？
本文主要的内容如下：
2001 – 神经语言模型
2008 – 多任务学习
2013 – 词嵌入
2013 – NLP 神经网络
2014 – sequence-to-sequence 模型
2015 – 注意力机制
2015 – 基于记忆的网络
2018 – 预训练语言模型
其他的里程碑事件

传统算法里程碑事件
2001 – 神经语言模型
语言建模任务指的是给定前一个单词去预测文本中的下一个单词。它可能是比较简单的语言处理任务，具体的实际应用场景包括 智能键盘 、电子邮件回复建议（Kannan 等人， 2016）、拼写自动更正等。正如很多人所知，语言建模有着丰富的历史。其中比较经典的方法基于 n-grams ，并使用平滑处理不可见的 n-grams（Kneser & Ney, 1995）。

第一个神经语言模型是 Bengio 等人在 2001 年提出的前馈神经网络，如图 1 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728399726069727.png'/>图 1：前馈神经网络语言模型（Bengio 等，2001；2003）

这个模型将从表 C 中查找到的 n 个单词作为输入向量表征。这种向量被现在的学者们称做“词嵌入”。这些词嵌入级联后被输入到一个隐藏层中，该隐藏层的输出又被输入到 softmax 层。更多关于模型的信息，请看 这篇文章 。

最近，在语言建模技术方面，前馈神经网络被循环神经网络（RNNs；Mikolov 等人，2010）和长短时记忆网络（LSTMs；格雷夫斯，2013）所取代。尽管近年来提出了许多扩展经典 LSTM 的新语言模型（请参阅 本页 以获得概述），经典的 LSTM 仍然作为一个强大的基线存在着（Melis 等人， 2018）。甚至 Bengio 等人的经典前馈神经网络在某些情况下也可以与更复杂的模型一较高下，因为这些模型通常只会考虑距离较近的单词（Daniluk 等人， 2017）。因此，如何更好地理解这种语言模型所捕获的信息也是一个比较热门的研究领域（Kuncoro 等人， 2018；布莱文斯等人，2018 年）。

语言建模通常是应用 RNN 时的第一步，对于这一点大家已经形成了共识。许多人是通过 Andrej 的博客 文章第一次接触到语言建模的。语言建模是一种非监督学习形式，Yann LeCun 也将其称为预测性学习，并将其作为获得基础常识的先决条件（ 参见 NIPS 2016 年的幻灯片）。语言建模最引人关注的一点是，尽管它很简单，但却是本文后面讨论的许多技术发展的核心：
词嵌入：word2vec 的目标是简化语言建模。
sequence-to-sequence 模型：这种模型通过一次预测一个单词生成一个输出序列。
预训练语言模型：这些方法使用来自语言模型的表述进行迁移学习。

反过来讲，这意味着近年来 NLP 的许多重要进展都可以归结为某些形式的语言建模。为了“真正”理解自然语言，仅仅从文本的原始形式中学习是不够的。我们需要新的方法和模型。


2008 – 多任务学习
多任务学习是在多个任务上训练的模型之间共享参数的一种通用方法。在神经网络中，可以通过给不同层施以不同的权重，来很容易地实现多任务学习。多任务学习的概念最初由 Rich Caruana 在 1993 年提出，并被应用于道路跟踪和肺炎预测（Caruana, 1998）。直观地说，多任务学习鼓励模型学习对许多任务有用的表述。这对于学习一般的、低级的表述形式、集中模型的注意力或在训练数据有限的环境中特别有用。想要更全面地了解多任务学习，请看 这篇文章 。

在 2008 年，Collobert 和 Weston 将多任务学习首次应用于 NLP 的神经网络。在他们的模型中，查询表（或单词嵌入矩阵）在两个接受不同任务训练的模型之间共享，如下面的图 2 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728400924094710.png'/>图 2：单词嵌入矩阵的共享（Collobert & Weston, 2008 年；Collobert 等人，2011）

词嵌入的共享，使得模型能够在词嵌入矩阵中协作和共享一般的低级信息，而这些低级信息所占的参数量往往是模型中最大的一部分。2008 年，Collobert 和 Weston 共同撰写的论文对多任务学习之外的其他应用还产生了一定的影响。它率先提出了一些想法，如对文字嵌入进行预训练以及使用卷积神经网络（CNNs）来处理文本数据。它获得了 ICML 2018 年的经典论文奖 （ 参见 本文的经典论文奖演讲）。

多任务学习现在被广泛地用于 NLP 任务。充分利用现有的或“人造”的任务进行训练，可以更好的提高 NLP 效率。有关不同辅助任务的概述，请看 这篇文章 。虽然参数的共享通常是预定义的，但是在优化过程中也可以学习到不同的共享模式（Ruder 等人， 2017）。随着对多任务模型泛化能力的评估，多任务学习越来越重要，最近还提出了多任务学习的专用标准（Wang 等人， 2018；McCann 等人，2018 年）。


2013 – 词嵌入
用稀疏向量表示文本，即所谓的 词袋模型 在 NLP 有着悠久的历史。正如上文中介绍的，早在 2001 年就开始使用密集向量表示词或词嵌入。Mikolov 等人在 2013 年提出的创新技术是通过去除隐藏层，逼近目标，进而使这些单词嵌入的训练更加高效。虽然这些技术变更本质上很简单，但它们与高效的 word2vec 配合使用，便能使大规模的词嵌入训练成为可能。

Word2vec 有两种风格，如下面的图 3 所示：连续字袋（CBOW）和 skip-gram。不过他们的目标不同：一个是根据周围的单词预测中心单词，而另一个则相反。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728401749507875.png'/>图 3：连续字袋和 skip-gram 架构（Mikolov 等人， 2013a；2013 b）

虽然这些嵌入在概念上与使用前馈神经网络学习的嵌入在概念上没有区别，但是在一个非常大的语料库上训练之后，它们就能够捕获诸如性别、动词时态和国家 – 首都关系等单词之间的特定关系，如下图 4 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728402413484256.png'/>图 4：word2vec（Mikolov 等人， 2013a；2013 b）

这些词语间关系的获得及其背后的意义引发了人们对嵌入技术的兴趣——人们开始大量研究这些线性关系形成的原理（Arora 等人， 2016；Mimno & Thompson, 2017；Antoniak & Mimno，2018 年；Wendlandt 等人，2018 年）。然而，推动词嵌入技术成为当前 NLP 的主流技术的却不是这些原理，而是在初始化时使用预训练的嵌入，因为这样做可以提高下游任务的性能。

虽然 word2vec 捕捉到的词间关系很直观、质量高得甚至有些神奇，但后来的研究表明，word2vec 本身并没有什幺特别之处：词嵌入也可以通过矩阵分解来学习（Pennington 等人，2014）；通过适当的调优，SVD 和 LSA 等经典的矩阵分解方法也得到了类似的结果（Levy 等人， 2015）。

从那以后，人们开始投入大量的精力去探索词嵌入的各个方面（从 原文引用的惊人数量 可以看出）。通过 这篇文章 ，我们可以看出一些趋势和未来的方向。尽管有许多发展进步，但到现在为止，word2vec 仍然是大众的首选。对 Word2vec 的使用范围已经不限于单词级别了：基于局部上下文学习嵌入的简单目标——带负抽样的 skip-gram 已被用于学习句子表示（Mikolov & Le, 2014；Kiros 等人，2015）。Word2vec 甚至还在网络（Grover & Leskovec, 2016）和生物序列（Asgari & Mofrad, 2015）等其他应用场景中发挥了作用。

一个比较有研究价值的技术方向是将不同语言的词嵌入到同一个空间中，以实现（零样本）跨语言迁移。以一种完全不受监督的方式（至少对于类似的语言来说）学习数据以实现一个好的推测效果变得越来越有可能（Conneau 等人，2018 年；Artetxe 等人，2018 年；Søgaard 等人，2018）。这种学习方式可被应用于语言资源缺乏的无监督机器翻译系统中（Lample 等人，2018;；Artetxe 等人，2018）。查看（Ruder 等人， 2018）以获得概述。


2013 – NLP 神经网络
2013 年和 2014 年是 NLP 问题开始引入神经网络模型的时期。使用最广泛的三种主要的神经网络是：循环神经网络、卷积神经网络和递归神经网络。

循环神经网络（RNNs）循环神经网络是处理 NLP 中普遍存在的动态输入序列的一个最佳的技术方案。Vanilla RNNs（Elman, 1990）很快被经典的长 – 短期记忆网络（Hochreiter & Schmidhuber，1997）所取代，它被证明 对消失和爆炸梯度问题更有弹性 。在 2013 年之前，RNN 仍被认为很难训练； Ilya Sutskever 的博士论文 为改变这种现状提供了一个关键性的例子。下面的图 5 对 LSTM 单元进行了可视化显示。双向 LSTM（Graves 等人， 2013）通常用于处理左右两边的上下文。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728403223620244.png'/>图 5：LSTM 网络（来源：Chris Olah）

卷积神经网络（CNNs） 卷积神经网络本来是广泛应用于计算机视觉领域的技术，现在也开始应用于语言（Kalchbrenner 等人， 2014；Kim 等人，2014）。文本的卷积神经网络只在两个维度上工作，其中滤波器（卷积核）只需要沿着时间维度移动。下面的图 6 显示了 NLP 中使用的典型 CNN。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728404093234745.png'/>图 6：文本卷积神经网络（Kim, 2014）

卷积神经网络的一个优点是它们比 RNN 更可并行化，因为其在每个时间步长的状态只依赖于本地上下文（通过卷积运算），而不是像 RNN 那样依赖过去所有的状态。使用膨胀卷积，可以扩大 CNN 的感受野，使网络有能力捕获更长的上下文（Kalchbrenner 等人， 2016）。CNN 和 LSTM 可以组合和叠加（Wang 等人， 2016），卷积也可以用来加速 LSTM（Bradbury 等人， 2017）。

递归神经网络RNN 和 CNN 都将语言视为一个序列。然而，从语言学的角度来看，语言本质上是 层次化的 ：单词被组合成高阶短语和从句，这些短语和从句本身可以根据一组生产规则递归地组合。将句子视为树而不是序列的语言学启发思想产生了递归神经网络（Socher 等人， 2013），如下图 7 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728404734327836.png'/>图 7：递归神经网络（Socher 等人， 2013）

递归神经网络从下到上构建序列的表示，这一点不同于从左到右或从右到左处理句子的 RNN。在树的每个节点上，通过组合子节点的结果来计算新的结果。由于树也可以被视为在 RNN 上强加不同的处理顺序，所以 LSTM 自然地也被扩展到树上（Tai 等人， 2015）。

RNN 和 LSTM 可以扩展到使用层次结构。单词嵌入不仅可以在本地学习，还可以在语法语境中学习（Levy & Goldberg, 2014）；语言模型可以基于句法堆栈生成单词（Dyer 等人， 2016）；图卷积神经网络可以基于树结构运行（Bastings 等人， 2017）。


2014 – sequence-to-sequence 模型
2014 年，Sutskever 等人提出了 sequence-to-sequence 模型。这是一个使用神经网络将一个序列映射到另一个序列的通用框架。在该框架中，编码器神经网络逐符号处理一个句子，并将其压缩为一个向量表示；然后，一个解码器神经网络根据编码器状态逐符号输出预测值，并将之前预测的符号作为每一步的输入，如下图 8 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728405469075559.png'/>图 8：sequence-to-sequence 模型（Sutskever 等人， 2014）

机器翻译是对这个框架比较成功的应用。2016 年，谷歌宣布将开始用神经 MT 模型取代基于单片短语的 MT 模型（Wu 等人， 2016）。 根据 Jeff Dean 的说法 ，这意味着用 500 行神经网络模型替换 50 万行基于短语的 MT 代码。

由于其灵活性，这个框架现在是自然语言生成任务的首选框架，其中不同的模型承担了编码器和解码器的角色。重要的是，解码器模型不仅可以解码一个序列，而且可以解码任意表征。例如，可以基于图像生成标题（Vinyals 等人， 2015）（如下图 9 所示）、基于表生成文本（Lebret 等人， 2016）和基于应用程序中源代码更改描述（Loyola 等人， 2017）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728406157980794.png'/>图 9：基于图像生成标题（Vinyals 等人，2015）

sequence-to-sequence 学习甚至可以应用于 NLP 中输出具有特定结构的结构化预测任务。为了简单起见，输出被线性化，如下面的图 10 所示，用于进行选区解析。神经网络已经证明了在有足够数量的训练数据进行选区分析（Vinyals 等人，2015）和命名实体识别（Gillick 等人， 2016）的情况下，直接学习可以产生这种线性化输出的能力。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728407015071412.png'/>图 10：线性化选区解析树（Vinyals 等人，2015）

序列和解码器的编码器通常基于 RNN，但可以使用其他模型类型。新的体系结构主要来源于 MT 的贡献，它是 sequence-to-sequence 模型体系结构的主要开发者。最新的模型有 deep LSTMs（Wu 等人，2016；tional encoders 、Kalchbrenner 等人，2016；Gehring 等人， Transformer 、Vaswani 等人，2017）和 LSTM 与 Transformer 的结合体（Chen 等人， 2018）。


2015 – 注意力机制
注意力机制（Bahdanau 等人， 2015）是神经网络机器翻译（NMT）的核心创新之一，也是使 NMT 模型胜过经典的基于短语的 MT 系统的关键思想。sequence-to-sequence 模型的主要瓶颈是需要将源序列的全部内容压缩为一个固定大小的向量。注意力机制通过允许解码器回头查看源序列隐藏状态来缓解这一问题，然后将其加权平均作为额外输入提供给解码器，如下面的图 11 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728407872231977.png'/>
图 11：Attention（Bahdanau 等人， 2015）

注意力机制有很多不同的形式（Luong 等人，2015）。 这里  有一个简短的概述。注意力机制广泛适用于任何需要根据输入的特定部分做出决策的任务，并且效果不错。它已被应用于一致性解析（Vinyals 等人，2015）、阅读理解（Hermann 等人，2015）和一次性学习（Vinyals 等人，2016）等诸多领域。输入甚至不需要是一个序列，即可以包含其他表示，如图像字幕（Xu 等人， 2015），如下图 12 所示。注意力机制的一个额外的功能是，它提供了一种少见的功能，我们可以通过检查输入的哪些部分与基于注意力权重的特定输出相关来了解模型的内部工作方式。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728409185148843.png'/>图 12：图像字幕模型中的视觉注意力，预测模型在生成“飞盘”时所关注的内容。（Xu 等人， 2015）

注意力机制也不仅仅局限于观察输入序列；可以使用 self-attention 查看句子或文档中的周围单词，以获得更佳的上下文敏感的词表示。Transformer 架构的核心是多层次的自我关注（Vaswani 等人， 2017），这是目前 NMT 最先进的模型。


2015 – 基于记忆的网络
注意力机制可以看作是模糊记忆的一种形式。记忆由模型的隐藏状态组成，模型选择从记忆中检索内容。想要更详细地了解注意力及其与记忆的联系，请看 这篇文章 。研究者们提出了许多具有更明确记忆的模型。这些模型有不同的变体，如神经图灵机（Graves 等 ，2014）、记忆网络（Weston 等 ，2015）和端到端记忆网络（Sukhbaatar 等，2015）、动态记忆网络（Kumar 等 ，2015）、神经微分计算机（Graves 等，2016）和循环实体网络（Henaff 等，2017）。

记忆的访问通常基于与当前状态的相似度，类似于注意力，通常可以写入和读取。模型在如何实现和利用内存方面有所不同。例如，端到端记忆网络多次处理输入，并更新记忆以实现多个推理步骤。神经图灵机也有一个基于位置的寻址，这允许他们学习简单的计算机程序，如排序。基于记忆的模型通常应用于一些特定任务中，如语言建模和阅读理解。在这些任务中，长时间保存信息应该很有用。记忆的概念是非常通用的：知识库或表可以充当记忆，而记忆也可以根据整个输入或它的特定部分填充。


2018 – 预训练语言模
预训练的词嵌入与上下文无关，仅用于初始化模型中的第一层。最近几个月，一系列监督型任务被用于神经网络的预训练（Conneau 等人，2017；McCann 等人，2017；Subramanian 等人，2018 年）。相反，语言模型只需要无标签的文本；因此，训练可以扩展到数十亿个令牌、新域和新语言。预训练语言模型于 2015 年被首次提出（Dai & Le, 2015）；直到最近，它们才被证明在各种任务中效果还是不错的。语言模型嵌入可以作为目标模型中的特征（Peters 等人，2018 年），或者使用语言模型对目标任务数据进行微调（Ramachandran 等人，2017 年；霍华德 & 鲁德出版社，2018 年）。添加语言模型嵌入可以在许多不同的任务中提供比最先进的技术更大的改进，如下面的图 13 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728409864861426.png'/>图 13：嵌入到最先进的语言模型中的改进（Peters 等人，2018）

预训练的语言模型已经被证明可以用更少的数据进行学习。由于语言模型只需要无标记的数据，因此对于标记数据稀缺的低资源语言尤其有用。有关预训练语言模型潜力的更多信息， 请参阅本文 。

其他里程碑事件
其他一些技术发展没有上面提到的那样流行，但仍然有广泛的影响。

基于字符的表示在字符上使用 CNN 或 LSTM 以获得基于字符的词表示的做法现在相当普遍，特别是对于形态信息重要或有许多未知单词的丰富的语言和任务，效果更加明显。据我所知，序列标签使用基于字符的表示（Lample 等人，2016；普兰克等人，2016），可以减轻在计算成本增加的情况下必须处理固定词汇表的需要，并支持完全基于字符的 NMT（Ling 等人， 2016；Lee 等人，2017）。

对抗学习对抗学习方法已经在 ML 领域掀起了风暴，在 NLP 中也有不同形式的应用。对抗性的例子越来越被广泛使用，它不仅是作为一种工具来探究模型和理解它们的失败案例，而且也使自身更加鲁棒（Jia & Liang， 2017）。（虚拟）对抗性训练，即最坏情况扰动（Miyato 等人，2017）和领域对抗性损失（Ganin 等人， 2016；Kim 等人，2017），同样可以使模型更加鲁棒。生成对抗网络（GANs）对于自然语言生成还不是很有效（Semeniuta 等人， 2018），但在匹配分布时很有用（Conneau 等人， 2018）。

强化学习强化学习已被证明对具有时间依赖性的任务有效，例如在训练期间选择数据（Fang 等人， 2017；Wu 等人， 2018）和建模对话（Liu 等人， 2018）。RL 对于直接优化不可微的末端度量（如 ROUGE 或 BLEU）也有效，反而在汇总中优化替代损失（如交叉熵）（Paulus 等人， 2018；Celikyilmaz 等人，2018）和机器翻译场景效果就不明显了（Ranzato 等人，2016）。类似地，逆向强化学习在过于复杂而无法指定数据的情况下也很有用，比看图说话任务（Wang 等人， 2018）。

非神经网络算法的里程碑事件
在 1998 年和接下来的几年里， FrameNet 项目诞生了（Baker 等人， 1998），这指导了 语义角色 标注 的任务。这是一种浅语义解析的形式，至今仍在积极研究开发中。在本世纪初，与自然语言学习会议（CoNLL）一起组织的共享任务促进了核心 NLP 任务的研究，如组块（Tjong Kim Sang 等人， 2000）、命名实体识别（Tjong Kim Sang 等人， 2003）和依赖解析（Buchholz 等人， 2006）等。许多 CoNLL 共享任务数据集现在仍然被用作评估的标准。

2001 年，条件随机场（CRF；Lafferty 等人， 2001）成为了最具影响力的序列标注方法类别之一，获得了 ICML 2011 的最佳论文奖 。CRF 层是当前最先进的序列标注问题模型的核心部分，这些模型具有标签间的相互依赖性，如命名实体识别（Lample 等，2016）。

2002 年，双语互译质量评估辅助工具（BLEU；Papineni 等人，2002）给出了双语互译质量度量标准，这使得 MT 系统得以扩展。其现在仍然是 MT 评估的标准度量标准。同年，结构感知机（Collins，2002）问世，为结构化感知工作奠定了基础。在同一次会议上，情感分析也成了最受欢迎和广泛研究的 NLP 任务之一（Pang 等人， 2002）。这三篇论文都获得了 2018 年 NAACL 最佳论文奖 。

2003 年引入了潜在狄利克雷分配（LDA；Blei 等人，2003），这是机器学习中应用最广泛的技术之一，至今仍是主题建模的标准方法。在 2004 年，有学者提出了比 SVM 更适合于捕获结构化数据中的相关性的新最大边缘模型（Taskar 等人， 2004a；2004b）。

2006 年，OntoNotes（Hovy 等人， 2006）介绍了一个具有多个注释和高注释协议的大型多语言语料库。OntoNotes 已被用于训练和评估各种任务，如依赖解析和引用解析。Milne 和 Witten（2008）在 2008 年介绍了利用维基百科丰富机器学习方法的方案。到目前为止，Wikipedia 是训练 ML 方法最有用的资源之一，无论是用于实体链接和消除歧义、语言建模、知识库还是其他各种任务。

2009 年，提出了远程监督的概念（Mintz 等人， 2009）。远程监督利用启发式或现有知识库中的信息生成带有噪声的模式，可用于从大型语料库中自动提取示例。远程监督现已被广泛应用，并且已经是关系提取、信息提取、情感分析等领域的常用技术。

英文原文： A Review of the Neural History of Natural Language Processing
## 判别式（discriminative）模型和生成式(generative)模型的核心区别是什么？
本题解析来源：https://www.zhihu.com/question/35866596/answer/236886066

在监督学习下，模型可以分为判别式模型与生成式模型。

根据经验，A批模型（神经网络模型、SVM、perceptron、LR、DT……）与B批模型（NB、LDA……），有啥区别不？（这个问题需要一些模型使用经验）应该是这样的：
1. A批模型是这么工作的，他们直接将数据的Y（或者label），根据所提供的features，学习，最后画出了一个明显或者比较明显的边界（具体怎么做到的？通过复杂的函数映射，或者决策叠加等等mechanism），这一点线性LR、线性SVM应该很明显吧。 

2. B批模型是这么工作的，他们先从训练样本数据中，将所有的数据的分布情况摸透，然后最终确定一个分布，来作为我的所有的输入数据的分布，并且他是一个联合分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035990526332970.svg'/>   (注意<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035991554313137.svg'/>包含所有的特征<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035992531381823.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035993717483536.svg'/>包含所有的label)。然后我来了新的样本数据（inference），好，通过学习来的模型的联合分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035995664047739.svg'/>，再结合新样本给的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035997142504104.svg'/>，通过条件概率就能出来<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036000361353863.svg'/>：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036001491062177.svg'/>

1. 判别式模型
那么A批模型对应了判别式模型。根据上面的两句话的区别，可以知道判别模型的特征了，所以有句话说：判别模型是直接对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036003810017449.svg'/>建模，就是说，直接根据X特征来对Y建模训练。

具体地，我的训练过程是确定构件  模型里面“复杂映射关系”中的参数，完了再去inference一批新的sample。

所以判别式模型的特征总结如下：
1) 对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036005749731553.svg'/>建模
2) 对所有的样本只构建一个模型，确认总体判别边界
3) 观测到输入什么特征，就预测最可能的label
4) 另外，判别式的优点是：对数据量要求没生成式的严格，速度也会快，小数据量下准确率也会好些。

2. 生成式模型
同样，B批模型对应了生成式模型。并且需要注意的是，在模型训练中，我学习到的是X与Y的联合模型<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036009086309819.svg'/>，也就是说，我在训练阶段是只对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036010615518794.svg'/>建模，我需要确定维护这个联合概率分布的所有的信息参数。完了之后在inference再对新的sample计算<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036015019525080.svg'/>，导出<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036016480767790.svg'/> ,但这已经不属于建模阶段了。

结合NB过一遍生成式模型的工作流程。学习阶段，建模：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415703601835560797.svg'/>（当然，NB具体流程去隔壁参考）,然后<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036022766630471.svg'/>。
另外，LDA也是这样，只是他更过分，需要确定很多个概率分布，而且建模抽样都蛮复杂的。

所以生成式总结下有如下特点：
① 对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036024830103287.svg'/>建模
② 这里我们主要讲分类问题，所以是要对每个label（<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036026167421345.svg'/>）都需要建模，最终选择最优概率的label为结果，所以没有什么判别边界。（对于序列标注问题，那只需要构件一个model）
③ 中间生成联合分布，并可生成采样数据。
④ 生成式模型的优点在于，所包含的信息非常齐全，我称之为“上帝信息”，所以不仅可以用来输入label，还可以干其他的事情。生成式模型关注结果是如何产生的。但是生成式模型需要非常充足的数据量以保证采样到了数据本来的面目，所以速度相比之下，慢。

这一点明白后，后面讲到的HMM与CRF的区别也会非常清晰。
最后identity the picture below：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036028335909703.jpg'/>
## 如何通俗理解隐马尔可夫模型HMM？
作者：Yang Eninala​，杜克大学 生物化学博士
链接：https://www.zhihu.com/question/20962240

隐马尔可夫（HMM）好讲，简单易懂不好讲。我认为 @者也的回答没什么错误，不过我想说个更通俗易懂的例子。我希望我的读者不是专家，而是对这个问题感兴趣的入门者，所以我会多阐述数学思想，少写公式。霍金曾经说过，你多写一个公式，就会少一半的读者。所以时间简史这本关于物理的书和麦当娜关于性的书卖的一样好。我会效仿这一做法，写最通俗易懂的答案。

还是用最经典的例子，掷骰子。

假设我手里有三个不同的骰子。
第一个骰子是我们平常见的骰子（称这个骰子为D6），6个面，每个面（1，2，3，4，5，6）出现的概率是1/6。
第二个骰子是个四面体（称这个骰子为D4），每个面（1，2，3，4）出现的概率是1/4。
第三个骰子有八个面（称这个骰子为D8），每个面（1，2，3，4，5，6，7，8）出现的概率是1/8。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415496212405785727.jpg'/>

假设我们开始掷骰子，我们先从三个骰子里挑一个，挑到每一个骰子的概率都是1/3。然后我们掷骰子，得到一个数字，1，2，3，4，5，6，7，8中的一个。不停的重复上述过程，我们会得到一串数字，每个数字都是1，2，3，4，5，6，7，8中的一个。例如我们可能得到这么一串数字（掷骰子10次）：1 6 3 5 2 7 3 5 2 4

这串数字叫做可见状态链。但是在隐马尔可夫模型中，我们不仅仅有这么一串可见状态链，还有一串隐含状态链。在这个例子里，这串隐含状态链就是你用的骰子的序列。比如，隐含状态链有可能是：D6 D8 D8 D6 D4 D8 D6 D6 D4 D8

一般来说，HMM中说到的马尔可夫链其实是指隐含状态链，因为隐含状态（骰子）之间存在转换概率（transition probability）。在我们这个例子里，D6的下一个状态是D4，D6，D8的概率都是1/3。D4，D8的下一个状态是D4，D6，D8的转换概率也都一样是1/3。这样设定是为了最开始容易说清楚，但是我们其实是可以随意设定转换概率的。比如，我们可以这样定义，D6后面不能接D4，D6后面是D6的概率是0.9，是D8的概率是0.1。这样就是一个新的HMM。

同样的，尽管可见状态之间没有转换概率，但是隐含状态和可见状态之间有一个概率叫做输出概率（emission probability）。就我们的例子来说，六面骰（D6）产生1的输出概率是1/6。产生2，3，4，5，6的概率也都是1/6。我们同样可以对输出概率进行其他定义。比如，我有一个被赌场动过手脚的六面骰子，掷出来是1的概率更大，是1/2，掷出来是2，3，4，5，6的概率是1/10。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415496317172103652.jpeg'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963177265294242.jpeg'/>

其实对于HMM来说，如果提前知道所有隐含状态之间的转换概率和所有隐含状态到所有可见状态之间的输出概率，做模拟是相当容易的。但是应用HMM模型时候呢，往往是缺失了一部分信息的，有时候你知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；有时候你只是看到了很多次掷骰子的结果，剩下的什么都不知道。如果应用算法去估计这些缺失的信息，就成了一个很重要的问题。这些算法我会在下面详细讲。

回到正题，和HMM模型相关的算法主要分为三类，分别解决三种问题：
1）知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道每次掷出来的都是哪种骰子（隐含状态链）。这个问题呢，在语音识别领域呢，叫做解码问题。这个问题其实有两种解法，会给出两个不同的答案。每个答案都对，只不过这些答案的意义不一样。
第一种解法求最大似然状态路径，说通俗点呢，就是我求一串骰子序列，这串骰子序列产生观测结果的概率最大。
第二种解法呢，就不是求一组骰子序列了，而是求每次掷出的骰子分别是某种骰子的概率。比如说我看到结果后，我可以求得第一次掷骰子是D4的概率是0.5，D6的概率是0.3，D8的概率是0.2。
第一种解法我会在下面说到，但是第二种解法我就不写在这里了，如果大家有兴趣，我们另开一个问题继续写吧。

2）还是知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道掷出这个结果的概率。看似这个问题意义不大，因为你掷出来的结果很多时候都对应了一个比较大的概率。问这个问题的目的呢，其实是检测观察到的结果和已知的模型是否吻合。如果很多次结果都对应了比较小的概率，那么就说明我们已知的模型很有可能是错的，有人偷偷把我们的骰子給换了。

3）知道骰子有几种（隐含状态数量），不知道每种骰子是什么（转换概率），观测到很多次掷骰子的结果（可见状态链），我想反推出每种骰子是什么（转换概率）。这个问题很重要，因为这是最常见的情况。很多时候我们只有可见结果，不知道HMM模型里的参数，我们需要从可见结果估计出这些参数，这是建模的一个必要步骤。

问题阐述完了，下面就开始说解法。（0号问题在上面没有提，只是作为解决上述问题的一个辅助）

0.一个简单问题
其实这个问题实用价值不高。由于对下面较难的问题有帮助，所以先在这里提一下。

知道骰子有几种，每种骰子是什么，每次掷的都是什么骰子，根据掷骰子掷出的结果，求产生这个结果的概率。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963191446207141.jpeg'/>

解法无非就是概率相乘：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963195175430992.png'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963195950738756.png'/>

1.看见不可见的，破解骰子序列
这里我说的是第一种解法，解最大似然路径问题。

举例来说，我知道我有三个骰子，六面骰，四面骰，八面骰。我也知道我掷了十次的结果（1 6 3 5 2 7 3 5 2 4），我不知道每次用了那种骰子，我想知道最有可能的骰子序列。

其实最简单而暴力的方法就是穷举所有可能的骰子序列，然后依照第零个问题的解法把每个序列对应的概率算出来。然后我们从里面把对应最大概率的序列挑出来就行了。如果马尔可夫链不长，当然可行。如果长的话，穷举的数量太大，就很难完成了。

另外一种很有名的算法叫做Viterbi algorithm. 要理解这个算法，我们先看几个简单的列子。
首先，如果我们只掷一次骰子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963210314993492.jpeg'/>

看到结果为1.对应的最大概率骰子序列就是D4，因为D4产生1的概率是1/4，高于1/6和1/8.
把这个情况拓展，我们掷两次骰子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963215041149237.jpeg'/>

结果为1，6.这时问题变得复杂起来，我们要计算三个值，分别是第二个骰子是D6，D4，D8的最大概率。显然，要取到最大概率，第一个骰子必须为D4。这时，第二个骰子取到D6的最大概率是
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963245064702512.png'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963247286074053.png'/>

同上，我们可以计算第三个骰子是D6或D8时的最大概率。我们发现，第三个骰子取到D4的概率最大。而使这个概率最大时，第二个骰子为D6，第一个骰子为D4。所以最大概率骰子序列就是D4 D6 D4。

写到这里，大家应该看出点规律了。既然掷骰子一二三次可以算，掷多少次都可以以此类推。我们发现，我们要求最大概率骰子序列时要做这么几件事情。首先，不管序列多长，要从序列长度为1算起，算序列长度为1时取到每个骰子的最大概率。然后，逐渐增加长度，每增加一次长度，重新算一遍在这个长度下最后一个位置取到每个骰子的最大概率。因为上一个长度下的取到每个骰子的最大概率都算过了，重新计算的话其实不难。当我们算到最后一位时，就知道最后一位是哪个骰子的概率最大了。然后，我们要把对应这个最大概率的序列从后往前推出来。

2.谁动了我的骰子？
比如说你怀疑自己的六面骰被赌场动过手脚了，有可能被换成另一种六面骰，这种六面骰掷出来是1的概率更大，是1/2，掷出来是2，3，4，5，6的概率是1/10。你怎么办么？答案很简单，算一算正常的三个骰子掷出一段序列的概率，再算一算不正常的六面骰和另外两个正常骰子掷出这段序列的概率。如果前者比后者小，你就要小心了。

比如说掷骰子的结果是：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415496325727916489.jpeg'/>

要算用正常的三个骰子掷出这个结果的概率，其实就是将所有可能情况的概率进行加和计算。同样，简单而暴力的方法就是把穷举所有的骰子序列，还是计算每个骰子序列对应的概率，但是这回，我们不挑最大值了，而是把所有算出来的概率相加，得到的总概率就是我们要求的结果。这个方法依然不能应用于太长的骰子序列（马尔可夫链）。

我们会应用一个和前一个问题类似的解法，只不过前一个问题关心的是概率最大值，这个问题关心的是概率之和。解决这个问题的算法叫做前向算法（forward algorithm）。

首先，如果我们只掷一次骰子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963266094334650.jpeg'/>

看到结果为1.产生这个结果的总概率可以按照如下计算，总概率为0.18：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963269969259385.jpeg'/>

把这个情况拓展，我们掷两次骰子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963278953070319.jpeg'/>

看到结果为1，6.产生这个结果的总概率可以按照如下计算，总概率为0.05：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415496328485517326.jpeg'/>

继续拓展，我们掷三次骰子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963294735647584.jpeg'/>

看到结果为1，6，3.产生这个结果的总概率可以按照如下计算，总概率为0.03：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963298847740924.jpeg'/>

同样的，我们一步一步的算，有多长算多长，再长的马尔可夫链总能算出来的。用同样的方法，也可以算出不正常的六面骰和另外两个正常骰子掷出这段序列的概率，然后我们比较一下这两个概率大小，就能知道你的骰子是不是被人换了。

3.掷一串骰子出来，让我猜猜你是谁
这个算法就是著名的EM算法，详见：https://www.julyedu.com/question/big/kp_id/23/ques_id/1007

上述算法呢，其实用到了递归，逆向推导，循环这些方法，我只不过用很直白的语言写出来了。如果你们去看专业书籍呢，会发现更加严谨和专业的描述。毕竟，我只做了会其意，要知其形，还是要看书的。
## 预训练方法 BERT和OpenAI GPT有什么区别？
1.GPT在BooksCorpus(800M单词)训练；BERT在BooksCorpus(800M单词)和维基百科(2,500M单词)训练
2.GPT使用一种句子分隔符([SEP])和分类符词块([CLS])，它们仅在微调时引入；BERT在预训练期间学习[SEP]，[CLS]和句子A/B嵌入
3.GPT用一个批量32,000单词训练1M步；BERT用一个批量128,000单词训练1M步
4.GPT对所有微调实验使用的5e-5相同学习率；BERT选择特定于任务的微调学习率，在开发集表现最佳
## 了解词嵌入的来龙去脉么？
本题解析来源：@Scofield_Phil，https://blog.csdn.net/Scotfield_msn/article/details/69075227 

Indexing:
〇、序
一、DeepNLP的核心关键：语言表示（Representation）
二、NLP词的表示方法类型
   1、词的独热表示one-hot representation
   2、词的分布式表示distributed representation
三、NLP语言模型

四、词的分布式表示
    1. 基于矩阵的分布表示
    2. 基于聚类的分布表示
    3. 基于神经网络的分布表示，词嵌入（ word embedding）
五、词嵌入（ word embedding）
    1、概念
    2、理解
六、神经网络语言模型与word2vec
    1、神经网络语言模型
    2.word2vec与CBOW、Skip-gram
    3.个人对word embedding的理解


〇、序
之前一段时间，在结合深度学习做NLP的时候一直有思考一些问题，其中有一个问题算是最核心一个：究竟深度网络是怎么做到让各种NLP任务解决地如何完美呢？到底我的数据在NN中发什么了什么呢？

并且，不少的terms like： 词向量、word embedding、分布式表示、word2vec、glove等等，这一锅粥的名词术语分别代表什么，他们具体的关系是什么，他们是否处于平级关系？

出于对知识结构追求完整梳理的强迫症的老毛病，于是不停地查资料、思考、keep revolving……

然后就感觉有一点小进展了。想到，不如将个人对其的理解，无论对错，先拿出来跟peer分享下，或许能交换出更有意义的东西呢？

整篇文章的构架是按照属于概念在逻辑上的先后大小顺序，一层一层一级一级地往下剖析、比较、说明。

另外说明下，here整篇文字内容相对是比较入门，甚至有的点可能描述的不太客观正确，限于当前的认知水平……还请您海涵，希望您在评论中指正！

一、DeepNLP的核心关键：语言表示（Representation）
最近有一个新名词：Deep Learning + NLP =  DeepNLP。当常规的机器学习Machine Learning升级发展到了一定的阶段后，慢慢的被后起的深度学习Deep Learning夺势而去，并如火如荼地引领了一波新高潮，因为Deep Learning有machine learning过而不及之处！那当Deep Learning进入NLP领域，自然是要横扫ACL一批paper才是。事实也是这样的。

先提下数据特征表示问题。数据表示是机器学习的核心问题，在过去的Machine Learning阶段，大量兴起特征工程，人工设计大量的特征解决数据的有效表示问题。而到了Deep Learning，想都别想，end-2-end，一步到位，hyper-parameter自动帮你选择寻找关键的特征参数。

那么，Deep Learning如何能在NLP中发挥出应有的real power呢？很明显，先不提如何设计出很强势的网络结构，不提如何在NLP中引入基于NN的解决例如情感分析、实体识别、机器翻译、文本生成这些高级任务，咱们首先得把语言表示这一关过了——如何让语言表示成为NN能够处理的数据类型。

我们看看图像和语音是怎么表示数据的：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211539066450273.jpg'/>

在语音中，用音频频谱序列向量所构成的matrix作为前端输入喂给NN进行处理，good；在图像中，用图片的像素构成的matrix展平成vector后组成的vector序列喂给NN进行处理，good；那在自然语言处理中呢？噢你可能知道或者不知道，将每一个词用一个向量表示出来！想法是挺简单的，对，事实上就是这么简单，然而真有这么简单吗？可能没这么简单。

有人提到，图像、语音属于比较自然地低级数据表示形式，在图像和语音领域，最基本的数据是信号数据，我们可以通过一些距离度量，判断信号是否相似，在判断两幅图片是否相似时，只需通过观察图片本身就能给出回答。而语言作为人类在进化了几百万年所产生的一种高层的抽象的思维信息表达的工具，其具有高度抽象的特征，文本是符号数据，两个词只要字面不同，就难以刻画它们之间的联系，即使是“麦克风”和“话筒”这样的同义词，从字面上也难以看出这两者意思相同（语义鸿沟现象），可能并不是简单地一加一那么简单就能表示出来，而判断两个词是否相似时，还需要更多的背景知识才能做出回答。

那么据上是不是可以自信地下一个结论呢：如何有效地表示出语言句子是决定NN能发挥出强大拟合计算能力的关键前提！


二、NLP词的表示方法类型
接下来将按照上面的思路，引出各种词的表示方法。按照现今目前的发展，词的表示分为独热表示one-hot、分布式表示distributed。

1、词的独热表示one-hot representation
NLP 中最直观，也是到目前为止最常用的词表示方法是 One-hot Representation，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。关于one-hot编码的资料很多，街货，这里简单举个栗子说明：

                “话筒”表示为 [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ...]
                “麦克”表示为 [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ...]

每个词都是茫茫 0 海中的一个 1。这种 One-hot Representation 如果采用稀疏方式存储，会是非常的简洁：也就是给每个词分配一个数字 ID。比如刚才的例子中，话筒记为 3，麦克记为 8（假设从 0 开始记）。如果要编程实现的话，用 Hash 表给每个词分配一个编号就可以了。这么简洁的表示方法配合上最大熵、SVM、CRF 等等算法已经很好地完成了 NLP 领域的各种主流任务。

现在我们分析他的不当处。1、向量的维度会随着句子的词的数量类型增大而增大；2、任意两个词之间都是孤立的，根本无法表示出在语义层面上词语词之间的相关信息，而这一点是致命的。

2、词的分布式表示distributed representation
传统的独热表示（ one-hot representation）仅仅将词符号化，不包含任何语义信息。如何将语义融入到词表示中？Harris 在 1954 年提出的分布假说（ distributional hypothesis）为这一设想提供了理论基础：上下文相似的词，其语义也相似。Firth 在 1957 年对分布假说进行了进一步阐述和明确：词的语义由其上下文决定（ a word is characterized by thecompany it keeps）。

到目前为止，基于分布假说的词表示方法，根据建模的不同，主要可以分为三类：基于矩阵的分布表示、基于聚类的分布表示和基于神经网络的分布表示。尽管这些不同的分布表示方法使用了不同的技术手段获取词表示，但由于这些方法均基于分布假说，它们的核心思想也都由两部分组成：一、选择一种方式描述上下文；二、选择一种模型刻画某个词（下文称“目标词”）与其上下文之间的关系。


三、NLP语言模型
在详细介绍词的分布式表示之前，需要将NLP中的一个关键概念描述清楚：语言模型。语言模型包括文法语言模型和统计语言模型。一般我们指的是统计语言模型。之所以要将语言模型摆在词表示方法之前，是因为后面的表示方法马上要用到这一概念。

统计语言模型： 统计语言模型把语言（词的序列）看作一个随机事件，并赋予相应的概率来描述其属于某种语言集合的可能性。给定一个词汇集合 V，对于一个由 V 中的词构成的序列S = ⟨w1, · · · , wT ⟩ ∈ Vn，统计语言模型赋予这个序列一个概率P(S)，来衡量S 符合自然语言的语法和语义规则的置信度。

用一句简单的话说，就语言模型就是计算一个句子的概率大小的这种模型。有什么意义呢？一个句子的打分概率越高，越说明他是更合乎人说出来的自然句子。

就是这么简单。常见的统计语言模型有N元文法模型（N-gram Model），最常见的是unigram model、bigram model、trigram model等等。形式化讲，统计语言模型的作用是为一个长度为 m 的字符串确定一个概率分布 P(w1; w2; :::; wm)，表示其存在的可能性，其中 w1 到 wm 依次表示这段文本中的各个词。一般在实际求解过程中，通常采用下式计算其概率值：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211544519075211.png'/>
 
同时通过这些方法均也可以保留住一定的词序信息，这样就能把一个词的上下文信息capture住。
具体的语言模型详情属于街货，详细请自行搜索。


四、词的分布式表示
1. 基于矩阵的分布表示
基于矩阵的分布表示通常又称为分布语义模型，在这种表示下，矩阵中的一行，就成为了对应词的表示，这种表示描述了该词的上下文的分布。由于分布假说认为上下文相似的词，其语义也相似，因此在这种表示下，两个词的语义相似度可以直接转化为两个向量的空间距离。

常见到的Global Vector 模型（ GloVe模型）是一种对“词-词”矩阵进行分解从而得到词表示的方法，属于基于矩阵的分布表示。

2. 基于聚类的分布表示
基于聚类的分布表示我也还不是太清楚，所以就不做具体描述。

3. 基于神经网络的分布表示，词嵌入（ word embedding）
基于神经网络的分布表示一般称为词向量、词嵌入（ word embedding）或分布式表示（ distributed representation）。这正是我们的主角today。

神经网络词向量表示技术通过神经网络技术对上下文，以及上下文与目标词之间的关系进行建模。由于神经网络较为灵活，这类方法的最大优势在于可以表示复杂的上下文。在前面基于矩阵的分布表示方法中，最常用的上下文是词。如果使用包含词序信息的 n-gram 作为上下文，当 n 增加时， n-gram 的总数会呈指数级增长，此时会遇到维数灾难问题。而神经网络在表示 n-gram 时，可以通过一些组合方式对 n 个词进行组合，参数个数仅以线性速度增长。有了这一优势，神经网络模型可以对更复杂的上下文进行建模，在词向量中包含更丰富的语义信息。


五、词嵌入（ word embedding）
1、概念
基于神经网络的分布表示又称为词向量、词嵌入，神经网络词向量模型与其它分布表示方法一样，均基于分布假说，核心依然是上下文的表示以及上下文与目标词之间的关系的建模。

前面提到过，为了选择一种模型刻画某个词（下文称“目标词”）与其上下文之间的关系，我们需要在词向量中capture到一个词的上下文信息。同时，上面我们恰巧提到了统计语言模型正好具有捕捉上下文信息的能力。那么构建上下文与目标词之间的关系，最自然的一种思路就是使用语言模型。从历史上看，早期的词向量只是神经网络语言模型的副产品。

2001年， Bengio 等人正式提出神经网络语言模型（ Neural Network Language Model ，NNLM），该模型在学习语言模型的同时，也得到了词向量。所以请注意一点：词向量可以认为是神经网络训练语言模型的副产品。

2、理解
前面提过，one-hot表示法具有维度过大的缺点，那么现在将vector做一些改进：
1、将vector每一个元素由整形改为浮点型，变为整个实数范围的表示；
2、将原来稀疏的巨大维度压缩嵌入到一个更小维度的空间。

如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211548737052650.png'/>
 
这也是词向量又名词嵌入的缘由了。


六、神经网络语言模型与word2vec
好了，到目前为止我们已经对的分布式表示以及词嵌入的概念的层级关系有了个理性的认识了，那这跟word2vec有什么联系？

1、神经网络语言模型
上面说，通过神经网络训练语言模型可以得到词向量，那么，究竟有哪些类型的神经网络语言模型呢？个人所知，大致有这么些个：

a) Neural Network Language Model ，NNLM
b) Log-Bilinear Language Model， LBL
c) Recurrent Neural Network based Language Model，RNNLM
d) Collobert 和 Weston 在2008 年提出的 C&W 模型
e) Mikolov 等人提出了 CBOW（ Continuous Bagof-Words）和 Skip-gram 模型

到这，估计有人看到了两个熟悉的term：CBOW、skip-gram，有看过word2vec的同学应该对此有所了解。我们继续。


2.word2vec与CBOW、Skip-gram
现在我们正式引出最火热的另一个term：word2vec。

上面提到的5个神经网络语言模型，只是个在逻辑概念上的东西，那么具体我们得通过设计将其实现出来，而实现CBOW（ Continuous Bagof-Words）和 Skip-gram 语言模型的工具正是well-known word2vec！另外，C&W 模型的实现工具是SENNA。

所以说，分布式词向量并不是word2vec的作者发明的，他只是提出了一种更快更好的方式来训练语言模型罢了。分别是：连续词袋模型Continous Bag of Words Model(CBOW)和Skip-Gram Model，这两种都是可以训练出词向量的方法，再具体代码操作中可以只选择其一，不过据论文说CBOW要更快一些。

顺便说说这两个语言模型。统计语言模型statistical language model就是给你几个词，在这几个词出现的前提下来计算某个词出现的（事后）概率。CBOW也是统计语言模型的一种，顾名思义就是根据某个词前面的C个词或者前后C个连续的词，来计算某个词出现的概率。Skip-Gram Model相反，是根据某个词，然后分别计算它前后出现某几个词的各个概率。

以“我爱北京天安门”这句话为例。假设我们现在关注的词是“爱”，C＝2时它的上下文分别是“我”，“北京天安门”。CBOW模型就是把“我” “北京天安门” 的one hot表示方式作为输入，也就是C个1xV的向量，分别跟同一个VxN的大小的系数矩阵W1相乘得到C个1xN的隐藏层hidden layer，然后C个取平均所以只算一个隐藏层。这个过程也被称为线性激活函数(这也算激活函数？分明就是没有激活函数了)。

然后再跟另一个NxV大小的系数矩阵W2相乘得到1xV的输出层，这个输出层每个元素代表的就是词库里每个词的事后概率。输出层需要跟ground truth也就是“爱”的one hot形式做比较计算loss。

这里需要注意的就是V通常是一个很大的数比如几百万，计算起来相当费时间，除了“爱”那个位置的元素肯定要算在loss里面，word2vec就用基于huffman编码的Hierarchical softmax筛选掉了一部分不可能的词，然后又用nagetive samping再去掉了一些负样本的词所以时间复杂度就从O(V)变成了O(logV)。Skip gram训练过程类似，只不过输入输出刚好相反。

补充下，Word embedding的训练方法大致可以分为两类：一类是无监督或弱监督的预训练；一类是端对端（end to end）的有监督训练。无监督或弱监督的预训练以word2vec和auto-encoder为代表。这一类模型的特点是，不需要大量的人工标记样本就可以得到质量还不错的embedding向量。不过因为缺少了任务导向，可能和我们要解决的问题还有一定的距离。因此，我们往往会在得到预训练的embedding向量后，用少量人工标注的样本去fine-tune整个模型。

相比之下，端对端的有监督模型在最近几年里越来越受到人们的关注。与无监督模型相比，端对端的模型在结构上往往更加复杂。同时，也因为有着明确的任务导向，端对端模型学习到的embedding向量也往往更加准确。例如，通过一个embedding层和若干个卷积层连接而成的深度神经网络以实现对句子的情感分类，可以学习到语义更丰富的词向量表达。

3.个人对word embedding的理解
现在，词向量既能够降低维度，又能够capture到当前词在本句子中上下文的信息（表现为前后距离关系），那么我们对其用来表示语言句子词语作为NN的输入是非常自信与满意的。

另外一点很实用的建议，在你做某一项具体的NLP任务时如你要用到词向量，那么我建议你：
要么1、选择使用别人训练好的词向量，注意，得使用相同语料内容领域的词向量；
要么2、自己训练自己的词向量。我建议是前者，因为……坑太多了。


References：
《How to Generate a Good Word Embedding?》,Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao
《基于神经网络的词和文档语义向量表示方法研究》，来斯惟
《面向自然语言处理的分布式表示学习》，邱锡鹏
《Deep Learning 实战之 word2vec》
http://www.cnblogs.com/iloveai/p/word2vec.html

http://www.hankcs.com/nlp/word2vec.html
http://licstar.net/archives/328
https://zhuanlan.zhihu.com/p/22477976
http://blog.csdn.net/itplus/article/details/37969519
http://www.tuicool.com/articles/fmuyamf
http://licstar.net/archives/620#comment-1542
http://blog.csdn.net/ycheng_sjtu/article/details/48520293
## 如何通俗理解深度学习中的注意力机制
本题解析来源：https://blog.csdn.net/qq_40027052/article/details/78421155

最近两年，注意力模型（Attention Model）被广泛使用在自然语言处理、图像识别及语音识别等各种不同类型的深度学习任务中，是深度学习技术中最值得关注与深入了解的核心技术之一。本文以机器翻译为例，深入浅出地介绍了深度学习中注意力机制的原理及关键计算机制，同时也抽象出其本质思想，并介绍了注意力模型在图像及语音等领域的典型应用场景。

注意力模型最近几年在深度学习各个领域被广泛使用，无论是图像处理、语音识别还是自然语言处理的各种不同类型的任务中，都很容易遇到注意力模型的身影。所以，了解注意力机制的工作原理对于关注深度学习技术发展的技术人员来说有很大的必要。

人类的视觉注意力
从注意力模型的命名方式看，很明显其借鉴了人类的注意力机制，因此，我们首先简单介绍人类视觉的选择性注意力机制。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211742747672925.1'/>
图1 人类的视觉注意力

视觉注意力机制是人类视觉所特有的大脑信号处理机制。人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段，是人类在长期进化中形成的一种生存机制，人类视觉注意力机制极大地提高了视觉信息处理的效率与准确性。

图1形象化展示了人类在看到一副图像时是如何高效分配有限的注意力资源的，其中红色区域表明视觉系统更关注的目标，很明显对于图1所示的场景，人们会把注意力更多投入到人的脸部，文本的标题以及文章首句等位置。

深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。

Encoder-Decoder框架
要了解深度学习中的注意力模型，就不得不先谈Encoder-Decoder框架，因为目前大多数注意力模型附着在Encoder-Decoder框架下，当然，其实注意力模型可以看作一种通用的思想，本身并不依赖于特定框架，这点需要注意。

Encoder-Decoder框架可以看作是一种深度学习领域的研究模式，应用场景异常广泛。图2是文本处理领域里常用的Encoder-Decoder框架最抽象的一种表示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211744751311155.2'/>
图2 抽象的文本处理领域的Encoder-Decoder框架

文本处理领域的Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对< Source,Target >，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。Source和Target可以是同一种语言，也可以是两种不同的语言。而Source和Target分别由各自的单词序列构成：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415521174572929437.3'/>

Encoder顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211747814699010.4'/>

对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息y1,y2……yi-1来生成i时刻要生成的单词yi：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211748555662960.5'/>

每个yi都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target。
如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题的Encoder-Decoder框架；
如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要的Encoder-Decoder框架；
如果Source是一句问句，Target是一句回答，那么这是问答系统或者对话机器人的Encoder-Decoder框架。
由此可见，在文本处理领域，Encoder-Decoder的应用领域相当广泛。

Encoder-Decoder框架不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用。
比如对于语音识别来说，图2所示的框架完全适用，区别无非是Encoder部分的输入是语音流，输出是对应的文本信息；
而对于“图像描述”任务来说，Encoder部分的输入是一副图片，Decoder的输出是能够描述图片语义内容的一句描述语。
一般而言，文本处理和语音识别的Encoder部分通常采用RNN模型，图像处理的Encoder一般采用CNN模型。

Attention模型
本节先以机器翻译作为例子讲解最常见的Soft Attention模型的基本原理，之后抛离Encoder-Decoder框架抽象出了注意力机制的本质思想，然后简单介绍最近广为使用的Self Attention的基本思路。

Soft Attention模型
图2中展示的Encoder-Decoder框架是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Target中每个单词的生成过程如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211749984814036.6'/>

其中f是Decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的，没有任何区别。而语义编码C是由句子Source的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词，y1，y2还是y3，其实句子Source中任意单词对生成某个目标单词yi来说影响力都是相同的，这是为何说这个模型没有体现出注意力的缘由。这类似于人类看到眼前的画面，但是眼中却没有注意焦点一样。

如果拿机器翻译来解释这个分心模型的Encoder-Decoder框架更好理解，比如输入的是英文句子：Tom chase Jerry，Encoder-Decoder框架逐步生成中文单词：“汤姆”，“追逐”，“杰瑞”。在翻译“杰瑞”这个中文单词的时候，分心模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要，但是分心模型是无法体现这一点的，这就是为何说它没有引入注意力的原因。没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。

上面的例子中，如果引入Attention模型的话，应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：

（Tom,0.3）(Chase,0.2) (Jerry,0.5)

每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词yi的时候，原先都是相同的中间语义表示C会被替换成根据当前生成单词而不断变化的Ci。

理解Attention模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。增加了注意力模型的Encoder-Decoder框架理解起来如图3所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415521175116518819.7'/>
图3 引入注意力模型的Encoder-Decoder框架

即生成目标句子单词的过程成了下面的形式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211752485511920.8'/>

而每个Ci可能对应着不同的源语句子单词的注意力分配概率分布，比如对于上面的英汉翻译来说，其对应的信息可能如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211753566542139.9'/>

其中，f2函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值；g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和，即下列公式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211754138153314.10'/>

其中，Lx代表输入句子Source的长度，aij代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数，而hj则是Source输入句子中第j个单词的语义编码。假设Ci下标i就是上面例子所说的“ 汤姆” ，那么Lx就是3，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)分别是输入句子每个单词的语义编码，对应的注意力模型权值则分别是0.6,0.2,0.2，所以g函数本质上就是个加权求和函数。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的中间语义表示Ci的形成过程类似图4。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415521175545482914.11'/>
图4 Attention的形成过程

这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，如何知道Attention模型所需要的输入句子单词注意力分配概率分布值呢？就是说“汤姆”对应的输入句子Source中各个单词的概率分布：(Tom,0.6)(Chase,0.2) (Jerry,0.2) 是如何得到的呢？

为了便于说明，我们假设对图2的非Attention模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图2的框架转换为图5。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211756860639756.12'/>
图5 RNN作为具体模型的Encoder-Decoder框架

那么用图6可以较为便捷地说明注意力分配概率分布值的通用计算过程。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211758070033907.13'/>
图6 注意力分配概率计算

对于采用RNN的Decoder来说，在时刻i，如果要生成yi单词，我们是可以知道Target在生成yi之前的时刻i-1时，隐层节点i-1时刻的输出值Hi-1的，而我们的目的是要计算生成yi时输入句子中的单词“Tom”、“Chase”、“Jerry”对yi来说的注意力分配概率分布，那么可以用Target输出句子i-1时刻的隐层节点状态Hi-1去一一和输入句子Source中每个单词对应的RNN隐层节点状态hj进行对比，即通过函数F(hj,Hi-1)来获得目标单词yi和每个输入单词对应的对齐可能性，这个F函数在不同论文里可能会采取不同的方法，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。绝大多数Attention模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在F的定义上可能有所不同。图7可视化地展示了在英语-德语翻译系统中加入Attention机制后，Source和Target两个句子每个单词对应的注意力分配概率分布。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415521175955491524.14'/>
图7 英语-德语翻译的注意力概率分布

上述内容就是经典的Soft Attention模型的基本思想，那么怎么理解Attention模型的物理含义呢？一般在自然语言处理应用里会把Attention模型看作是输出Target句子中某个单词和输入Source句子每个单词的对齐模型，这是非常有道理的。目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤，而注意力模型其实起的是相同的作用。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211760936424713.15'/>
图8 Google 神经网络机器翻译系统结构图

图8所示即为Google于2016年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了60%，其架构就是上文所述的加上Attention机制的Encoder-Decoder框架，主要区别无非是其Encoder和Decoder使用了8层叠加的LSTM模型。

Attention机制的本质思想
如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211762137451769.16'/>
图9 Attention机制的本质思想

我们可以这样来看待Attention机制（参考图9）：将Source中的构成元素想象成是由一系列的< Key,Value >数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415521176349162958.17'/>

其中， Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。

当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。从图9可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。

至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211765427968810.18'/>
图10 三阶段计算Attention过程

在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个Keyi，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211766341219640.19'/>

第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211766949656168.20'/>

第二阶段的计算结果ai即为Valuei对应的权重系数，然后进行加权求和即可得到Attention数值：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211768186014632.21'/>

通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。

Self Attention模型
通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。Self Attention也经常被称为intra Attention（内部Attention），最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型。在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。而Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。

如果是常规的Target不等于Source情形下的注意力计算，其物理含义正如上文所讲，比如对于机器翻译来说，本质上是目标语单词和源语单词之间的一种单词对齐机制。那么如果是Self Attention机制，一个很自然的问题是：通过Self Attention到底学到了哪些规律或者抽取出了哪些特征呢？或者说引入Self Attention有什么增益或者好处呢？我们仍然以机器翻译中的Self Attention来说明，图11和图12是可视化地表示Self Attention在同一个英语句子内单词间产生的联系。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211769243433697.22'/>
图11 可视化Self Attention实例

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211770048194399.23'/>
图12 可视化Self Attention实例

从两张图（图11、图12）可以看出，Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）。很明显，引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。

Attention机制的应用
前文有述，Attention机制在深度学习的各种应用领域都有广泛的使用场景。上文在介绍过程中我们主要以自然语言处理中的机器翻译任务作为例子，下面分别再从图像处理领域和语音识别选择典型应用实例来对其应用做简单说明。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211771159077828.24'/>
图13 图片-描述任务的Encoder-Decoder框架

图片描述（Image-Caption）是一种典型的图文结合的深度学习应用，输入一张图片，人工智能系统输出一句描述句子，语义等价地描述图片所示内容。很明显这种应用场景也可以使用Encoder-Decoder框架来解决任务目标，此时Encoder输入部分是一张图片，一般会用CNN来对图片进行特征抽取，Decoder部分使用RNN或者LSTM来输出自然语言句子（参考图13）。此时如果加入Attention机制能够明显改善系统输出效果，Attention模型在这里起到了类似人类视觉选择性注意的机制，在输出某个实体单词的时候会将注意力焦点聚焦在图片中相应的区域上。图14给出了根据给定图片生成句子“A person is standing on a beach with a surfboard.”过程时每个单词对应图片中的注意力聚焦区域。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211772324247639.25'/>
图14 图片生成句子中每个单词时的注意力聚焦区域

图15给出了另外四个例子形象地展示了这种过程，每个例子上方左侧是输入的原图，下方句子是人工智能系统自动产生的描述语句，上方右侧图展示了当AI系统产生语句中划横线单词的时候，对应图片中聚焦的位置区域。比如当输出单词dog的时候，AI系统会将注意力更多地分配给图片中小狗对应的位置。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211773028957634.26'/>
图15 图像描述任务中Attention机制的聚焦作用

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211775024135448.27'/>
图16 语音识别中音频序列和输出字符之间的Attention

语音识别的任务目标是将语音流信号转换成文字，所以也是Encoder-Decoder的典型应用场景。Encoder部分的Source输入是语音流信号，Decoder部分输出语音对应的字符串流。图16可视化地展示了在Encoder-Decoder框架中加入Attention机制后，当用户用语音说句子 how much would a woodchuck chuck 时，输入部分的声音特征信号和输出字符之间的注意力分配概率分布情况，颜色越深代表分配到的注意力概率越高。从图中可以看出，在这个场景下，Attention机制起到了将输出字符和输入语音信号进行对齐的功能。

上述内容仅仅选取了不同AI领域的几个典型Attention机制应用实例，Encoder-Decoder加Attention架构由于其卓越的实际效果，目前在深度学习领域里得到了广泛的使用，了解并熟练使用这一架构对于解决实际问题会有极大帮助。
## 请详细说说Transformer （超详细图解，一图胜千言）
本文解析来源：https://blog.csdn.net/longxinchen_ml/article/details/86533005，原英文链接：https://jalammar.github.io/illustrated-transformer/

编者按：前一段时间谷歌推出的BERT模型在11项NLP任务中夺得SOTA结果，引爆了整个NLP界。而BERT取得成功的一个关键因素是Transformer的强大作用。谷歌的Transformer模型最早是用于机器翻译任务，当时达到了SOTA效果。Transformer改进了RNN最被人诟病的训练慢的缺点，利用self-attention机制实现快速并行。并且Transformer可以增加到非常深的深度，充分发掘DNN模型的特性，提升模型准确率。在本文中，我们将研究Transformer模型，把它掰开揉碎，理解它的工作原理。

正文：
Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。

在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。
Attention is All You Need：https://arxiv.org/abs/1706.03762

从宏观的视角开始
首先将这个模型看成是一个黑箱操作。在机器翻译中，就是输入一种语言，输出另一种语言。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845827584775818.png'/>

那么拆开这个黑箱，我们可以看到它是由编码组件、解码组件和它们之间的连接组成。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845828062714956.png'/>

编码组件部分由一堆编码器（encoder）构成（论文中是将6个编码器叠在一起——数字6没有什么神奇之处，你也可以尝试其他数字）。解码组件部分也是由相同数量（与编码器对应）的解码器（decoder）组成的。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845829057658997.png'/>

所有的编码器在结构上都是相同的，但它们没有共享参数。每个解码器都可以分解成两个子层。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845842850978652.jpg'/>

从编码器输入的句子首先会经过一个自注意力（self-attention）层，这层帮助编码器在对每个单词编码时关注输入句子的其他单词。我们将在稍后的文章中更深入地研究自注意力。

自注意力层的输出会传递到前馈（feed-forward）神经网络中。每个位置的单词对应的前馈神经网络都完全一样（译注：另一种解读就是一层窗口为一个单词的一维卷积神经网络）。

解码器中也有编码器的自注意力（self-attention）层和前馈（feed-forward）层。除此之外，这两个层之间还有一个注意力层，用来关注输入句子的相关部分（和seq2seq模型的注意力作用相似）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845844714514848.jpg'/>

将张量引入图景
我们已经了解了模型的主要部分，接下来我们看一下各种向量或张量（译注：张量概念是矢量概念的推广，可以简单理解矢量是一阶张量、矩阵是二阶张量。）是怎样在模型的不同部分中，将输入转化为输出的。

像大部分NLP应用一样，我们首先将每个输入单词通过词嵌入算法转换为词向量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845833046907804.png'/>

每个单词都被嵌入为512维的向量，我们用这些简单的方框来表示这些向量。

词嵌入过程只发生在最底层的编码器中。所有的编码器都有一个相同的特点，即它们接收一个向量列表，列表中的每个向量大小为512维。在底层（最开始）编码器中它就是词向量，但是在其他编码器中，它就是下一层编码器的输出（也是一个向量列表）。向量列表大小是我们可以设置的超参数——一般是我们训练集中最长句子的长度。

将输入序列进行词嵌入之后，每个单词都会流经编码器中的两个子层。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845847673986901.jpg'/>

接下来我们看看Transformer的一个核心特性，在这里输入序列中每个位置的单词都有自己独特的路径流入编码器。在自注意力层中，这些路径之间存在依赖关系。而前馈（feed-forward）层没有这些依赖关系。因此在前馈（feed-forward）层时可以并行执行各种路径。

然后我们将以一个更短的句子为例，看看编码器的每个子层中发生了什么。

现在我们开始“编码”
如上述已经提到的，一个编码器接收向量列表作为输入，接着将向量列表中的向量传递到自注意力层进行处理，然后传递到前馈神经网络层中，将输出结果传递到下一个编码器中。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684584927375001.jpg'/>

输入序列的每个单词都经过自编码过程。然后，他们各自通过前向传播神经网络——完全相同的网络，而每个向量都分别通过它。

从宏观视角看自注意力机制
不要被我用自注意力这个词弄迷糊了，好像每个人都应该熟悉这个概念。其实我之也没有见过这个概念，直到读到Attention is All You Need 这篇论文时才恍然大悟。让我们精炼一下它的工作原理。

例如，下列句子是我们想要翻译的输入句子：

The animal didn’t cross the street because it was too tired

这个“it”在这个句子是指什么呢？它指的是street还是这个animal呢？这对于人类来说是一个简单的问题，但是对于算法则不是。

当模型处理这个单词“it”的时候，自注意力机制会允许“it”与“animal”建立联系。

随着模型处理输入序列的每个单词，自注意力会关注整个输入序列的所有单词，帮助模型对本单词更好地进行编码。

如果你熟悉RNN（循环神经网络），回忆一下它是如何维持隐藏层的。RNN会将它已经处理过的前面的所有单词/向量的表示与它正在处理的当前单词/向量结合起来。而自注意力机制会将所有相关单词的理解融入到我们正在处理的单词中。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845850836579196.png'/>

当我们在编码器#5（栈中最上层编码器）中编码“it”这个单词的时，注意力机制的部分会去关注“The Animal”，将它的表示的一部分编入“it”的编码中。

请务必检查Tensor2Tensor notebook ，在里面你可以下载一个Transformer模型，并用交互式可视化的方式来检验。

从微观视角看自注意力机制
首先我们了解一下如何使用向量来计算自注意力，然后来看它实怎样用矩阵来实现。

计算自注意力的第一步就是从每个编码器的输入向量（每个单词的词向量）中生成三个向量。也就是说对于每个单词，我们创造一个查询向量、一个键向量和一个值向量。这三个向量是通过词嵌入与三个权重矩阵后相乘创建的。

可以发现这些新向量在维度上比词嵌入向量更低。他们的维度是64，而词嵌入和编码器的输入/输出向量的维度是512. 但实际上不强求维度更小，这只是一种基于架构上的选择，它可以使多头注意力（multiheaded attention）的大部分计算保持不变。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845853367578284.jpg'/>

X1与WQ权重矩阵相乘得到q1, 就是与这个单词相关的查询向量。最终使得输入序列的每个单词的创建一个查询向量、一个键向量和一个值向量。

什么是查询向量、键向量和值向量向量？

它们都是有助于计算和理解注意力机制的抽象概念。请继续阅读下文的内容，你就会知道每个向量在计算注意力机制中到底扮演什么样的角色。

计算自注意力的第二步是计算得分。假设我们在为这个例子中的第一个词“Thinking”计算自注意力向量，我们需要拿输入句子中的每个单词对“Thinking”打分。这些分数决定了在编码单词“Thinking”的过程中有多重视句子的其它部分。

这些分数是通过打分单词（所有输入句子的单词）的键向量与“Thinking”的查询向量相点积来计算的。所以如果我们是处理位置最靠前的词的自注意力的话，第一个分数是q1和k1的点积，第二个分数是q1和k2的点积。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845856798161316.jpg'/>

第三步和第四步是将分数除以8(8是论文中使用的键向量的维数64的平方根，这会让梯度更稳定。这里也可以使用其它值，8只是默认值)，然后通过softmax传递结果。softmax的作用是使所有单词的分数归一化，得到的分数都是正值且和为1。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845858096672999.jpg'/>

这个softmax分数决定了每个单词对编码当下位置（“Thinking”）的贡献。显然，已经在这个位置上的单词将获得最高的softmax分数，但有时关注另一个与当前单词相关的单词也会有帮助。

第五步是将每个值向量乘以softmax分数(这是为了准备之后将它们求和)。这里的直觉是希望关注语义上相关的单词，并弱化不相关的单词(例如，让它们乘以0.001这样的小数)。

第六步是对加权值向量求和（译注：自注意力的另一种解释就是在编码某个单词时，就是将所有单词的表示（值向量）进行加权求和，而权重是通过该词的表示（键向量）与被编码词表示（查询向量）的点积并通过softmax得到。），然后即得到自注意力层在该位置的输出(在我们的例子中是对于第一个单词)。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845866637300867.jpg'/>

这样自自注意力的计算就完成了。得到的向量就可以传给前馈神经网络。然而实际中，这些计算是以矩阵形式完成的，以便算得更快。那我们接下来就看看如何用矩阵实现的。

通过矩阵运算实现自注意力机制
第一步是计算查询矩阵、键矩阵和值矩阵。为此，我们将将输入句子的词嵌入装进矩阵X中，将其乘以我们训练的权重矩阵(WQ，WK，WV)。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641568458696369372.png'/>

x矩阵中的每一行对应于输入句子中的一个单词。我们再次看到词嵌入向量 (512，或图中的4个格子)和q/k/v向量(64，或图中的3个格子)的大小差异。

最后，由于我们处理的是矩阵，我们可以将步骤2到步骤6合并为一个公式来计算自注意力层的输出。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845871490011733.png'/>

自注意力的矩阵运算形式

“大战多头怪”
通过增加一种叫做“多头”注意力（“multi-headed” attention）的机制，论文进一步完善了自注意力层，并在两方面提高了注意力层的性能：

1.它扩展了模型专注于不同位置的能力。在上面的例子中，虽然每个编码都在z1中有或多或少的体现，但是它可能被实际的单词本身所支配。如果我们翻译一个句子，比如“The animal didn’t cross the street because it was too tired”，我们会想知道“it”指的是哪个词，这时模型的“多头”注意机制会起到作用。

2.它给出了注意力层的多个“表示子空间”（representation subspaces）。接下来我们将看到，对于“多头”注意机制，我们有多个查询/键/值权重矩阵集(Transformer使用八个注意力头，因此我们对于每个编码器/解码器有八个矩阵集合)。这些集合中的每一个都是随机初始化的，在训练之后，每个集合都被用来将输入词嵌入(或来自较低编码器/解码器的向量)投影到不同的表示子空间中。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684587581642812.jpg'/>

在“多头”注意机制下，我们为每个头保持独立的查询/键/值权重矩阵，从而产生不同的查询/键/值矩阵。和之前一样，我们拿X乘以WQ/WK/WV矩阵来产生查询/键/值矩阵。

如果我们做与上述相同的自注意力计算，只需八次不同的权重矩阵运算，我们就会得到八个不同的Z矩阵。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845876557715677.png'/>

这给我们带来了一点挑战。前馈层不需要8个矩阵，它只需要一个矩阵(由每一个单词的表示向量组成)。所以我们需要一种方法把这八个矩阵压缩成一个矩阵。那该怎么做？其实可以直接把这些矩阵拼接在一起，然后用一个附加的权重矩阵WO与它们相乘。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845878591825568.jpg'/>

这几乎就是多头自注意力的全部。这确实有好多矩阵，我们试着把它们集中在一个图片中，这样可以一眼看清。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845879611656792.jpg'/>

既然我们已经摸到了注意力机制的这么多“头”，那么让我们重温之前的例子，看看我们在例句中编码“it”一词时，不同的注意力“头”集中在哪里：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845880564049405.png'/>

当我们编码“it”一词时，一个注意力头集中在“animal”上，而另一个则集中在“tired”上，从某种意义上说，模型对“it”一词的表达在某种程度上是“animal”和“tired”的代表。

然而，如果我们把所有的attention都加到图示里，事情就更难解释了：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845881721069536.png'/>

使用位置编码表示序列的顺序
到目前为止，我们对模型的描述缺少了一种理解输入单词顺序的方法。

为了解决这个问题，Transformer为每个输入的词嵌入添加了一个向量。这些向量遵循模型学习到的特定模式，这有助于确定每个单词的位置，或序列中不同单词之间的距离。这里的直觉是，将位置向量添加到词嵌入中使得它们在接下来的运算中，能够更好地表达的词与词之间的距离。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845883192685943.jpg'/>

为了让模型理解单词的顺序，我们添加了位置编码向量，这些向量的值遵循特定的模式。

如果我们假设词嵌入的维数为4，则实际的位置编码如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845884574651402.jpg'/>

尺寸为4的迷你词嵌入位置编码实例

这个模式会是什么样子？

在下图中，每一行对应一个词向量的位置编码，所以第一行对应着输入序列的第一个词。每行包含512个值，每个值介于1和-1之间。我们已经对它们进行了颜色编码，所以图案是可见的。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845885532148322.png'/>

20字(行)的位置编码实例，词嵌入大小为512(列)。你可以看到它从中间分裂成两半。这是因为左半部分的值由一个函数(使用正弦)生成，而右半部分由另一个函数(使用余弦)生成。然后将它们拼在一起而得到每一个位置编码向量。

原始论文里描述了位置编码的公式(第3.5节)。你可以在 get_timing_signal_1d()中看到生成位置编码的代码。这不是唯一可能的位置编码方法。然而，它的优点是能够扩展到未知的序列长度(例如，当我们训练出的模型需要翻译远比训练集里的句子更长的句子时)。

残差模块
在继续进行下去之前，我们需要提到一个编码器架构中的细节：在每个编码器中的每个子层（自注意力、前馈网络）的周围都有一个残差连接，并且都跟随着一个“层-归一化”步骤。

层-归一化步骤：https://arxiv.org/abs/1607.06450 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845887369372446.jpg'/>

如果我们去可视化这些向量以及这个和自注意力相关联的层-归一化操作，那么看起来就像下面这张图描述一样：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845889917117177.jpg'/>

解码器的子层也是这样样的。如果我们想象一个2 层编码-解码结构的transformer，它看起来会像下面这张图一样：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845897927214300.jpg'/>

解码组件
既然我们已经谈到了大部分编码器的概念，那么我们基本上也就知道解码器是如何工作的了。但最好还是看看解码器的细节。

编码器通过处理输入序列开启工作。顶端编码器的输出之后会变转化为一个包含向量K（键向量）和V（值向量）的注意力向量集 。这些向量将被每个解码器用于自身的“编码-解码注意力层”，而这些层可以帮助解码器关注输入序列哪些位置合适：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156846894583861613.gif'/>
在完成编码阶段后，则开始解码阶段。解码阶段的每个步骤都会输出一个输出序列（在这个例子里，是英语翻译的句子）的元素。

接下来的步骤重复了这个过程，直到到达一个特殊的终止符号，它表示transformer的解码器已经完成了它的输出。每个步骤的输出在下一个时间步被提供给底端解码器，并且就像编码器之前做的那样，这些解码器会输出它们的解码结果 。另外，就像我们对编码器的输入所做的那样，我们会嵌入并添加位置编码给那些解码器，来表示每个单词的位置。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156846899939997439.gif'/>
而那些解码器中的自注意力层表现的模式与编码器不同：在解码器中，自注意力层只被允许处理输出序列中更靠前的那些位置。在softmax步骤前，它会把后面的位置给隐去（把它们设为-inf）。

这个“编码-解码注意力层”工作方式基本就像多头自注意力层一样，只不过它是通过在它下面的层来创造查询矩阵，并且从编码器的输出中取得键/值矩阵。

最终的线性变换和Softmax层
解码组件最后会输出一个实数向量。我们如何把浮点数变成一个单词？这便是线性变换层要做的工作，它之后就是Softmax层。

线性变换层是一个简单的全连接神经网络，它可以把解码组件产生的向量投射到一个比它大得多的、被称作对数几率（logits）的向量里。

不妨假设我们的模型从训练集中学习一万个不同的英语单词（我们模型的“输出词表”）。因此对数几率向量为一万个单元格长度的向量——每个单元格对应某一个单词的分数。

接下来的Softmax 层便会把那些分数变成概率（都为正数、上限1.0）。概率最高的单元格被选中，并且它对应的单词被作为这个时间步的输出。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845944079222297.jpg'/>

这张图片从底部以解码器组件产生的输出向量开始。之后它会转化出一个输出单词。

训练部分总结
既然我们已经过了一遍完整的transformer的前向传播过程，那我们就可以直观感受一下它的训练过程。

在训练过程中，一个未经训练的模型会通过一个完全一样的前向传播。但因为我们用有标记的训练集来训练它，所以我们可以用它的输出去与真实的输出做比较。

为了把这个流程可视化，不妨假设我们的输出词汇仅仅包含六个单词：“a”, “am”, “i”, “thanks”, “student”以及 “”（end of sentence的缩写形式）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845946021955055.png'/>

我们模型的输出词表在我们训练之前的预处理流程中就被设定好。

一旦我们定义了我们的输出词表，我们可以使用一个相同宽度的向量来表示我们词汇表中的每一个单词。这也被认为是一个one-hot 编码。所以，我们可以用下面这个向量来表示单词“am”：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684595265463151.jpg'/>

例子：对我们输出词表的one-hot 编码

接下来我们讨论模型的损失函数——这是我们用来在训练过程中优化的标准。通过它可以训练得到一个结果尽量准确的模型。

损失函数
比如说我们正在训练模型，现在是第一步，一个简单的例子——把“merci”翻译为“thanks”。

这意味着我们想要一个表示单词“thanks”概率分布的输出。但是因为这个模型还没被训练好，所以不太可能现在就出现这个结果。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845954355490127.jpg'/>

因为模型的参数（权重）都被随机的生成，（未经训练的）模型产生的概率分布在每个单元格/单词里都赋予了随机的数值。我们可以用真实的输出来比较它，然后用反向传播算法来略微调整所有模型的权重，生成更接近结果的输出。

你会如何比较两个概率分布呢？我们可以简单地用其中一个减去另一个。更多细节请参考交叉熵和KL散度。

交叉熵：https://colah.github.io/posts/2015-09-Visual-Information/
KL散度：https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained

但注意到这是一个过于简化的例子。更现实的情况是处理一个句子。例如，输入“je suis étudiant”并期望输出是“i am a student”。那我们就希望我们的模型能够成功地在这些情况下输出概率分布：

每个概率分布被一个以词表大小（我们的例子里是6，但现实情况通常是3000或10000）为宽度的向量所代表。

第一个概率分布在与“i”关联的单元格有最高的概率

第二个概率分布在与“am”关联的单元格有最高的概率

以此类推，第五个输出的分布表示“”关联的单元格有最高的概率
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845957766480817.jpg'/>

依据例子训练模型得到的目标概率分布

在一个足够大的数据集上充分训练后，我们希望模型输出的概率分布看起来像这个样子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845959033771441.jpg'/>

我们期望训练过后，模型会输出正确的翻译。当然如果这段话完全来自训练集，它并不是一个很好的评估指标（参考：交叉验证，链接https://www.youtube.com/watch?v=TIgfjmp-4BA）。注意到每个位置（词）都得到了一点概率，即使它不太可能成为那个时间步的输出——这是softmax的一个很有用的性质，它可以帮助模型训练。

因为这个模型一次只产生一个输出，不妨假设这个模型只选择概率最高的单词，并把剩下的词抛弃。这是其中一种方法（叫贪心解码）。另一个完成这个任务的方法是留住概率最靠高的两个单词（例如I和a），那么在下一步里，跑模型两次：其中一次假设第一个位置输出是单词“I”，而另一次假设第一个位置输出是单词“me”，并且无论哪个版本产生更少的误差，都保留概率最高的两个翻译结果。然后我们为第二和第三个位置重复这一步骤。这个方法被称作集束搜索（beam search）。在我们的例子中，集束宽度是2（因为保留了2个集束的结果，如第一和第二个位置），并且最终也返回两个集束的结果（top_beams也是2）。这些都是可以提前设定的参数。

再进一步
我希望通过上文已经让你们了解到Transformer的主要概念了。如果你想在这个领域深入，我建议可以走以下几步：阅读Attention Is All You Need，Transformer博客和Tensor2Tensor announcement，以及看看Łukasz Kaiser的介绍，了解模型和细节。

Attention Is All You Need：https://arxiv.org/abs/1706.03762
Transformer博客：https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html
Tensor2Tensor announcement：https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html
Łukasz Kaiser的介绍：https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb

接下来可以研究的工作：
Depthwise Separable Convolutions for Neural Machine Translation
https://arxiv.org/abs/1706.03059

One Model To Learn Them All
https://arxiv.org/abs/1706.05137

Discrete Autoencoders for Sequence Models
https://arxiv.org/abs/1801.09797

Generating Wikipedia by Summarizing Long Sequences
https://arxiv.org/abs/1801.10198

Image Transformer
https://arxiv.org/abs/1802.05751

Training Tips for the Transformer Model
https://arxiv.org/abs/1804.00247

Self-Attention with Relative Position Representations
https://arxiv.org/abs/1803.02155

Fast Decoding in Sequence Models using Discrete Latent Variables
https://arxiv.org/abs/1803.03382

Adafactor: Adaptive Learning Rates with Sublinear Memory Cost
https://arxiv.org/abs/1804.04235
## 如何通俗理解LDA主题模型
本题解析来源于July在CSDN上超过20万阅读量的LDA笔记《通俗理解LDA主题模型》，特原封不动的转至于此。


0 前言
    印象中，最开始听说“LDA”这个名词，是缘于rickjin在2013年3月写的一个LDA科普系列，叫LDA数学八卦，我当时一直想看来着，记得还打印过一次，但不知是因为这篇文档的前序铺垫太长（现在才意识到这些“铺垫”都是深刻理解LDA 的基础，但如果没有人帮助初学者提纲挈领、把握主次、理清思路，则很容易陷入LDA的细枝末节之中），还是因为其中的数学推导细节太多，导致一直没有完整看完过。

    2013年12月，在我组织的Machine Learning读书会第8期上，@夏粉_百度 讲机器学习中排序学习的理论和算法研究，@沈醉2011 则讲主题模型的理解。又一次碰到了主题模型，当时貌似只记得沈博讲了一个汪峰写歌词的例子，依然没有理解LDA到底是怎样一个东西（但理解了LDA之后，再看沈博主题模型的PPT会很赞）。

    直到昨日下午，机器学习班 第12次课上，邹讲完LDA之后，才真正明白LDA原来是那么一个东东！上完课后，趁热打铁，再次看LDA数学八卦，发现以前看不下去的文档再看时竟然一路都比较顺畅，一口气看完大部。看完大部后，思路清晰了，知道理解LDA，可以分为下述5个步骤：

一个函数：gamma函数
四个分布：二项分布、多项分布、beta分布、Dirichlet分布
一个概念和一个理念：共轭先验和贝叶斯框架
两个模型：pLSA、LDA（在本文第4 部分阐述）
一个采样：Gibbs采样
    本文便按照上述5个步骤来阐述，希望读者看完本文后，能对LDA有个尽量清晰完整的了解。同时，本文基于邹讲LDA的PPT、rickjin的LDA数学八卦及其它参考资料写就，可以定义为一篇学习笔记或课程笔记，当然，后续不断加入了很多自己的理解。若有任何问题，欢迎随时于本文评论下指出，thanks。


1 gamma函数
1.0 整体把握LDA
    关于LDA有两种含义，一种是线性判别分析（Linear Discriminant Analysis），一种是概率主题模型：隐含狄利克雷分布（Latent Dirichlet Allocation，简称LDA），本文讲后者。

    另外，我先简单说下LDA的整体思想，不然我怕你看了半天，铺了太长的前奏，却依然因没见到LDA的影子而显得“心浮气躁”，导致不想再继续看下去。所以，先给你吃一颗定心丸，明白整体框架后，咱们再一步步抽丝剥茧，展开来论述。

    按照wiki上的介绍，LDA由Blei, David M.、Ng, Andrew Y.、Jordan于2003年提出，是一种主题模型，它可以将文档集 中每篇文档的主题以概率分布的形式给出，从而通过分析一些文档抽取出它们的主题（分布）出来后，便可以根据主题（分布）进行主题聚类或文本分类。同时，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的关系。

    此外，一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。

    人类是怎么生成文档的呢？LDA的这三位作者在原始论文中给了一个简单的例子。比如假设事先给定了这几个主题：Arts、Budgets、Children、Education，然后通过学习训练，获取每个主题Topic对应的词语。如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223030724224382.1'/>

    然后以一定的概率选取上述某个主题，再以一定的概率选取那个主题下的某个单词，不断的重复这两步，最终生成如下图所示的一篇文章（其中不同颜色的词语分别对应上图中不同主题下的词）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223031682919494.2'/>

    而当我们看到一篇文章后，往往喜欢推测这篇文章是如何生成的，我们可能会认为作者先确定这篇文章的几个主题，然后围绕这几个主题遣词造句，表达成文。

    LDA就是要干这事：根据给定的一篇文档，反推其主题分布。

    通俗来说，可以假定认为人类是根据上述文档生成过程写成了各种各样的文章，现在某小撮人想让计算机利用LDA干一件事：你计算机给我推测分析网络上各篇文章分别都写了些啥主题，且各篇文章中各个主题出现的概率大小（主题分布）是啥。

    然，就是这么一个看似普通的LDA，一度吓退了不少想深入探究其内部原理的初学者。难在哪呢，难就难在LDA内部涉及到的数学知识点太多了。

    在LDA模型中，一篇文档生成的方式如下：

    ①从狄利克雷分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223041580429184.3'/>中取样生成文档 i 的主题分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223042353367220.4'/>
    ②从主题的多项式分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223043433138956.5'/>中取样生成文档i第 j 个词的主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223044046358704.6'/>
    ③从狄利克雷分布中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223047393316093.7'/>取样生成主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223048937470750.8'/>对应的词语分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223049586264826.9'/>
    ④从词语的多项式分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223050433074241.10'/>中采样最终生成词语<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223050950540711.11'/>

    其中，类似Beta分布是二项式分布的共轭先验概率分布，而狄利克雷分布（Dirichlet分布）是多项式分布的共轭先验概率分布。

    此外，LDA的图模型结构如下图所示（类似贝叶斯网络结构）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223057373957026.12'/>

    恩，不错，短短6句话整体概括了整个LDA的主体思想！但也就是上面短短6句话，却接连不断或重复出现了二项分布、多项式分布、beta分布、狄利克雷分布（Dirichlet分布）、共轭先验概率分布、取样，那么请问，这些都是啥呢？

    这里先简单解释下二项分布、多项分布、beta分布、Dirichlet 分布这4个分布。

 二项分布（Binomial distribution）。
    二项分布是从伯努利分布推进的。伯努利分布，又称两点分布或0-1分布，是一个离散型的随机分布，其中的随机变量只有两类取值，非正即负{+，-}。而二项分布即重复n次的伯努利试验，记为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415522306648555491.13'/>。简言之，只做一次实验，是伯努利分布，重复做了n次，是二项分布。二项分布的概率密度函数为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223067794562016.14'/>

    对于k = 0, 1, 2, ..., n，其中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223069542303939.15'/>是二项式系数（这就是二项分布的名称的由来），又记为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223070680695592.16'/>。回想起高中所学的那丁点概率知识了么：想必你当年一定死记过这个二项式系数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415522307199634074.17'/>就是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415522307255987551.18'/>。

多项分布，是二项分布扩展到多维的情况。
    多项分布是指单次试验中的随机变量的取值不再是0-1的，而是有多种离散值可能（1,2,3...,k）。比如投掷6个面的骰子实验，N次实验结果服从K=6的多项分布。其中
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223074876480664.19'/>

    多项分布的概率密度函数为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223075581464852.20'/>

Beta分布，二项分布的共轭先验分布。
    给定参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415522307904324346.21'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223080157054835.22'/>，取值范围为[0,1]的随机变量 x 的概率密度函数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415522308114994355.23'/>

    其中：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223082621142936.24'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223083618227062.25'/>

   注：便是所谓的gamma函数，下文会具体阐述。

Dirichlet分布，是beta分布在高维度上的推广。
    Dirichlet分布的的密度函数形式跟beta分布的密度函数如出一辙：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223112271512633.31'/>

    其中
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223112796786823.32'/>

    至此，我们可以看到二项分布和多项分布很相似，Beta分布和Dirichlet 分布很相似，而至于“Beta分布是二项式分布的共轭先验概率分布，而狄利克雷分布（Dirichlet分布）是多项式分布的共轭先验概率分布”这点在下文中说明。

    OK，接下来，咱们就按照本文开头所说的思路：“一个函数：gamma函数，四个分布：二项分布、多项分布、beta分布、Dirichlet分布，外加一个概念和一个理念：共轭先验和贝叶斯框架，两个模型：pLSA、LDA（文档-主题，主题-词语），一个采样：Gibbs采样”一步步详细阐述，争取给读者一个尽量清晰完整的LDA。

    （当然，如果你不想深究背后的细节原理，只想整体把握LDA的主体思想，可直接跳到本文第4 部分，看完第4部分后，若还是想深究背后的细节原理，可再回到此处开始看）

1.1 gamma函数
    咱们先来考虑一个问题（此问题1包括下文的问题2-问题4皆取材自LDA数学八卦）：

问题1 随机变量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223090075485218.26'/>
把这n 个随机变量排序后得到顺序统计量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223090967158461.27'/>
然后请问<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223091925303405.28'/>的分布是什么。

    为解决这个问题，可以尝试计算<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223093225086067.29'/>落在区间[x,x+Δx]的概率。即求下述式子的值：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223095916322370.30'/>

    首先，把 [0,1] 区间分成三段 [0,x)，[x,x+Δx]，(x+Δx,1]，然后考虑下简单的情形：即假设n 个数中只有1个落在了区间 [x,x+Δx]内，由于这个区间内的数X(k)是第k大的，所以[0,x)中应该有 k−1 个数，(x+Δx,1] 这个区间中应该有n−k 个数。如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223121355760865.33'/>

    从而问题转换为下述事件E：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223122063787678.34'/>

    对于上述事件E，有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223122755489443.35'/>

    其中，o(Δx)表示Δx的高阶无穷小。显然，由于不同的排列组合，即n个数中有一个落在 [x,x+Δx]区间的有n种取法，余下n−1个数中有k−1个落在[0,x)的有<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223127124703337.36'/>种组合，所以和事件E等价的事件一共有<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223127986924382.37'/>个。

    如果有2个数落在区间[x,x+Δx]呢？如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223128658913270.38'/>

    类似于事件E，对于2个数落在区间[x,x+Δx]的事件E’：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223130769986750.39'/>

    有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223131599874421.40'/>

   从上述的事件E、事件E‘中，可以看出，只要落在[x,x+Δx]内的数字超过一个，则对应的事件的概率就是 o(Δx)。于是乎有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223149499756040.41'/>

    从而得到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223153365411196.42'/>的概率密度函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223154232926834.43'/>为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641552231548865276.44'/>

    至此，本节开头提出的问题得到解决。然仔细观察<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223157626755370.45'/>的概率密度函数，发现式子的最终结果有阶乘，联想到阶乘在实数上的推广函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223159439752741.46'/>：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223160618449295.47'/>

    两者结合是否会产生奇妙的效果呢？考虑到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223163335912349.48'/>具有如下性质：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223164012353331.49'/>

    故将<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223166328509447.50'/>代入到的概率密度函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223167028426475.51'/>中，可得：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223168292466822.52'/>

    然后取<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223171484240359.53'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223172045156108.54'/>，转换<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223172857689698.55'/>得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223173788644765.56'/>

    如果熟悉beta分布的朋友，可能会惊呼：哇，竟然推出了beta分布！


2 beta分布
2.1 beta分布
    在概率论中，beta是指一组定义在（0,1）区间的连续概率分布，有两个参数α和β，且α,β＞0。

    beta分布的概率密度函数是：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231207547653659.1'/>

  其中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231204010221212.3'/>便是函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231204560600459.4'/>：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231214890576308.1'/>

随机变量X服从参数为的beta分布通常写作：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229714554930785.png'/>。

2.2 Beta-Binomial 共轭
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229718746243542.png'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231219212539276.2'/>

熟悉贝叶斯方法（不熟悉的没事，参见此文第一部分）的朋友心里估计又犯“嘀咕”了，这不就是贝叶斯式的思考过程么？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229722237463745.png'/>

  回顾下贝叶斯派思考问题的固定模式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229725615314415.png'/>

上述思考模式意味着，新观察到的样本信息将修正人们以前对事物的认知。换言之，在得到新的样本信息之前，人们对的认知是先验分布π(θ)，在得到新的样本信息X后，人们对θ的认知为π(θ|X)。

    类比到现在这个问题上，我们也可以试着写下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415523122275489906.3'/>

   其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231227566978481.4'/>对应的是二项分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231228693653066.5'/>的计数。

    更一般的，对于非负实数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231229667562070.6'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231230168992630.7'/>，我们有如下关系
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231231826118016.8'/>

    针对于这种观测到的数据符合二项分布，参数的先验分布和后验分布都是Beta分布的情况，就是Beta-Binomial共轭。换言之，Beta分布是二项式分布的共轭先验概率分布。

    二项分布和Beta分布是共轭分布意味着，如果我们为二项分布的参数p选取的先验分布是Beta分布，那么以p为参数的二项分布用贝叶斯估计得到的后验分布仍然服从Beta分布。

    此外，如何理解参数α和β所表达的意义呢？α、β可以认为形状参数，通俗但不严格的理解是，α和β共同控制Beta分布的函数“长的样子”：形状千奇百怪，高低胖瘦，如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229756384798734.png'/>


2.3 共轭先验分布
    什么又是共轭呢？轭的意思是束缚、控制，共轭从字面上理解，则是共同约束，或互相约束。

    在贝叶斯概率理论中，如果后验概率P(θ|x)和先验概率p(θ)满足同样的分布律，那么，先验分布和后验分布被叫做共轭分布，同时，先验分布叫做似然函数的共轭先验分布。

    比如，某观测数据服从概率分布P(θ)时，当观测到新的X数据时，我们一般会遇到如下问题：

·可否根据新观测数据X，更新参数θ？
·根据新观测数据可以在多大程度上改变参数θ，即
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231235822904796.9'/>

·当重新估计θ的时候，给出新参数值θ的新概率分布，即P(θ|x)。
    事实上，根据根据贝叶斯公式可知：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231236453266717.10'/>

      其中，P(x|θ)表示以预估θ为参数的x概率分布，可以直接求得，P(θ)是已有原始的θ概率分布。
    所以，如果我们选取P(x|θ)的共轭先验作为P(θ)的分布，那么P(x|θ)乘以P(θ)，然后归一化的结果P(θ|x)跟和P(θ)的形式一样。换句话说，先验分布是P(θ)，后验分布是P(θ|x)，先验分布跟后验分布同属于一个分布族，故称该分布族是θ的共轭先验分布（族）。

    举个例子。投掷一个非均匀硬币，可以使用参数为θ的伯努利模型，θ为硬币为正面的概率，那么结果x的分布形式为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415523123964620658.11'/>

其共轭先验为beta分布，具有两个参数α和β，称为超参数（hyperparameters）。且这两个参数决定了θ参数，其Beta分布形式为
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231241271314136.12'/>

然后计算后验概率
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231242356335304.13'/>

归一化这个等式后会得到另一个Beta分布，从而证明了Beta分布确实是伯努利分布的共轭先验分布。

2.4 从beta分布推广到Dirichlet 分布
    接下来，咱们来考察beta分布的一个性质。

    如果<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229779552176762.png'/>，则有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231263691991014.14'/>

注意到上式最后结果的右边积分
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231264996810441.15'/>

   其类似于概率分布，而对于这个分布有
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231267979918048.16'/>

    从而求得
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231299175015654.21'/>

    的结果为
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231299718546134.22'/>

 最后将此结果带入E（P）的计算式，得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231300527577258.23'/>

最后的这个结果意味着对于Beta 分布的随机变量，其均值（期望）可以用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231311169628707.24'/>来估计。此外，狄利克雷Dirichlet 分布也有类似的结论，即如果<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231312235227334.25'/>，同样可以证明有下述结论成立：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231312887026070.26'/>

那什么是Dirichlet 分布呢？简单的理解Dirichlet 分布就是一组连续多变量概率分布，是多变量普遍化的beta分布。为了纪念德国数学家约翰·彼得·古斯塔夫·勒热纳·狄利克雷（Peter Gustav Lejeune Dirichlet）而命名。狄利克雷分布常作为贝叶斯统计的先验概率。


3 Dirichlet 分布
3.1 Dirichlet 分布
    根据wikipedia上的介绍，维度K ≥ 2（x1,x2…xK-1维，共K个）的狄利克雷分布在参数α1, ..., αK > 0上、基于欧几里得空间RK-1里的勒贝格测度有个概率密度函数，定义为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240414332330366.27'/>

其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240419717553514.30'/>相当于是多项beta函数
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641552404182645672.29'/>

    且<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240422584074653.31'/>

    此外，x1+x2+…+xK-1+xK=1，x1,x2…xK-1>0，且在(K-1)维的单纯形上，其他区域的概率密度为0。
    当然，也可以如下定义Dirichlet 分布
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240427272704404.33'/>

    其中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240429788762649.34'/>称为Dirichlet 分布的归一化系数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240431290405971.35'/>

   且根据Dirichlet分布的积分为1（概率的基本性质），可以得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240433938538250.36'/>

3.2 Dirichlet-Multinomial 共轭
    下面，在2.2节问题2的基础上继续深入，引出问题3。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265255074788118.1'/>，
排序后对应的顺序统计量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265256651524124.2'/>，
问<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265257489886146.3'/>的联合分布是什么？

    为了简化计算，取x3满足x1+x2+x3=1,但只有x1,x2是变量，如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265261057547216.4'/>

    从而有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265262453871123.5'/>

    于是我们得到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265267979274685.6'/>的联合分布为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415526526886519885.7'/>

    观察上述式子的最终结果，可以看出上面这个分布其实就是3维形式的 Dirichlet 分布
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265272189207353.8'/>

    令<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265272917507727.9'/>，于是分布密度可以写为
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265277366683357.10'/>

    这个就是一般形式的3维 Dirichlet 分布，即便<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265290079110244.11'/>延拓到非负实数集合，以上概率分布也是良定义的。

    将Dirichlet分布的概率密度函数取对数，绘制对称Dirichlet分布的图像如下图所示（截取自wikipedia上）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265290955067777.gif'/>

    上图中，取K=3，也就是有两个独立参数x1,x2，分别对应图中的两个坐标轴，第三个参数始终满足x3=1-x1-x2且α1=α2=α3=α，图中反映的是参数α从α=(0.3, 0.3, 0.3)变化到(2.0, 2.0, 2.0)时的概率对数值的变化情况。

    为了论证Dirichlet分布是多项式分布的共轭先验概率分布，下面咱们继续在上述问题3的基础上再进一步，提出问题4。

① 问题4 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265317275584434.13'/>，排序后对应的顺序统计量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265318230535687.14'/>
② 令<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265319252084598.15'/>,<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265319879551800.16'/>,③<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265320814461814.17'/>（此处的p3非变量，只是为了表达方便），现在要猜测<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265321669607942.18'/>；
③<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265332219024969.19'/>，Yi中落到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265333267422880.20'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265333853453858.21'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265335340907058.22'/>三个区间的个数分别为 m1,m2,m3，m=m1+m2+m3；
④问后验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265338581905745.23'/>的分布是什么。

   为了方便讨论，记<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265346181281799.24'/>，及<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265348350761128.25'/>，根据已知条件“<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265350124385705.26'/>，Yi中落到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265351269767948.27'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265351725235046.28'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265352645486574.29'/>三个区间的个数分别为 m1,m2”，可得<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265358817258273.30'/>、<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415526535953785854.31'/>分别是这m+n个数中第<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265360948108935.32'/>大、第<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265361659953478.33'/>大的数。于是，后验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265362898411793.34'/>应该为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265363849875110.35'/>，即一般化的形式表示为：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265364434267239.36'/>。

    同样的，按照贝叶斯推理的逻辑，可将上述过程整理如下：

①我们要猜测参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265390663085151.1'/>，其先验分布为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265391457469294.2'/>；
②数据Yi落到三个区间<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265392382060271.3'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265393093165149.4'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265393692361971.5'/>的个数分别为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265400758288001.6'/>，所以<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415526540295154371.7'/>服从多项分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265405353911281.8'/>
③在给定了来自数据提供的知识<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415526540662937979.9'/>后，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265407534466969.10'/>的后验分布变为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265408067825039.11'/>

    上述贝叶斯分析过程的直观表述为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265412656626144.12'/>

    令<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265415634287599.13'/>，可把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265416576979635.14'/>从整数集合延拓到实数集合，从而得到更一般的表达式如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265418255923943.15'/>

    针对于这种观测到的数据符合多项分布，参数的先验分布和后验分布都是Dirichlet 分布的情况，就是Dirichlet-Multinomial 共轭。换言之，至此已经证明了Dirichlet分布的确就是多项式分布的共轭先验概率分布。

    意味着，如果我们为多项分布的参数p选取的先验分布是Dirichlet分布，那么以p为参数的多项分布用贝叶斯估计得到的后验分布仍然服从Dirichlet分布。

    进一步，一般形式的Dirichlet 分布定义如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265423696979493.16'/>

    而对于给定的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265424520230749.17'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265430564190441.18'/>，其多项分布为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265429665098453.19'/>

    结论是：Dirichlet分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265429077652257.20'/>和多项分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265428574395056.21'/>是共轭关系。

4 主题模型LDA
 在开始下面的旅程之前，先来总结下我们目前所得到的最主要的几个收获：
通过上文的第2.2节，我们知道beta分布是二项式分布的共轭先验概率分布：
“对于非负实数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535172306510478.jpg'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351721212669411.png'/>，我们有如下关系
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351725130307553.png'/>
其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351776279339979.png'/>对应的是二项分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351777836965389.png'/>的计数。针对于这种观测到的数据符合二项分布，参数的先验分布和后验分布都是Beta分布的情况，就是Beta-Binomial 共轭。”

通过上文的3.2节，我们知道狄利克雷分布（Dirichlet分布）是多项式分布的共轭先验概率分布：
“ 把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351782973597967.png'/>从整数集合延拓到实数集合，从而得到更一般的表达式如下：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351786241796470.png'/>
 针对于这种观测到的数据符合多项分布，参数的先验分布和后验分布都是Dirichlet 分布的情况，就是 Dirichlet-Multinomial 共轭。 ”

以及贝叶斯派思考问题的固定模式：
先验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351796643995114.png'/>+ 样本信息<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351798095493072.png'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351798691125902.png'/>后验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351799854779787.png'/>

    上述思考模式意味着，新观察到的样本信息将修正人们以前对事物的认知。换言之，在得到新的样本信息之前，人们对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351804248040092.png'/>的认知是先验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351805677607437.png'/>，在得到新的样本信息<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351806970884361.png'/>后，人们对的认知为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351807977911931.png'/>。
顺便提下频率派与贝叶斯派各自不同的思考方式：
频率派把需要推断的参数θ看做是固定的未知常数，即概率虽然是未知的，但最起码是确定的一个值，同时，样本X 是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本X 的分布；
而贝叶斯派的观点则截然相反，他们认为待估计的参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351831867203184.png'/>是随机变量，服从一定的分布，而样本X 是固定的，由于样本是固定的，所以他们重点研究的是参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351834420951277.png'/>的分布。

  OK，在杀到终极boss——LDA模型之前，再循序渐进理解基础模型：Unigram model、mixture of unigrams model，以及跟LDA最为接近的pLSA模型。

   为了方便描述，首先定义一些变量：
1. <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535185486898741.png'/>表示词，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351858832767877.png'/>表示所有单词的个数（固定值）
2 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351862485749542.png'/>表示主题，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351865867996166.png'/>是主题的个数（预先给定，固定值）
3 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351869938370218.png'/>表示语料库，其中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535187439814166.png'/>是语料库中的文档数（固定值）
4 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351878044686506.png'/>表示文档，其中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351885884888045.png'/>表示一个文档中的词数（随机变量）

4.1 各个基础模型
4.1.1 Unigram model
对于文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415536018804693961.png'/>，用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351898332010957.png'/>表示词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351901293720939.png'/>的先验概率，生成文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351908620697437.png'/>的概率为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351912492396105.png'/>
 
其图模型为（图中被涂色的w表示可观测变量，N表示一篇文档中总共N个单词，M表示M篇文档）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351917291347195.png'/>
 
 或为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351920664655666.png'/>
 
unigram model假设文本中的词服从Multinomial分布，而我们已经知道Multinomial分布的先验分布为Dirichlet分布。
 上图中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351925688181359.png'/>表示在文本中观察到的第n个词，n∈[1,N]表示该文本中一共有N个单词。加上方框表示重复，即一共有N个这样的随机变量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351932891807587.png'/>。其中，p和α是隐含未知变量：

p是词服从的Multinomial分布的参数
α是Dirichlet分布（即Multinomial分布的先验分布）的参数。

一般α由经验事先给定，p由观察到的文本中出现的词学习得到，表示文本中出现每个词的概率。

4.1.2 Mixture of unigrams model
    该模型的生成过程是：给某个文档先选择一个主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351952988658030.png'/>,再根据该主题生成文档，该文档中的所有词都来自一个主题。假设主题有<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351960621703324.png'/>，生成文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351965060615535.png'/>的概率为：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351970337977230.png'/>

其图模型为（图中被涂色的w表示可观测变量，未被涂色的z表示未知的隐变量，N表示一篇文档中总共N个单词，M表示M篇文档）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351974151443443.png'/>

4.2 PLSA模型
    啊哈，长征两万五，经过前面这么长的铺垫，终于快要接近LDA模型了！因为跟LDA模型最为接近的便是下面要阐述的这个pLSA模型，理解了pLSA模型后，到LDA模型也就一步之遥——给pLSA加上贝叶斯框架，便是LDA。

4.2.1 pLSA模型下生成文档
    OK，在上面的Mixture of unigrams model中，我们假定一篇文档只有一个主题生成，可实际中，一篇文章往往有多个主题，只是这多个主题各自在文档中出现的概率大小不一样。比如介绍一个国家的文档中，往往会分别从教育、经济、交通等多个主题进行介绍。那么在pLSA中，文档是怎样被生成的呢？

    假设你要写M篇文档，由于一篇文档由各个不同的词组成，所以你需要确定每篇文档里每个位置上的词。

    再假定你一共有K个可选的主题，有V个可选的词，咱们来玩一个扔骰子的游戏。

1. 假设你每写一篇文档会制作一颗K面的“文档-主题”骰子（扔此骰子能得到K个主题中的任意一个），和K个V面的“主题-词项” 骰子（每个骰子对应一个主题，K个骰子对应之前的K个主题，且骰子的每一面对应要选择的词项，V个面对应着V个可选的词）。

比如可令K=3，即制作1个含有3个主题的“文档-主题”骰子，这3个主题可以是：教育、经济、交通。然后令V = 3，制作3个有着3面的“主题-词项”骰子，其中，教育主题骰子的3个面上的词可以是：大学、老师、课程，经济主题骰子的3个面上的词可以是：市场、企业、金融，交通主题骰子的3个面上的词可以是：高铁、汽车、飞机。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351982937840324.png'/>

2. 每写一个词，先扔该“文档-主题”骰子选择主题，得到主题的结果后，使用和主题结果对应的那颗“主题-词项”骰子，扔该骰子选择要写的词。

 先扔“文档-主题”的骰子，假设（以一定的概率）得到的主题是教育，所以下一步便是扔教育主题筛子，（以一定的概率）得到教育主题筛子对应的某个词：大学。

 上面这个投骰子产生词的过程简化下便是：“先以一定的概率选取主题，再以一定的概率选取词”。事实上，一开始可供选择的主题有3个：教育、经济、交通，那为何偏偏选取教育这个主题呢？其实是随机选取的，只是这个随机遵循一定的概率分布。比如可能选取教育主题的概率是0.5，选取经济主题的概率是0.3，选取交通主题的概率是0.2，那么这3个主题的概率分布便是{教育：0.5，经济：0.3，交通：0.2}，我们把各个主题z在文档d中出现的概率分布称之为主题分布，且是一个多项分布。

同样的，从主题分布中随机抽取出教育主题后，依然面对着3个词：大学、老师、课程，这3个词都可能被选中，但它们被选中的概率也是不一样的。比如大学这个词被选中的概率是0.5，老师这个词被选中的概率是0.3，课程被选中的概率是0.2，那么这3个词的概率分布便是{大学：0.5，老师：0.3，课程：0.2}，我们把各个词语w在主题z下出现的概率分布称之为词分布，这个词分布也是一个多项分布。

所以，选主题和选词都是两个随机的过程，先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后从该教育主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。

3. 最后，你不停的重复扔“文档-主题”骰子和”主题-词项“骰子，重复N次（产生N个词），完成一篇文档，重复这产生一篇文档的方法M次，则完成M篇文档。
    
上述过程抽象出来即是PLSA的文档生成模型。在这个过程中，我们并未关注词和词之间的出现顺序，所以pLSA是一种词袋方法。具体说来，该模型假设一组共现(co-occurrence)词项关联着一个隐含的主题类别<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351998213382297.png'/>。同时定义：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352002150071997.png'/>表示海量文档中某篇文档被选中的概率。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352006183724999.png'/>表示词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352009676765455.png'/>在给定文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352013258884194.png'/>中出现的概率。

怎么计算得到呢？针对海量文档，对所有文档进行分词后，得到一个词汇列表，这样每篇文档就是一个词语的集合。对于每个词语，用它在文档中出现的次数除以文档中词语总的数目便是它在文档中出现的概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352019545721319.png'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352024270338597.png'/>表示具体某个主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352029443891821.png'/>在给定文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352033453911092.png'/>下出现的概率。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352039853393421.png'/>表示具体某个词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352043684085659.png'/>在给定主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352047799253493.png'/>下出现的概率，与主题关系越密切的词，其条件概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535205203254211.png'/>越大。
 
利用上述的第1、3、4个概率，我们便可以按照如下的步骤得到“文档-词项”的生成模型：

1.按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352062899758278.png'/>选择一篇文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352064564521102.png'/>
2.选定文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535206908447580.png'/>后，从主题分布中按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155360862879631619.png'/>选择一个隐含的主题类别<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352074435875593.png'/>
3.选定<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352077286470568.png'/>后，从词分布中按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352080024544288.png'/>
选择一个词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352082182451662.png'/>
  
所以pLSA中生成文档的整个过程便是选定文档生成主题，确定主题生成词。
  

4.2.1 根据文档反推其主题分布
反过来，既然文档已经产生，那么如何根据已经产生好的文档反推其主题呢？这个利用看到的文档推断其隐藏的主题（分布）的过程（其实也就是产生文档的逆过程），便是主题建模的目的：自动地发现文档集中的主题（分布）。

    换言之，人类根据文档生成模型写成了各类文章，然后丢给了计算机，相当于计算机看到的是一篇篇已经写好的文章。现在计算机需要根据一篇篇文章中看到的一系列词归纳出当篇文章的主题，进而得出各个主题各自不同的出现概率：主题分布。即文档d和单词w是可被观察到的，但主题z却是隐藏的。

    如下图所示（图中被涂色的d、w表示可观测变量，未被涂色的z表示未知的隐变量，N表示一篇文档中总共N个单词，M表示M篇文档）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352089953160215.png'/>

上图中，文档d和词w是我们得到的样本（样本随机，参数虽未知但固定，所以pLSA属于频率派思想。区别于下文要介绍的LDA中：样本固定，参数未知但不固定，是个随机变量，服从一定的分布，所以LDA属于贝叶斯派思想），可观测得到，所以对于任意一篇文档，其<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352102141399349.png'/>是已知的。
 
从而可以根据大量已知的文档-词项信息<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352105911187250.png'/>，训练出文档-主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352109183646967.png'/>和主题-词项<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352112551770813.png'/>，如下公式所示：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352116356129118.png'/>

 故得到文档中每个词的生成概率为：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352119519512651.png'/>

由于<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352126947332565.png'/>可事先计算求出，而<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352131773110152.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535213557938318.png'/>未知，所以<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352138829704316.png'/>就是我们要估计的参数（值），通俗点说，就是要最大化这个θ。

  用什么方法进行估计呢，常用的参数估计方法有极大似然估计MLE、最大后验证估计MAP、贝叶斯估计等等。因为该待估计的参数中含有隐变量z，所以我们可以考虑EM算法。

4.2.1.1 EM算法的简单介绍
    EM算法，全称为Expectation-maximization algorithm，为期望最大算法，其基本思想是：首先随机选取一个值去初始化待估计的值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352147339279118.png'/>，然后不断迭代寻找更优的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352150834369608.png'/>使得其似然函数likelihood<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352154639134153.png'/>比原来的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352157815713092.png'/>要大。换言之，假定现在得到了<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352161771937404.png'/>，想求<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155360898816678576.png'/>，使得
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352173565268236.png'/>
 
EM的关键便是要找到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352178841278499.png'/>的一个下界<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352185797295694.png'/>（注：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352189266615936.png'/>，其中，X表示已经观察到的随机变量），然后不断最大化这个下界，通过不断求解下界<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535219387766440.png'/>的极大化，从而逼近要求解的似然函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352197891781163.png'/>。
 
所以EM算法的一般步骤为：
1. 随机选取或者根据先验知识初始化<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352204413956527.png'/>；
2. 不断迭代下述两步
①给出当前的参数估计<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352209568676129.png'/>，计算似然函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352211731064424.png'/>的下界<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535221606467251.png'/>
②重新估计参数θ，即求<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535222196165359.png'/>，使得<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535222814050175.png'/>
3. 上述第二步后，如果<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352230788002509.png'/>收敛（即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352233865245342.png'/>收敛）则退出算法，否则继续回到第二步。

上述过程好比在二维平面上，有两条不相交的曲线，一条曲线在上（简称上曲线<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352239311710575.png'/>），一条曲线在下（简称下曲线<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352243013947692.png'/>），下曲线为上曲线的下界。现在对上曲线未知，只已知下曲线，为了求解上曲线的最高点，我们试着不断增大下曲线，使得下曲线不断逼近上曲线，下曲线在某一个点达到局部最大值并与上曲线在这点的值相等，记录下这个值，然后继续增大下曲线，寻找下曲线上与上曲线上相等的值，迭代到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352246021459246.png'/>收敛（即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352250118457515.png'/>收敛）停止，从而利用当前下曲线上的局部最大值当作上曲线的全局最大值（换言之，EM算法不保证一定能找到全局最优值）。如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352254171513644.png'/>

  以下是详细介绍。

  假定有训练集<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352259269706536.png'/>，包含m个独立样本，希望从中找到该组数据的模型p(x,z)的参数。   

  然后通过极大似然估计建立目标函数--对数似然函数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352263149431888.png'/>

  这里，z是隐随机变量，直接找到参数的估计是很困难的。我们的策略是建立<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352267759665504.png'/>的下界，并且求该下界的最大值；重复这个过程，直到收敛到局部最大值。

    令Qi是z的某一个分布，Qi≥0，且结合Jensen不等式，有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352271697349975.png'/>

  为了寻找尽量紧的下界，我们可以让使上述等号成立，而若要让等号成立的条件则是：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352274895011896.png'/>
 
 换言之，有以下式子成立：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535227756859447.png'/>，且由于有：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535228017333416.png'/>

  所以可得：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352283565659113.png'/>
 最终得到EM算法的整体框架如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352286271375942.png'/>

OK，EM算法还会在本博客后面的博文中具体阐述。接下来，回到pLSA参数的估计问题上。

4.2.1.2 EM算法估计pLSA的两未知参数
    首先尝试从矩阵的角度来描述待估计的两个未知变量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352291631971718.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352292434633816.png'/>。
假定用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352297030254289.png'/>表示词表<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352301189888408.png'/>在主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352302913692625.png'/>上的一个多项分布，则<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352304964410693.png'/>可以表示成一个向量，每个元素<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352308442000767.png'/>表示词项<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352311344378316.png'/>出现在主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352313382645402.png'/>中的概率，即
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352317021943030.png'/>

用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352320939984026.png'/>表示所有主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352324316569651.png'/>在文档上的一个多项分布，则<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352333393092979.png'/>可以表示成一个向量，每个元素<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352336563166032.png'/>表示主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352338527941490.png'/>出现在文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352340930640951.png'/>中的概率，即
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535234352433311.png'/>
   
  这样，巧妙的把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352348486881632.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352350645765621.png'/>转换成了两个矩阵。换言之，最终我们要求解的参数是这两个矩阵：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352353073972994.png'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352355184296622.png'/>

 由于词和词之间是相互独立的，所以整篇文档N个词的分布为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352357788773508.png'/>
  
  再由于文档和文档之间也是相互独立的，所以整个语料库中词的分布为（整个语料库M篇文档，每篇文档N个词）：
  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352361858829873.png'/>

  其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352364935525345.png'/>表示词项<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535236667245054.png'/>在文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352368213803366.png'/>中的词频，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352371760112539.png'/>表示文档di中词的总数，显然有<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352374898103137.png'/>。
从而得到整个语料库的词分布的对数似然函数（下述公式中有个小错误，正确的应该是：N为M，M为
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352379054242048.png'/>

 现在，我们需要最大化上述这个对数似然函数来求解参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352381489436443.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352385566987101.png'/>。对于这种含有隐变量的最大似然估计，可以使用EM算法。EM算法，分为两个步骤：先E-step，后M-step。

E-step：假定参数已知，计算此时隐变量的后验概率。
    利用贝叶斯法则，可以得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352389044058754.png'/>

M-step：带入隐变量的后验概率，最大化样本分布的对数似然函数，求解相应的参数。

 观察之前得到的对数似然函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352394396615182.png'/>的结果，由于文档长度<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352396955817109.png'/>可以单独计算，所以去掉它不影响最大化似然函数。此外，根据E-step的计算结果，把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352399941382410.png'/>，于是我们只要最大化下面这个函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535240327201289.png'/>即可（下述公式中有个小错误，正确的应该是：N为M，M为N）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352406233499612.png'/>
 
这是一个多元函数求极值问题，并且已知有如下约束条件（下述公式中有个小错误，正确的应该是：M为N）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352409455110165.png'/>

熟悉凸优化的朋友应该知道，一般处理这种带有约束条件的极值问题，常用的方法便是拉格朗日乘数法，即通过引入拉格朗日乘子将约束条件和多元（目标）函数融合到一起，转化为无约束条件的极值问题。

 这里我们引入两个拉格朗日乘子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352413738698728.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352416366512507.png'/>，从而写出拉格朗日函数（下述公式中有个小错误，正确的应该是：N为M，M为N）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352420359093828.png'/>

因为我们要求解的参数是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352422618590365.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352424574090968.png'/>，所以分别对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352426669008186.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352427787047989.png'/>求偏导，然后令偏导结果等于0，得到（下述公式中有个小错误，正确的应该是：N为M，M为N）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535243064050167.png'/>

  消去拉格朗日乘子，最终可估计出参数和（下述公式中有个小错误，正确的应该是：N为M，M为N）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352434487426261.png'/>

综上，在pLSA中：
1.由于<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352440312429474.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352442333506579.png'/>未知，所以我们用EM算法去估计<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352445692171967.png'/>这个参数的值。
2.而后，用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352449631508144.png'/>表示词项<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535245186332266.png'/>出现在主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352453435224270.png'/>中的概率，即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352456134811955.png'/>，用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352457840200732.png'/>表示主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352459424484384.png'/>出现在文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352460946637132.png'/>中的概率，即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352463463822127.png'/>，从而把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535246568171006.png'/>转换成了“主题-词项”矩阵Φ（主题生成词），把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352467848466871.png'/>转换成了“文档-主题”矩阵Θ（文档生成主题）。
3.最终求解出<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352471759175481.png'/>、<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352473630066744.png'/>。

4.3 LDA模型
    
事实上，理解了pLSA模型，也就差不多快理解了LDA模型，因为LDA就是在pLSA的基础上加层贝叶斯框架，即LDA就是pLSA的贝叶斯版本（正因为LDA被贝叶斯化了，所以才需要考虑历史先验知识，才加的两个先验参数）。

4.3.1 pLSA跟LDA的对比：生成文档与参数估计

    在pLSA模型中，我们按照如下的步骤得到“文档-词项”的生成模型：
1.按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359322481186453.png'/>选择一篇文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359323551097493.png'/>
2.选定文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359325085150489.png'/>后，确定文章的主题分布
3.从主题分布中按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359328613609280.png'/>选择一个隐含的主题类别<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359329788656813.png'/>
4.选定<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359331176906780.png'/>后，确定主题下的词分布
5.从词分布中按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535933335424985.png'/>选择一个词 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359334640009226.png'/>”

下面，咱们对比下本文开头所述的LDA模型中一篇文档生成的方式是怎样的：

1.按照先验概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359337712851481.png'/>选择一篇文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359338815076728.png'/>
2.从狄利克雷分布（即Dirichlet分布）<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359341151542134.png'/>中取样生成文档 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359342523720499.png'/>的主题分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359344162706964.png'/>，换言之，主题分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359345336716218.png'/>由超参数为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359347046693480.png'/>的Dirichlet分布生成
3.从主题的多项式分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359349648689986.png'/>中取样生成文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359351216238589.png'/>第 j 个词的主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359354911479692.png'/>
4.从狄利克雷分布（即Dirichlet分布）<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359359147094570.png'/>中取样生成主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359360763058204.png'/>对应的词语分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359363613691817.png'/>，换言之，词语分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359364720601037.png'/>由参数为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359366134741704.png'/>的Dirichlet分布生成
5.从词语的多项式分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359368732723273.png'/>中采样最终生成词语<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359371844169016.png'/>
 
从上面两个过程可以看出，LDA在PLSA的基础上，为主题分布和词分布分别加了两个Dirichlet先验。

    继续拿之前讲解PLSA的例子进行具体说明。如前所述，在PLSA中，选主题和选词都是两个随机的过程，先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后从该主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359376162963529.png'/>

而在LDA中，选主题和选词依然都是两个随机的过程，依然可能是先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后再从该主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。

    那PLSA跟LDA的区别在于什么地方呢？区别就在于：

PLSA中，主题分布和词分布是唯一确定的，能明确的指出主题分布可能就是{教育：0.5，经济：0.3，交通：0.2}，词分布可能就是{大学：0.5，老师：0.3，课程：0.2}。
但在LDA中，主题分布和词分布不再唯一确定不变，即无法确切给出。例如主题分布可能是{教育：0.5，经济：0.3，交通：0.2}，也可能是{教育：0.6，经济：0.2，交通：0.2}，到底是哪个我们不再确定（即不知道），因为它是随机的可变化的。但再怎么变化，也依然服从一定的分布，即主题分布跟词分布由Dirichlet先验随机确定。
   
看到这，你可能凌乱了，你说面对多个主题或词，各个主题或词被抽中的概率不一样，所以抽取主题或词是随机抽取，还好理解。但现在你说主题分布和词分布本身也都是不确定的，这是怎么回事？没办法，谁叫Blei等人“强行”给PLSA安了个贝叶斯框架呢，正因为LDA是PLSA的贝叶斯版本，所以主题分布跟词分布本身由先验知识随机给定。

    进一步，你会发现：
pLSA中，主题分布和词分布确定后，以一定的概率（<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359387526978616.png'/>、<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359390838572878.png'/>）分别选取具体的主题和词项，生成好文档。而后根据生成好的文档反推其主题分布、词分布时，最终用EM算法（极大似然估计思想）求解出了两个未知但固定的参数的值：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641553593940883943.png'/>（由<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359396128718352.png'/>转换而来）和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359399361339549.png'/>（由<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359400856737205.png'/>转换而来）。

文档d产生主题z的概率，主题z产生单词w的概率都是两个固定的值。

举个文档d产生主题z的例子。给定一篇文档d，主题分布是一定的，比如{ P(zi|d), i = 1,2,3 }可能就是{0.4,0.5,0.1}，表示z1、z2、z3，这3个主题被文档d选中的概率都是个固定的值：P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1，如下图所示（图截取自沈博PPT上）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359406378585932.png'/>

但在贝叶斯框架下的LDA中，我们不再认为主题分布（各个主题在文档中出现的概率分布）和词分布（各个词语在某个主题下出现的概率分布）是唯一确定的（而是随机变量），而是有很多种可能。但一篇文档总得对应一个主题分布和一个词分布吧，怎么办呢？LDA为它们弄了两个Dirichlet先验参数，这个Dirichlet先验为某篇文档随机抽取出某个主题分布和词分布。
文档d产生主题z（准确的说，其实是Dirichlet先验为文档d生成主题分布Θ，然后根据主题分布Θ产生主题z）的概率，主题z产生单词w的概率都不再是某两个确定的值，而是随机变量。
还是再次举下文档d具体产生主题z的例子。给定一篇文档d，现在有多个主题z1、z2、z3，它们的主题分布{ P(zi|d), i = 1,2,3 }可能是{0.4,0.5,0.1}，也可能是{0.2,0.2,0.6}，即这些主题被d选中的概率都不再认为是确定的值，可能是P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1，也有可能是P(z1|d) = 0.2、P(z2|d) = 0.2、P(z3|d) = 0.6等等，而主题分布到底是哪个取值集合我们不确定（为什么？这就是贝叶斯派的核心思想，把未知参数当作是随机变量，不再认为是某一个确定的值），但其先验分布是dirichlet 分布，所以可以从无穷多个主题分布中按照dirichlet 先验随机抽取出某个主题分布出来。如下图所示（图截取自沈博PPT上）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359413518476943.png'/>

换言之，LDA在pLSA的基础上给这两参数（<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359415874805605.png'/>、<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359417519361279.png'/>加了两个先验分布的参数（贝叶斯化）：一个主题分布的先验分布Dirichlet分布
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359419613790061.png'/>，和一个词语分布的先验分布Dirichlet分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359421690163061.png'/>。

    综上，LDA真的只是pLSA的贝叶斯版本，文档生成后，两者都要根据文档去推断其主题分布和词语分布（即两者本质都是为了估计给定文档生成主题，给定主题生成词语的概率），只是用的参数推断方法不同，在pLSA中用极大似然估计的思想去推断两未知的固定参数，而LDA则把这两参数弄成随机变量，且加入dirichlet先验。

    所以，pLSA跟LDA的本质区别就在于它们去估计未知参数所采用的思想不同，前者用的是频率派思想，后者用的是贝叶斯派思想。
    好比，我去一朋友家：

按照频率派的思想，我估计他在家的概率是1/2，不在家的概率也是1/2，是个定值。
而按照贝叶斯派的思想，他在家不在家的概率不再认为是个定值1/2，而是随机变量。比如按照我们的经验（比如当天周末），猜测他在家的概率是0.6，但这个0.6不是说就是完全确定的，也有可能是0.7。如此，贝叶斯派没法确切给出参数的确定值（0.3,0.4，0.6,0.7，0.8,0.9都有可能），但至少明白在哪个范围或哪些取值（0.6,0.7，0.8,0.9）更有可能，哪个范围或哪些取值（0.3,0.4） 不太可能。进一步，贝叶斯估计中，参数的多个估计值服从一定的先验分布，而后根据实践获得的数据（例如周末不断跑他家），不断修正之前的参数估计，从先验分布慢慢过渡到后验分布。
    
OK，相信已经解释清楚了。如果是在机器学习班上face-to-face，更好解释和沟通。

4.3.2 LDA生成文档过程的进一步理解

    上面说，LDA中，主题分布 —— 比如{ P(zi), i =1,2,3 }等于{0.4,0.5,0.1}或{0.2,0.2,0.6} —— 是由dirichlet先验给定的，不是根据文档产生的。所以，LDA生成文档的过程中，先从dirichlet先验中“随机”抽取出主题分布，然后从主题分布中“随机”抽取出主题，最后从确定后的主题对应的词分布中“随机”抽取出词。

    那么，dirichlet先验到底是如何“随机”抽取主题分布的呢？

    事实上，从dirichlet分布中随机抽取主题分布，这个过程不是完全随机的。为了说清楚这个问题，咱们得回顾下dirichlet分布。事实上，如果我们取3个事件的话，可以建立一个三维坐标系，类似xyz三维坐标系，这里，我们把3个坐标轴弄为p1、p2、p3，如下图所示：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359431634073119.png'/>

在这个三维坐标轴所划分的空间里，每一个坐标点(p1,p2,p3)就对应着一个主题分布，且某一个点(p1,p2,p3)的大小表示3个主题z1、z2、z3出现的概率大小（因为各个主题出现的概率和为1，所以p1+p2+p3 = 1，且p1、p2、p3这3个点最大取值为1）。比如(p1,p2,p3) = (0.4,0.5,0.1)便对应着主题分布{ P(zi), i =1,2,3 } = {0.4,0.5,0.1}。

    可以想象到，空间里有很多这样的点(p1,p2,p3)，意味着有很多的主题分布可供选择，那dirichlet分布如何选择主题分布呢？把上面的斜三角形放倒，映射到底面的平面上，便得到如下所示的一些彩图（3个彩图中，每一个点对应一个主题分布，高度代表某个主题分布被dirichlet分布选中的概率，且选不同的，dirichlet 分布会偏向不同的主题分布）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359435897199172.png'/>

我们来看上图中左边这个图，高度就是代表dirichlet分布选取某个坐标点(p1,p2,p3)（这个点就是一个主题分布）的概率大小。如下图所示，平面投影三角形上的三个顶点上的点：A=(0.9,0.05,0.05)、B=(0.05,0.9,0.05)、C=(0.05,0.05,0.9)各自对应的主题分布被dirichlet分布选中的概率值很大，而平面三角形内部的两个点：D、E对应的主题分布被dirichlet分布选中的概率值很小。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359441826947541.png'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359442527123381.png'/>
 
所以虽然说dirichlet分布是随机选取任意一个主题分布的，但依然存在着P(A) = P(B) = P(C) >> P(D) = P(E)，即dirichlet分布还是“偏爱”某些主题分布的。至于dirichlet分布的参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359444870976857.png'/>是如何决定dirichlet分布的形状的，可以从dirichlet分布的定义和公式思考。

此外，就算说“随机”选主题也是根据主题分布来“随机”选取，这里的随机不是完全随机的意思，而是根据各个主题出现的概率值大小来抽取。比如当dirichlet先验为文档d生成的主题分布{ P(zi), i =1,2,3 }是{0.4,0.5,0.1}时，那么主题z2在文档d中出现的概率便是0.5。所以，从主题分布中抽取主题，这个过程也不是完全随机的，而是按照各个主题出现的概率值大小进行抽取。

4.3.3 pLSA跟LDA的概率图对比

    接下来，对比下LDA跟pLSA的概率模型图模型，左图是pLSA，右图是LDA（右图不太规范，z跟w都得是小写， 其中，阴影圆圈表示可观测的变量，非阴影圆圈表示隐变量，箭头表示两变量间的条件依赖性conditional dependency，方框表示重复抽样，方框右下角的数字代表重复抽样的次数）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359454253285997.png'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359454968917172.png'/>

对应到上面右图的LDA，只有W / w是观察到的变量，其他都是隐变量或者参数，其中，Φ表示词分布，Θ表示主题分布，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359458595491102.png'/>是主题分布Θ的先验分布（即Dirichlet 分布）的参数，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535946061014763.png'/>是词分布Φ的先验分布（即Dirichlet 分布）的参数，N表示文档的单词总数，M表示文档的总数。

    所以，对于一篇文档d中的每一个单词，LDA根据先验知识<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359463185900462.png'/>确定某篇文档的主题分布θ，然后从该文档所对应的多项分布（主题分布）θ中抽取一个主题z，接着根据先验知识<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535946517204555.png'/>确定当前主题的词语分布ϕ，然后从主题z所对应的多项分布（词分布）ϕ中抽取一个单词w。然后将这个过程重复N次，就产生了文档d。

    换言之：
1.假定语料库中共有M篇文章，每篇文章下的Topic的主题分布是一个从参数为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359468293917039.png'/>的Dirichlet先验分布中采样得到的Multinomial分布，每个Topic下的词分布是一个从参数为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359470361659360.png'/>的Dirichlet先验分布中采样得到的Multinomial分布。
2.对于某篇文章中的第n个词，首先从该文章中出现的每个主题的Multinomial分布（主题分布）中选择或采样一个主题，然后再在这个主题对应的词的Multinomial分布（词分布）中选择或采样一个词。不断重复这个随机生成过程，直到M篇文章全部生成完成。

    综上，M 篇文档会对应于 M 个独立的 Dirichlet-Multinomial 共轭结构，K 个 topic 会对应于 K 个独立的 Dirichlet-Multinomial 共轭结构。

其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359475913343454.png'/>→θ→z 表示生成文档中的所有词对应的主题，显然<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535947715124992.png'/>→θ 对应的是Dirichlet 分布，θ→z 对应的是 Multinomial 分布，所以整体是一个 Dirichlet-Multinomial 共轭结构，如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359480762738464.png'/>

类似的，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359482582107994.png'/>→φ→w，容易看出， 此时β→φ对应的是 Dirichlet 分布， φ→w 对应的是 Multinomial 分布， 所以整体也是一个Dirichlet-Multinomial 共轭结构，如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359486495256002.png'/>

4.3.4 pLSA跟LDA参数估计方法的对比

    上面对比了pLSA跟LDA生成文档的不同过程，下面，咱们反过来，假定文档已经产生，反推其主题分布。那么，它们估计未知参数所采用的方法又有什么不同呢？

在pLSA中，我们使用EM算法去估计“主题-词项”矩阵Φ（由<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359492797010796.png'/>转换得到）和“文档-主题”矩阵Θ（由<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359494649312679.png'/>转换得到）这两个参数，而且这两参数都是个固定的值，只是未知，使用的思想其实就是极大似然估计MLE。

而在LDA中，估计Φ、Θ这两未知参数可以用变分(Variational inference)-EM算法，也可以用gibbs采样，前者的思想是最大后验估计MAP（MAP与MLE类似，都把未知参数当作固定的值），后者的思想是贝叶斯估计。贝叶斯估计是对MAP的扩展，但它与MAP有着本质的不同，即贝叶斯估计把待估计的参数看作是服从某种先验分布的随机变量。
关于贝叶斯估计再举个例子。假设中国的大学只有两种：理工科和文科，这两种学校数量的比例是1:1，其中，理工科男女比例7:1，文科男女比例1:7。某天你被外星人随机扔到一个校园，问你该学校可能的男女比例是多少？然后，你实际到该校园里逛了一圈，看到的5个人全是男的，这时候再次问你这个校园的男女比例是多少？

1.因为刚开始时，有先验知识，所以该学校的男女比例要么是7:1，要么是1:7，即P(比例为7:1) = 1/2，P(比例为1:7) = 1/2。
2.然后看到5个男生后重新估计男女比例，其实就是求P(比例7:1|5个男生）= ？，P(比例1:7|5个男生) = ？
3.用贝叶斯公式<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359514141108834.png'/>可得：P(比例7:1|5个男生) = P(比例7:1)*P(5个男生|比例7:1) / P(5个男生)，P(5个男生)是5个男生的先验概率，与学校无关，所以是个常数；类似的，P(比例1:7|5个男生) = P((比例1:7)*P(5个男生|比例1:7)/P(5个男生)。
4.最后将上述两个等式比一下，可得：P(比例7:1|5个男生)/P(比例1:7|5个男生) = {P((比例7:1)*P(5个男生|比例7:1)} / { P(比例1:7)*P(5个男生|比例1:7)}。

由于LDA把要估计的主题分布和词分布看作是其先验分布是Dirichlet分布的随机变量，所以，在LDA这个估计主题分布、词分布的过程中，它们的先验分布（即Dirichlet分布）事先由人为给定，那么LDA就是要去求它们的后验分布（LDA中可用gibbs采样去求解它们的后验分布，得到期望<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359522940867967.png'/>、<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359527491936344.png'/>）！

  此外，不厌其烦的再插一句，在LDA中，主题分布和词分布本身都是多项分布，而由上文3.2节可知“Dirichlet分布是多项式分布的共轭先验概率分布”，因此选择Dirichlet 分布作为它们的共轭先验分布。意味着为多项分布的参数p选取的先验分布是Dirichlet分布，那么以p为参数的多项分布用贝叶斯估计得到的后验分布仍然是Dirichlet分布。

4.3.5 LDA参数估计：Gibbs采样

    理清了LDA中的物理过程，下面咱们来看下如何学习估计。

    类似于pLSA，LDA的原始论文中是用的变分-EM算法估计未知参数，后来发现另一种估计LDA未知参数的方法更好，这种方法就是：Gibbs Sampling，有时叫Gibbs采样或Gibbs抽样，都一个意思。Gibbs抽样是马尔可夫链蒙特卡尔理论（MCMC）中用来获取一系列近似等于指定多维概率分布（比如2个或者多个随机变量的联合概率分布）观察样本的算法。

    OK，给定一个文档集合，w是可以观察到的已知变量，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359532295718453.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359533165831980.png'/>是根据经验给定的先验参数，其他的变量z，θ和φ都是未知的隐含变量，需要根据观察到的变量来学习估计的。根据LDA的图模型，可以写出所有变量的联合分布：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359536591356059.png'/>

注：上述公式中及下文中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359542072756136.png'/>等价上文中定义的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359543426315732.png'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359546885021084.png'/>等价于上文中定义的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359548691118426.png'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359551783463011.png'/>等价于上文中定义的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359553751938575.png'/>，等价于上文中定义的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359555428029823.png'/>。

因为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359557999852117.png'/>产生主题分布θ，主题分布θ确定具体主题，且<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359559851337359.png'/>产生词分布φ、词分布φ确定具体词，所以上述式子等价于下述式子所表达的联合概率分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359563427448219.png'/>：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359565643480339.png'/>

其中，第一项因子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359570837415527.png'/>表示的是根据确定的主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359573958414593.png'/>和词分布的先验分布参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359576639474534.png'/>
采样词的过程，第二项因子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359580311650615.png'/>是根据主题分布的先验分布参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359582066245634.png'/>采样主题的过程，这两项因子是需要计算的两个未知参数。

    由于这两个过程是独立的，所以下面可以分别处理，各个击破。

第一个因子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359585480164890.png'/>，可以根据确定的主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359586943966272.png'/>和从先验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359588773349861.png'/>取样得到的词分布Φ产生：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359590962076577.png'/>

   由于样本中的词服从参数为主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359618289654499.png'/>的独立多项分布，这意味着可以把上面对词的乘积分解成分别对主题和对词的两层乘积：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359593565890753.png'/>

其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359596066791425.png'/>是词 t 在主题 k 中出现的次数。

    回到第一个因子上来。目标分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359597966127928.png'/>需要对词分布Φ积分，且结合我们之前在3.1节定义的Dirichlet 分布的归一化系数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359611537715401.png'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359613776670830.png'/>

  可得：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359622645015940.png'/>

这个结果可以看作K个Dirichlet-Multinomial模型的乘积。
现在开始求第二个因子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359625338257227.png'/>。类似于<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359627036094269.png'/>的步骤，先写出条件分布，然后分解成两部分的乘积：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535962945646534.png'/>

其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359631142393470.png'/>表示的单词 i 所属的文档，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359633983813018.png'/>是主题 k 在文章 m 中出现的次数。

    对主题分布Θ积分可得：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359636767049375.png'/>

 综合第一个因子和第二个因子的结果，得到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359639459843092.png'/>的联合分布结果为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359644266955465.png'/>

接下来，有了联合分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359647047163864.png'/>，咱们便可以通过联合分布来计算在给定可观测变量 w 下的隐变量 z 的条件分布（后验分布）<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155361082156196393.png'/>来进行贝叶斯分析。

换言之，有了这个联合分布后，要求解第m篇文档中的第n个词（下标为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359654142524587.png'/>的词）的全部条件概率就好求了。

 先定义几个变量。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535965893140882.png'/>表示除去<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535966138089608.png'/>的词，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359664740690253.png'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359666565248711.png'/>

然后，排除当前词的主题分配，即根据其他词的主题分配和观察到的单词来计算当前词主题的概率公式为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359668959930366.png'/>

勘误：考虑到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359671922591402.png'/>，所以上述公式的第二行的分子，非p(w,z) *p(z)，而是p(w|z)*p(z)。

    且有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359674945226332.png'/>

    最后一步，便是根据Markov链的状态获取主题分布的参数Θ和词分布的参数Φ。

    换言之根据贝叶斯法则和Dirichlet先验，以及上文中得到的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359681388661477.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359683243973640.png'/>自被分解成两部分乘积的结果，可以计算得到每个文档上Topic的后验分布和每个Topic下的词的后验分布分别如下（据上文可知：其后验分布跟它们的先验分布一样，也都是Dirichlet 分布）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359685989285980.png'/>

其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359691132878833.png'/>是构成文档m的主题数向量，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359693536235563.png'/>是构成主题k的词项数向量。

  此外，别忘了上文中2.4节所述的Dirichlet的一个性质，如下：
“ 如果<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359696734950792.png'/>，同样可以证明有下述结论成立：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359699468234589.png'/>

即：如果<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359702552311065.png'/>，则<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359706273738100.png'/>中的任一元素<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359707051302300.png'/>的期望是：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359709377025673.png'/>

可以看出，超参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359713397744124.png'/>的直观意义就是事件先验的伪计数(prior pseudo-count)。 ”
    所以，最终求解的Dirichlet 分布期望为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359715726876809.png'/>

然后将<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359717666161966.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359724431736088.png'/>的结果代入之前得到的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359727179740137.png'/>的结果中，可得：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359729581510899.png'/>

    仔细观察上述结果，可以发现，式子的右半部分便是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359731996365266.png'/>，这个概率的值对应着<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359734227139103.png'/>的路径概率。如此，K 个topic 对应着K条路径，Gibbs Sampling 便在这K 条路径中进行采样，如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359739411606863.png'/>

何等奇妙，就这样，Gibbs Sampling通过求解出主题分布和词分布的后验分布，从而成功解决主题分布和词分布这两参数未知的问题。

5 读者微评
 

    本文发表后，部分热心的读者在微博上分享了他们自己理解LDA的心得，也欢迎更多朋友分享你的理解心得（比如评论在本文下，或评论在微博上），从而在分享、讨论的过程中让更多人可以更好的理解：

@SiNZeRo：lda 如果用em就是 map估计了. lda本意是要去找后验分布 然后拿后验分布做bayesian分析. 比如theta的期望 . 而不是把先验作为正则化引入。最后一点gibbs sampling其实不是求解的过程 是去explore后验分布 去采样 用于求期望.
@研究者July：好问题好建议，这几天我陆续完善下！//@帅广应s：LDA这个东西该怎么用？可以用在哪些地方？还有就是Gibbs抽样的原理是什么？代码怎么实现？如果用EM来做，代码怎么实现？ LDA模型的变形和优化有哪些？LDA不适用于解决哪类的问题？总之，不明白怎么用，参数怎么调优？ 

@xiangnanhe：写的很好，4.1.3节中的那两个图很赞，非常直观的理解了LDA模型加了先验之后在学参数的时候要比PLSI更灵活；PLSI在学参数的过程中比较容易陷入local minimum然后overfitting。

@asker2：无论是pLSA中，还是LDA中，主题分布和词分布本身是固定的存在，但都未知。pLSA跟LDA的区别在于，去探索这两个未知参数的方法或思想不一样。pLSA是求到一个能拟合文本最好的参数（分布），这个值就认为是真实的参数。但LDA认为，其实我们没法去完全求解出主题分布、词分布到底是什么参数，我们只能把它们当成随机变量，通过缩小其方差（变化度）来尽量让这个随机变量变得更“确切”。换言之，我们不再求主题分布、词分布的具体值，而是通过这些分布生成的观测值（即实际文本）来反推分布的参数的范围，即在什么范围比较可能，在什么范围不太可能。所以，其实这就是一种贝叶斯分析的思想，虽然无法给出真实值具体是多少，但可以按照经验给一个相对合理的真实值服从的先验分布，然后从先验出发求解其后验分布。
..
 

6 参考文献与推荐阅读
1.Blei, David M.; Ng, Andrew Y.; Jordan, Michael I. Latent Dirichlet allocation（LDA原始论文）：http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf。
2 Blei. Probabilistic Topic Models：http://www.cs.princeton.edu/~blei/papers/Blei2012.pdf，一网友的翻译：http://www.cnblogs.com/siegfang/archive/2013/01/30/2882391.html；
3 一堆wikipedia，比如隐含狄利克雷分布LDA的wiki：http://zh.wikipedia.org/wiki/éå«çå©åé·åå¸，狄利克雷分布的wiki：http://zh.wikipedia.org/wiki/çå©åé·åå¸；
4.从贝叶斯方法谈到贝叶斯网络 ；
5.rickjin的LDA数学八卦（力荐，本文部分图片和公式来自于此文档）网页版：http://www.flickering.cn/tag/lda/，PDF版：http://emma.memect.com/t/9756da9a47744de993d8df13a26e04e38286c9bc1c5a0d2b259c4564c6613298/LDA；

6.Thomas Hofmann.Probabilistic Latent Semantic Indexing（pLSA原始论文）：http://cs.brown.edu/~th/papers/Hofmann-SIGIR99.pdf；
7.Gregor Heinrich.Parameter estimation for text analysis（关于Gibbs 采样最精准细致的论述）：http://www.arbylon.net/publications/text-est.pdf；
8.Probabilistic latent semantic analysis (pLSA)：http://blog.tomtung.com/2011/10/plsa/http://blog.tomtung.com/2011/10/plsa/。
9.《概率论与数理统计教程第二版 茆诗松等人著》，如果忘了相关统计分布，建议复习此书或此文第二部分；
10.《支持向量机通俗导论：理解SVM的三层境界》，第二部分关于拉格朗日函数的讨论；

11.机器学习班第11次课上，邹博讲EM & GMM的PPT：http://pan.baidu.com/s/1i3zgmzF；
12.机器学习班第12次课上，邹博讲主题模型LDA的PPT：http://pan.baidu.com/s/1jGghtQm；
13.主题模型之pLSA：http://blog.jqian.net/post/plsa.html；
14主题模型之LDA：http://blog.jqian.net/post/lda.html；
15.搜索背后的奥秘——浅谈语义主题计算：http://www.semgle.com/search-engine-algorithms-mystery-behind-
search-on-the-calculation-of-semantic-topic；

16.LDA的EM推导：http://www.cnblogs.com/hebin/archive/2013/04/25/3043575.html；
17.Machine Learning读书会第8期上，沈博讲主题模型的PPT：http://vdisk.weibo.com/s/zrFL6OXKgKMAf；
18.Latent Dirichlet Allocation （LDA）- David M.Blei：http://www.xperseverance.net/blogs/2012/03/17/；
19.用GibbsLDA做Topic Modeling：http://weblab.com.cityu.edu.hk/blog/luheng/2011/06/24/ç¨gibbsldaåtopic-modeling/#comment-87；
20.主题模型在文本挖掘中的应用：http://net.pku.edu.cn/~zhaoxin/Topic-model-xin-zhao-wayne.pdf；

21.二项分布和多项分布，beta分布的对比：http://www.cnblogs.com/wybang/p/3206719.html；
22.LDA简介：http://cos.name/2010/10/lda_topic_model/；
23.LDA的相关论文、工具库：http://site.douban.com/204776/widget/notes/12599608/note/287085506/；
24.一个网友学习LDA的心得：http://www.xuwenhao.com/2011/03/20/suggestions-for-programmers-to-learn-lda/；
25.http://blog.csdn.net/hxxiaopei/article/details/7617838；

26.主题模型LDA及其在微博推荐&广告算法中的应用：http://www.wbrecom.com/?p=136；
27.LDA发明人之一Blei 写的毕业论文：http://www.cs.princeton.edu/~blei/papers/Blei2004.pdf；
28.LDA的一个C实现：http://www.cs.princeton.edu/~blei/lda-c/index.html；
29.LDA的一些其它资料：http://www.xperseverance.net/blogs/2012/03/657/。
## Word2Vec中为什么使用负采样（negtive sample）？
解析一
如七月在线推荐就业班的专家讲师李老师所言
负采样这个点引入word2vec非常巧妙，两个作用：
1.加速了模型计算
2.保证了模型训练的效果，其一 模型每次只需要更新采样的词的权重，不用更新所有的权重，那样会很慢，其二 中心词其实只跟它周围的词有关系，位置离着很远的词没有关系，也没必要同时训练更新，作者这点非常聪明。

解析二
下述解析来源于：https://zhuanlan.zhihu.com/p/29488930
1. 随机梯度下降法有什么问题？
通过对代价函数求权重的梯度，我们可以一次性对所有的参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415527417807244433.svg'/>进行优化，但是如果每次等全部计算完成再优化升级，我们将等待很长时间（对于很大的语料库来说）。

所以我们采用随机梯度下降（ Stochastic Gradient Descent），也就是说每次完成一次计算就进行升级。

但是，还有两个问题导致目前的模型效率低下！
第一个问题，我们每次只对窗口中出现的几个单词进行升级，但是在计算梯度的过程中，我们是对整个参数矩阵进行运算，这样参数矩阵中的大部分值都是0。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274196765063064.jpg'/>

计算效率低下！

第二个问题：我们使用的目标函数是softmax函数   
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274198388176499.svg'/>

我们观察分母，分母需要把窗口中所有单词的“得分”都算出来再求和，效率低下！

2. 使用负采样
负采样的核心思想是：计算目标单词和窗口中的单词的真实单词对“得分”，再加一些“噪声”，即词表中的随机单词和目标单词的“得分”。

真实单词对“得分”和“噪声”作为代价函数。
每次优化参数，只关注代价函数中涉及的词向量。

下面给出公式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274230878559348.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274232155160337.svg'/>

采用上述公式解决了之前说的两个问题：

我们仅对K个参数进行采样
我们放弃softmax函数，采用sigmoid函数，这样就不存在先求一遍窗口中所有单词的‘“得分”的情况了。

3. 计算梯度
既然代价函数已经更新了，那么我们需要对梯度进行更新。

首先考虑一下，我们想要求导的目标，也就是对谁求导？

答案是，对我们想要优化的参数求导，前面说了，负采样的目的是不需要对整个向量矩阵 U或 V 进行优化，而是仅对求代价过程中涉及的词向量进行优化，因此，求导对象是目标向量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274269955206683.svg'/>,窗口中的其他词向量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641552742710818869.svg'/>和负采样时随机选取的词向量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274272164152465.svg'/> 。

此篇文章关注的问题不是求导的过程，因此下面直接给出梯度：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274273898566277.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274275076208119.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274276112212624.svg'/>
## 什么是TF-IDF算法？
TF-IDF(term frequency–inverse document frequency)是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。

TF-IDF有两层意思，一层是"词频"（Term Frequency，缩写为TF），另一层是"逆文档频率"（Inverse Document Frequency，缩写为IDF）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156630878613221494.jpg'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156630878961416696.jpg'/>

假设我们现在有一片长文叫做《量化系统架构设计》词频高在文章中往往是停用词，“的”，“是”，“了”等，这些在文档中最常见但对结果毫无帮助、需要过滤掉的词，用TF可以统计到这些停用词并把它们过滤。当高频词过滤后就只需考虑剩下的有实际意义的词。

但这样又会遇到了另一个问题，我们可能发现"量化"、"系统"、"架构"这三个词的出现次数一样多。这是不是意味着，作为关键词，它们的重要性是一样的？事实上系统应该在其他文章比较常见，所以在关键词排序上，“量化”和“架构”应该排在“系统”前面，这个时候就需要IDF，IDF会给常见的词较小的权重，它的大小与一个词的常见程度成反比。

当有TF(词频)和IDF(逆文档频率)后，将这两个词相乘，就能得到一个词的TF-IDF的值。某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由大到小排序，排在最前面的几个词，就是该文章的关键词。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156630881464111037.jpg'/>

TF-IDF算法步骤

第一步，计算词频：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156630882773571227.jpg'/>
考虑到文章有长短之分，为了便于不同文章的比较，进行"词频"标准化。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415718241246797631.jpg'/>
第二步，计算逆文档频率：

这时，需要一个语料库（corpus），用来模拟语言的使用环境。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157182414255715037.jpg'/>
如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数。

第三步，计算TF-IDF：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157182415370411571.jpg'/>
可以看到，TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。所以，自动提取关键词的算法就很清楚了，就是计算出文档的每个词的TF-IDF值，然后按降序排列，取排在最前面的几个词。

优缺点
TF-IDF的优点是简单快速，而且容易理解。缺点是有时候用词频来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。如果要体现词的上下文结构，那么你可能需要使用word2vec算法来支持。

示例代码
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156630890881435667.jpg'/>

本题解析来源：https://zhuanlan.zhihu.com/p/31197209
## 请说说word2vec的简要理解
本题解析来源：https://blog.csdn.net/lilong117194/article/details/81979522?tdsourcetag=s_pcqq_aiomsg

在自然语言处理领域中，本文向量化是文本表示的一种重要方式。在当前阶段，对文本的大部分研究都是通过词向量化实现的，但同时也有一部分研究将句子作为文本处理的基本单元，也就是doc2vec和str2vec技术。

1. word2vec简介
大家很熟悉的词袋(bag of words)模型是最早的以词语为基本处理单元的文本向量化算法，所谓的词袋模型就是借助于词典把文本转化为一组向量，下面是两个简单的文本示例：

john likes to watch movies, mary likes too.
john also likes to watch football games.

现假设词典如下： 
{"john":1,"likes":2,"to":3,"watch":4, "movies":5,"also":6,"football":7,"games":8,"mary":9 "too":10} 
在这个自己构建的词典中，每个单词都有一个唯一的索引，那么上述的两个文本就可以基于这个暂时的词典来构建其文本的向量表示，如下： 
[1,2,1,1,1,0,0,0,1,1] 
[1,1,1,1,0,1,1,1,0,0] 

由此可以看出此向量的构建是根据该词在词典出现的次数而构成的，比如第一条文本中的”likes”,这个词在文本中出现了2次，所以基于词袋的文本向量是根据词出现的次数构建的。但是此向量与文本中单词出现的顺序没有关系，只是一种频率的表示，该方法容易实现，但是有很大的问题：
a)维数灾难：假如词典包含10000个单词，那么每个文本需要使用10000维的向量表示，那么向量的很多位置必定是0，如此稀疏的高维向量会严重影响计算速度。
b)这样构成的向量无法保存词序信息，而词序对于自然语言处理又是那么的重要。
c)存在语义鸿沟

例如：关于数据稀疏的问题 
自然语言处理经常把字词转为离散的单独的符号，也就是One-Hot Encoder。
杭州 [0,0,0,0,0,0,0,1,0,……，0,0,0,0,0,0,0]
上海 [0,0,0,0,1,0,0,0,0,……，0,0,0,0,0,0,0]
宁波 [0,0,0,1,0,0,0,0,0,……，0,0,0,0,0,0,0]
北京 [0,0,0,0,0,0,0,0,0,……，1,0,0,0,0,0,0]

比如上面的这个例子，在语料库中，杭州、上海、宁波、北京各对应一个向量，向量中只有一个值为1，其余都为0。但是使用One-Hot Encoder有以下问题。一方面，城市编码是随机的，向量之间相互独立，看不出城市之间可能存在的关联关系。其次，向量维度的大小取决于语料库中字词的多少。如果将世界所有城市名称对应的向量合为一个矩阵的话，那这个矩阵过于稀疏，并且会造成维度灾难。

现在随着互联网的发展，大量的无标注数据产生，此时的word2vec技术即是利用神经网络从大量的无标注的文本中提取有用的信息而产生的。

为什么说word2vec能提取有用的信息呢？ 
我们知道词语是表达语义的基本单元，而词袋模型只是简单的将词语符号化，举个不太恰当的比喻就是：现在有”一麻袋”的词语，而我们要处理的文本就像是从一个麻袋中无序得（不分先后顺序）抽出麻袋中所有的词，再查看文本中出现的个数，注意这里的从麻袋中抽取词的过程是无序的，也就是只是简单的统计文本中有没有出现该词和该词出现了几次，所以对于词袋模型，文本的语序特征就丧失了，也就丧失了语义的信息。

此时我们需要一个模型就是能在使文本向量化的同时也保留了词序的信息。分布式假说的提出就是解决了语义信息的问题。该方法的思想是：上下文相似的词，其语义也相似，随后就有了基于上下文分布表示词义的方法，这就是“词空间模型“。Word2Vec可以将One-Hot Encoder转化为低维度的连续值，也就是稠密向量，并且其中意思相近的词将被映射到向量空间中相近的位置。而使用神经网络可以灵活的对上下文进行建模，也因此成为用的比较多的方法。

2. 模型简介
one-hot向量作为word2vec的输入，通过word2vec训练低维词向量（word embedding） 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156820904924499143.png'/>
输入层：One-Hot Vector 
隐藏层：没有激活函数，也就是线性的单元。 
输出层：维度跟输入层的维度一样，用的是Softmax回归。 
我们要获取的dense vector其实就是Hidden Layer的输出单元。有的地方定为Input Layer和Hidden Layer之间的权重，其实说的是一回事。

下面用具体的例子看下： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156820906914753774.png'/>
可以看出： 
输入层：5个神经元 
隐藏层：3个神经元 
所以权重矩阵是5x3的大小，可以看出权重矩阵中的[10,12,19]和前向传播后[10,12,19]是一样的。

3. CBOW模式
word2vec主要分为CBOW（Continuous Bag of Words）和Skip-Gram两种模式。CBOW是从原始语句推测目标字词；而Skip-Gram正好相反，是从目标字词推测出原始语句。CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。

CBOW模型的理解： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156820922755035768.png'/>
CBOW模型结构图
1 输入层：上下文单词的onehot. {假设单词向量空间dim为V，也就是词典的大小。上下文单词个数为C}。
2 所有onehot分别乘以共享的输入权重矩阵W(V*N矩阵，N为自己设定的数，N也是隐藏层的神经元个数，初始化权重矩阵W）。
3 所得的向量 {因为是onehot所以为向量} 相加求平均作为隐层向量, size为1*N。
4 乘以输出权重矩阵W′W′(N*V)。
5 得到向量 (1*V) ，激活函数处理得到V-dim概率分布，概率最大的index所指示的单词为预测出的中间词(target word)。
6 与true label的onehot做比较，误差越小越好。

所以，需要定义loss function（一般为交叉熵代价函数），采用梯度下降算法更新W和W′W′。训练完毕后，输入层的每个单词与矩阵W相乘得到的向量的就是我们想要的词向量（word embedding），这个矩阵（所有单词的word embedding）也叫做look up table（其实这个look up table就是矩阵W自身），也就是说，任何一个单词的onehot乘以这个矩阵都将得到自己的词向量。有了look up table就可以免去训练过程直接查表得到单词的词向量了。

案例： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156820926741308823.png'/>

5. Skip-Gram模式
从直观上理解，Skip-Gram是给定input word来预测上下文。

接下来我们来看看如何训练我们的神经网络。假如我们有一个句子“The dog barked at the mailman”。

首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；

有了input word以后，我们再定义一个叫做skip_window的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设置skip_window=2，那么我们最终获得窗口中的词（包括input word在内）就是[‘The’, ‘dog’，’barked’, ‘at’]。skip_window=2代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小span=2x2=4。

另一个参数叫num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当skip_window=2，num_skips=2时，我们将会得到两组 (input word, output word) 形式的训练数据，即 (‘dog’, ‘barked’)，(‘dog’, ‘the’)。

神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词是output word的可能性。这句话有点绕，我们来看个栗子。第二步中我们在设置skip_window和num_skips=2的情况下获得了两组训练数据。假如我们先拿一组数据 (‘dog’, ‘barked’) 来训练神经网络，那么模型通过学习这个训练样本，会告诉我们词汇表中每个单词是“barked”的概率大小。

模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。举个栗子，如果我们向神经网络模型中输入一个单词“中国“，那么最终模型的输出概率中，像“英国”， ”俄罗斯“这种相关词的概率将远高于像”苹果“，”蝈蝈“非相关词的概率。因为”英国“，”俄罗斯“在文本中更大可能在”中国“的窗口中出现。我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。

面的图中给出了一些我们的训练样本的例子。我们选定句子“The quick brown fox jumps over lazy dog”，设定我们的窗口大小为2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。Training Samples（输入， 输出） 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156820979228136457.png'/>
我们的模型将会从每对单词出现的次数中习得统计结果。例如，我们的神经网络可能会得到更多类似（“中国“，”英国“）这样的训练样本对，而对于（”英国“，”蝈蝈“）这样的组合却看到的很少。因此，当我们的模型完成训练后，给定一个单词”中国“作为输入，输出的结果中”英国“或者”俄罗斯“要比”蝈蝈“被赋予更高的概率。

再次提醒，最终我们需要的是训练出来的权重矩阵。

5. 训练优化
此时注意到，这个训练过程的参数规模非常巨大。 
假设语料库中有30000个不同的单词，hidden layer取128，word2vec两个权值矩阵维度都是[30000,128]，在使用SGD对庞大的神经网络进行学习时，将是十分缓慢的。而且，你需要大量的训练数据来调整许多权重，避免过度拟合。数以百万计的重量数十亿倍的训练样本意味着训练这个模型将是一个野兽。 
一般来说，有两种加速算法：Hierarchical Softmax、Negative Sampling等方式来解决。

参考： 
https://blog.csdn.net/mylove0414/article/details/61616617 
https://blog.csdn.net/free356/article/details/79445895
## 如何理解Word2vec 之 Skip-Gram 模型
本题解析来源：https://zhuanlan.zhihu.com/p/27234078

什么是Word2Vec和Embeddings？
Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，它被大量地用在自然语言处理（NLP）中。那么它是如何帮助我们做自然语言处理呢？

Word2Vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。
本质上，Embedding其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。

我们从直观角度上来理解一下，cat这个单词和kitten属于语义上很相近的词，而dog和kitten则不是那么相近，iphone这个单词和kitten的语义就差的更远了。通过对词汇表中单词进行这种数值表示方式的学习（也就是将单词转换为词向量），能够让我们基于这样的数值进行向量化的操作从而得到一些有趣的结论。

比如说，如果我们对词向量kitten、cat以及dog执行这样的操作：kitten - cat + dog，那么最终得到的嵌入向量（embedded vector）将与puppy这个词向量十分相近。

第一部分
模型
Word2Vec模型中，主要有CBOW和Skip-Gram两种模型，从直观上理解，CBOW是给定上下文来预测input word，而Skip-Gram是给定input word来预测上下文。本篇文章仅讲解Skip-Gram模型。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821561640625665.png'/>
Skip-Gram模型的基础形式非常简单，为了更清楚地解释模型，我们先从最一般的基础模型来看Word2Vec（下文中所有的Word2Vec都是指Skip-Gram模型）。

Word2Vec模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。基于训练数据建模的过程，我们给它一个名字叫“Fake Task”，意味着建模并不是我们最终的目的。

上面提到的这种方法实际上会在无监督特征学习（unsupervised feature learning）中见到，最常见的就是自编码器（auto-encoder）：通过在隐层将输入进行编码压缩，继而在输出层将数据解码恢复初始状态，训练完成后，我们会将输出层“砍掉”，仅保留隐层。

The Fake Task
我们在上面提到，训练模型的真正目的是获得模型基于训练数据学得的隐层权重。为了得到这些权重，我们首先要构建一个完整的神经网络作为我们的“Fake Task”，后面再返回来看通过“Fake Task”我们如何间接地得到这些词向量。

接下来我们来看看如何训练我们的神经网络。假如我们有一个句子“The dog barked at the mailman（狗对邮递员吠叫）”。

首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；
有了input word以后，我们再定义一个叫做skip_window的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设置<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821596247104635.svg'/>，那么我们最终获得窗口中的词（包括input word在内）就是[&#39;The&#39;, &#39;dog&#39;，&#39;barked&#39;, &#39;at&#39;]。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821612511990114.svg'/>代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821615820640129.svg'/>。

另一个参数叫num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821618970912257.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821619542372541.svg'/>时，我们将会得到两组 (input word, output word) 形式的训练数据，即 (&#39;dog&#39;, &#39;barked&#39;)，(&#39;dog&#39;, &#39;the&#39;)。

神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词是output word的可能性。这句话有点绕，我们来看个栗子。

第二步中我们在设置skip_window和num_skips=2的情况下获得了两组训练数据。假如我们先拿一组数据 (&#39;dog&#39;, &#39;barked&#39;) 来训练神经网络，那么模型通过学习这个训练样本，会告诉我们词汇表中每个单词是“barked”的概率大小。模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。

举个栗子，如果我们向神经网络模型中输入一个单词“Soviet“，那么最终模型的输出概率中，像“Union”， ”Russia“这种相关词的概率将远高于像”watermelon“，”kangaroo“非相关词的概率。因为”Union“，”Russia“在文本中更大可能在”Soviet“的窗口中出现。

我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。下面的图中给出了一些我们的训练样本的例子。我们选定句子“The quick brown fox jumps over lazy dog”，设定我们的窗口大小为2（<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821622585857030.svg'/>），也就是说我们仅选输入词前后各两个词和输入词进行组合。

下图中，蓝色代表input word，方框内代表位于窗口内的单词。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821625740244978.png'/>
我们的模型将会从每对单词出现的次数中习得统计结果。例如，我们的神经网络可能会得到更多类似（“Soviet“，”Union“）这样的训练样本对，而对于（”Soviet“，”Sasquatch“）这样的组合却看到的很少。

因此，当我们的模型完成训练后，给定一个单词”Soviet“作为输入，输出的结果中”Union“或者”Russia“要比”Sasquatch“被赋予更高的概率。

模型细节
我们如何来表示这些单词呢？

首先，我们都知道神经网络只能接受数值输入，我们不可能把一个单词字符串作为输入，因此我们得想个办法来表示这些单词。最常用的办法就是基于训练文档来构建我们自己的词汇表（vocabulary）再对单词进行one-hot编码。

假设从我们的训练文档中抽取出10000个唯一不重复的单词组成词汇表。我们对这10000个单词进行one-hot编码，得到的每个单词都是一个10000维的向量，向量每个维度的值只有0或者1，假如单词ants在词汇表中的出现位置为第3个，那么ants的向量就是一个第三维度取值为1，其他维都为0的10000维的向量（<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821632794376147.svg'/>）。

还是上面的例子，“The dog barked at the mailman”，那么我们基于这个句子，可以构建一个大小为5的词汇表（忽略大小写和标点符号）：("the", "dog", "barked", "at", "mailman")，我们对这个词汇表的单词进行编号0-4。那么”dog“就可以被表示为一个5维向量[0, 1, 0, 0, 0]。

模型的输入如果为一个10000维的向量，那么输出也是一个10000维度（词汇表的大小）的向量，它包含了10000个概率，每一个概率代表着当前词是输入样本中output word的概率大小。

下图是我们神经网络的结构：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821635134131103.png'/>
隐层没有使用任何激活函数，但是输出层使用了sotfmax。

我们基于成对的单词来对神经网络进行训练，训练样本是 ( input word, output word ) 这样的单词对，input word和output word都是one-hot编码的向量。最终模型的输出是一个概率分布。

隐层

说完单词的编码和训练样本的选取，我们来看下我们的隐层。如果我们现在想用300个特征来表示一个单词（即每个词可以被表示为300维的向量）。那么隐层的权重矩阵应该为10000行，300列（隐层有300个结点）。
Google在最新发布的基于Google news数据集训练的模型中使用的就是300个特征的词向量。词向量的维度是一个可以调节的超参数（在Python的gensim包中封装的Word2Vec接口默认的词向量大小为100， window_size为5）。

看下面的图片，左右两张图分别从不同角度代表了输入层-隐层的权重矩阵。左图中每一列代表一个10000维的词向量和隐层单个神经元连接的权重向量。从右边的图来看，每一行实际上代表了每个单词的词向量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821637292037709.png'/>
所以我们最终的目标就是学习这个隐层的权重矩阵。
我们现在回来接着通过模型的定义来训练我们的这个模型。上面我们提到，input word和output word都会被我们进行one-hot编码。仔细想一下，我们的输入被one-hot编码以后大多数维度上都是0（实际上仅有一个位置为1），所以这个向量相当稀疏，那么会造成什么结果呢。如果我们将一个1 x 10000的向量和10000 x 300的矩阵相乘，它会消耗相当大的计算资源，为了高效计算，它仅仅会选择矩阵中对应的向量中维度值为1的索引行（这句话很绕），看图就明白。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821638986334887.png'/>
我们来看一下上图中的矩阵运算，左边分别是1 x 5和5 x 3的矩阵，结果应该是1 x 3的矩阵，按照矩阵乘法的规则，结果的第一行第一列元素为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821640674382966.svg'/>，同理可得其余两个元素为12，19。如果10000个维度的矩阵采用这样的计算方式是十分低效的。

为了有效地进行计算，这种稀疏状态下不会进行矩阵乘法计算，可以看到矩阵的计算的结果实际上是矩阵对应的向量中值为1的索引，上面的例子中，左边向量中取值为1的对应维度为3（下标从0开始），那么计算结果就是矩阵的第3行（下标从0开始）—— [10, 12, 19]，这样模型中的隐层权重矩阵便成了一个”查找表“（lookup table），进行矩阵计算时，直接去查输入向量中取值为1的维度下对应的那些权重值。隐层的输出就是每个输入单词的“嵌入词向量”。

输出层

经过神经网络隐层的计算，ants这个词会从一个1 x 10000的向量变成1 x 300的向量，再被输入到输出层。输出层是一个softmax回归分类器，它的每个结点将会输出一个0-1之间的值（概率），这些所有输出层神经元结点的概率之和为1。

下面是一个例子，训练样本为 (input word: “ants”， output word: “car”) 的计算示意图。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821642423711066.png'/>
直觉上的理解

下面我们将通过直觉来进行一些思考。

如果两个不同的单词有着非常相似的“上下文”（也就是窗口单词很相似，比如“Kitty climbed the tree”和“Cat climbed the tree”），那么通过我们的模型训练，这两个单词的嵌入向量将非常相似。

那么两个单词拥有相似的“上下文”到底是什么含义呢？比如对于同义词“intelligent”和“smart”，我们觉得这两个单词应该拥有相同的“上下文”。而例如”engine“和”transmission“这样相关的词语，可能也拥有着相似的上下文。

实际上，这种方法实际上也可以帮助你进行词干化（stemming），例如，神经网络对”ant“和”ants”两个单词会习得相似的词向量。词干化（stemming）就是去除词缀得到词根的过程。

第二部分
第一部分我们了解skip-gram的输入层、隐层、输出层。在第二部分，会继续深入讲如何在skip-gram模型上进行高效的训练。

在第一部分讲解完成后，我们会发现Word2Vec模型是一个超级大的神经网络（权重矩阵规模非常大）。

举个栗子，我们拥有10000个单词的词汇表，我们如果想嵌入300维的词向量，那么我们的输入-隐层权重矩阵和隐层-输出层的权重矩阵都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。百万数量级的权重矩阵和亿万数量级的训练样本意味着训练这个模型将会是个灾难（太凶残了）。

Word2Vec的作者在它的第二篇论文中强调了这些问题，下面是作者在第二篇论文中的三个创新：
1 将常见的单词组合（word pairs）或者词组作为单个“words”来处理。
2 对高频次单词进行抽样来减少训练样本的个数。
3 对优化目标采用“negative sampling”方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担。事实证明，对常用词抽样并且对优化目标采用“negative sampling”不仅降低了训练过程中的计算负担，还提高了训练的词向量的质量。

Word pairs and "phases"
论文的作者指出，一些单词组合（或者词组）的含义和拆开以后具有完全不同的意义。比如“Boston Globe”是一种报刊的名字，而单独的“Boston”和“Globe”这样单个的单词却表达不出这样的含义。因此，在文章中只要出现“Boston Globe”，我们就应该把它作为一个单独的词来生成其词向量，而不是将其拆开。同样的例子还有“New York”，“United Stated”等。

在Google发布的模型中，它本身的训练样本中有来自Google News数据集中的1000亿的单词，但是除了单个单词以外，单词组合（或词组）又有3百万之多。

如果你对模型的词汇表感兴趣，可以点击这里，你还可以直接浏览这个词汇表。

如果想了解这个模型如何进行文档中的词组抽取，可以看论文中“Learning Phrases”这一章，对应的代码word2phrase.c被发布在这里。

对高频词抽样

在第一部分的讲解中，我们展示了训练样本是如何从原始文档中生成出来的，这里我再重复一次。我们的原始文本为“The quick brown fox jumps over the laze dog”，如果我使用大小为2的窗口，那么我们可以得到图中展示的那些训练样本。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821649048207731.png'/>
但是对于“the”这种常用高频单词，这样的处理方式会存在下面两个问题：
①当我们得到成对的单词训练样本时，("fox", "the") 这样的训练样本并不会给我们提供关于“fox”更多的语义信息，因为“the”在每个单词的上下文中几乎都会出现。
②由于在文本中“the”这样的常用词出现概率很大，因此我们将会有大量的（”the“，...）这样的训练样本，而这些样本数量远远超过了我们学习“the”这个词向量所需的训练样本数。

Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。

如果我们设置窗口大小<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821654393333842.svg'/>（即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821654950932640.svg'/>），并且从我们的文本中删除所有的“the”，那么会有下面的结果：
i)由于我们删除了文本中所有的“the”，那么在我们的训练样本中，“the”这个词永远也不会出现在我们的上下文窗口中。
ii)当“the”作为input word时，我们的训练样本数至少会减少10个。

这句话应该这么理解，假如我们的文本中仅出现了一个“the”，那么当这个“the”作为input word时，我们设置span=10，此时会得到10个训练样本 ("the", ...) ，如果删掉这个“the”，我们就会减少10个训练样本。实际中我们的文本中不止一个“the”，因此当“the”作为input word的时候，至少会减少10个训练样本。上面提到的这两个影响结果实际上就帮助我们解决了高频词带来的问题。

抽样率

word2vec的C语言代码实现了一个计算在词汇表中保留某个词概率的公式。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821660035913093.svg'/>是一个单词，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821660960057105.svg'/>是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415682166216556831.svg'/>这个单词在所有语料中出现的频次。举个栗子，如果单词“peanut”在10亿规模大小的语料中出现了1000次，那么<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821664260389439.svg'/>。

在代码中还有一个参数叫“sample”，这个参数代表一个阈值，默认值为0.001（在gensim包中的Word2Vec类说明中，这个参数默认为0.001，文档中对这个参数的解释为“ threshold for configuring which higher-frequency words are randomly downsampled”）。这个值越小意味着这个单词被保留下来的概率越小（即有越大的概率被我们删除）。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821667218406991.svg'/>代表着保留某个单词的概率：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821667767554195.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821669356397987.png'/>
图中x轴代表着<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415682167169523557.svg'/>，即单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821672898807262.svg'/>在语料中出现频率，y轴代表某个单词被保留的概率。对于一个庞大的语料来说，单个单词的出现频率不会很大，即使是常用词，也不可能特别大。

从这个图中，我们可以看到，随着单词出现频率的增高，它被采样保留的概率越来越小，我们还可以看到一些有趣的结论：
当时<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821677797780491.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821678357190252.svg'/>。当单词在语料中出现的频率小于0.0026时，它是100%被保留的，这意味着只有那些在语料中出现频率超过0.26%的单词才会被采样。
当时<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821679039763958.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821679552009052.svg'/>，意味着这一部分的单词有50%的概率被保留。
当时<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821680087665003.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821680494439768.svg'/>，意味着这部分单词以3.3%的概率被保留。如果你去看那篇论文的话，你会发现作者在论文中对函数公式的定义和在C语言代码的实现上有一些差别，但我认为C语言代码的公式实现是更权威的一个版本。

负采样（negative sampling）

训练一个神经网络意味着要输入训练样本并且不断调整神经元的权重，从而不断提高对目标的准确预测。每当神经网络经过一个训练样本的训练，它的权重就会进行一次调整。

正如我们上面所讨论的，vocabulary的大小决定了我们的Skip-Gram神经网络将会拥有大规模的权重矩阵，所有的这些权重需要通过我们数以亿计的训练样本来进行调整，这是非常消耗计算资源的，并且实际中训练起来会非常慢。

负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。

当我们用训练样本 ( input word: "fox"，output word: "quick") 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的vocabulary大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为“negative” word。

当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。
在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。

回忆一下我们的隐层-输出层拥有300 x 10000的权重矩阵。如果使用了负采样的方法我们仅仅去更新我们的positive word-“quick”的和我们选择的其他5个negative words的结点对应的权重，共计6个输出神经元，相当于每次只更新<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821683822681342.svg'/>个权重。对于3百万的权重来说，相当于只计算了0.06%的权重，这样计算效率就大幅度提高。

如何选择negative words

我们使用“一元模型分布（unigram distribution）”来选择“negative words”。

要注意的一点是，一个单词被选作negative sample的概率跟它出现的频次有关，出现频次越高的单词越容易被选作negative words。

在word2vec的C语言实现中，你可以看到对于这个概率的实现公式。每个单词被选为“negative words”的概率计算公式与其出现的频次有关。

代码中的公式实现如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821686197707259.svg'/>
每个单词被赋予一个权重，即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821687325457026.svg'/>， 它代表着单词出现的频次。

公式中开3/4的根号完全是基于经验的，论文中提到这个公式的效果要比其它公式更加出色。你可以在google的搜索栏中输入“plot y = x^(3/4) and y = x”，然后看到这两幅图（如下图），仔细观察x在[0,1]区间内时y的取值，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821691465707304.svg'/>有一小段弧形，取值在<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821692590826333.svg'/>函数之上。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821694160224328.png'/>
负采样的C语言实现非常的有趣。unigram table有一个包含了一亿个元素的数组，这个数组是由词汇表中每个单词的索引号填充的，并且这个数组中有重复，也就是说有些单词会出现多次。那么每个单词的索引在这个数组中出现的次数该如何决定呢，有公式<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415682169593130473.svg'/>，也就是说计算出的负采样概率*1亿=单词在表中出现的次数。

有了这张表以后，每次去我们进行负采样时，只需要在0-1亿范围内生成一个随机数，然后选择表中索引号为这个随机数的那个单词作为我们的negative word即可。一个单词的负采样概率越大，那么它在这个表中出现的次数就越多，它被选中的概率就越大。

到目前为止，Word2Vec中的Skip-Gram模型就讲完了，对于里面具体的数学公式推导细节这里并没有深入。这篇文章只是对于实现细节上的一些思想进行了阐述。
## 请用图形象的解释word2vec（一图胜千言）
本题解析来源：https://blog.csdn.net/longxinchen_ml/article/details/89077048#commentBox，英文原文：https://jalammar.github.io/illustrated-word2vec/

嵌入（embedding）是机器学习中最迷人的想法之一。 如果你曾经使用Siri、Google Assistant、Alexa、Google翻译，甚至智能手机键盘进行下一词预测，那么你很有可能从这个已经成为自然语言处理模型核心的想法中受益。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844468755940872.png'/>
在过去的几十年中，嵌入技术用于神经网络模型已有相当大的发展。尤其是最近，其发展包括导致BERT和GPT2等尖端模型的语境化嵌入。

BERT：
https://jalammar.github.io/illustrated-bert/

Word2vec是一种有效创建词嵌入的方法，它自2013年以来就一直存在。但除了作为词嵌入的方法之外，它的一些概念已经被证明可以有效地创建推荐引擎和理解时序数据。在商业的、非语言的任务中。像Airbnb、阿里巴巴、Spotify这样的公司都从NLP领域中提取灵感并用于产品中，从而为新型推荐引擎提供支持。

在这篇文章中，我们将讨论嵌入的概念，以及使用word2vec生成嵌入的机制。让我们从一个例子开始，熟悉使用向量来表示事物。你是否知道你的个性可以仅被五个数字的列表（向量）表示？

个性嵌入：你是什么样的人？
如何用0到100的范围来表示你是多么内向/外向（其中0是最内向的，100是最外向的）？ 你有没有做过像MBTI那样的人格测试，或者五大人格特质测试？ 如果你还没有，这些测试会问你一系列的问题，然后在很多维度给你打分，内向/外向就是其中之一。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684451535247366.png'/>

五大人格特质测试测试结果示例。它可以真正告诉你很多关于你自己的事情，并且在学术、人格和职业成功方面都具有预测能力。此处可以找到测试结果。

假设我的内向/外向得分为38/100。 我们可以用这种方式绘图：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844516154531694.png'/>

让我们把范围收缩到-1到1:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844520232134004.png'/>
当你只知道这一条信息的时候，你觉得你有多了解这个人？了解不多。人很复杂，让我们添加另一测试的得分作为新维度。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844520941566848.png'/>

我们可以将两个维度表示为图形上的一个点，或者作为从原点到该点的向量。我们拥有很棒的工具来处理即将上场的向量们。

我已经隐藏了我们正在绘制的人格特征，这样你会渐渐习惯于在不知道每个维度代表什么的情况下，从一个人格的向量表示中获得价值信息。

我们现在可以说这个向量部分地代表了我的人格。当你想要将另外两个人与我进行比较时，这种表示法就有用了。假设我被公共汽车撞了，我需要被性格相似的人替换，那在下图中，两个人中哪一个更像我？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844521531016577.png'/>

处理向量时，计算相似度得分的常用方法是余弦相似度：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844522551918807.png'/>

1号替身在性格上与我更相似。指向相同方向的向量（长度也起作用）具有更高的余弦相似度。

再一次，两个维度还不足以捕获有关不同人群的足够信息。心理学已经研究出了五个主要人格特征（以及大量的子特征），所以让我们使用所有五个维度进行比较：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844523345738278.png'/>

使用五个维度的问题是我们不能在二维平面绘制整齐小箭头了。这是机器学习中的常见问题，我们经常需要在更高维度的空间中思考。 但好在余弦相似度仍然有效，它适用于任意维度：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844524467774046.png'/>

余弦相似度适用于任意数量的维度。这些得分比上次的得分要更好，因为它们是根据被比较事物的更高维度算出的。

在本节的最后，我希望提出两个中心思想：
1.我们可以将人和事物表示为代数向量（这对机器来说很棒！）。
2.我们可以很容易地计算出相似的向量之间的相互关系。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844525211426089.png'/>

词嵌入
通过上文的理解，我们继续看看训练好的词向量实例（也被称为词嵌入）并探索它们的一些有趣属性。

这是一个单词“king”的词嵌入（在维基百科上训练的GloVe向量）：
[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]

这是一个包含50个数字的列表。通过观察数值我们看不出什么，但是让我们稍微给它可视化，以便比较其它词向量。我们把所有这些数字放在一行：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844527173787486.png'/>

让我们根据它们的值对单元格进行颜色编码（如果它们接近2则为红色，接近0则为白色，接近-2则为蓝色）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844527990576253.png'/>

我们将忽略数字并仅查看颜色以指示单元格的值。现在让我们将“king”与其它单词进行比较：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844529089103826.png'/>

看看“Man”和“Woman”彼此之间是如何比它们任一一个单词与“King”相比更相似的？ 这暗示你一些事情。这些向量图示很好的展现了这些单词的信息/含义/关联。

这是另一个示例列表（通过垂直扫描列来查找具有相似颜色的列）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844530092331232.png'/>

有几个要点需要指出：
1.所有这些不同的单词都有一条直的红色列。 它们在这个维度上是相似的（虽然我们不知道每个维度是什么）
2.你可以看到“woman”和“girl”在很多地方是相似的，“man”和“boy”也是一样
3.“boy”和“girl”也有彼此相似的地方，但这些地方却与“woman”或“man”不同。这些是否可以总结出一个模糊的“youth”概念？可能吧。
4.除了最后一个单词，所有单词都是代表人。 我添加了一个对象“water”来显示类别之间的差异。你可以看到蓝色列一直向下并在 “water”的词嵌入之前停下了。
5.“king”和“queen”彼此之间相似，但它们与其它单词都不同。这些是否可以总结出一个模糊的“royalty”概念？

类比
展现嵌入奇妙属性的著名例子是类比。我们可以添加、减去词嵌入并得到有趣的结果。一个著名例子是公式：“king”-“man”+“woman”：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844530810251399.png'/>

在python中使用Gensim库，我们可以添加和减去词向量，它会找到与结果向量最相似的单词。该图像显示了最相似的单词列表，每个单词都具有余弦相似性。

我们可以像之前一样可视化这个类比：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844531824302089.png'/>

由“king-man + woman”生成的向量并不完全等同于“queen”，但“queen”是我们在此集合中包含的400,000个字嵌入中最接近它的单词。

现在我们已经看过训练好的词嵌入，接下来让我们更多地了解训练过程。 但在我们开始使用word2vec之前，我们需要看一下词嵌入的父概念：神经语言模型。

语言模型
如果要举自然语言处理最典型的例子，那应该就是智能手机输入法中的下一单词预测功能。这是个被数十亿人每天使用上百次的功能。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684453231639228.png'/>

下一单词预测是一个可以通过语言模型实现的任务。语言模型会通过单词列表(比如说两个词)去尝试预测可能紧随其后的单词。

在上面这个手机截屏中，我们可以认为该模型接收到两个绿色单词(thou shalt)并推荐了一组单词(“not” 就是其中最有可能被选用的一个)：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844533326231028.png'/>

我们可以把这个模型想象为这个黑盒:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844534289421319.png'/>

但事实上，该模型不会只输出一个单词。实际上，它对所有它知道的单词(模型的词库，可能有几千到几百万个单词)的按可能性打分，输入法程序会选出其中分数最高的推荐给用户。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844535045920372.png'/>

自然语言模型的输出就是模型所知单词的概率评分，我们通常把概率按百分比表示，但是实际上，40%这样的分数在输出向量组是表示为0.4

自然语言模型(请参考Bengio 2003)在完成训练后，会按如下中所示法人三步完成预测：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844536035278081.png'/>

第一步与我们最相关，因为我们讨论的就是Embedding。模型在经过训练之后会生成一个映射单词表所有单词的矩阵。在进行预测的时候，我们的算法就是在这个映射矩阵中查询输入的单词，然后计算出预测值:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844536687698599.png'/>

现在让我们将重点放到模型训练上，来学习一下如何构建这个映射矩阵。

语言模型训练
相较于大多数其他机器学习模型，语言模型有一个很大有优势，那就是我们有丰富的文本来训练语言模型。所有我们的书籍、文章、维基百科、及各种类型的文本内容都可用。相比之下，许多其他机器学习的模型开发就需要手工设计数据或者专门采集数据。

我们通过找常出现在每个单词附近的词，就能获得它们的映射关系。机制如下：

1.先是获取大量文本数据(例如所有维基百科内容)
2. 然后我们建立一个可以沿文本滑动的窗(例如一个窗里包含三个单词)
3. 利用这样的滑动窗就能为训练模型生成大量样本数据。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684453784736340.png'/>

当这个窗口沿着文本滑动时，我们就能(真实地)生成一套用于模型训练的数据集。为了明确理解这个过程，我们看下滑动窗是如何处理这个短语的:

“Thou shalt not make a machine in the likeness of a human mind” ~Dune

在一开始的时候，窗口锁定在句子的前三个单词上:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844538544645411.png'/>

我们把前两个单词单做特征，第三个单词单做标签:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844539597558241.png'/>

这时我们就生产了数据集中的第一个样本，它会被用在我们后续的语言模型训练中。

接着，我们将窗口滑动到下一个位置并生产第二个样本:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844540444667243.png'/>

这时第二个样本也生成了。

不用多久，我们就能得到一个较大的数据集，从数据集中我们能看到在不同的单词组后面会出现的单词:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844541131034754.png'/>

在实际应用中，模型往往在我们滑动窗口时就被训练的。但是我觉得将生成数据集和训练模型分为两个阶段会显得更清晰易懂一些。除了使用神经网络建模之外，大家还常用一项名为N-gams的技术进行模型训练。

如果想了解现实产品从使用N-gams模型到使用神经模型的转变，可以看一下Swiftkey (我最喜欢的安卓输入法)在2015年的发表一篇博客，文中介绍了他们的自然语言模型及该模型与早期N-gams模型的对比。我很喜这个例子，因为这个它能告诉你如何在营销宣讲中把Embedding的算法属性解释清楚。

顾及两头
根据前面的信息进行填空:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844542392507445.png'/>

在空白前面，我提供的背景是五个单词(如果事先提及到‘bus’)，可以肯定，大多数人都会把bus填入空白中。但是如果我再给你一条信息——比如空白后的一个单词，那答案会有变吗？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844542995294732.png'/>

这下空白处改填的内容完全变了。这时’red’这个词最有可能适合这个位置。从这个例子中我们能学到，一个单词的前后词语都带信息价值。事实证明，我们需要考虑两个方向的单词(目标单词的左侧单词与右侧单词)。那我们该如何调整训练方式以满足这个要求呢，继续往下看。

Skipgram模型
我们不仅要考虑目标单词的前两个单词，还要考虑其后两个单词。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844544844737440.png'/>

如果这么做，我们实际上构建并训练的模型就如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844545454377802.png'/>

上述的这种架构被称为连续词袋(CBOW)，在一篇关于word2vec的论文中有阐述。

还有另一种架构，它不根据前后文(前后单词)来猜测目标单词，而是推测当前单词可能的前后单词。我们设想一下滑动窗在训练数据时如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844551639257097.png'/>

绿框中的词语是输入词，粉框则是可能的输出结果

这里粉框颜色深度呈现不同，是因为滑动窗给训练集产生了4个独立的样本:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844552333293804.png'/>

这种方式称为Skipgram架构。我们可以像下图这样将展示滑动窗的内容。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844553075616690.png'/>
这样就为数据集提供了4个样本:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844556060192397.png'/>
然后我们移动滑动窗到下一个位置:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844557654635876.png'/>

这样我们又产生了接下来4个样本:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844560054138247.png'/>

在移动几组位置之后，我们就能得到一批样本:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844576031016522.png'/>

重新审视训练过程
现在我们已经从现有的文本中获得了Skipgram模型的训练数据集，接下来让我们看看如何使用它来训练一个能预测相邻词汇的自然语言模型。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684457694057923.png'/>

从数据集中的第一个样本开始。我们将特征输入到未经训练的模型，让它预测一个可能的相邻单词。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844578099644532.png'/>

该模型会执行三个步骤并输入预测向量(对应于单词表中每个单词的概率)。因为模型未经训练，该阶段的预测肯定是错误的。但是没关系，我们知道应该猜出的是哪个单词——这个词就是我训练集数据中的输出标签:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844579274876441.png'/>

目标单词概率为1，其他所有单词概率为0，这样数值组成的向量就是“目标向量”。

模型的偏差有多少？将两个向量相减，就能得到偏差向量:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684457997052686.png'/>

现在这一误差向量可以被用于更新模型了，所以在下一轮预测中，如果用not作为输入，我们更有可能得到thou作为输出了。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844580832335579.png'/>

这其实就是训练的第一步了。我们接下来继续对数据集内下一份样本进行同样的操作，直到我们遍历所有的样本。这就是一轮（epoch）了。我们再多做几轮（epoch），得到训练过的模型，于是就可以从中提取嵌入矩阵来用于其他应用了。

以上确实有助于我们理解整个流程，但这依然不是word2vec真正训练的方法。我们错过了一些关键的想法。

负例采样
回想一下这个神经语言模型计算预测值的三个步骤：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844581688372943.png'/>

从计算的角度来看，第三步非常昂贵 - 尤其是当我们将需要在数据集中为每个训练样本都做一遍（很容易就多达数千万次）。我们需要寻找一些提高表现的方法。

一种方法是将目标分为两个步骤：
1.生成高质量的词嵌入（不要担心下一个单词预测）。
2.使用这些高质量的嵌入来训练语言模型（进行下一个单词预测）。

在本文中我们将专注于第1步（因为这篇文章专注于嵌入）。要使用高性能模型生成高质量嵌入，我们可以改变一下预测相邻单词这一任务：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844582543625632.png'/>

将其切换到一个提取输入与输出单词的模型，并输出一个表明它们是否是邻居的分数（0表示“不是邻居”，1表示“邻居”）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844585866655467.png'/>

这个简单的变换将我们需要的模型从神经网络改为逻辑回归模型——因此它变得更简单，计算速度更快。

这个开关要求我们切换数据集的结构——标签值现在是一个值为0或1的新列。它们将全部为1，因为我们添加的所有单词都是邻居。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684458643093615.png'/>

现在的计算速度可谓是神速啦——在几分钟内就能处理数百万个例子。但是我们还需要解决一个漏洞。如果所有的例子都是邻居（目标：1），我们这个”天才模型“可能会被训练得永远返回1——准确性是百分百了，但它什么东西都学不到，只会产生垃圾嵌入结果。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844594770863315.png'/>

为了解决这个问题，我们需要在数据集中引入负样本 - 不是邻居的单词样本。我们的模型需要为这些样本返回0。模型必须努力解决这个挑战——而且依然必须保持高速。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844595838706243.png'/>

对于我们数据集中的每个样本，我们添加了负面示例。它们具有相同的输入字词，标签为0。

但是我们作为输出词填写什么呢？我们从词汇表中随机抽取单词
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844596819454389.png'/>

这个想法的灵感来自噪声对比估计。我们将实际信号（相邻单词的正例）与噪声（随机选择的不是邻居的单词）进行对比。这导致了计算和统计效率的巨大折衷。

噪声对比估计
http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf

基于负例采样的Skipgram（SGNS）
我们现在已经介绍了word2vec中的两个（一对）核心思想：负例采样，以及skipgram。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684459775546353.png'/>

Word2vec训练流程
现在我们已经了解了skipgram和负例采样的两个中心思想，可以继续仔细研究实际的word2vec训练过程了。

在训练过程开始之前，我们预先处理我们正在训练模型的文本。在这一步中，我们确定一下词典的大小（我们称之为vocab_size，比如说10,000）以及哪些词被它包含在内。

在训练阶段的开始，我们创建两个矩阵——Embedding矩阵和Context矩阵。这两个矩阵在我们的词汇表中嵌入了每个单词（所以vocab_size是他们的维度之一）。第二个维度是我们希望每次嵌入的长度（embedding_size——300是一个常见值，但我们在前文也看过50的例子）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844599257221521.png'/>

在训练过程开始时，我们用随机值初始化这些矩阵。然后我们开始训练过程。在每个训练步骤中，我们采取一个相邻的例子及其相关的非相邻例子。我们来看看我们的第一组：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844599990371684.png'/>

现在我们有四个单词：输入单词not和输出/上下文单词: thou（实际邻居词），aaron和taco（负面例子）。我们继续查找它们的嵌入——对于输入词，我们查看Embedding矩阵。对于上下文单词，我们查看Context矩阵（即使两个矩阵都在我们的词汇表中嵌入了每个单词）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844600598212911.png'/>

然后，我们计算输入嵌入与每个上下文嵌入的点积。在每种情况下，结果都将是表示输入和上下文嵌入的相似性的数字。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684460147082011.png'/>

现在我们需要一种方法将这些分数转化为看起来像概率的东西——我们需要它们都是正值，并且 处于0到1之间。sigmoid这一逻辑函数转换正适合用来做这样的事情啦。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844602121676776.png'/>

现在我们可以将sigmoid操作的输出视为这些示例的模型输出。您可以看到taco得分最高，aaron最低，无论是sigmoid操作之前还是之后。

既然未经训练的模型已做出预测，而且我们确实拥有真实目标标签来作对比，那么让我们计算模型预测中的误差吧。为此我们只需从目标标签中减去sigmoid分数。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844602716214910.png'/>

error = target - sigmoid_scores

这是“机器学习”的“学习”部分。现在，我们可以利用这个错误分数来调整not、thou、aaron和taco的嵌入，使我们下一次做出这一计算时，结果会更接近目标分数。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844603139960024.png'/>

训练步骤到此结束。我们从中得到了这一步所使用词语更好一些的嵌入（not，thou，aaron和taco）。我们现在进行下一步（下一个相邻样本及其相关的非相邻样本），并再次执行相同的过程。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684460369423321.png'/>

当我们循环遍历整个数据集多次时，嵌入会继续得到改进。然后我们就可以停止训练过程，丢弃Context矩阵，并使用Embeddings矩阵作为下一项任务的已被训练好的嵌入。

窗口大小和负样本数量
word2vec训练过程中的两个关键超参数是窗口大小和负样本的数量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844605320678823.png'/>

不同的任务适合不同的窗口大小。一种启发式方法是，使用较小的窗口大小（2-15）会得到这样的嵌入：两个嵌入之间的高相似性得分表明这些单词是可互换的（注意，如果我们只查看附近距离很近的单词，反义词通常可以互换——例如，好的和坏的经常出现在类似的语境中）。使用较大的窗口大小（15-50，甚至更多）会得到相似性更能指示单词相关性的嵌入。在实际操作中，你通常需要对嵌入过程提供指导以帮助读者得到相似的”语感“。Gensim默认窗口大小为5（除了输入字本身以外还包括输入字之前与之后的两个字）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844605973404618.png'/>

负样本的数量是训练训练过程的另一个因素。原始论文认为5-20个负样本是比较理想的数量。它还指出，当你拥有足够大的数据集时，2-5个似乎就已经足够了。Gensim默认为5个负样本。

结论
我希望您现在对词嵌入和word2vec算法有所了解。我也希望现在当你读到一篇提到“带有负例采样的skipgram”（SGNS）的论文（如顶部的推荐系统论文）时，你已经对这些概念有了更好的认识。
## 请简要说说word2vec的来龙去脉/前世今生？
本题解析来源：https://www.cnblogs.com/iloveai/p/word2vec.html，July和助教远根特给此文补充了相关例子和表格，以为更通俗形象、一目了然。

2013年，Google开源了一款用于词向量计算的工具——word2vec，引起了工业界和学术界的关注。首先，word2vec可以在百万数量级的词典和上亿的数据集上进行高效地训练；其次，该工具得到的训练结果——词向量（word embedding），可以很好地度量词与词之间的相似性。

随着深度学习（Deep Learning）在自然语言处理中应用的普及，很多人误以为word2vec是一种深度学习算法。其实word2vec算法的背后是一个浅层神经网络，是一个计算word vector的开源工具。所以，当我们在说word2vec算法或模型的时候，其实指的是其背后用于计算word vector的CBoW模型和Skip-gram模型。

接下来，本文将从统计语言模型出发，尽可能详细地介绍word2vec工具背后的算法模型的来龙去脉。

Statistical Language Model
在深入word2vec算法的细节之前，我们首先回顾一下自然语言处理中的一个基本问题：如何计算一段文本序列在某种语言下出现的概率？之所为称其为一个基本问题，是因为它在很多NLP任务中都扮演着重要的角色。

例如，在机器翻译的问题中，如果我们知道了目标语言中每句话的概率，就可以从候选集合中挑选出最合理的句子做为翻译结果返回。统计语言模型给出了这一类问题的一个基本解决框架。对于一段文本序列S=w1,w2,...,wT，它的概率可以表示为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904223641520182.png'/>
即将序列的联合概率转化为一系列条件概率的乘积。

问题变成了如何去预测这些给定previous words下的条件概率：p(wt|w1,w2,...,wt−1)

由于其巨大的参数空间，这样一个原始的模型在实际中并没有什么卵用。我们更多的是采用其简化版本——Ngram模型：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904231571943173.png'/>

常见的如bigram模型（N=2N=2）和trigram模型（N=3N=3）。事实上，由于模型复杂度和预测精度的限制，我们很少会考虑N>3N>3的模型。

我们可以用最大似然法去求解Ngram模型的参数——等价于去统计每个Ngram的条件词频。

为了避免统计中出现的零概率问题（一段从未在训练集中出现过的Ngram片段会使得整个序列的概率为0），人们基于原始的Ngram模型进一步发展出了back-off trigram模型（用低阶的bigram和unigram代替零概率的trigram），和interpolated trigram模型（将条件概率表示为unigram、bigram、trigram三者的线性函数）。此处不再赘述。感兴趣者可进一步阅读相关的文献[3]。

Distributed Representation
不过，Ngram模型仍有其局限性。首先，由于参数空间的爆炸式增长，它无法处理更长程的context（N>3N>3）。

其次，它没有考虑词与词之间内在的联系性。例如，考虑"the cat is walking in the bedroom"这句话。如果我们在训练语料中看到了很多类似“the dog is walking in the bedroom”或是“the cat is running in the bedroom”这样的句子，那么，即使我们没有见过这句话，也可以从“cat”和“dog”（“walking”和“running”）之间的相似性，推测出这句话的概率[3]。
然而， Ngram模型做不到。这是因为，Ngram本质上是将词当做一个个孤立的原子单元（atomic unit）去处理的。这种处理方式对应到数学上的形式是一个个离散的one-hot向量（除了一个词典索引的下标对应的方向上是1，其余方向上都是0）。

例如，对于一个大小为5的词典：
{"I", "love", "nature", "luaguage", "processing"}，
“nature”对应的one-hot向量为：
[0,0,1,0,0]。

显然，one-hot向量的维度等于词典的大小。这在动辄上万甚至百万词典的实际应用中，面临着巨大的维度灾难问题（the curse of dimensionality）。

于是，人们就自然而然地想到，能否用一个连续的稠密向量去刻画一个word的特征呢？这样，我们不仅可以直接刻画词与词之间的相似度，还可以建立一个从向量到概率的平滑函数模型，使得相似的词向量可以映射到相近的概率空间上。这个稠密连续向量也被称为word的distributed representation[3]。

事实上，这个概念在信息检索（Information Retrieval）领域早就已经被广泛地使用了。只不过，在IR领域里，这个概念被称为向量空间模型（Vector Space Model，以下简称VSM）。

VSM是基于一种Statistical Semantics Hypothesis[4]：语言的统计特征隐藏着语义的信息（Statistical pattern of human word usage can be used to figure out what people mean）。例如，两篇具有相似词分布的文档可以被认为是有着相近的主题。

这个Hypothesis有很多衍生版本。其中，比较广为人知的两个版本是Bag of Words Hypothesis和Distributional Hypothesis。前者是说，一篇文档的词频（而不是词序）代表了文档的主题；后者是说，上下文环境相似的两个词有着相近的语义。后面我们会看到，word2vec算法也是基于Distributional的假设。

那么，VSM是如何将稀疏离散的one-hot词向量映射为稠密连续的distributional representation的呢？

简单来说，基于Bag of Words Hypothesis，我们可以构造一个term-document矩阵A：
矩阵的行Ai对应着词典里的一个word；
矩阵的列Aj对应着训练语料里的一篇文档；
矩阵里的元素Aij代表着word wi在文档Dj中出现的次数（或频率）。
那么，我们就可以提取行向量做为word的语义向量（不过，在实际应用中，我们更多的是用列向量做为文档的主题向量）。

恩，屏幕前的你现在是不是想问，有没例子呀？当然有的，我们基于这两个相似语句，例如
语句A：the dog is walking in the bedrom
语句B：the cat is running in the bedroom
构造term-document矩阵A，用于表示每一个词分别语句A和语句B中出现的次数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156905711095687650.png'/>
则提取行向量作为word的语义向量，思考：这是基于词频的，也可以基于此得到主题分布，但是缺少语义的考虑即上下文context。

类似地，我们可以基于Distributional Hypothesis构造一个word-context的矩阵。此时，矩阵的列变成了context里的word，矩阵的元素也变成了一个context窗口里word的共现次数。

举个例子：通过分析如下三个相似语句
语句A：I like deep learning.
语句B：I like NLP.
语句C：I enjoy flying.
构造word-content矩阵（设置窗口的大小为1），表示每个词与这三个语句中其他词相邻的次数（这个时候，必须上表格，便可瞬间清晰、一目了然）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156905680876441470.png'/>

注意，这两类矩阵的行向量所计算的相似度有着细微的差异：
term-document矩阵会给经常出现在同一篇document里的两个word赋予更高的相似度；
而word-context矩阵会给那些有着相同context的两个word赋予更高的相似度。

后者相对于前者是一种更高阶的相似度，因此在传统的信息检索领域中得到了更加广泛的应用。不过，这种co-occurrence矩阵仍然存在着数据稀疏性和维度灾难的问题。为此，人们提出了一系列对矩阵进行降维的方法（如LSI／LSA等）。这些方法大都是基于SVD的思想，将原始的稀疏矩阵分解为两个低秩矩阵乘积的形式。

关于VSM更多的介绍，可以进一步阅读文末的参考文献[4]。

Neural Network Language Model
接下来，让我们回到对统计语言模型的讨论。鉴于Ngram等模型的不足，2003年，Bengio等人发表了一篇开创性的文章：A neural probabilistic language model[3]。在这篇文章里，他们总结出了一套用神经网络建立统计语言模型的框架（Neural Network Language Model，以下简称NNLM），并首次提出了word embedding的概念（虽然没有叫这个名字），从而奠定了包括word2vec在内后续研究word representation learning的基础。

NNLM模型的基本思想可以概括如下：
①假定词表中的每一个word都对应着一个连续的特征向量；
②假定一个连续平滑的概率模型，输入一段词向量的序列，可以输出这段序列的联合概率；
③同时学习词向量的权重和概率模型里的参数。
值得注意的一点是，这里的词向量也是要学习的参数。

在03年的论文里，Bengio等人采用了一个简单的前向反馈神经网络f(wt−n+1,...,wt)来拟合一个词序列的条件概率p(wt|w1,w2,...,wt−1)。
整个模型的网络结构见下图：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904095373575974.png'/>

我们可以将整个模型拆分成两部分加以理解：
    i)首先是一个线性的embedding层。它将输入的N−1个one-hot词向量，通过一个共享的D×V的矩阵C，映射为N−1个分布式的词向量（distributed vector）。其中，V是词典的大小，D是embedding向量的维度（一个先验参数）。C矩阵里存储了要学习的word vector。
    ii)其次是一个简单的前向反馈神经网络g。它由一个tanh隐层和一个softmax输出层组成。

通过将embedding层输出的N−1N−1个词向量映射为一个长度为VV的概率分布向量，从而对词典中的word在输入context下的条件概率做出预估：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904307614253337.png'/>

我们可以通过最小化一个cross-entropy的正则化损失函数来调整模型的参数θ：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904311795942433.png'/>

其中，模型的参数θ包括了embedding层矩阵C的元素，和前向反馈神经网络模型g里的权重。这是一个巨大的参数空间。不过，在用SGD学习更新模型的参数时，并不是所有的参数都需要调整（例如未在输入的context中出现的词对应的词向量）。计算的瓶颈主要是在softmax层的归一化函数上（需要对词典中所有的word计算一遍条件概率）。

然而，抛却复杂的参数空间，我们不禁要问，为什么这样一个简单的模型会取得巨大的成功呢？仔细观察这个模型就会发现，它其实在同时解决两个问题：一个是统计语言模型里关注的条件概率p(wt|context)的计算；一个是向量空间模型里关注的词向量的表达。而这两个问题本质上并不独立。通过引入连续的词向量和平滑的概率模型，我们就可以在一个连续空间里对序列概率进行建模，从而从根本上缓解数据稀疏性和维度灾难的问题。

另一方面，以条件概率p(wt|context)为学习目标去更新词向量的权重，具有更强的导向性，同时也与VSM里的Distributional Hypothesis不谋而合。CBoW & Skip-gram Model铺垫了这么多，终于要轮到主角出场了。

不过在主角正式登场前，我们先看一下NNLM存在的几个问题。一个问题是，同Ngram模型一样，NNLM模型只能处理定长的序列。在03年的论文里，Bengio等人将模型能够一次处理的序列长度N提高到了5，虽然相比bigram和trigram已经是很大的提升，但依然缺少灵活性。因此，Mikolov等人在2010年提出了一种RNNLM模型[7]，用递归神经网络代替原始模型里的前向反馈神经网络，并将embedding层与RNN里的隐藏层合并，从而解决了变长序列的问题。

另一个问题就比较严重了。NNLM的训练太慢了。即便是在百万量级的数据集上，即便是借助了40个CPU进行训练，NNLM也需要耗时数周才能给出一个稍微靠谱的解来。显然，对于现在动辄上千万甚至上亿的真实语料库，训练一个NNLM模型几乎是一个impossible mission。这时候，还是那个Mikolov站了出来。他注意到，原始的NNLM模型的训练其实可以拆分成两个步骤：
用一个简单模型训练出连续的词向量；
基于词向量的表达，训练一个连续的Ngram神经网络模型。
而NNLM模型的计算瓶颈主要是在第二步。

如果我们只是想得到word的连续特征向量，是不是可以对第二步里的神经网络模型进行简化呢？Mikolov是这么想的，也是这么做的。他在2013年一口气推出了两篇paper，并开源了一款计算词向量的工具——至此，word2vec横空出世，主角闪亮登场。

下面，我将带领大家简单剖析下word2vec算法的原理。有了前文的基础，理解word2vec算法就变得很简单了。首先，我们对原始的NNLM模型做如下改造：
1 移除前向反馈神经网络中非线性的hidden layer，直接将中间层的embedding layer与输出层的softmax layer连接；
2 忽略上下文环境的序列信息：输入的所有词向量均汇总到同一个embedding layer；
3 将future words纳入上下文环境

得到的模型称之为CBoW模型（Continuous Bag-of-Words Model），也是word2vec算法的第一个模型：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904097679110071.png'/>

从数学上看，CBoW模型等价于一个词袋模型的向量乘以一个embedding矩阵，从而得到一个连续的embedding向量。这也是CBoW模型名称的由来。CBoW模型依然是从context对target word的预测中学习到词向量的表达。反过来，我们能否从target word对context的预测中学习到word vector呢？答案显然是可以的：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904099658168993.png'/>
这个模型被称为Skip-gram模型（名称源于该模型在训练时会对上下文环境里的word进行采样）。如果将Skip-gram模型的前向计算过程写成数学形式，我们得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904107834080692.png'/>
其中，Vi是embedding层矩阵里的列向量，也被称为wi的input vector。Uj是softmax层矩阵里的行向量，也被称为wj的output vector。

因此，Skip-gram模型的本质是计算输入word的input vector与目标word的output vector之间的余弦相似度，并进行softmax归一化。我们要学习的模型参数正是这两类词向量。

然而，直接对词典里的V个词计算相似度并归一化，显然是一件极其耗时的impossible mission。为此，Mikolov引入了两种优化算法：层次Softmax（Hierarchical Softmax）和负采样（Negative Sampling）。

Hierarchical Softmax[5]
层次Softmax的方法最早由Bengio在05年引入到语言模型中。它的基本思想是将复杂的归一化概率分解为一系列条件概率乘积的形式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904345491578560.png'/>

其中，每一层条件概率对应一个二分类问题，可以通过一个简单的逻辑回归函数去拟合。这样，我们将对V个词的概率归一化问题，转化成了对logV个词的概率拟合问题。

我们可以通过构造一颗分类二叉树来直观地理解这个过程。首先，我们将原始字典D划分为两个子集D1、D2，并假设在给定context下，target word属于子集D1的概率p(wt∈D1|context)服从logistical function的形式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904122895000025.png'/>
其中，UDroot和Vwt都是模型的参数。

接下来，我们可以对子集D1和D2进一步划分。重复这一过程，直到集合里只剩下一个word。这样，我们就将原始大小为V的字典DD转换成了一颗深度为logV的二叉树。树的叶子节点与原始字典里的word一一对应；非叶节点则对应着某一类word的集合。显然，从根节点出发到任意一个叶子节点都只有一条唯一路径——这条路径也编码了这个叶子节点所属的类别。

同时，从根节点出发到叶子节点也是一个随机游走的过程。因此，我们可以基于这颗二叉树对叶子节点出现的似然概率进行计算。例如，对于训练样本里的一个target word wt，假设其对应的二叉树编码为{1,0,1,...,1}，则我们构造的似然函数为：
p(wt|context)=p(D1=1|context)p(D2=0|D1=1)…p(wt|Dk=1)

乘积中的每一项都是一个逻辑回归的函数。

我们可以通过最大化这个似然函数来求解二叉树上的参数——非叶节点上的向量，用来计算游走到某一个子节点的概率。层次Softmax是一个很巧妙的模型。它通过构造一颗二叉树，将目标概率的计算复杂度从最初的V降低到了logV的量级。不过付出的代价是人为增强了词与词之间的耦合性。例如，一个word出现的条件概率的变化，会影响到其路径上所有非叶节点的概率变化，间接地对其他word出现的条件概率带来不同程度的影响。因此，构造一颗有意义的二叉树就显得十分重要。实践证明，在实际的应用中，基于Huffman编码的二叉树可以满足大部分应用场景的需求。

Negative Sampling[6]
负采样的思想最初来源于一种叫做Noise-Contrastive Estimation的算法[6]，原本是为了解决那些无法归一化的概率模型的参数预估问题。与改造模型输出概率的层次Softmax算法不同，NCE算法改造的是模型的似然函数。以Skip-gram模型为例，其原始的似然函数对应着一个Multinomial的分布。在用最大似然法求解这个似然函数时，我们得到一个cross-entropy的损失函数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904138341967126.png'/>

式中的p(wt+j|wt)是一个在整个字典上归一化了的概率。而在NCE算法中，我们构造了这样一个问题：对于一组训练样本，我们想知道，target word的出现，是来自于context的驱动，还是一个事先假定的背景噪声的驱动？显然，我们可以用一个逻辑回归的函数来回答这个问题：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904147178192128.png'/>
这个式子给出了一个target word w来自于context驱动的概率。其中，k是一个先验参数，表明噪声的采样频率。p(w|context)是一个非归一化的概率分布，这里采用softmax归一化函数中的分子部分。pn(w)pn(w)则是背景噪声的词分布。通常采用word的unigram分布。

通过对噪声分布的kk采样，我们得到一个新的数据集：。其中，label标记了数据的来源（真实数据分布还是背景噪声分布？）。在这个新的数据集上，我们就可以用最大化上式中逻辑回归的似然函数来求解模型的参数。而Mikolov在2013年的论文里提出的负采样算法， 是NCE的一个简化版本。在这个算法里，Mikolov抛弃了NCE似然函数中对噪声分布的依赖，直接用原始softmax函数里的分子定义了逻辑回归的函数，进一步简化了计算：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904153361440918.png'/>
此时，模型相应的目标函数变为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641569041599475384.png'/>
J(θ)=logσ(Uo⋅Vi)+∑j=1kEwj∼pn(w)[logσ(−Uj⋅Vi)]J(θ)=log⁡σ(Uo⋅Vi)+∑j=1kEwj∼pn(w)[log⁡σ(−Uj⋅Vi)]除了这里介绍的层次Softmax和负采样的优化算法，Mikolov在13年的论文里还介绍了另一个trick：下采样（subsampling）。其基本思想是在训练时依概率随机丢弃掉那些高频的词：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904166546122186.png'/>
其中，tt=是一个先验参数，一般取为10^5。f(w)是w在语料中出现的频率。实验证明，这种下采样技术可以显著提高低频词的词向量的准确度。

Beyond the Word Vector
介绍完word2vec模型的算法和原理，我们来讨论一些轻松点的话题——模型的应用。13年word2vec模型横空出世后，人们最津津乐道的是它学到的向量在语义和语法相似性上的应用——尤其是这种相似性居然对数学上的加减操作有意义[8]！

最经典的一个例子是，v("King")−v("Man")+v("Woman")=v("Queen")。然而，这种例子似乎并没有太多实际的用途。

除此之外，word2vec模型还被应用于机器翻译和推荐系统领域。Machine Translation[9]与后来提出的在sentence level上进行机器翻译的RNN模型不同，word2vec模型主要是用于词粒度上的机器翻译。具体来说，我们首先从大量的单语种语料中学习到每种语言的word2vec表达，再借助一个小的双语语料库学习到两种语言word2vec表达的线性映射关系WW。构造的损失函数为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904177816033839.png'/>
在翻译的过程中，我们首先将源语言的word2vec向量通过矩阵WW映射到目标语言的向量空间上；再在目标语言的向量空间中找出与投影向量距离最近的word做为翻译的结果返回。

其原理是，不同语言学习到的word2vec向量空间在几何上具有一定的同构性。映射矩阵W本质上是一种空间对齐的线性变换。

Item2Vec[11]
本质上，word2vec模型是在word-context的co-occurrence矩阵基础上建立起来的。因此，任何基于co-occurrence矩阵的算法模型，都可以套用word2vec算法的思路加以改进。比如，推荐系统领域的协同过滤算法。协同过滤算法是建立在一个user-item的co-occurrence矩阵的基础上，通过行向量或列向量的相似性进行推荐。如果我们将同一个user购买的item视为一个context，就可以建立一个item-context的矩阵。进一步的，可以在这个矩阵上借鉴CBoW模型或Skip-gram模型计算出item的向量表达，在更高阶上计算item间的相似度。关于word2vec更多应用的介绍，可以进一步参考这篇文献[10]。

Word Embedding
最后，我想简单阐述下我对word embedding的几点思考。不一定正确，也欢迎大家提出不同的意见。

Word embedding最早出现于Bengio在03年发表的开创性文章中[3]。通过嵌入一个线性的投影矩阵（projection matrix），将原始的one-hot向量映射为一个稠密的连续向量，并通过一个语言模型的任务去学习这个向量的权重。这一思想后来被广泛应用于包括word2vec在内的各种NLP模型中。

Word embedding的训练方法大致可以分为两类：
一类是无监督或弱监督的预训练；
一类是端对端（end to end）的有监督训练。

无监督或弱监督的预训练以word2vec和auto-encoder为代表。这一类模型的特点是，不需要大量的人工标记样本就可以得到质量还不错的embedding向量。不过因为缺少了任务导向，可能和我们要解决的问题还有一定的距离。因此，我们往往会在得到预训练的embedding向量后，用少量人工标注的样本去fine-tune整个模型。

相比之下，端对端的有监督模型在最近几年里越来越受到人们的关注。与无监督模型相比，端对端的模型在结构上往往更加复杂。同时，也因为有着明确的任务导向，端对端模型学习到的embedding向量也往往更加准确。例如，通过一个embedding层和若干个卷积层连接而成的深度神经网络以实现对句子的情感分类，可以学习到语义更丰富的词向量表达。

Word embedding的另一个研究方向是在更高层次上对sentence的embedding向量进行建模。我们知道，word是sentence的基本组成单位。一个最简单也是最直接得到sentence embedding的方法是将组成sentence的所有word的embedding向量全部加起来——类似于CBoW模型。显然，这种简单粗暴的方法会丢失很多信息。
另一种方法借鉴了word2vec的思想——将sentence或是paragraph视为一个特殊的word，然后用CBoW模型或是Skip-gram进行训练[12]。

这种方法的问题在于，对于一篇新文章，总是需要重新训练一个新的sentence2vec。此外，同word2vec一样，这个模型缺少有监督的训练导向。个人感觉比较靠谱的是第三种方法——基于word embedding的端对端的训练。Sentence本质上是word的序列。因此，在word embedding的基础上，我们可以连接多个RNN模型或是卷积神经网络，对word embedding序列进行编码，从而得到sentence embedding。这方面的工作已有很多。有机会，我会再写一篇关于sentence embedding的综述。

References
[1]: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013, January 17). Efficient Estimation of Word Representations in Vector Space. arXiv.org.
[2]: Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013, October 17). Distributed Representations of Words and Phrases and their Compositionality. arXiv.org.
[3]: Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). A neural probabilistic language model. The Journal of Machine Learning Research, 3, 1137–1155.
[4]: Turney, P. D., & Pantel, P. (2010). From frequency to meaning: vector space models of semantics. Journal of Artificial Intelligence Research, 37(1).

[5]: Morin, F., & Bengio, Y. (2005). Hierarchical Probabilistic Neural Network Language Model. Aistats.
[6]: Mnih, A., & Kavukcuoglu, K. (2013). Learning word embeddings efficiently with noise-contrastive estimation, 2265–2273.
[7]: Mikolov, T., Karafiát, M., Burget, L., & Cernocký, J. (2010). Recurrent neural network based language model. Interspeech.
[8]: Mikolov, T., Yih, W., & Zweig, G. (2013). Linguistic Regularities in Continuous Space Word Representations. Hlt-Naacl.

[9]: Mikolov, T., Le, Q. V., & Sutskever, I. (2013, September 17). Exploiting Similarities among Languages for Machine Translation. arXiv.org.
[10]: Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011). Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12(Aug), 2493–2537.
[11]: Barkan, O., & Koenigstein, N. (2016, March 14). Item2Vec: Neural Item Embedding for Collaborative Filtering. arXiv.org.
[12]: Le, Q. V., & Mikolov, T. (2014, May 16). Distributed Representations of Sentences and Documents. arXiv.org
.

## 了解什么是正则表达式么？
本题解析来源：https://www.liaoxuefeng.com/wiki/897692888725344/923056128128864，配图则来自七月在线自然语言处理课程。

字符串是编程时涉及到的最多的一种数据结构，对字符串进行操作的需求几乎无处不在。比如判断一个字符串是否是合法的Email地址，虽然可以编程提取@前后的子串，再分别判断是否是单词和域名，但这样做不但麻烦，而且代码难以复用。

正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。

所以我们判断一个字符串是否是合法的Email的方法是：
1 创建一个匹配Email的正则表达式；
2 用该正则表达式去匹配用户的输入来判断是否合法。

因为正则表达式也是用字符串表示的，所以，我们要首先了解如何用字符来描述字符。在正则表达式中，如果直接给出字符，就是精确匹配。用\d可以匹配一个数字，\w可以匹配一个字母或数字，所以：
  &#39;00\d&#39;可以匹配&#39;007&#39;，但无法匹配&#39;00A&#39;；
  &#39;\d\d\d&#39;可以匹配&#39;010&#39;；
  &#39;\w\w\d&#39;可以匹配&#39;py3&#39;；
.可以匹配任意字符，
所以：
&#39;py.&#39;可以匹配&#39;pyc&#39;、&#39;pyo&#39;、&#39;py!&#39;等等。

要匹配变长的字符，在正则表达式中，用*表示任意个字符（包括0个），用+表示至少一个字符，用?表示0个或1个字符，用{n}表示n个字符，用{n,m}表示n-m个字符：

来看一个复杂的例子：\d{3}\s+\d{3,8}。我们来从左到右解读一下：
  \d{3}表示匹配3个数字，例如&#39;010&#39;；
  \s可以匹配一个空格（也包括Tab等空白符），所以\s+表示至少有一个空格，例如匹配&#39; &#39;，&#39; &#39;等；
  \d{3,8}表示3-8个数字，例如&#39;1234567&#39;。
综合起来，上面的正则表达式可以匹配以任意个空格隔开的带区号的电话号码。

如果要匹配&#39;010-12345&#39;这样的号码呢？由于&#39;-&#39;是特殊字符，在正则表达式中，要用&#39;\&#39;转义，所以，上面的正则是\d{3}\-\d{3,8}。

但是，仍然无法匹配&#39;010 - 12345&#39;，因为带有空格。所以我们需要更复杂的匹配方式。

进阶
要做更精确地匹配，可以用[]表示范围，比如：
  [0-9a-zA-Z\_]可以匹配一个数字、字母或者下划线；
  [0-9a-zA-Z\_]+可以匹配至少由一个数字、字母或者下划线组成的字符串，比如&#39;a100&#39;，&#39;0_Z&#39;，&#39;Py3000&#39;等等；
  [a-zA-Z\_][0-9a-zA-Z\_]*可以匹配由字母或下划线开头，后接任意个由一个数字、字母或者下划线组成的字符串，也就是Python合法的变量；
  [a-zA-Z\_][0-9a-zA-Z\_]{0, 19}更精确地限制了变量的长度是1-20个字符（前面1个字符+后面最多19个字符）。

A|B可以匹配A或B，所以(P|p)ython可以匹配&#39;Python&#39;或者&#39;python&#39;。
  ^表示行的开头，^\d表示必须以数字开头。
  $表示行的结束，\d$表示必须以数字结束。
你可能注意到了，py也可以匹配&#39;python&#39;，但是加上^py$就变成了整行匹配，就只能匹配&#39;py&#39;了。

re模块
有了准备知识，我们就可以在Python中使用正则表达式了。Python提供re模块，包含所有正则表达式的功能。由于Python的字符串本身也用\\u8f6c义，所以要特别注意：
s = &#39;ABC\\-001&#39; # Python的字符串
# 对应的正则表达式字符串变成：
# &#39;ABC\-001&#39;
因此我们强烈建议使用Python的r前缀，就不用考虑转义的问题了：
s = r&#39;ABC\-001&#39; 
# Python的字符串
# 对应的正则表达式字符串不变：
# &#39;ABC\-001&#39;

先看看如何判断正则表达式是否匹配：
>>> import re
>>> re.match(r&#39;^\d{3}\-\d{3,8}$&#39;, &#39;010-12345&#39;)
>>> re.match(r&#39;^\d{3}\-\d{3,8}$&#39;, &#39;010 12345&#39;)
>>>

match()方法判断是否匹配，如果匹配成功，返回一个Match对象，否则返回None。常见的判断方法就是：
test = &#39;用户输入的字符串
&#39;if re.match(r&#39;正则表达式&#39;, test):
    print &#39;ok&#39;
else:
    print &#39;failed&#39;

切分字符串用
正则表达式切分字符串比用固定的字符更灵活，请看正常的切分代码：
>>> &#39;a b   c&#39;.split(&#39; &#39;)
[&#39;a&#39;, &#39;b&#39;, &#39;&#39;, &#39;&#39;, &#39;c&#39;]

嗯，无法识别连续的空格，用正则表达式试试：
>>> re.split(r&#39;\s+&#39;, &#39;a b   c&#39;)
[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]

无论多少个空格都可以正常分割。加入,试试：
>>> re.split(r&#39;[\s\,]+&#39;, &#39;a,b, c  d&#39;)
[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]

再加入;试试：
>>> re.split(r&#39;[\s\,\;]+&#39;, &#39;a,b;; c  d&#39;)
[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]

如果用户输入了一组标签，下次记得用正则表达式来把不规范的输入转化成正确的数组。

分组
除了简单地判断是否匹配之外，正则表达式还有提取子串的强大功能。用()表示的就是要提取的分组（Group）。比如：
^(\d{3})-(\d{3,8})$分别定义了两个组，可以直接从匹配的字符串中提取出区号和本地号码：
>>> m = re.match(r&#39;^(\d{3})-(\d{3,8})$&#39;, &#39;010-12345&#39;)
>>> m

>>> m.group(0)
&#39;010-12345&#39;
>>> m.group(1)
&#39;010&#39;
>>> m.group(2)
&#39;12345&#39;

如果正则表达式中定义了组，就可以在Match对象上用group()方法提取出子串来。

注意到group(0)永远是原始字符串，group(1)、group(2)……表示第1、2、……个子串。提取子串非常有用。

来看一个更凶残的例子：
>>> t = &#39;19:05:30&#39;
>>> m = re.match(r&#39;^(0[0-9]|1[0-9]|2[0-3]|[0-9])\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])$&#39;, t)>>> m.groups()
(&#39;19&#39;, &#39;05&#39;, &#39;30&#39;)

这个正则表达式可以直接识别合法的时间。但是有些时候，用正则表达式也无法做到完全验证，比如识别日期：
&#39;^(0[1-9]|1[0-2]|[0-9])-(0[1-9]|1[0-9]|2[0-9]|3[0-1]|[0-9])$&#39;

对于&#39;2-30&#39;，&#39;4-31&#39;这样的非法日期，用正则还是识别不了，或者说写出来非常困难，这时就需要程序配合识别了。

贪婪匹配
最后需要特别指出的是，正则匹配默认是贪婪匹配，也就是匹配尽可能多的字符。举例如下，匹配出数字后面的0：
>>> re.match(r&#39;^(\d+)(0*)$&#39;, &#39;102300&#39;).groups()
(&#39;102300&#39;, &#39;&#39;)

由于\d+采用贪婪匹配，直接把后面的0全部匹配了，结果0*只能匹配空字符串了。

必须让\d+采用非贪婪匹配（也就是尽可能少匹配），才能把后面的0匹配出来，加个?就可以让\d+采用非贪婪匹配：
>>> re.match(r&#39;^(\d+?)(0*)$&#39;, &#39;102300&#39;).groups()
(&#39;1023&#39;, &#39;00&#39;)

编译
当我们在Python中使用正则表达式时，re模块内部会干两件事情：
  i)编译正则表达式，如果正则表达式的字符串本身不合法，会报错；
  ii)用编译后的正则表达式去匹配字符串。

如果一个正则表达式要重复使用几千次，出于效率的考虑，我们可以预编译该正则表达式，接下来重复使用时就不需要编译这个步骤了，直接匹配：
>>> import re
# 编译:
>>> re_telephone = re.compile(r&#39;^(\d{3})-(\d{3,8})$&#39;)
# 使用：
>>> re_telephone.match(&#39;010-12345&#39;).groups()
(&#39;010&#39;, &#39;12345&#39;)
>>> re_telephone.match(&#39;010-8086&#39;).groups()
(&#39;010&#39;, &#39;8086&#39;)

编译后生成Regular Expression对象，由于该对象自己包含了正则表达式，所以调用对应的方法时不用给出正则字符串。

练习与总结
请尝试写一个验证Email地址的正则表达式。
版本一应该可以验证出类似的Email：
someone@gmail.com
bill.gates@microsoft.com

版本二可以验证并提取出带名字的Email地址： tom@voyager.org

最后为方便快速查阅，附一张正则表达式的快速查阅图
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156912802151684336.png'/>
## 如何理解NNLM（Neural Network Language Model）模型？
本题解析来源：https://blog.csdn.net/qq_39422642/article/details/78658309?tdsourcetag=s_pcqq_aiomsg，参考：https://shomy.top/2017/07/28/word2vec-all/

1 基本概念
传统的机器翻译，自然语言处理多是基于规则的，现在更多的是基于模型，规则隐含的参数里。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415692507657682586.png'/>

词编码 
每个词都用不同的含义，而要被机器所识别，就必须要把词进行编码，同时词编码时要保证词的相似性。图像识别的时候，对图像在RGB三个颜色通道中看他们的相似性就可以了，但是，无论中文还是英文，词都太多了，他是人造的，很难保持像图片这样的信息，所以我们希望能对词进行编码，保持它所包含的信息量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925077029995836.png'/>
因此，我们希望能有一个对应关系，如图，这些数字在空间中的表示能有一个对应关系。这不就是和机器学习差不多吗？很多机器学习的预测都是寻找一个对应关系，也就是数据（X）和预测的东西（Y）的对应。机器翻译其实原理也差不多。 


向量空间子结构 
我们希望找到这样一个关系，可以作为机器学习/深度学习的输入.
VKing−VQueen+VWomen=VMan
VKing−VQueen+VWomen=VMan


这个有没有感觉呢？其实换成这样，你可能更好理解:
VKing−VQueen=VMan−VWomen
VKing−VQueen=VMan−VWomen
我们就是希望找到King,QueenKing,Queen之间的差异，隐含的关系，然后通过一个dense vector表示。

One-Hot 
最简单的一种想法，就是对一句话用one-hot编码:比如对于这句话：
John likes to watch movies,Mary likes too.
John also likes to watch football games.

"John":1,"likes":2,"to":3,"watch":4,"movies":5,"also":6,"football":7,"games":8,"Mary":9,"too":10

用one-hot可以表示为：
John:[1,0,0,0,0,0,0,0,0,0]
likes:[0,1,0,0,0,0,0,0,0,0]
...
too:[0,0,0,0,0,0,0,0,0,1]

但事实是，这样做耗费的资源太多，而且不能很好的表征每句话的特性。

Bag of words(词袋模型) 
另一种方法则是词袋模型，它相当于一个词袋，不考虑词/句之间的相关性，只要出现了该词，就会记为1，再次出现就会+1。比如前面的那句话：
John likes to watch movies,Mary likes too.

可以表示为
[1,2,1,1,1,0,0,0,1,1]

与其相似的是binary weighting,它就是看一下每个词是否出现，出现记为1，没出现记为0
[1,1,1,1,1,0,0,0,1,1]

但他们都有一个缺点，不能将每个单词所代表的中心表示出来，比如John 和to,watch他们都是1，怎么直到那个词更重要呢？

TF-IDF 
因此，就有tf-idf解决这个问题，它的主要思路就是有两方面： 
A—第一就是如果这个词在我们当前文档出现的频率非常高，说明它在当前文档应该是比较重要的。 
B-但如果它在所有的文档中出现的频次都非常好，大家可能就会觉得这个词应该不是那么重要的。

比如中文的“的“，或者我们上面那两个句子中的to. 
因此，tf-idf就是一个在当前文档和所有文档中权衡他们的重要性，然后计算出每个词的重要度的方法。

语言模型 作为Word Embedding的背景, 语言模型(Language Model)也是很有必要简要介绍一下。
统计语言模型就是用来计算一个句子的概率分布简单来说，就是计算一个句子的概率, 语言模型用处很广泛,比如机器翻译中, 如何挑选一个概率尽可能大的句子也就是尽量靠谱的句子返回. 假设一个长度为m的句子,包含词:[(w1,w2,w3,..,wm), 
那么这个句子的概率,也就是这m个词共现的概率:<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925419251452802.png'/>

一般情况, 语言模型都是为了使得条件概率: P(wt|w1,w2,..,wt−1)最大化, 不过考虑到近因效应, 当前词与距离它比较近的n个词更加相关，而非前面所有的词都有关, 因此上述公式可以近似为:<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925443879348816.png'/>
上述便是经典的n-gram模型的近似表示方式。

语言模型可以考虑到词之间的前后关系，缺点就是n-gram随着预料的增多，离散化越严重，最终导致数据稀疏的问题。

分布式表示 
如果数据的维度过高，经常需要用到分布式来表示，它是个什么东西呢？ 
比如有这么一个例子：
红色的大型卡车，黄色的中型轿车，蓝色的小型电动车

可以发现，这几个词之间都有一定的模式存在，我们可以通过这些模式，把表达的空间压缩的三个维度：颜色，车型，那个品牌的车。然后他们做一个笛卡尔积就可以有:
需要记忆的单元数=颜色 X 型号 X 车型

其中，在现代统计自然语言处理中，有一个非常有洞见的想法，就是：
能否用一个词附近的其他词来表示该词？

就像你认识一个新认识的朋友，想直到他的收入，你可以看一下他周围10个朋友的收入大致是多少，来推断他的收入是多少一样。

共现矩阵 
基于前面那个想法，就有了这样一个局域窗口，这个窗口的主要作用就是看一下周围的几个词才好，窗口的长度一般设为5-10。就像你看你朋友的收入一样，你是看周边的五个人还是10个人呢？

那他具体怎么表示呢？ 
假设为三句话： 
I like deep learning,
I like NLP,
I enjoy flying

假设局域窗口为1，可以得到这样的对称矩阵 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925104327717944.png'/>

可以发现I 的前后一共出现了2次like，所以第一行第二列是2。
其实共现矩阵的本质代表着词与词之间的连接，且不考虑顺序。因为他是对称的，所以一般会取行向量或者列向量来表示词向量。

其中面临的问题就是向量的维数随着词典的大小呈先行增长，且对于模型有着严重的稀疏性问题。

2.NNLM（Neural Network Language Model）神经网络语言模型
NNLM的基本思想
在一开始的时候，做自然语言处理可以发现很多问题，比如很多情况下，要做平滑处理等，因此深度学习慢慢开始火了之后，就有人说，不然咱来试一下神经网络来做这个吧。


他的本质就是：直接从语言模型出发，将模型最优化的过程转换为求词向量表示的过程。 
最优的方向是这个目标函数：用“我爱自然语言处理“为例,窗长度为n-1：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925110098567176.png'/>
这个是n=3的通俗表达。 

概率p满足归一化条件，因为词典中的每个词都有一个概率:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925108395493047.png'/>

NNLM原理 
神经语言模型(NNLM)最初由Bengio提出的A Neural Probabilistic Language Mode最为经典， word2vec便是从其中简化训练而来. Bengio通过下面的一个三层神经网络来计算P(wt|wt−1,wt−2...wt−n(+1))。
其原理图如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925110832083434.png'/>
首先第一层输入就是前n−1个词wt−(n+1),...,wt−1 去预测第t个词是wt 的概率. 这里面的矩阵C∈|V|×d维护着词汇表中所有词的词向量, 其中|V|是词汇表中词的个数, d是词向量的维度. 然后根据输入的前n−1个词, 在C中找到它们对应的词向量, 然后直接串联起来成为一个维度为(n−1)d的向量x 作为接下来三层神经网络的输入，后面就是普通神经网络了。

需要说明的是,因为我们那要预测概率最大的wt, 因此最后输出层的神经元应该与词汇表大小同样为|V|, 这里使用使用softmax函数归一化输出层的值到[0,1], 代表可能的每个词的概率. 此外在原文中, 存在一些直连边, 也就是上图中的虚线, 从输入层直接到输出层，是一个线性变换。Bingo在文中表示, 直连边的存在大幅降低迭代次数, 但对语言模型效果无提升, 随着计算能力的提高, 后续的工作基本都去掉了直连边。

整个模型可以解决类似这样的问题：假设我们有一分文本，我们可以通过他的前N−1个词预测他的第N个词应该是什么？

projection layer 
举个例子，有这样一个句子：“我爱自然语言处理“，拆开就是‘我’，’爱’，’自然’，’语言’，一共四个词，预测一下，下一个词是什么？ 
如果每个词都给一个索引表示，’我’为0，’爱’为1。

C矩阵为投影矩阵，其中词典的维数为v，假设v=10000。 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925113218237384.png'/>
那么‘我‘和‘爱‘的one-hot向量表示为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415692511387257335.png'/>

每个列向量都有D行，那么一个词对应的列向量乘以这个矩阵C就可以得到
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925115965306031.png'/>

这样的乘法相当于把一个句子中的一个词取出来了，将每个词的结果进行拼接，就可以得到一句话，这句话的向量表示为
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925115378871189.png'/>的列向量。

总结一下这一层做的事：主要是把一句话用one-hot向量表示，通过一个权重矩阵，得到表示这一句话的词向量。

hidden layer 
这一部分主要做的就是将上一层的输出作为输入，进行全连接，然后一般会有个tanh，来处理这些数据。

SoftMax层 
隐层出来之后，接一个SoftMax分类器，预测一下，在这10000个词的词表中，出现每个单词出现概率有多大。因此拿到的是一个10000X1 的概率向量。 

因为我们有标准答案，就是”我爱自然语言”的第五个词应该是“处理“，如果预测出来的不准确，就可以通过定义一个交叉熵损失函数来计算损失，通过BP算法来调整参数C。
## 请详细推导下word2vec(Xin Rong牛论文的解读)
本题解析来源：https://shomy.top/2017/07/28/word2vec-all/ 

目前需要做Network Embedding方面的课题，而复杂网络本身就经常借鉴NLP的一些算法模型, Embedding也不例外. 因此先从Word Embedding入手。之前对Word Embedding(暂且翻译为词嵌入或者词向量)的理解就是将单词根据某种特征转为数值向量，再来做其他工作比如文本分类的工作。而word2vec则是word embedding的一种模型，也是目前使用最广的词向量模型, 由Google的Mikolov团队2013年提出。之前仅仅能够使用第三方库来训练直接使用, 对其中原理并没有多少理解, 这篇博客则比较完整的从背景知识到原理，参数训练等方面整理一下word2Vec。

Mikolov的两篇文章中涉及word2vec的细节甚少. 有不少人都对此模型作出了更加详细的解释, 本文主要沿着Rong, X.word2vec Parameter Learning Explained这篇文章的思路来整理一下，很多公式参考于这篇文章。

参考:
Mikolov, T.(2013). Distributed Representations of Words and Phrases and their Compositionality.
Mikolov, T.(2013). Efficient Estimation of Word Representations in Vector Space.
Rong, X. (2014). word2vec Parameter Learning Explained.

背景
神经网络
引入word2vec之前需要先对神经网络的知识有一定了解, 这里只贴一张图说明一个简单三层神经网络，如下图, x=[x1,x2,...xK]是模型的输入, 中间经过与权重矩阵<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932500382113185.png'/>运算, 矩阵运算结果再经过非线性激活函数得到隐层的结果h, 从隐层到输出层同理. 这样从输入层到输出层既有线性变换,又有非线性变换, 因此可以更好刻画出输入变量的特征。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932503956394300.png'/>
神经语言模型
作为Word Embedding的背景, 语言模型(Language Model)也是很有必要简要介绍一下.

统计语言模型就是用来计算一个句子的概率分布

简单来说，就是计算一个句子的概率, 语言模型用处很广泛,比如机器翻译中, 如何挑选一个概率尽可能大的句子也就是尽量靠谱的句子返回. 假设一个长度为m的句子,包含词:[(w1,w2,w3,..,wm)，那么这个句子的概率,也就是这m个词共现的概率:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932513663330365.png'/>

一般情况, 语言模型都是为了使得条件概率: P(wt|w1,w2,..,wt−1)最大化, 不过考虑到近因效应, 当前词与距离它比较近的n个词更加相关(一般n不超过5),而非前面所有的词都有关, 因此上述公式可以近似为:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415693252738649414.png'/>

上述便是经典的n-gram模型的近似表示方式. 下面需要介绍一下神经语言模型(NNLM), 最初由Bengio提出的A Neural Probabilistic Language Mode最为经典, word2vec便是从其中简化训练而来. Bengio通过下面的一个三层神经网络来计算P(wt|wt−1,wt−2...wt−n(+1))：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932530668515851.png'/>
首先第一层输入就是前n−1个词wt−(n+1),...,wt−1wt−(n+1),...,wt−1 去预测第t个词是wt的概率. 这里面的矩阵C∈|V|×d维护着词汇表中所有词的词向量, 其中|V|是词汇表中词的个数, d是词向量的维度. 然后根据输入的前n−1个词, 在C中找到它们对应的词向量, 然后直接串联起来成为一个维度为(n−1)d的向量x 作为接下来三层神经网络的输入, 后面就是普通神经网络了。

需要说明的是,因为我们那要预测概率最大的wt, 因此最后输出层的神经元应该与词汇表大小同样为|V|, 这里使用使用softmax函数归一化输出层的值到[0,1], 代表可能的每个词的概率. 此外在原文中, 存在一些直连边, 也就是上图中的虚线, 从输入层直接到输出层, 是一个线性变换, Bingo在文中表示, 直连边的存在大幅降低迭代次数, 但对语言模型效果无提升, 随着计算能力的提高, 后续的工作基本都去掉了直连边.

神经语言模型构建完成之后,就是训练参数了. 这里的参数包括词向量矩阵C, 以及三层神经网络的权重, 偏置等参数. 训练数据就是大堆大堆的预料库. 训练结束之后, 语言模型得到了, 词向量也得到了. 换言之, 词向量是这个语言模型的副产品. 但是这个模型的缺点就是速度问题, 因为词汇表往往很大,几十万几百王, 训练起来就很耗时, Bengo仅仅训练5个epoch就花了3周, 这还是40个CPU并行训练的结果. 因此才会有了后续好多的优化工作, word2vec便是其中一个.

Word2Vec
简介
背景介绍完毕, 终于到主角了. word2vec是google于2013年的Distributed Representations ofWords and Phrases and their Compositionality 以及后续的Distributed Representations ofWords and Phrases and their Compositionality 两篇文章中提出的一种高效训练词向量的模型, 基本出发点是上下文相似的两个词,它们的词向量也应该相似, 比如香蕉和梨在句子中可能经常出现在相同的上下文中，因此这两个词的表示向量应该就比较相似.

word2vec模型中比较重要的概念是词汇的上下文, 说白了就是一个词周围的词, 比如wt的范围为1的上下文就是wt−1和wt+1. 在word2vec中提出两个模型(假设上下文窗口为3)

CBOW(Continuous Bag-of-Word): 以上下文词汇预测当前词: wt−1,wt+1去预测 wt
SkipGram: 以当前词预测其上下文词汇: wt 去预测wt−1,wt+1

两个模型图示如下
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932542888568931.png'/>
下面将会从最简单的上下文只有一个词的情形入手, 然后扩展到CBOW以及Skip-gram, 介绍原理以及参数训练过程. 关于word2vec的训练这里将会从完全的BP神经网络的过程来介绍。

One-Word Model
首先先看简化版入手: 输入输出都只有一个词, 如下图示:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933399243251430.png'/>
首先说明符号:
V: 词汇表长度; N: 隐层神经元个数, 同时也是词向量维度
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933416211290600.png'/>：输入层到隐层的权重矩阵, 其实就是词向量矩阵,其中每一行代表一个词的词向量
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933423931286960.png'/>：隐层到输出层的权重矩阵, 其中每一列也可以看作额外的一种词向量

下面从神经网络的前向过程开始介绍:
我们需要做的是用输入的词去预测输出的词. 其中 输入层的单词wI使用one-hot来表示的, 即在上图中x1,x2,x3,...,xV只有xk为1, 其余为0, 其中k可以是输入的词在词汇表中的索引下标。之后就是经过词向量矩阵W连接输入层和隐层. 其中由于X中只有一个1, 因此经过与W相乘, 相当于取出W中的第k行，实际也就是输入单词的wI的N维的词向量，使用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933436784247204.png'/>表示，来作为隐层的值，注意word2vec的隐层并没有激活函数:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933441348742956.png'/>

然后考虑从隐层的h到输出层Y, 同样h经过矩阵W′相乘，得到一个V×1的向量u:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415693345038428885.png'/>
其中u中的每个元素<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933461833468244.png'/>就是W′的第j列：用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933466565509846.png'/>表示，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933466565509846.png'/>与h做内积得到: <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933470292881474.png'/>，含义就是词汇表中第j个词的分数，我们的目的就是要根据输入词wI去预测输出的词，因此预测的词就取分数最高的即可。

这里为了方便概率表示，使用softmax将u归一化到[0,1]之间, 从而作为输出词的概率, 其实是一个多项分布, 也就是上图中的y:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933478828014034.png'/>

其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933486213632671.png'/>与<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933486591104120.png'/>都称为词w的词向量，一般使用前者作为词向量，而非后者，原因后续会解释。至此前向过程完成，就是给定一个词作为输入，来预测它的上下文词，还是比较简单的，属于简化版的神经语言模型。这个过程中需要用到的参数有两个词向量矩阵W,W′，下面就是重点了，介绍如何根据语料库来训练模型，更新参数，得到最终的词向量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933532510144847.png'/>
到此为止， 一个训练样本的反向传播训练过程就为止了。 我们可以看到，对于输入层到隐层的矩阵W，我们每次训练只需要更新一行向量即可，而对于隐层到输出层的矩阵W′的所有N×V个元素都需要更新一遍，这里的计算量还是很大的。这一节主要比较细致的介绍了最简单的输入输出只有一个单词的情况的推理和训练的过程，后面的CBOW(上下文预测单词)以及SG(单词预测上下文)均基于这一节扩展开来。

CBOW Model
这一部分讲word2vec的第一个形式: Continurous Bag-Of-Word，模型图示如下:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933536021583652.png'/>
跟上一个模型唯一的不同就是输入不再是一个词wI, 而是多个词，上图中一共有C个单词: x1k,x2k,...,xCk，每个x都是one-hot表示。 这样隐层的h的计算就会不同了: 之前一个单词的模型是直接取出W的一行<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933544723147001.png'/>作为h的值，在CBOW中则是取出W中输入的所有C个单词的词向量，然后直接取平均，如下:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933540692252654.png'/>

后面隐层到输出层的过程与One-Word Model 一模一样，包括目标函数定义， 反向传播训练等。将W′的更新公式照抄下来如下,依旧是每次都需要更新所有的行:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933556266255795.png'/>

隐层神经元的梯度也相同:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933561236111035.png'/>

下面考虑输入层到隐层稍微有些不同，在One-Word Model里面因为输入只有一个词，因此每次训练只更新这个词对应到W的那一行，但是在CBOW里面有多个词，这里采取的策略是将hh的梯度均摊到每个词上，因此每次训练会更新W中的C行，如下:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933564744121022.png'/>

到此为止 CBOW 的推理和训练过程也介绍完毕，基本跟One-Word Model 一样。

SkipGram Model
现在开始介绍word2vec的第二种形式: SkipGram(根据单词预测上下文)，这个模型与One-Word Model不同的地方在于，SG的输出有多个词，而非One-Word 中输出只有一个词，这样输出层就不是一个多项分布了，而是C个多项分布了，

模型图示如下：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933585910070681.png'/>

因此从输入层到隐层部分与One-Word Model 相同，隐层神经元的计算方式如下:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933591963138753.png'/>

因为输出层是有C个单词， 因此有C个多项分布: y1,y2...yC, 因此前向计算的过程也需要分开计算，如下公式，用来计算第c个输出单词的预测的多项分布中第j项，相比One-Word Model 多了一个c参数:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933595936280024.png'/>

需要主要的是这C个输出向量是相互独立的，可以当做是独立的C个One-Word Model 中的输出向量，相互之间没有影响，并且从图中也可以看出，连接隐层与C个输出层的参数矩阵W′是共享的，于是便有: <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933602574149387.png'/>

这里的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415693360812860055.png'/>的含义与One Word Model 中相同，都代表W′的第j列，同时也是词汇表中第j个单词的一种词向量(虽然实际中不用)。从前向后 根据上述公式计算出C个输出向量之后，在每个V维向量中选取概率最大的作为输出的单词，这样根据输出单词wI就得到了C个输出单词，也就达到了根据单词预测上下文的目的。

下面开始介绍SG的反向传播训练的过程，这个跟前面的有些许的不同, 首先是损失函数:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933612843278322.png'/>

前面说过输出的C个词是相互独立，因此<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933616694598396.png'/>, 此外<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933621342815638.png'/>的含义同One-Word Model 中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933624099921471.png'/>一样，都代表训练的真实的输出单词在词汇表的下标。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933629761306020.png'/>

优化
复杂度
前面的CBOW与SG模型是标准的word2vec模型，或者说是神经语言模型的简化版，去掉了隐层的激活函数，其余的变化不大，因此训练效率还是很低的。

我们分析下训练的复杂度。首先明确需要学习的两个词向量矩阵W,W′，从前面的推导中知道对于每一个训练样本，CBOW更新W的C行，SG更新W其中一行，也就是每次更新有限个词的词向量。但是对于W′则不同了，正如前面一直提到的，无论是CBOW还是SG，对每个训练样本(或者Mini Batch)从梯度更新中需要对W′的所有V×N个元素，也就是词汇表中所有V个单词都需要更新词向量，考虑现实任务词汇表一般是几十万，上百万千万级别的， 这个计算成本是巨大的。

关于计算成本大的原因，除了上面提到的训练部分，还有就是在每次前向计算的时候，隐层到输出层的softmax函数计算输出层V个元素，计算量也是很大，这样整个模型现实意义不大。

考虑到计算量大的部分都是在隐层到输出层上，尤其是W′的更新。因此word2vec使用了两种优化策略: Hierarchical Softmax 和 Negative Sampling。二者的出发点一致，就是在每个训练样本中，不再完全计算或者更新W′这个矩阵。二者都不再显示使用W′这个矩阵。

因此这也就解释了前面说的为什么不用W′作为最终词向量。在多一句，其实上述训练和推理的复杂度很大的根本原因是softmax的分母上的∑，因此在求梯度的时候，就会有V次的计算。因此下面的两种方法其实是对softmax的优化，不仅仅限制在word2vec.两种优化方式使得word2vec的训练速度大大提升，并且词向量的质量几乎没有下降，这也是word2vec在NLP领域如此流行的原因。

这里只介绍其中一种优化算法：Hierarchical SoftMax。

Hierarchical SoftMax
首先Hierarchical SoftMax(HS)并不是word2vec提出来的, 而是之前Bengio在2005年最早提出来专门为了加速计算神经语言模型中的softmax的一种方式, 这里介绍如何在word2vec中使用. 

HS主要基于哈夫曼树(一种二叉数)将计算量大的部分变为了一种二分类的问题. 先看下面的图，原来的模型在隐层之后通过W′连接输出层, 现在HS则去掉了W′, 隐层h直接与下面的二叉树的root节点相连：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932079953438540.png'/>
其中图中白色的叶子节点表示词汇表中所有的|V|个词, 黑色节点表示非叶子节点, 每一个叶子节点也就是每一个单词, 都对应唯一的一条从root节点出发的路径。而我们的目的是使的w=wO这条路径的概率最大，即: P(w=wO|wI)最大, 此时每一个分支都代表一个选择, 向左转还是向右转. 所以如何判断向左还是向右呢? 

我们用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932096590847829.png'/>表示从root到叶子节点w的路径上的第j个非叶子节点, 并且每个非叶子节点都对应一个向量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932102811228403.png'/>，维度与h相同, 然后使用一个sigmod函数: <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932131541126578.png'/>, 结合向量的内积, 来判断该向左还是向右, 如下, 第n个节点向左 以及向右的概率定义：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415693213649981981.png'/>

有了上述的概率, 我们可以重新定义P(wO|wi)了：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932160782034408.png'/>

其中I()是指示函数, 条件成立值为1, 反之为-1. 而L(w)表示整条路径的长度, 这样整个概率就是从root节点到叶子节点这条路径的概率, 这样我们在训练的时候, 通过训练样本来更新非叶子节点的参数v′w.

举个例子, 比如上图中的加粗的黑色路径: (n(w2,1),n(w2,2),n(w2,3),w2(n(w2,1),n(w2,2),n(w2,3),w2 , 就是说假设有一个训练样本是(wI,w2), 我们需要使得P(wO=w2|wI)概率最大:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932197471243141.png'/>

并且在一个非叶子节点处, 向左向右的概率和为1, 因此一直分裂下去,最后的和肯定还是1. 因此可以很容易得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932202684423110.png'/>

这一点的证明是有必要的, 因为在原始的softmax本身保证了所有单词的概率和是1, 而通过上式也知道了通过HS得到的输出层仍然是一个概率多项分布, 输出所有的单词概率和为1.

讲完了从前向后的如何得到输出单词的概率的过程, 下面开始后向传播的训练过程.。

首先需要明确的是训练的参数: 输入层与隐层的词向量矩阵W, 以及二叉树的非叶子节点对应的向量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932216066412765.png'/>。

为了书写方便,下面简化一部分的符号: 用[I]表示前面的指示函数I(n(w,j+1)==left), 使用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932228316836686.png'/>表示<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641569322289906486.png'/>。

对于一组训练数据, 损失函数的定义与前面相同, 最大似然(注意这里以One-Word Model为例，CBOW与Skip-Gram按照上述的思路完全一样)：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932238064727751.png'/>

之后便可以逐项求梯度了, 先考虑<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415693224974527821.png'/>, 注意<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932250552539245.png'/>：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932251433225928.png'/>

之后对[I]分情况讨论, 可得:<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932256676898368.png'/>

这里如果[I]=1, 那么tj=1, 否则 tj=0, 这个公式与前面的yj−tj很类似, 可以理解为预测值与实际值的差别。

有了上述的梯度,就可以很简单的求出<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932281131851034.png'/>的梯度了：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932278177987766.png'/>

有了梯度,便可以更新了, 具体公式还是梯度下降：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932286990541555.png'/>

也就是说对于一个训练样本， 我们只需要更新L(w)−1个向量就好了， 而未优化的版本需要更新V个， 相当于时间复杂度从O(V)O(V)降到了O(logV), 这个提升还是非常大的。
虽然在考察空间复杂度方面，HS的二叉树的非叶子节点有V−1个，也就是我们需要V−1存储<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932298814615118.png'/>,优化之前则是V个， 空间复杂度相同， 但总体而言，时间复杂度大大降低了。

然后考虑隐层h的梯度，因为我们的优化目标都是在隐层到输出层，因此前面的几乎不变， 跟One-Word Model 一样，路径上的非叶子节点的表达式都含有hh，因此需要对梯度求和：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932307786980718.png'/>
其实跟前面很一样了， 只需要替代下导数就好了， 后面就不再赘述了。整个Hierarchical Softmax的优化算法介绍完了，隐层到输出层的计算量从O(V), 利用二叉树(更加具体来说是哈夫曼树）降为了O(logV)。

## word2vec 相比之前的 Word Embedding 方法好在什么地方？
本题解析来源：https://www.zhihu.com/question/53011711

解析一
@邱锡鹏：Word2vec训练方面采用的HSoftmax以及负采样确实可以认为是创新不大。但Word2vec流行的主要原因也不在于此。主要原因在于以下3点：
1. 极快的训练速度。以前的语言模型优化的目标是MLE，只能说词向量是其副产品。Mikolov应该是第一个提出抛弃MLE（和困惑度）指标，就是要学习一个好的词嵌入。如果不追求MLE，模型就可以大幅简化，去除隐藏层。再利用HSoftmax以及负采样的加速方法，可以使得训练在小时级别完成。而原来的语言模型可能需要几周时间。
2. 一个很酷炫的man-woman=king-queen的示例。这个示例使得人们发现词嵌入还可以这么玩，并促使词嵌入学习成为了一个研究方向，而不再仅仅是神经网络中的一些参数。
3. word2vec里有大量的tricks，比如噪声分布如何选？如何采样？如何负采样？等等。这些tricks虽然摆不上台面，但是对于得到一个好的词向量至关重要。

举一个生活中的例子，语言模型和word2vec的关系可以类比于单反相机和美颜手机，它们的受众不一样。就照片质量（MLE）而言，单反肯定好。但如果更关心美颜（词嵌入）和便携性（训练速度），美颜手机就更受欢迎。

更多的资料可以参考：
https://nndl.github.io/ch12.pdf 

解析二
@吴海波：最近正好写了篇相关的文章，原文如下：

引子
此时再谈Word2Vec，特别是ELMo、Bert相继大火后，有点炒冷饭的意味。但有一个点，很多同学可能好奇过却没有深究：自然语言处理里面应用Distribution representation做Embedding，word2vec不是第一个，对比Matrix Fatorication的方法，比如SVD、LSA，为何Word2Vec出来后，Emebedding变的这么火？

面试的时候，如果聊得时间有多，会顺便问下这个问题，大多数同学都没有想过这个问题，能提到Word2Vec比如SVD之类的更容易训练更多的数据，已经寥寥无几。

其实这个问题学术界相关的研究很多，但不熟悉的同学往往不知道该用什么关键词，而Word2Vec的相关的文献不计其数，导致真正有价值的信息被埋没了，比如上面这个问题，你如果搜why word2vec so good，出来的大多是描述其原理的（知乎上就有很好的讨论帖[2][3]），需要搜的key是word2vec matrix fatorication，这就需要你了解一些相关的背景。准确的定义问题的描述，往往比问题的答案更重要。

本文参考了一些相关论文，尽量少引入公式（主要是我懒，公式编辑太麻烦了），尝试解答上述问题。

背景知识
Word2Vec
详尽的原理，网上有很多非常好的资料，这里不再赘述。Word2Vec和Deep Learing的关系并不深，至少一点也不Deep。13、14年那段时间，很多学者都在尝试解释它到底学到了什么。无论是CBOW还是Skip-Gram，本质还是要基于word和context做文章，即可以理解为模型在学习word和context的co-occurrence。

参考[1]本文重点关注Skip-Gram with Negative Sample，简称为SGNS，让我们来回顾下它的目标函数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157034184239331650.jpg'/>

再介绍论文[1]如何将它一步步变成和Matrix Factorization等价前，我们先来了解下MF的背景。

Matrix Factorization
MF即矩阵分解，典型的算法有SVD、LSA，是一种常见的技术，在推荐、NLP都有应用。据资料显示，百度在2010年之前，就做过大规模分布式的plsa算法。简单来讲，MF就是将一个矩阵分解成多个矩阵的乘积。让我们回顾下word2vec，最终每一个word都会有两个向量：V_word和V_context，假设存在一个矩阵W = V_word * V_context，word2vec也可以理解成是对矩阵W的矩阵分解。那么，问题就变成这个matrix W是什么？

Pointwise Mutual Information（PMI）
论文[1]中有将SGNS的目标函数，一步步的变换，变成下面的形式（具体见论文）
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157034185955190239.jpg'/>

上述公式中，左边的部分就是大名鼎鼎的PMI，即w和c同时出现的次数/(w出现的次数 + c出现的次数)。和MF对比，就多了个常量logk。PMI是dense matrix，对模型优化带来很多困难，所以一般会用sparse matrix PPMI（positive PMI）来替代它。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157034187556666755.jpg'/>

到此，可以说明word2vec和MF并没有本质的区别，是一种implicit的MF，正如论文[1]的标题。既然其本质差别不大，是否MF的测试效果也能和word2vec一致呢。首先，在word similar上，SVD和Word2vec差距并不大，但在word analogies上，word2vec要明显好于SVD。

Why
训练大规模语料的能力
这个当然是核心优点，虽然SVD有SMF的版本去处理大数据，LSA也有大数据的方案，但谁都没有word2vec的简单易实现。google一开始的开源代码，根本不需要分布式就能跑以前不能跑的数据规模。

Simple is Power！

举个不恰当的例子：很多人怀疑牛顿是先发明了微积分，后面才做出了很多重要的工作，为了装逼在他的著作里面用传统的数学方法重写了一遍，各种奇技淫巧。当然这个逼装的有点过，导致后面和莱布尼茨关于微积分的发明权吵了一辈子。

很多论文，因为word2vec的成果，有了方向后再去回溯传统，其价值自然远不如开创者。数据量的问题，在之前可能都不是核心问题，再加上LSA模型稳定，在小数据集上效果挺好，数据规模增长一点不一定能看出收益，而不像word2vec，在数据集小的时候，可能还不如CountVectorizer，必须往大数据去。

再一次，定义问题是什么，比找到方法难。

MF suffer from unobserved value
在word-context matrix中，这是个常见的问题。另外，对matrix中的value做不同的weight也不是件容易的事情。最重要的是，MF类的算法在analogies task上有明显劣势。即当初那个经典的word2vec展示case：king - queen = man - woman，即词向量可以相互做加减，具有实际意义。

这个case有点特殊，并不是所有的词向量都成立，但确实比MF类的模型总体上好。比较酷炫，其原理我还没有找到特别好的解释，如果有童鞋知道，求告知。

融合二者
Glove是其中代表，即结合了global matrix factorization and local context window methods。论文[5]中给出了不错的实验数据，但也有不少人质疑它，比如论文[6]。另外，word2vec比glove简单多了，也更容易被大家接受。我们自己实践来看，glove也没有明显增量。

最后
这篇是好奇心驱动的产物，实用价值并不高，只为答疑解惑。前期在搜索过程中看了不少无效的资料，浪费了不少时间，希望本文能给感兴趣的同学一些帮助。

参考文献
[1] Neural Word Embedding as Implicit Matrix Factorization
[2] word2vec 相比之前的 Word Embedding 方法好在什么地方？
[3] 词向量，LDA，word2vec三者的关系是什么
[4] What is the connection between PCA and Word2Vec in terms of word embedding? Is there an empiric superiority between the two?
[5]GloVe: Global Vectors for Word Representation 

[6]Improving Distributional Similarity with Lessons Learned from Word Embeddings
[7]Linguistic Regularities in Sparse and Explicit Word Representations
[8]ALL-BUT-THE-TOP: SIMPLE AND EFFECTIVE POSTPROCESSING FOR WORD REPRESENTATIONS
[9]Enriching Word Vectors with Subword Information

解析三
@李韶华：词嵌入模型效果好不好的关键之一，是用上下文词预测当前词的formulation，即采用的回归函数。

Hinton等07年和08年的log bilinear language model之前的工作都采用的是 softmax([上下文词向量,当前词向量]的线性变换) 的形式，softmax里边可以简化认为是一些向量的线性和。但几个向量的线性和不能很好的抓住这几个向量在隐空间一些维度上取值接近的特点，所以效果并不好。

07年的Three New Graphical Models for Statistical Language Modelling里，三个模型之一是log bilinear language model (LBL), 题目中08年的论文扩展了这个方法，得到Hierarchical Log Bilinear Language model。

为了叙述简单，下面把这两种方法统称为LBL。LBL使用了 softmax(上下文词向量的线性变换 * 当前词向量) 的形式，点乘在抓两个向量在一些维度上取值接近方面，比相加要好得多，这是词向量模型发展的一个重大突破。

word2vec使用的也是LBL。那么和之前的方法有什么区别呢？08年的Hierarchical LBL里，用的是这样的回归函数:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157034315393488470.svg'/>

这里的Ci都是矩阵，不同位置对应不同的矩阵。
word2vec的CBOW用的是（skip-gram我觉得和CBOW基本是等价的，效果也类似，但CBOW的概率解释好些，所以拿它来比较）:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157034316295567998.svg'/>

可见它移除了变换矩阵Ci和偏移量bw. 实际上我们事后诸葛来看，变换矩阵Ci也的确是多余的，因为 两个词经常一块出现两个词在某方面有相似语义两个向量在某些维度取值类似，那么直接点乘就可以了，用Ci变换一下，反而有可能把本来相似的维度变得不同，从而让学出来的向量不能很好满足“相似词在有些维度上取值相近”的训练效果。

显而易见，移除Ci会极大的提高运算速度，使word2vec在大语料上训练非常可行。两个LBL模型训练语料都是1000w单词左右，而word2vec即使用wikipedia这样>20亿词规模的语料也只需几小时，大语料下得出的词向量当然会抓住更多的语法语义规律，从而更准确。

所以我觉得，word2vec的成功，印证了一句话：Less is more.
## 说说NLP中的预训练技术发展史：从Word Embedding到Bert模型
本题解析的作者：张俊林，链接：https://zhuanlan.zhihu.com/p/49271699

July注：本文是了解bert历史的最佳文章，把整个来龙去脉介绍的高屋建瓴、通俗细致，全文主要有这些内容：预训练在图像领域的应用、从语言模型到Word Embedding、从Word Embedding到ELMO、从Word Embedding到GPT、Bert的诞生。

当然，在此之前，建议先通过此文了解word2vec：https://blog.csdn.net/v_JULY_v/article/details/102708459，这是理解bert的关键，其次则是Transformer，关于Transformer，推荐此文 https://www.julyedu.com/question/big/kp_id/30/ques_id/2912。

因为我也是这么过来的，之前bert刚火起来的时候，就看到俊林老师这篇文章，当时看的不甚了解，及至后来先学习word2vec和Transformer之后，再看此文，你会觉得真是高屋建瓴，甚至醍醐灌顶，是关于bert中文介绍少有的好文章。
话休絮烦，以下即为张俊林老师所写的正文。

Bert最近很火，应该是最近最火爆的AI进展，网上的评价很高，那么Bert值得这么高的评价吗？我个人判断是值得。那为什么会有这么高的评价呢？是因为它有重大的理论或者模型创新吗？

其实并没有，从模型创新角度看一般，创新不算大。但是架不住效果太好了，基本刷新了很多NLP的任务的最好性能，有些任务还被刷爆了，这个才是关键。另外一点是Bert具备广泛的通用性，就是说绝大部分NLP任务都可以采用类似的两阶段模式直接去提升效果，这个第二关键。客观的说，把Bert当做最近两年NLP重大进展的集大成者更符合事实。

本文的主题是自然语言处理中的预训练过程，会大致说下NLP中的预训练技术是一步一步如何发展到Bert模型的，从中可以很自然地看到Bert的思路是如何逐渐形成的，Bert的历史沿革是什么，继承了什么，创新了什么，为什么效果那么好，主要原因是什么，以及为何说模型创新不算太大，为何说Bert是近年来NLP重大进展的集大成者。

我们一步一步来讲，而串起来这个故事的脉络就是自然语言的预训练过程，但是落脚点还是在Bert身上。要讲自然语言的预训练，得先从图像领域的预训练说起。

图像领域的预训练自从深度学习火起来后，预训练过程就是做图像或者视频领域的一种比较常规的做法，有比较长的历史了，而且这种做法很有效，能明显促进应用的效果。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156040081288495.png'/>

那么图像领域怎么做预训练呢，上图展示了这个过程，我们设计好网络结构以后，对于图像来说一般是CNN的多层叠加网络结构，可以先用某个训练集合比如训练集合A或者训练集合B对这个网络进行预先训练，在A任务上或者B任务上学会网络参数，然后存起来以备后用。假设我们面临第三个任务C，网络结构采取相同的网络结构，在比较浅的几层CNN结构，网络参数初始化的时候可以加载A任务或者B任务学习好的参数，其它CNN高层参数仍然随机初始化。

之后我们用C任务的训练数据来训练网络，此时有两种做法，
一种是浅层加载的参数在训练C任务过程中不动，这种方法被称为“Frozen”;
另外一种是底层网络参数尽管被初始化了，在C任务训练过程中仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”，顾名思义，就是更好地把参数进行调整使得更适应当前的C任务。

一般图像或者视频领域要做预训练一般都这么做。这么做有几个好处，首先，如果手头任务C的训练集合数据量较少的话，现阶段的好用的CNN比如Resnet/Densenet/Inception等网络结构层数很深，几百万上千万参数量算起步价，上亿参数的也很常见，训练数据少很难很好地训练这么复杂的网络，但是如果其中大量参数通过大的训练集合比如ImageNet预先训练好直接拿来初始化大部分网络结构参数，然后再用C任务手头比较可怜的数据量上Fine-tuning过程去调整参数让它们更适合解决C任务，那事情就好办多了。

这样原先训练不了的任务就能解决了，即使手头任务训练数据也不少，加个预训练过程也能极大加快任务训练的收敛速度，所以这种预训练方式是老少皆宜的解决方案，另外疗效又好，所以在做图像处理领域很快就流行开来。

那么新的问题来了，为什么这种预训练的思路是可行的？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154534918309172.jpg'/>

目前我们已经知道，对于层级的CNN结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构，如上图所示，如果我们手头是个人脸识别任务，训练好网络后，把每层神经元学习到的特征可视化肉眼看一看每层学到了啥特征，你会看到最底层的神经元学到的是线段等特征，图示的第二个隐层学到的是人脸五官的轮廓，第三层学到的是人脸的轮廓，通过三步形成了特征的层级结构，越是底层的特征越是所有不论什么领域的图像都会具备的比如边角线弧线等底层基础特征，越往上抽取出的特征越与手头任务相关。

正因为此，所以预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务网络参数的原因。而高层特征跟任务关联较大，实际可以不用使用，或者采用Fine-tuning用新数据集合清洗掉高层无关的特征抽取器。

一般我们喜欢用ImageNet来做网络的预训练，主要有两点，一方面ImageNet是图像领域里有超多事先标注好训练数据的数据集合，分量足是个很大的优势，量越大训练出的参数越靠谱；另外一方面因为ImageNet有1000类，类别多，算是通用的图像数据，跟领域没太大关系，所以通用性好，预训练完后哪哪都能用，是个万金油。分量足的万金油当然老少通吃，人人喜爱。

听完上述话，如果你是具备研究素质的人，也就是说具备好奇心，你一定会问下面这个问题：”既然图像领域预训练这么好用，那干嘛自然语言处理不做这个事情呢？是不是搞NLP的人比搞CV的傻啊？就算你傻，你看见人家这么做，有样学样不就行了吗？这不就是创新吗，也许能成，万一成了，你看，你的成功来得就是这么突然!”

嗯，好问题，其实搞NLP的人一点都不比你傻，早就有人尝试过了，不过总体而言不太成功而已。听说过word embedding吗？2003年出品，陈年技术，馥郁芳香。word embedding其实就是NLP里的早期预训练技术。当然也不能说word embedding不成功，一般加到下游任务里，都能有1到2个点的性能提升，只是没有那么耀眼的成功而已。没听过？那下面就把这段陈年老账讲给你听听。

Word Embedding考古史这块大致讲讲Word Embedding的故事，很粗略，因为网上关于这个技术讲的文章太多了，汗牛冲动，我不属牛，此刻更没有流汗，所以其实丝毫没有想讲Word Embedding的冲动和激情，但是要说预训练又得从这开始，那就粗略地讲讲，主要是引出后面更精彩的部分。在说Word Embedding之前，先更粗略地说下语言模型，因为一般NLP里面做预训练一般的选择是用语言模型任务来做。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157155009414062228.jpg'/>

什么是语言模型？其实看上面这张PPT上扣下来的图就明白了，为了能够量化地衡量哪个句子更像一句人话，可以设计如上图所示函数，核心函数P的思想是根据句子里面前面的一系列前导单词预测后面跟哪个单词的概率大小（理论上除了上文之外，也可以引入单词的下文联合起来预测单词出现概率）。句子里面每个单词都有个根据上文预测自己的过程，把所有这些单词的产生概率乘起来，数值越大代表这越像一句人话。

语言模型压下暂且不表，我隐约预感到我这么讲你可能还是不太会明白，但是大概这个意思，不懂的可以去网上找，资料多得一样地汗牛冲动（July注：关于语言模型还可以看看这篇 https://www.julyedu.com/question/big/kp_id/30/ques_id/2984）。

假设现在让你设计一个神经网络结构，去做这个语言模型的任务，就是说给你很多语料做这个事情，训练好一个神经网络，训练好之后，以后输入一句话的前面几个单词，要求这个网络输出后面紧跟的单词应该是哪个，你会怎么做？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154560273799310.jpg'/>

你可以像上图这么设计这个网络结构，这其实就是大名鼎鼎的中文人称“神经网络语言模型”，英文小名NNLM的网络结构，用来做语言模型。这个工作有年头了，是个陈年老工作，是Bengio 在2003年发表在JMLR上的论文。它生于2003，火于2013，以后是否会不朽暂且不知，但是不幸的是出生后应该没有引起太大反响，沉寂十年终于时来运转沉冤得雪，在2013年又被NLP考古工作者从海底湿淋淋地捞出来了祭入神殿。

为什么会发生这种技术奇遇记？你要想想2013年是什么年头，是深度学习开始渗透NLP领域的光辉时刻，万里长征第一步，而NNLM可以算是南昌起义第一枪。在深度学习火起来之前，极少有人用神经网络做NLP问题，如果你10年前坚持用神经网络做NLP，估计别人会认为你这人神经有问题。所谓红尘滚滚，谁也挡不住历史发展趋势的车轮，这就是个很好的例子。

上面是闲话，闲言碎语不要讲，我们回来讲一讲NNLM的思路。先说训练过程，现在看其实很简单，见过RNN、LSTM、CNN后的你们回头再看这个网络甚至显得有些简陋。学习任务是输入某个句中单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154570412977637.svg'/>前面句子的t-1个单词，要求网络正确预测单词Bert，即最大化：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154572430609079.svg'/>

前面任意单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154574554305681.svg'/>用Onehot编码（比如：0001000）作为原始单词输入，之后乘以矩阵Q后获得向量 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154576598939623.svg'/>，每个单词的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715457745240528.svg'/>拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。

这个<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154578489621011.svg'/>是什么？这其实就是单词对应的Word Embedding值，那个矩阵Q包含V行，V代表词典大小，每一行内容代表对应单词的Word embedding值。只不过Q的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵Q，当这个网络训练好之后，矩阵Q的内容被正确赋值，每一行代表一个单词对应的Word embedding值。

所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵Q，这就是单词的Word Embedding是被如何学会的。2013年最火的用语言模型做Word Embedding的工具是Word2Vec，后来又出了Glove。

Word2Vec是怎么工作的呢？看下图。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154582635057288.jpg'/>

Word2Vec的网络结构其实和NNLM是基本类似的，只是这个图长得清晰度差了点，看上去不像，其实它们是亲兄弟。不过这里需要指出：尽管网络结构相近，而且也是做语言模型任务，但是其训练方法不太一样。

Word2Vec有两种训练方法，一种叫CBOW，核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词；
第二种叫做Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。

而你回头看看，NNLM是怎么训练的？是输入一个单词的上文，去预测这个单词。这是有显著差异的。为什么Word2Vec这么处理？原因很简单，因为Word2Vec和NNLM不一样，NNLM的主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而word embedding只是无心插柳的一个副产品。

但是Word2Vec目标不一样，它单纯就是要word embedding的，这是主产品，所以它完全可以随性地这么去训练网络。为什么要讲Word2Vec呢？这里主要是要引出CBOW的训练方法，BERT其实跟它有关系，后面会讲它们之间是如何的关系，当然它们的关系BERT作者没说，是我猜的，至于我猜的对不对，后面你看后自己判断。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154605888272540.jpg'/>

使用Word2Vec或者Glove，通过做语言模型任务，就可以获得每个单词的Word Embedding，那么这种方法的效果如何呢？上图给了网上找的几个例子，可以看出有些例子效果还是很不错的，一个单词表达成Word Embedding后，很容易找出语义相近的其它词汇。

我们的主题是预训练，那么问题是Word Embedding这种做法能算是预训练吗？这其实就是标准的预训练过程。要理解这一点要看看学会Word Embedding后下游任务是怎么用它的。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156048711181137.png'/>

假设如上图所示，我们有个NLP的下游任务，比如QA，就是问答问题，所谓问答问题，指的是给定一个问题X，给定另外一个句子Y,要判断句子Y是否是问题X的正确答案。问答问题假设设计的网络结构如上图所示，这里不展开讲了，懂得自然懂，不懂的也没关系，因为这点对于本文主旨来说不关键，关键是网络如何使用训练好的Word Embedding的。

它的使用方法其实和前面讲的NNLM是一样的，句子中每个单词以Onehot形式作为输入，然后乘以学好的Word Embedding矩阵Q，就直接取出单词对应的Word Embedding了。

这乍看上去好像是个查表操作，不像是预训练的做法是吧？其实不然，那个Word Embedding矩阵Q其实就是网络Onehot层到embedding层映射的网络参数矩阵。所以你看到了，使用Word Embedding等价于什么？等价于把Onehot层到embedding层的网络用预训练好的参数矩阵Q初始化了。这跟前面讲的图像领域的低层预训练过程其实是一样的，区别无非Word Embedding只能初始化第一层网络参数，再高层的参数就无能为力了。

下游NLP任务在使用Word Embedding的时候也类似图像有两种做法，一种是Frozen，就是Word Embedding那层网络参数固定不动；另外一种是Fine-Tuning，就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新掉。

上面这种做法就是18年之前NLP领域里面采用预训练的典型做法，之前说过，Word Embedding其实对于很多下游NLP任务是有帮助的，只是帮助没有大到闪瞎忘记戴墨镜的围观群众的双眼而已。

那么新问题来了，为什么这样训练及使用Word Embedding的效果没有期待中那么好呢？答案很简单，因为Word Embedding有问题呗。这貌似是个比较弱智的答案，关键是Word Embedding存在什么问题？这其实是个好问题。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715605432899758.png'/>

这片在Word Embedding头上笼罩了好几年的乌云是什么？是多义词问题。我们知道，多义词是自然语言中经常出现的现象，也是语言灵活性和高效性的一种体现。多义词对Word Embedding来说有什么负面影响？

如上图所示，比如多义词Bank，有两个常用含义，但是Word Embedding在对bank这个单词进行编码的时候，是区分不开这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。

所以word embedding无法区分多义词的不同语义，这就是它的一个比较严重的问题。你可能觉得自己很聪明，说这可以解决啊，确实也有很多研究人员提出很多方法试图解决这个问题，但是从今天往回看，这些方法看上去都成本太高或者太繁琐了，有没有简单优美的解决方案呢？ELMO提供了一种简洁优雅的解决方案。

从Word Embedding到ELMO
ELMO是“Embedding from Language Models”的简称，其实这个名字并没有反应它的本质思想，提出ELMO的论文题目：“Deep contextualized word representation”更能体现其精髓，而精髓在哪里？在deep contextualized这个短语，一个是deep，一个是context，其中context更关键。

在此之前的Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变，所以对于比如Bank这个词，它事先学好的Word Embedding中混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含money等词）明显可以看出它代表的是“银行”的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。这是为何说它是静态的，这也是问题所在。

ELMO的本质思想是：我事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。

所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715606044100672.png'/>

ELMO采用了典型的两阶段过程，第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。

上图展示的是其预训练过程，它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715463197090044.svg'/>的上下文去正确预测单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154632618450065.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154633882353118.svg'/>之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。

图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154634360481822.svg'/>的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM叠加。

这个网络结构其实在NLP中是很常用的。使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154642594810124.svg'/>，句子中每个单词都能得到对应的三个Embedding：
最底层是单词的Word Embedding；
往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；
再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。

也就是说，ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156022749639923.png'/>

上面介绍的是ELMO的第一阶段：预训练阶段。那么预训练好网络结构后，如何给下游任务使用呢？上图展示了下游任务的使用过程，比如我们的下游任务仍然是QA问题，此时对于问句X，我们可以先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding，之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。

然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。对于上图所示下游任务QA中的回答句子Y来说也是如此处理。因为ELMO给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。至于为何这么做能够达到区分多义词的效果，你可以想一想，其实比较容易想明白原因。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156065876593589.png'/>

上面这个图是TagLM采用类似ELMO的思路做命名实体识别任务的过程，其步骤基本如上述ELMO的思路，所以此处不展开说了。TagLM的论文发表在2017年的ACL会议上，作者就是AllenAI里做ELMO的那些人，所以可以将TagLM看做ELMO的一个前导工作。

前几天这个PPT发出去后有人质疑说FastAI的在18年4月提出的ULMFiT才是抛弃传统Word Embedding引入新模式的开山之作，我深不以为然。
首先TagLM出现的更早而且模式基本就是ELMO的思路；
另外ULMFiT使用的是三阶段模式，在通用语言模型训练之后，加入了一个领域语言模型预训练过程，而且论文重点工作在这块，方法还相对比较繁杂，这并不是一个特别好的主意，因为领域语言模型的限制是它的规模往往不可能特别大，精力放在这里不太合适，放在通用语言模型上感觉更合理；
再者，尽管ULFMiT实验做了6个任务，但是都集中在分类问题相对比较窄，不如ELMO验证的问题领域广，我觉得这就是因为第二步那个领域语言模型带来的限制。
所以综合看，尽管ULFMiT也是个不错的工作，但是重要性跟ELMO比至少还是要差一档，当然这是我个人看法。

每个人的学术审美口味不同，我个人一直比较赞赏要么简洁有效体现问题本质，要么思想特别游离现有框架脑洞开得异常大的工作，所以ULFMiT我看论文的时候就感觉看着有点难受，觉得这工作没抓住重点而且特别麻烦，但是看ELMO论文感觉就赏心悦目，觉得思路特别清晰顺畅，看完暗暗点赞，心里说这样的文章获得NAACL2018最佳论文当之无愧，比ACL很多最佳论文也好得不是一点半点，这就是好工作带给一个有经验人士的一种在读论文时候就能产生的本能的感觉，也就是所谓的这道菜对上了食客的审美口味。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154658139852177.jpg'/>

前面我们提到静态Word Embedding无法解决多义词的问题，那么ELMO引入上下文动态调整单词的embedding后多义词问题解决了吗？解决了，而且比我们期待的解决得还要好。

上图给了个例子，对于Glove训练出的Word Embedding来说，多义词比如play，根据它的embedding找出的最接近的其它单词大多数集中在体育领域，这很明显是因为训练数据中包含play的句子中体育领域的数量明显占优导致；而使用ELMO，根据上下文动态调整后的embedding不仅能够找出对应的“演出”的相同语义的句子，而且还可以保证找出的句子中的play对应的词性也是相同的，这是超出期待之处。之所以会这样，是因为我们上面提到过，第一层LSTM编码了很多句法信息，这在这里起到了重要作用。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154672595377892.jpg'/>

ELMO经过这般操作，效果如何呢？实验效果见上图，6个NLP任务中性能都有幅度不同的提升，最高的提升达到25%左右，而且这6个任务的覆盖范围比较广，包含句子语义关系判断，分类任务，阅读理解等多个领域，这说明其适用范围是非常广的，普适性强，这是一个非常好的优点。

那么站在现在这个时间节点看，ELMO有什么值得改进的缺点呢？
首先，一个非常明显的缺点在特征抽取器选择方面，ELMO使用了LSTM而不是新贵Transformer，Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响，很多研究已经证明了Transformer提取特征的能力是要远强于LSTM的。如果ELMO采取Transformer作为特征提取器，那么估计Bert的反响远不如现在的这种火爆场面。
另外一点，ELMO采取双向拼接这种融合特征的能力可能比Bert一体化的融合特征方式弱，但是，这只是一种从道理推断产生的怀疑，目前并没有具体实验说明这一点。我们如果把ELMO这种预训练方法和图像领域的预训练方法对比，发现两者模式看上去还是有很大差异的。

除了以ELMO为代表的这种基于特征融合的预训练方法外，NLP里还有一种典型做法，这种做法和图像领域的方式就是看上去一致的了，一般将这种方法称为“基于Fine-tuning的模式”，而GPT就是这一模式的典型开创者。

从Word Embedding到GPT
GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156074526382168.png'/>

GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：
首先，特征抽取器不是用的RNN，而是用的Transformer，上面提到过它的特征抽取能力要强于RNN，这个选择很明显是很明智的；
其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，所谓“单向”的含义是指：语言模型训练的任务目标是根据<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154689157643717.svg'/>单词的上下文去正确预测单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154689157643717.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154689157643717.svg'/>之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。

ELMO在做语言模型预训练的时候，预测单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154689157643717.svg'/>同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文。这个选择现在看不是个太好的选择，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到Word Embedding中，是很吃亏的，白白丢掉了很多信息。

这里强行插入一段简单提下Transformer，尽管上面提到了，但是说的还不完整，补充两句。首先，Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器，注意力这个机制在此被发扬光大，从任务的配角不断抢戏，直到Transformer一跃成为踢开RNN和CNN传统特征提取器，荣升头牌，大红大紫。

你问了：什么是注意力机制？这里再插个广告，对注意力不了解的可以参考鄙人16年出品17年修正的文章：“深度学习中的注意力模型”：https://www.julyedu.com/question/big/kp_id/30/ques_id/2911，补充下相关基础知识，如果不了解注意力机制你肯定会落后时代的发展。

而介绍Transformer比较好的文章可以参考以下两篇文章：一个是Jay Alammar可视化地介绍Transformer的博客文章The Illustrated Transformer（July注，其中文题库版见：https://www.julyedu.com/question/big/kp_id/30/ques_id/2912），非常容易理解整个机制，建议先从这篇看起；然后可以参考哈佛大学NLP研究组写的“The Annotated Transformer. ”：http://nlp.seas.harvard.edu/2018/04/03/attention.html，代码原理双管齐下，讲得非常清楚。我相信上面两个文章足以让你了解Transformer了，所以这里不展开介绍。

其次，我的判断是Transformer在未来会逐渐替代掉RNN成为主流的NLP工具，RNN一直受困于其并行计算能力，这是因为它本身结构的序列性依赖导致的，尽管很多人在试图通过修正RNN结构来修正这一点，但是我不看好这种模式，因为给马车换轮胎不如把它升级到汽车，这个道理很好懂，更何况目前汽车的雏形已经出现了，干嘛还要执着在换轮胎这个事情呢？是吧？

再说CNN，CNN在NLP里一直没有形成主流，CNN的最大优点是易于做并行计算，所以速度快，但是在捕获NLP的序列关系尤其是长距离特征方面天然有缺陷，不是做不到而是做不好，目前也有很多改进模型，但是特别成功的不多。综合各方面情况，很明显Transformer同时具备并行性好，又适合捕获长距离特征，没有理由不在赛跑比赛中跑不过RNN和CNN。

好了，题外话结束，我们再回到主题，接着说GPT。上面讲的是GPT如何进行第一阶段的预训练，那么假设预训练好了网络模型，后面下游任务怎么用？它有自己的个性，和ELMO的方式大有不同。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156078381376885.png'/>

上图展示了GPT在第二阶段如何使用。

首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的。
然后，在做下游任务的时候，利用第一步预训练好的参数初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了，这是个非常好的事情。
再次，你可以用手头的任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决手头的问题。就是这样。

看到了么？这有没有让你想起最开始提到的图像领域如何做预训练的过程（请参考上图那句非常容易暴露年龄的歌词）？对，这跟那个模式是一模一样的。这里引入了一个新问题：对于NLP各种花样的不同任务，怎么改造才能靠近GPT的网络结构呢？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715472108350789.jpg'/>

GPT论文给了一个改造施工图如上，其实也很简单：对于分类问题，不用怎么动，加上一个起始和终结符号即可；对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可；对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154730583116276.jpg'/>

GPT的效果是非常令人惊艳的，在12个任务里，9个达到了最好的效果，有些任务性能提升非常明显。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154732491204502.jpg'/>

那么站在现在的时间节点看，GPT有什么值得改进的地方呢？其实最主要的就是那个单向语言模型，如果改造成双向的语言模型任务估计也没有Bert太多事了。当然，即使如此GPT也是非常非常好的一个工作，跟Bert比，其作者炒作能力亟待提升。

Bert的诞生
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154737146796255.jpg'/>

我们经过跋山涉水，终于到了目的地Bert模型了。Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型，当然另外一点是语言模型的数据规模要比GPT大。所以这里Bert的预训练过程不必多讲了。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156085031331286.png'/>

第二阶段，Fine-Tuning阶段，这个阶段的做法和GPT是一样的。当然，它也面临着下游任务网络结构改造的问题，在改造任务方面Bert和GPT有些不同，下面简单介绍一下。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715474627832825.jpg'/>

在介绍Bert如何改造下游任务之前，先大致说下NLP的几类问题，说这个是为了强调Bert的普适性有多强。通常而言，绝大部分NLP问题可以归入上图所示的四类任务中：
一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。
第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。
第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；
第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156088092600404.png'/>

对于种类如此繁多而且各具特点的下游NLP任务，Bert如何改造输入输出部分使得大部分NLP任务都可以使用Bert预训练好的模型参数呢？
上图给出示例，对于句子关系类任务，很简单，和GPT类似，加上一个起始和终结符号，句子之间加个分隔符即可。
对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。
对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；
对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。

从这里可以看出，上面列出的NLP四大任务里面，除了生成类任务外，Bert其它都覆盖到了，而且改造起来很简单直观。尽管Bert论文没有提，但是稍微动动脑子就可以想到，其实对于机器翻译或者文本摘要，聊天机器人这种生成式任务，同样可以稍作改造即可引入Bert的预训练成果。只需要附着在S2S结构上，encoder部分是个深度Transformer结构，decoder部分也是个深度Transformer结构。根据任务选择不同的预训练数据初始化encoder和decoder即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个Transformer结构上加装隐层产生输出也是可以的。

不论如何，从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154759012126361.jpg'/>

Bert采用这种两阶段方式解决各种NLP任务效果如何？在11个各种类型的NLP任务中达到目前最好的效果，某些任务性能有极大的提升。一个新模型好不好，效果才是王道。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156091744887247.png'/>

到这里我们可以再梳理下几个模型之间的演进关系。从上图可见，Bert其实和ELMO及GPT存在千丝万缕的关系，
比如如果我们把GPT预训练阶段换成双向语言模型，那么就得到了Bert；
而如果我们把ELMO的特征抽取器换成Transformer，那么我们也会得到Bert。

所以你可以看出：Bert最关键两点，一点是特征抽取器采用Transformer；第二点是预训练的时候采用双向语言模型。那么新问题来了：对于Transformer来说，怎么才能在这个结构上做双向语言模型任务呢？

乍一看上去好像不太好搞。我觉得吧，其实有一种很直观的思路，怎么办？看看ELMO的网络结构图，只需要把两个LSTM替换成两个Transformer，一个负责正向，一个负责反向特征提取，其实应该就可以。当然这是我自己的改造，Bert没这么做。

那么Bert是怎么做的呢？我们前面不是提过Word2Vec吗？我前面肯定不是漫无目的地提到它，提它是为了在这里引出那个CBOW训练方法，所谓写作时候埋伏笔的“草蛇灰线，伏脉千里”，大概就是这个意思吧？

前面提到了CBOW方法，它的核心思想是：在做语言模型任务的时候，我把要预测的单词抠掉，然后根据它的上文Context-Before和下文Context-after去预测单词。其实Bert怎么做的？Bert就是这么做的。从这里可以看到方法间的继承关系。当然Bert作者没提Word2Vec及CBOW方法，这是我的判断，Bert作者说是受到完形填空任务的启发，这也很可能，但是我觉得他们要是没想到过CBOW估计是不太可能的。

从这里可以看出，在文章开始我说过Bert在模型方面其实没有太大创新，更像一个最近几年NLP重要技术的集大成者，原因在于此，当然我不确定你怎么看，是否认同这种看法，而且我也不关心你怎么看。其实Bert本身的效果好和普适性强才是最大的亮点。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156095587160714.png'/>

那么Bert本身在模型和方法角度有什么创新呢？就是论文中指出的Masked 语言模型和Next Sentence Prediction。而Masked语言模型上面讲了，本质思想其实是CBOW，但是细节方面有改进。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154773536828846.jpg'/>

Masked双向语言模型向上图展示这么做：随机选择语料中15%的单词，把它抠掉，也就是用[Mask]掩码代替原始单词，然后要求模型去正确预测被抠掉的单词。但是这里有个问题：训练过程大量看到[mask]标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对[mask]这个标记的，但是实际使用又见不到这个标记，这自然会有问题。

为了避免这个问题，Bert改造了一下，15%的被上天选中要执行[mask]替身这项光荣任务的单词中，只有80%真正被替换成[mask]标记，10%被狸猫换太子随机替换成另外一个单词，10%情况这个单词还待在原地不做改动。这就是Masked双向语音模型的具体做法。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154778357827261.jpg'/>

至于说“Next Sentence Prediction”，指的是做语言模型预训练的时候，分两种情况选择两个句子，一种是选择语料中真正顺序相连的两个句子；另外一种是第二个句子从语料库中抛色子，随机选择一个拼到第一个句子后面。我们要求模型除了做上述的Masked语言模型任务外，附带再做个句子关系预测，判断第二个句子是不是真的是第一个句子的后续句子。

之所以这么做，是考虑到很多NLP任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。所以可以看到，它的预训练是个多任务过程。这也是Bert的一个创新。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154781152753624.jpg'/>

上面这个图给出了一个我们此前利用微博数据和开源的Bert做预训练时随机抽出的一个中文训练实例，从中可以体会下上面讲的masked语言模型和下句预测任务。训练数据就长这种样子。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154783471696873.jpg'/>

顺带讲解下Bert的输入部分，也算是有些特色。它的输入部分是个线性序列，两个句子通过分隔符分割，最前面和最后增加两个标识符号。每个单词有三个embedding：
位置信息embedding，这是因为NLP中单词顺序是很重要的特征，需要在这里对位置信息进行编码；
单词embedding,这个就是我们之前一直提到的单词embedding；
第三个是句子embedding，因为前面提到训练数据都是由两个句子构成的，那么每个句子有个句子整体的embedding项对应给每个单词。

把单词对应的三个embedding叠加，就形成了Bert的输入。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157157778492670494.png'/>

至于Bert在预训练的输出部分如何组织，可以参考上图的注释。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154788099931636.jpg'/>

我们说过Bert效果特别好，那么到底是什么因素起作用呢？如上图所示，对比试验可以证明，跟GPT相比，双向语言模型起到了最主要的作用，对于那些需要看到下文的任务来说尤其如此。而预测下个句子来说对整体性能来说影响不算太大，跟具体任务关联度比较高。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154789784307560.jpg'/>

最后，我讲讲我对Bert的评价和看法，我觉得Bert是NLP里里程碑式的工作，对于后面NLP的研究和工业应用会产生长久的影响，这点毫无疑问。但是从上文介绍也可以看出，从模型或者方法角度看，Bert借鉴了ELMO，GPT及CBOW，主要提出了Masked 语言模型及Next Sentence Prediction，但是这里Next Sentence Prediction基本不影响大局，而Masked LM明显借鉴了CBOW的思想。所以说Bert的模型没什么大的创新，更像最近几年NLP重要进展的集大成者，这点如果你看懂了上文估计也没有太大异议，如果你有大的异议，杠精这个大帽子我随时准备戴给你。

如果归纳一下这些进展就是：首先是两阶段模型，第一阶段双向语言模型预训练，这里注意要用双向而不是单向，第二阶段采用具体任务Fine-tuning或者做特征集成；第二是特征抽取要用Transformer作为特征提取器而不是RNN或者CNN；第三，双向语言模型可以采取CBOW的方法去做（当然我觉得这个是个细节问题，不算太关键，前两个因素比较关键）。

Bert最大的亮点在于效果好及普适性强，几乎所有NLP任务都可以套用Bert这种两阶段解决思路，而且效果应该会有明显提升。可以预见的是，未来一段时间在NLP应用领域，Transformer将占据主导地位，而且这种两阶段预训练方法也会主导各种应用。

另外，我们应该弄清楚预训练这个过程本质上是在做什么事情，本质上预训练是通过设计好一个网络结构来做语言模型任务，然后把大量甚至是无穷尽的无标注的自然语言文本利用起来，预训练任务把大量语言学知识抽取出来编码到网络结构中，当手头任务带有标注信息的数据有限时，这些先验的语言学特征当然会对手头任务有极大的特征补充作用，因为当数据有限的时候，很多语言学现象是覆盖不到的，泛化能力就弱，集成尽量通用的语言学知识自然会加强模型的泛化能力。

如何引入先验的语言学知识其实一直是NLP尤其是深度学习场景下的NLP的主要目标之一，不过一直没有太好的解决办法，而ELMO/GPT/Bert的这种两阶段模式看起来无疑是解决这个问题自然又简洁的方法，这也是这些方法的主要价值所在。

对于当前NLP的发展方向，我个人觉得有两点非常重要，一个是需要更强的特征抽取器，目前看Transformer会逐渐担当大任，但是肯定还是不够强的，需要发展更强的特征抽取器；
第二个就是如何优雅地引入大量无监督数据中包含的语言学知识，注意我这里强调地是优雅，而不是引入，此前相当多的工作试图做各种语言学知识的嫁接或者引入，但是很多方法看着让人牙疼，就是我说的不优雅。目前看预训练这种两阶段方法还是很有效的，也非常简洁，当然后面肯定还会有更好的模型出现。

完了，这就是自然语言模型预训练的发展史。

PS，July注：上文PPT下载地址：http://ccl.pku.edu.cn/doubtfire/NLP/Deep_Learning/%E4%BB%8EWord%20Embedding%E5%88%B0Bert%20ppt%20%E5%BC%A0%E4%BF%8A%E6%9E%97.pdf

## 如何理解Seq2Seq Attention模型：图解Seq2Seq Attention
本题解析来源：https://zhuanlan.zhihu.com/p/40920384 ，和https://blog.csdn.net/Irving_zhang/article/details/78889364

seq2seq是一个Encoder–Decoder结构的网络，它的输入是一个序列，输出也是一个序列，Encoder中将一个可变长度的信号序列变为固定长度的向量表达，Decoder将这个固定长度的向量变成可变长度的目标的信号序列。

首先，我们使用x={x1，x2，…，xn}代表输入的语句，y={y1,y2,…,yn}代表输出的语句，yt代表当前输出词。在理解seq2seq的过程中，我们要牢记我们的目标是：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820189856707261.jpg'/>
即输出的yt不仅依赖之前的输出{y1,y2,…,yt−1}，还依赖输入语句x，模型再怎么变化都是在上述公式的约束之下。

seq2seq最初模型
最早由bengio等人发表在computerscience上的论文：LearningPhraseRepresentationsusingRNNEncoder–Decoder 
forStatisticalMachineTranslation。

对于RNN来说，x={x1，x2，…，xt}代表输入，在每个时间步t，RNN的隐藏状态ht由下述公式更新：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820210449024097.png'/>
其中，f代表一个非线性函数。这时ht就是一个rnn_size的隐含状态。然后需要通过一个矩阵W将其转成一个symbol_size的输出，并通过softmax函数将其转化为概率，然后筛选出概率最大的symbol为输出symbol。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820217792527635.jpg'/>

以上是rnn的基本原理，接下来介绍论文中的seq2seq模型： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820221847717693.jpg'/>
模型包括encoder和decoder两个部分。
首先在encoder部分，将输入传到encoder部分，得到最后一个时间步长t的隐藏状态C，这就是RNNcell的基本功能。

其次是decoder部分，从上述模型的箭头中可以看出，decoder的隐藏状态ht就由ht−1，yt−1和C三部分构成。即： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820259193663983.jpg'/>
由此我们的到了decoder的隐藏状态，那么最后的输出yt从图中也可以看得出来由三部分得到，yt从图中也可以看得出来由三部分得到，ht−1，yt−1和C，即：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820266236752971.jpg'/>
到现在为止，我们就实现了上文最开始的目标。

seq2seq的改进模型
改进模型介绍2014年发表的论文SequencetoSequenceLearningwithNeuralNetworks。模型图： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820278511576601.jpg'/>
可以看到，该模型和第一个模型主要的区别在于从输入到输出有一条完整的流：ABC为encoder的输入，WXYZ为decoder的输入。将encoder最后得到的隐藏层的状态ht输入到decoder的第一个cell里，就不用像第一个模型一样，每一个decoder的cell都需要ht，因此从整体上看，从输入到输出像是一条“线性的数据流”。

本文的论文也提出来，ABC翻译为XYZ，将encoder的input变为“CBA”效果更好。即A和X的距离更近了，更有利于seq2seq模型的交流。

具体来说，encoder的过程如下图。这和我们之前的encoder都一样。 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820293919476479.jpg'/>
不同的是decoder的阶段：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820297935765869.jpg'/>

得到了encoderrepresention，即encoder的最后一个时间步长的隐层ht以后，输入到decoder的第一个cell里，然后通过一个激活函数和softmax层，得到候选的symbols，筛选出概率最大的symbol，然后作为下一个时间步长的输入，传到cell中。这样，我们就得到了我们最开始的目标。

seq2seq with attention
我们前面提到，距离decoder的第一个cell越近的输入单词，对decoder的影响越大。但这并不符合常理，这时就提出了attention机制，对于输出的每一个cell，都检测输入的sequence里每个单词的重要性，即论文Neural Machine Translation by Jointly Learning to Align and Translate。attention在NMT基于seq2seq的改进模型再进行改进。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820423226415848.jpg'/>
上图中，encoder和decoder都发生了变化。首先说encoder，使用了双向RNN，因为希望不仅能得到前向的词的顺序，还希望能够得到反向的词的顺序。使用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820447266351394.png'/>代表hj前向的隐层状态，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820448261022537.png'/>代表hj的反向隐层状态，hj的最终状态为将两者连接(concat)起来，即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820449073286618.png'/>。

再说decoder。我们再来回顾一下我们最开始的目标公式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820189856707261.jpg'/>
对于加入attention机制的seq2seq，每一个输出为公式如下。即对于时间步i的输出yi，由时间步i的隐藏状态si，由attention计算得到的输入内容ci和上一个输出yi-1得到。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820454744511338.jpg'/>

其中si是对于时间步i的隐藏状态，由下述公式计算。即对于时间步i的隐藏状态，由时间步i-1的隐藏状态si-1，由attention计算得到的输入内容ci和上一个输出yi-1得到。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820459313937216.jpg'/>
通过以上公式可以看出，加入attention的seq2seq比之前的seq2seq多了一个输入内容向量ci，那么这个ci是怎么得来的呢？和输入内容以及attention有什么关系呢？我们接着看下述公式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820464935278728.jpg'/>
即，对于decoder的时间步长i的隐藏状态si，ci等于Tx个输入向量[1,Tx]与其权重<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820483584567741.png'/>相乘求和。这个权重<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820483584567741.png'/>由这个公式得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820476762319690.jpg'/>
其中，eij由下面公式得到
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820479476665219.jpg'/>
总结一下，对于时间步i的隐藏状态si，可以通过求时间步i-1的隐藏状态si-1、输入内容的编码向量ci和上一个输出yi-1得到。输入内容编码ci是新加入的内容，可以通过计算输入句子中每个单词的权重，然后加权求和得到ci。
直观解释这个权重：对于decoder的si和encoder的hj的权重<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820483584567741.png'/>，就是上一个时间步长的隐藏状态si-1与encoder的hj通过非线性函数得到的。这样就把输入内容加入到解码的过程中，这和我们人类翻译的过程也是类似的，即对于当前输出的词，每一个输入给与的注意力是不一样的。

上面一堆公式是不是看懵了。好了别管了，接下来开始刷图吧。
大框架
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819613655535774.jpg'/>
想象一下翻译任务，input是一段英文，output是一段中文。
公式（直接跳过看图最佳）
输入： <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819666441379406.svg'/>
输出： <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415781966719389889.svg'/>
(1)  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819667877685337.svg'/>,Encoder方面接受的是每一个单词wordembedding，和上一个时间点的hiddenstate。输出的是这个时间点的hiddenstate。
(2)  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819668613974666.svg'/>，Decoder方面接受的是目标句子里单词的wordembedding，和上一个时间点的hiddenstate。
(3)  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819669118649057.svg'/>,contextvector是一个对于encoder输出的hiddenstates的一个加权平均。
(4) <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819670368675621.svg'/>,每一个encoder的hiddenstates对应的权重。
(5)  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819671335923896.svg'/>,通过decoder的hiddenstates加上encoder的hiddenstates来计算一个分数，用于计算权重(4)
(6)<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819672177541328.svg'/> ,将contextvector和decoder的hiddenstates串起来。
(7) <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819672968590209.svg'/>，计算最后的输出概率。

详细图
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819617176492580.jpg'/>
左侧为Encoder+输入，右侧为Decoder+输出。中间为Attention。

(1)<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819677645031312.svg'/> ,Encoder方面接受的是每一个单词wordembedding，和上一个时间点的hiddenstate。输出的是这个时间点的hiddenstate。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819621681817468.jpg'/>
从左边Encoder开始，输入转换为wordembedding,进入LSTM。LSTM会在每一个时间点上输出hiddenstates。如图中的h1,h2,...,h8。

(2)<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819678688484058.svg'/>，Decoder方面接受的是目标句子里单词的wordembedding，和上一个时间点的hiddenstate。
接下来进入右侧Decoder，输入为(1)句首符号，原始contextvector(为0)，以及从encoder最后一个hiddenstate:h8。LSTM的是输出是一个hiddenstate。（当然还有cellstate，这里没用到，不提。）
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819623777338747.jpg'/>

(3)  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819679648091265.svg'/>,contextvector是一个对于encoder输出的hiddenstates的一个加权平均。
(4) <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819680210877215.svg'/> ,每一个encoder的hiddenstates对应的权重。
(5) <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819680637877193.svg'/> ,通过decoder的hiddenstates加上encoder的hiddenstates来计算一个分数，用于计算权重(4)
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819627541167161.jpg'/>
Decoder的hiddenstate与Encoder所有的hiddenstates作为输入，放入Attention模块开始计算一个contextvector。之后会介绍attention的计算方法。

下一个时间点
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819628860823770.jpg'/>
来到时间点2，之前的contextvector可以作为输入和目标的单词串起来作为lstm的输入。之后又回到一个hiddnstate。以此循环。

(6)<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819683848695318.svg'/> ,将contextvector和decoder的hiddenstates串起来。
(7) <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819684211239337.svg'/>，计算最后的输出概率。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819631334788367.jpg'/>
另一方面，contextvector和decoder的hiddenstate合起来通过一系列非线性转换以及softmax最后计算出概率。

在luong中提到了三种score的计算方法。这里图解前两种：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819633397841742.jpg'/>
Attentionscorefunction: dot

输入是encoder的所有hiddenstatesH:大小为(hiddim,sequencelength)。decoder在一个时间点上的hiddenstate，s：大小为（hiddim,1）。
第一步：旋转H为（sequencelength,hiddim)与s做点乘得到一个大小为(sequencelength,1)的分数。
第二步：对分数做softmax得到一个合为1的权重。
第三步：将H与第二步得到的权重做点乘得到一个大小为(hiddim,1)的contextvector。
Attentionscorefunction: general
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819635740854076.jpg'/>
输入是encoder的所有hiddenstatesH:大小为(hiddim1,sequencelength)。decoder在一个时间点上的hiddenstate，s：大小为（hiddim2,1）。此处两个hiddenstate的纬度并不一样。
第一步：旋转H为（sequencelength,hiddim1)与 Wa[大小为hiddim1,hiddim2)] 做点乘，再和s做点乘得到一个大小为(sequencelength,1)的分数。
第二步：对分数做softmax得到一个合为1的权重。
第三步：将H与第二步得到的权重做点乘得到一个大小为(hiddim,1)的contextvector。

完结
看懂一个模型的最好办法就是在心里想一遍从输入到模型到输出每一个步骤里，tensor是如何流动的。希望对大家有帮助～点个赞吧

后记
写完这篇看图解说快有一年了，看到评论里的一些关于细节的问题我想在这里统一回复：本文主要介绍Attention机制的一个思路，其实就是一个对于hiddenstates的weightedaverage。至于怎么去用c，s，怎么去算，方法五花八门。如果你大概念不理解，你可以看看这篇文章。如果你对细节有疑问，正确的方法还是去看开源的代码。毕竟最后还是全靠实战。

# NLP
## 了解Google最新的模型bert么？
BERT (Bidirectional Encoder Representations from Transformers)

10月11日，Google AI Language 发布了论文
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding
提出的 BERT 模型在 11 个 NLP 任务上的表现刷新了记录，包括问答 Question Answering (SQuAD v1.1)，推理 Natural Language Inference (MNLI) 等：

GLUE ：General Language Understanding Evaluation
MNLI ：Multi-Genre Natural Language Inference
SQuAD v1.1 ：The Standford Question Answering Dataset
QQP ： Quora Question Pairs 
QNLI ： Question Natural Language Inference

SST-2 ：The Stanford Sentiment Treebank
CoLA ：The Corpus of Linguistic Acceptability 
STS-B ：The Semantic Textual Similarity Benchmark
MRPC ：Microsoft Research Paraphrase Corpus
RTE ：Recognizing Textual Entailment 
WNLI ：Winograd NLI
SWAG ：The Situations With Adversarial Generations

让我们先来看一下 BERT 在 Stanford Question Answering Dataset (SQuAD) 上面的排行榜吧：
https://rajpurkar.github.io/SQuAD-explorer/
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155721663860330387.png'/>
BERT 可以用来干什么？

BERT 可以用于问答系统，情感分析，垃圾邮件过滤，命名实体识别，文档聚类等任务中，作为这些任务的基础设施即语言模型，

BERT 的代码也已经开源：
https://github.com/google-research/bert
我们可以对其进行微调，将它应用于我们的目标任务中，BERT 的微调训练也是快而且简单的。

例如在 NER 问题上，BERT 语言模型已经经过 100 多种语言的预训练，这个是 top 100 语言的列表：
https://github.com/google-research/bert/blob/master/multilingual.md

只要在这 100 种语言中，如果有 NER 数据，就可以很快地训练 NER。

BERT 原理简述
BERT 的创新点在于它将双向 Transformer 用于语言模型，
之前的模型是从左向右输入一个文本序列，或者将 left-to-right 和 right-to-left 的训练结合起来。

实验的结果表明，双向训练的语言模型对语境的理解会比单向的语言模型更深刻，论文中介绍了一种新技术叫做 Masked LM（MLM），在这个技术出现之前是无法进行双向语言模型训练的。

BERT 利用了 Transformer 的 encoder 部分。
Transformer 是一种注意力机制，可以学习文本中单词之间的上下文关系的。
Transformer 的原型包括两个独立的机制，一个 encoder 负责接收文本作为输入，一个 decoder 负责预测任务的结果。
BERT 的目标是生成语言模型，所以只需要 encoder 机制。

Transformer 的 encoder 是一次性读取整个文本序列，而不是从左到右或从右到左地按顺序读取，
这个特征使得模型能够基于单词的两侧学习，相当于是一个双向的功能。

下图是 Transformer 的 encoder 部分，输入是一个 token 序列，先对其进行 embedding 称为向量，然后输入给神经网络，输出是大小为 H 的向量序列，每个向量对应着具有相同索引的 token。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155721672283337082.png'/>
图片 by Rani Horev

当我们在训练语言模型时，有一个挑战就是要定义一个预测目标，很多模型在一个序列中预测下一个单词，
“The child came home from ___”
双向的方法在这样的任务中是有限制的，为了克服这个问题，BERT 使用两个策略:

1. Masked LM (MLM)
在将单词序列输入给 BERT 之前，每个序列中有 15％ 的单词被 [MASK] token 替换。 然后模型尝试基于序列中其他未被 mask 的单词的上下文来预测被掩盖的原单词。

这样就需要：
i)在 encoder 的输出上添加一个分类层
ii)用嵌入矩阵乘以输出向量，将其转换为词汇的维度
iii)用 softmax 计算词汇表中每个单词的概率

BERT 的损失函数只考虑了 mask 的预测值，忽略了没有掩蔽的字的预测。这样的话，模型要比单向模型收敛得慢，不过结果的情境意识增加了。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155721677852106261.png'/>
图片 by Rani Horev

2. Next Sentence Prediction (NSP)
在 BERT 的训练过程中，模型接收成对的句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子。
在训练期间，50％ 的输入对在原始文档中是前后关系，另外 50％ 中是从语料库中随机组成的，并且是与第一句断开的。

为了帮助模型区分开训练中的两个句子，输入在进入模型之前要按以下方式进行处理：

在第一个句子的开头插入 [CLS] 标记，在每个句子的末尾插入 [SEP] 标记。
将表示句子 A 或句子 B 的一个句子 embedding 添加到每个 token 上。
给每个 token 添加一个位置 embedding，来表示它在序列中的位置。
为了预测第二个句子是否是第一个句子的后续句子，用下面几个步骤来预测：

整个输入序列输入给 Transformer 模型
用一个简单的分类层将 [CLS] 标记的输出变换为 2×1 形状的向量
用 softmax 计算 IsNextSequence 的概率
在训练 BERT 模型时，Masked LM 和 Next Sentence Prediction 是一起训练的，目标就是要最小化两种策略的组合损失函数。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155721681290293736.png'/>
如何使用 BERT?
BERT 可以用于各种NLP任务，只需在核心模型中添加一个层，例如：
a)在分类任务中，例如情感分析等，只需要在 Transformer 的输出之上加一个分类层
b)在问答任务（例如SQUAD v1.1）中，问答系统需要接收有关文本序列的 question，并且需要在序列中标记 answer。 可以使用 BERT 学习两个标记 answer 开始和结尾的向量来训练Q＆A模型。
c)在命名实体识别（NER）中，系统需要接收文本序列，标记文本中的各种类型的实体（人员，组织，日期等）。 可以用 BERT 将每个 token 的输出向量送到预测 NER 标签的分类层。

在 fine-tuning 中，大多数超参数可以保持与 BERT 相同，在论文中还给出了需要调整的超参数的具体指导（第3.5节）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155721683728608771.png'/>
学习资料：
https://arxiv.org/pdf/1810.04805.pdf
https://www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/
https://medium.com/syncedreview/best-nlp-model-ever-google-bert-sets-new-standards-in-11-language-tasks-4a2a189bc155
## 了解文本嵌入么？
什么是NLP？
自然语言处理（NLP） 是计算机科学，人工智能和语言学的交叉领域。目标是让计算机处理或“理解”自然语言，以执行语言翻译和问题回答等任务。

随着语音接口和聊天机器人的兴起，NLP正在成为信息时代最重要的技术之一，同时它也是人工智能的关键部分。充分理解和表达语言的含义是一个非常困难的目标。为什么？因为人类的语言很特别。

人类语言有什么特别之处？
1.人类语言是专门为传达说话人的意图而构建的系统。这不仅仅是一个环境信号，更是一个有意识的交流。
2.人类语言大多是离散/符号的/分类的信号系统，大概是因为信号可靠性更高。
3.一种语言的分类符号可以用几种方式编码为通信信号：声音，手势，写作，图像等。人类语言只是其中的一种。
4.人类语言是不明确的（与编程和其他正式语言不同）。 因此，在表达、学习和使用语言/情境/情境/文字/视觉知识对人类语言方面存在高度复杂性。

NLP应用到哪里？
从NLP研究领域衍生出了一批快速增长的应用程序。以下是其中几个：
1.拼写检查，关键字搜索，查找同义词；
2.从网站提取信息，例如：产品价格，日期，地点，人员或公司名称；
3.分类：长文档的积极/消极情绪；
4.机器翻译；
5.口语对话系统；
6.复杂的问答系统；

事实上，这些应用程序已经在现实中大量使用，从搜索到在线广告匹配 ; 从自动/辅助翻译到营销或财务/交易的情绪分析 ; 从语音识别到chatbots /对话代理（自动化客户支持，控制设备，订购商品）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946313784534369.jpg'/>

深度学习
大部分NLP技术都是由深度学习提供技术支持。近几年，深度学习才开始发挥作用，主要是因为：
·大量的训练数据；
·更快的机器和多核CPU / GPU；
·性能高的新模型和算法：有效的端到端联合系统学习、有效的使用上下文和任务间转换的学习方法，以及正则化优化方法。

在深度学习中，表示学习试图自动学习来自原始输入的良好特征或表示。而在机器学习中手动设计的特征通常过多且不完整，需要花费很长时间进行设计和验证。而且深度学习提供了一个非常灵活、通用且可学习的框架，用于呈现视觉和语言信息的世界。

最初，它在语音识别和计算机视觉等领域取得突破。最近，深度学习方法在许多不同的NLP任务中表现出了非常高的性能。这些模型通常可以通过单一的端到端模型进行训练，并且不需要传统的，特定于任务的特征工程。

下面简单介绍下文本嵌入（Text Embeddings）。

在传统的NLP中，我们将单词视为离散符号，然后可以用one-hot向量表示。向量的维度是整个词汇表中单词的数量。单词作为离散符号的问题在于，对于one-hot向量来说，没有自然的相似性概念。

因此，另一种方法是学习在向量本身中编码相似性。核心思想是一个词的含义是由经常出现在其旁边的单词给出的。

文本嵌入是字符串的实值向量表示。我们为每个单词建立一个密集的向量，选择它以便类似于类似上下文中出现的单词的向量。对于大多数NLP任务而言，词嵌入被认为是一个很好的起点。它们允许深度学习在较小的数据集上也是有效的，因为它们通常是深度学习体系的第一批输入，也是NLP中最流行的迁移学习方式。

在词嵌入中最流行的应该是Word2vec，它是由谷歌（Mikolov）开发的模型，另外一个是由斯坦福大学（彭宁顿，Socher和曼宁）开发的GloVe。

接着我们重点介绍这两种模型：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946332328697749.png'/>

在Word2vec中，我们有一个庞大的文本语料库，其中固定词汇表中的每个词都由一个向量表示。然后，我们通过文本中的每个位置t，其中有一个中心词c和上下文词o。

接下来，我们使用字向量的相似性Ç和Ò计算的概率ø给出Ç（或反之亦然）。我们不断调整单词向量来最大化这个概率。为了有效地训练Word2vec，我们可以从数据集中去除无意义的单词。这有助于提高模型的准确性。

Word2vec有两个变体值得一提：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946339451861219.png'/>

1.Skip-Gram：我们考虑一个包含k个连续项的上下文窗口。
然后，我们跳过其中一个单词，尝试学习一个神经网络，该网络可以获得除跳过的所有术语外的所有术语，并预测跳过的术语。
因此，如果两个单词在大语料库中反复共享相似的上下文，那么这些术语的嵌入向量将具有相似的向量。

2.Continuous Bag of Words：我们在一个大的语料库中获取大量的句子，每当我们看到一个词，我们就会联想到周围的词。
然后，我们将上下文单词输入到神经网络，并预测该上下文中心的单词。
当我们有数千个这样的上下文单词和中心单词时，我们就有了一个神经网络数据集的实例。我们训练神经网络，最后编码的隐藏层输出表示一个特定的词嵌入。

当我们通过大量的句子进行训练时，类似上下文中的单词会得到相似的向量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946350921665397.jpg'/>

对Skip-Gram和CBOW的一个吐槽就是它们都是基于窗口的模型，这意味着语料库的共现统计不能被有效使用，导致次优的嵌入（suboptimal embeddings）。

GloVe模型旨在通过捕捉一个字与整个观测语料库的结构嵌入的含义来解决这个问题。为此，该模型训练单词的全局共现次数，并通过最小化最小二乘误差来充分利用统计量，从而产生具有有意义子结构的单词向量空间。这样的做法足以保留单词与向量距离的相似性。

除了这两种文本嵌入外，还有许多最近开发的高级模型，包括FastText，Poincare嵌入，sense2vec，Skip-Thought，Adaptive Skip-Gram，我强烈建议你学习一下。

## 了解机器翻译中的NLP技术么？
机器翻译是语言理解的经典测试。它由语言分析和语言生成组成。大型机器翻译系统具有巨大的商业用途，给你一些值得注意的例子：

·谷歌翻译每天翻译1000亿字；
·Facebook使用机器翻译自动翻译帖子和评论中的文字，以打破语言障碍，让世界各地的人们相互交流；
·阿里巴巴使用机器翻译技术来实现跨境贸易，连接世界各地的买家和卖家；
·微软为Android、iOS和Amazon Fire上的最终用户和开发人员提供基于人工智能的翻译，无论他们是否可以访问互联网。

在传统的机器翻译系统中，我们必须使用平行语料库：一组文本，每个文本都被翻译成一种或多种不同于原文的其他语言。

例如，给定源语言f（例如法语）和目标语言e（例如英语），我们需要建立多个统计模型，包括使用贝叶斯规则的概率公式，训练的翻译模型p（f | e）平行语料库和语言模型p（e）在纯英文语料库上训练。这种方法跳过了数百个重要细节，需要大量的手工特征工程，整体而言它是一个非常复杂的系统。

神经机器翻译是通过一个称为递归神经网络（RNN）的大型人工神经网络对整个过程进行建模的方法。RNN是一个有状态的神经网络，它通过时间连接过去。神经元的信息不仅来自前一层，而且来自更前一层的信息。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946362363352968.png'/>

标准的神经机器翻译是一种端到端神经网络，其中，源语句由称为编码器的RNN 编码，目标词使用另一个称为解码器。RNN编码器一次读取一个源语句，然后在最后隐藏状态汇总整个源句子。RNN解码器使用反向传播学习这个汇总并返回翻译后的版本。

神经机器翻译从2014年的一项边缘研究领域发展到2016年广泛采用的领先机器翻译方式，那么，使用神经机器翻译的最大成功是什么？
1.端到端训练：NMT中的所有参数同时被优化，以最大限度地减少网络输出的损耗性能。
2.分布式表示的优势：NMT更好地利用单词和短语的相似性。
3.更好地探索上下文：NMT可以使用更多的上下文——源文本和部分目标文本以此进行更准确地翻译。
4.更流利的文本生成：深度学习文本生成质量高于平行语料库。

RNN的一个大问题是梯度消失（或爆炸）问题，其中取决于所使用的激活函数，随着时间的推移信息会迅速丢失。

直观地说，这不会成为一个很大问题，因为这些只是权重而不是神经元状态，但是时间的权重实际上是存储过去的信息的地方，如果权重达到0或1,000,000的值，那么以前的状态将不会提供很多信息。

因此，RNNs在记忆序列中的前几个单词时会表现的很困难，并且只能根据最近的单词进行预测。

长期/短期记忆（LSTM）网络试图通过引入门和明确定义的存储器单元来对抗梯度消失/爆炸问题。每个神经元都有一个存储单元和三个门：输入、输出和忘记。这些门的功能是通过停止或允许信息流来保护信息。
    ①输入门决定了来自上一层的多少信息存储在单元中；
    ②输出层在另一端获取任务，并确定下一层有多少单元知道该单元的状态。
    ③忘记门的作用起初看起来很奇怪，但有时候忘记门是个不错的设计：如果它正在学习一本书并开始新的一章，那么网络可能需要忘记前一章中的一些字符。

已经证明LSTM能够学习复杂的序列，例如像莎士比亚的写作或者创作原始音乐。请注意，这些门中的每一个都对前一个神经元中的一个单元具有权重，因此它们通常需要更多资源才能运行。

LSTM目前非常流行，并且在机器翻译中被广泛使用。除此之外，它是大多数序列标签任务的默认模型，其中有大量的数据。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946375621557650.jpg'/>

门控重复单元（GRU）是在LSTM的基础上变形得来的，也是神经机器翻译的扩展。它拥有更少的门，并且连接方式略有不同：它不是输入、输出和忘记门组成的，而是具有更新门。这个更新门决定了从最后一个状态开始保留多少信息以及从上一个层开始输入多少信息。

复位（reset）门的功能与LSTM的忘记（forget）门非常相似，但位置稍有不同。他们总是发出它们完整的状态因为他们没有输出门。在大多数情况下，它们的功能与LSTM非常相似，最大的不同之处在于GRUs稍快并且更容易运行（但表现力稍差）。

在实践中，这些往往会互相抵消，因为你需要一个更大的网络来重新获得一些表示能力，这反过来又抵消了性能的优势。在一些情况下，GRU可以胜过LSTM。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946381391394604.png'/>

除了这三大体系结构之外，过去几年神经​​机器翻译系统还有进一步的改进。以下是最显着的发展：
    a)用神经网络进行序列学习的序列证明了LSTM在神经机器翻译中的有效性。它提出了序列学习的一种通用的端到端方法，对序列结构进行了最少的假设。该方法使用多层Long Short Term Memory（LSTM）将输入序列映射为固定维度的向量，然后使用另一个深度LSTM从向量解码目标序列。
    b)通过联合学习对齐和翻译的神经机器翻译引入了NLP中的注意机制（将在下一篇文章中介绍）。认识到使用固定长度矢量是提高NMT性能的瓶颈，作者建议通过允许模型自动（软）搜索与预测目标相关的源句子部分来扩展，而不必将这些部分明确地形成为一个固定的长度。
    c)用于神经机器翻译的循环编码器上的卷积利用附加的卷积层增强NMT中的标准RNN编码器，以在编码器输出中捕捉更广泛的上下文。
谷歌的神经机器翻译，它解决了准确性和部署方便性的问题。该模型由一个深度LSTM网络组成，该网络包含8个编码器和8个解码器层，使用残余连接以及从解码器网络到编码器的注意力连接。
    d)Facebook AI研究人员不使用递归神经网络，而是使用卷积神经网络序列对NMT中的学习任务进行排序。
## 了解情感分析中的NLP技术么？
人际交往不仅仅是文字和其明确的含义，而且它还是微妙且复杂的。即使在完全基于文本的对话中，你也可以根据单词选择和标点符号判断客户是否感到愤怒。你可以阅读产品在天猫平台的评论，并了解评论者是否喜欢或不喜欢它，即使他们从未直接说过。

为了使计算机真正理解人类每天的交流方式，他们需要理解的不仅仅是客观意义上的词语定义、而且他们需要了解我们的情绪。

情绪分析是通过较小元素的语义组成来解释较大文本单元（实体、描述性术语、事实、论据、故事）的含义的过程。

传统情感分析的方法是将句子视为一个词袋，并查阅“积极”和“消极”单词的策划列表，以确定该句子的情绪。这需要手工设计的特征来捕捉情绪，所有这是非常耗时和不可扩展的。

用于情感分析的现代深度学习方法可用于形态学、语法和逻辑语义，其中最有效的是递归神经网络。顾名思义，递归神经网络开发的主要假设递归是描述语言的自然方式。递归在消歧方面很有用，有助于某些任务引用特定的短语，并且对于使用语法树结构的任务非常有效。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728521912439982.png'/>
递归神经网络非常适合具有嵌套层次结构和内部递归结构的设置。语法的句法规则是高度递归的，因此，我们利用递归神经网络！

使用RNN对句子进行建模的另一个好处是，我们现在可以输入任意长度的句子，这对于在NLP中使用神经网络来说是一个巨大的难题，使用非常聪明的技巧使句子的输入向量具有相同的大小，尽管句子的长度不相等。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728522658917987.png'/>
标准RNN是一种递归神经网络的最基本的版本。它具有最大边距结构预测架构，可以在复杂的场景图像和句子中成功地运用这种结构。它用于为自然语言句子提供有竞争力的语法分析器比如说Penn Treebank。

作为参考，Penn Treebank是第一个大型树形数据集，由华尔街日报三年（WSJ）收集的24,799个故事组成，它广泛用于句法注释。此外，它优于语义场景分割、注释和分类的替代方法。

然而，标准RNN并不能捕获语法短语的完整语法。在语法上解开RNN，也被称为成分矢量语法（CVG），这个方法是解决这个问题的一个重大升级。它使用语法解开的递归神经网络来学习句法语义和组合向量表示。该模型能够像标准RNN一样快速地进行训练和实施。

另一个演变是Matrix-Vector RNN，它能够捕获更长短语的组成含义。该模型为解析树中的每个节点分配一个向量和一个矩阵：向量用于捕获成分的固有含义，而矩阵捕获它如何改变相邻单词或短语的含义。而且该矩阵向量RNN可以在命题逻辑和自然语言中学习运算符的含义。

该模型在三个不同的实验中获得过不错的表示：
· 预测副词-形容词对的细粒度情感分布；
· 对电影评论的情感标签进行分类；
· 使用它们之间的句法路径对名词之间的语义关系（例如因果关系）进行分类。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728523343039565.png'/>
迄今为止用于情感分析的最强大的RNN模型是递归神经张量网络，其在每个节点处具有神经网络的树结构。该模型可用于边界分割，以确定哪些词组是积极的，哪些是消极的。

在Sentiment Treebank上接受训练时，该模型在几个指标上的表现优于所有以前的方法。
## 了解问答系统中涉及的NLP技术么？
问答（QA）系统的想法是直接从文档、对话、在线搜索和其他地方提取信息，以满足用户的信息需求。QA系统不是让用户阅读整个文档，而是更喜欢简短而简洁的答案。如今，QA系统可以非常容易地与其他NLP系统结合使用，并且一些QA系统甚至超越了对文本文档的搜索，并且可以从图片集合中提取信息。

事实上，大多数NLP问题都可以被视为一个问题回答问题。范例很简单：我们发出查询指令，机器提供响应。通过阅读文档或一组指令，智能系统应该能够回答各种各样的问题。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728507426066624.png'/>
强大的深度学习架构（称为动态内存网络（DMN））已针对QA问题进行了专门开发和优化。给定输入序列（知识）和问题的训练集，它可以形成情节记忆，并使用它们来产生相关答案。该体系结构具有以下组件：

· 语义内存模块（类似于知识库）被用来创建从输入句子的嵌入字序列预先训练手套载体。
· 输入模块处理与问题有关的输入矢量称为事实。该模块使用门控循环单元实现，GRU使网络能够了解当前正在考虑的句子是否相关或与答案无关。

· 问题模块逐字处理疑问词，并且使用输出相同权重的GRU输入模块的向量。事实和问题都被编码为嵌入。
· 情景记忆模块接收从输入中提取和编码的嵌入事实和问题载体。这使用了一个受大脑海马体启发的想法，它可以检索由某些反应触发的时间状态，如景点或声音。
· 答案生成模块，通过适当的响应，情景记忆应该包含回答问题所需的所有信息。该模块使用另一个GRU，使用正确序列的交叉熵错误分类进行训练，然后可以将其转换回自然语言。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728508087672647.png'/>
DMN不仅在质量保证方面做得非常好，而且在情感分析和词性标注方面也优于其他架构。自开发以来，动态内存网络已经有了重大改进，进一步提高其在问答环境中的准确性，包括：
· 用于视觉和文本问题的动态存储网络问答应用基本上是将DMN应用于图像，其内存和输入模块已升级，以便能够回答视觉问题。该模型改进了许多基准Visual Question Answering数据集的现有技术水平，而不支持事实监督。

· 用于问题应答的动态Coattention网络解决了从对应于不正确答案的局部最大值恢复的问题。它首先融合了问题和文件的共同依赖表示，以便集中于两 者的相关部分。然后，动态指向解码器迭代潜在的答案跨度，该迭代过程使模型能够从对应于不正确答案的初始局部最大值中恢复。
## 了解文本摘要中的NLP技术么？
人类很难手动汇总大型文本文档。文本摘要是NLP为源文档创建简短、准确和流畅的摘要问题。随着推送通知和文章摘要获得越来越多的注意力，为长文本生成智能且准确摘要的任务每天都在增长。

通过首先计算整个文本文档的单词频率来自动汇总文本。

然后，存储和排序100个最常用的单词。然后根据它包含的高频词数对每个句子进行评分，更高频率的词，价值更大。

最后，根据它们在原始文本中的位置来获取和排序前X个句子。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728501064833759.png'/>
文本摘要有两种基本方法：提取和抽象。前者从原始文本中提取单词和单词短语以创建摘要。后者是学习内部语言表示以生成更像人类的摘要，解释原始文本的意图。

提取摘要的方法是通过选择子集来工作。这是通过从实际文章中提取短语或句子以形成摘要来完成的，LexRank和TextRank是众所周知的摘要总结，它们都使用了Google PageRank算法的变体。
· LexRank是一种无监督的基于图形的算法，它使用IDF修改的余弦作为两个句子之间的相似性度量。该相似度用作两个句子之间的图形边缘的权重。LexRank还采用了智能后处理步骤，确保为摘要选择的顶级句子彼此不太相似。
· TextRank是一种类似于LexRank的算法，具有一些增强功能，例如使用词形化而不是词干，结合词性标注和命名实体分辨率，从文章中提取关键短语，以及根据这些短语提取摘要句子。除了文章摘要外，TextRank还从文章中提取了有意义的关键短语。

抽象概括的模型属于深度学习。使用深度学习的文本摘要已经取得了一定的突破。以下是一些NLP领域最大公司最显着的公布结果：
· Facebook的神经注意是一种神经网络架构，它利用基于本地注意力的模型，能够根据输入句子生成摘要中的每个单词。
· Google Brain的Sequence-to-Sequence模型遵循编码器-解码器架构。编码器负责读取源文档并将其编码为内部表示，解码器是一种语言模型，负责使用源文档的编码表示在输出摘要中生成每个单词。
· IBM Watson使用类似的序列到序列模型，但具有注意力和双向递归神经网络功能。
## 了解注意力机制么？
神经网络中的注意力机制是基于人类的视觉注意机制。人类的视觉注意力虽然存在不同的模型，但它们都基本上归结为能够以“高分辨率”聚焦于图像的某个区域，同时以“低分辨率”感知周围的图像，然后随着时间的推移调整焦点。

想象一下，你正在阅读一篇完整的文章：不是按顺序浏览每个单词或字符，而是潜意识地关注一些信息密度最高的句子并过滤掉其余部分。你的注意力有效地以分层方式捕获上下文信息，这样就可以在减少开销的同时做出决策。

那为什么这很重要？诸如LSTM和GRU之类的模型依赖于读取完整的句子并将所有信息压缩为固定长度的矢量。这需要基于文本统计属性的复杂特征工程，用几个单词表示的数百个单词的句子肯定会导致信息丢失，翻译不足等。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728485442341045.png'/>
通过注意力机制，我们不再尝试将全文编码为固定长度的矢量。相反，我们允许解码器在输出生成的每个步骤处理源语句的不同部分。我们让模型根据输入句子以及它到目前为止产生的内容来学习要注意的内容。

根据上面从基于注意力的神经机器翻译的有效方法的图像，蓝色表示编码器，红色表示解码器，因此我们可以看到上下文向量将所有单元格的输出作为输入来计算每个单元格的源语言单词的概率分布。解码器想要生成单个字，通过利用该机制，解码器可以捕获全局信息而不是仅基于一个隐藏状态进行推断。

除了机器翻译之外，注意力模型还可以处理各种其他NLP任务。在Show，Attend和Tell：使用视觉注意生成神经图像标题，作者将注意力机制应用于生成图像描述的问题。他们使用卷积神经网络对图像进行编码，使用具有注意力机制的递归神经网络来生成描述。通过可视化注意力，他们可以在生成单词时解释模型正在查看的内容：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728486099757508.png'/>
在语法作为外语中，作者使用具有注意力机制的递归神经网络来生成句子解析的树。可视化的注意力矩阵可以深入了解网络如何生成这些树：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728486624127618.png'/>
在阅读和理解的教学机器中，作者使用回归神经网络来阅读文本，阅读问题，然后产生答案。通过可视化关注矩阵，它们可以在尝试查找问题答案时显示网络的外观：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728487292365440.png'/>
然而，注意力机制需要付出代价。我们需要计算输入和输出字的每个组合的注意力值。如果你有一个100字的输入序列并生成一个100字的输出序列，那将是10,000个注意力值。如果你进行字符级计算并处理由数百个令牌组成的序列，上述机制可能变得非常昂贵。

自然语言处理已经解决的障碍
值得注意的是，研究人员不得不处理各种障碍：算法的局限性、模型的可扩展性、对人类语言的模糊理解。好消息是，这个领域的发展似乎是一个巨大的开源项目：研究人员不断构建更好的模型来解决现有问题并与社区分享他们的结果。

由于最近的学术研究进展，以下是NLP中已经解决的主要障碍：
· 没有单一的模型架构，跨任务具有一致的最新结果。例如，在Question Answering中，我们有强监督的端到端内存网络 ; 在情感分析中，我们有Tree-LSTM ; 在序列标记中，我们有双向LSTM-CRF。我之前在问题回答部分中提到的动态内存网络以某种方式解决了这一挑战，因为它可以在多个域中一致地运行。

· 机器学习中一种强大的方法是多任务学习，它共享相关任务之间的表示，以使模型能够更好地概括原始任务。然而，相关的多任务学习很难，因为它通常仅限于较低层，仅在任务相关时才有用，并且在所提出的模型中具有相同的解码器/分类器。在联合多任务模型中：为多个NLP任务增长，作者预先定义了一个由几个NLP任务组成的分层架构，作为多任务学习的联合模型。该模型包括字符n-gram和短路以及最先进的纯前馈解析器，能够执行依赖解析，多句子任务和联合训练。

· 另一个挑战是重复字表示的问题，其中模型中编码器和解码器的不同编码导致重复的参数/含义。对此最简单的解决方案是将单词向量联系在一起并联合训练单个权重，如“绑定单词向量” 和“单词分类器：语言建模的损失框架”中所示。

· 另一个障碍是，与诸如卷积神经网络或前馈神经网络相比，任何Deep NLP技术的基本构建块Recurrent Neural Networks相当慢。准递归神经网络采用RNN和CNN的最佳部分来提高训练速度，使用卷积跨越时间的并行性和跨越信道的并行性的元素级门控递归。这种方法比语言建模和情感分析中的任何其他模型更好，更快。

· 最后，在NLP中，架构搜索使用机器学习自动化人工神经网络设计的过程 非常缓慢，因为传统的手动过程需要大量的专业知识。如果我们可以使用AI为任何问题找到合适的架构怎么办？使用Google Brain进行强化学习的神经架构搜索是迄今为止开发的最可行的解决方案。作者使用循环网络生成神经网络的模型描述，并使用强化学习训练此RNN，以最大化验证集上生成的体系结构的预期准确性。
## 如何通俗理解Word2vec
小编注：考虑到下文（https://zhuanlan.zhihu.com/p/26306795）没有对比清楚2013年Mikolov原始论文与2014年Rong.X文章的区别，故建议初学者先看July在CSDN上写的Word2vec笔记，《如何通俗理解word2vec》：https://blog.csdn.net/v_JULY_v/article/details/102708459，更清晰、更易懂

1. 引子
大家好
我叫数据挖掘机
皇家布鲁斯特大学肄业
我喝最烈的果粒橙，钻最深的牛角尖
——执着如我

今天我要揭开Word2vec的神秘面纱
直窥其本质

相信我，这绝对是你看到的
最浅白易懂的 Word2vec 中文总结
（蛤？你问我为啥有这个底气？
且看下面，我的踩坑血泪史。。。）

2. Word2vec参考资料总结
(以下都是我踩过的坑，建议先跳过本节，阅读正文部分，读完全文回头再来看)

先大概说下我深挖 word2vec 的过程：先是按照惯例，看了 Mikolov 关于 Word2vec 的两篇原始论文，然而发现看完依然是一头雾水，似懂非懂，主要原因是这两篇文章省略了太多理论背景和推导细节；
然后翻出 Bengio 03年那篇JMLR和 Ronan 11年那篇JMLR，看完对语言模型、用CNN处理NLP任务有所了解，但依然无法完全吃透 word2vec；
这时候我开始大量阅读中英文博客，其中 北漂浪子 的一篇阅读量很多的博客吸引了我的注意，里面非常系统地讲解了 Word2vec 的前因后果，最难得的是深入剖析了代码的实现细节，看完之后细节方面了解了很多，不过还是觉得有些迷雾；
终于，我在 quora 上看到有人推荐 Xin Rong 的那篇英文paper，看完之后只觉醍醐灌顶，酣畅淋漓，相见恨晚，成为我首推的 Word2vec 参考资料。

下面我将详细列出我阅读过的所有 Word2vec 相关的参考资料，并给出评价

Mikolov 两篇原论文：
『Distributed Representations of Sentences and Documents』
在前人基础上提出更精简的语言模型（language model）框架并用于生成词向量，这个框架就是 Word2vec

『Efficient estimation of word representations in vector space』
专门讲训练 Word2vec 中的两个trick：hierarchical softmax 和 negative sampling
优点：Word2vec 开山之作，两篇论文均值得一读
缺点：只见树木，不见森林和树叶，读完不得要义。这里『森林』指 word2vec 模型的理论基础——即 以神经网络形式表示的语言模型，『树叶』指具体的神经网络形式、理论推导、hierarchical softmax 的实现细节等等

北漂浪子的博客：『深度学习word2vec 笔记之基础篇』
优点：非常系统，结合源码剖析，语言平实易懂
缺点：太啰嗦，有点抓不住精髓

Yoav Goldberg 的论文：『word2vec Explained- Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method』
优点：对 negative-sampling 的公式推导非常完备
缺点：不够全面，而且都是公式，没有图示，略显干枯

Xin Rong 的论文：『word2vec Parameter Learning Explained』：
！重点推荐！
理论完备由浅入深非常好懂，且直击要害，既有 high-level 的 intuition 的解释，也有细节的推导过程
一定要看这篇paper！一定要看这篇paper！一定要看这篇paper！
评论区 @huichan 告知了一条沉重的信息，Rong Xin 于2017年驾驶飞机失事，永远离开了我们。缅怀，R.I.P，愿他能在天堂继续开心地科研

来斯惟的博士论文『基于神经网络的词和文档语义向量表示方法研究』以及他的博客（网名：licstar）
可以作为更深入全面的扩展阅读，这里不仅仅有 word2vec，而是把词嵌入的所有主流方法通通梳理了一遍

几位大牛在知乎的回答：『word2vec 相比之前的 Word Embedding 方法好在什么地方？』
刘知远、邱锡鹏、李韶华等知名学者从不同角度发表对 Word2vec 的看法，非常值得一看

Sebastian 的博客：『On word embeddings - Part 2: Approximating the Softmax』
详细讲解了 softmax 的近似方法，Word2vec 的 hierarchical softmax 只是其中一种

3. 正文
你会在本文看到：
提纲挈领地讲解 word2vec 的理论精髓
学会用gensim训练词向量，并寻找相似词
你不会在本文看到

神经网络训练过程的推导
hierarchical softmax/negative sampling 等 trick 的理论和实现细节

3.1. 什么是 Word2vec?
在聊 Word2vec 之前，先聊聊 NLP (自然语言处理)。NLP 里面，最细粒度的是 词语，词语组成句子，句子再组成段落、篇章、文档。所以处理 NLP 的问题，首先就要拿词语开刀。

举个简单例子，判断一个词的词性，是动词还是名词。用机器学习的思路，我们有一系列样本(x,y)，这里 x 是词语，y 是它们的词性，我们要构建 f(x)->y 的映射，但这里的数学模型 f（比如神经网络、SVM）只接受数值型输入，而 NLP 里的词语，是人类的抽象总结，是符号形式的（比如中文、英文、拉丁文等等），所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec，就是词嵌入（ word embedding) 的一种。

我在前作『都是套路: 从上帝视角看透时间序列和数据挖掘』提到，大部分的有监督机器学习模型，都可以归结为：
f(x)->y

在 NLP 中，把 x 看做一个句子里的一个词语，y 是这个词语的上下文词语，那么这里的 f，便是 NLP 中经常出现的『语言模型』（language model），这个模型的目的，就是判断 (x,y) 这个样本，是否符合自然语言的法则，更通俗点说就是：词语x和词语y放在一起，是不是人话。

Word2vec 正是来源于这个思想，但它的最终目的，不是要把 f 训练得多么完美，而是只关心模型训练完后的副产物——模型参数（这里特指神经网络的权重），并将这些参数，作为输入 x 的某种向量化的表示，这个向量便叫做——词向量（这里看不懂没关系，下一节我们详细剖析）。

我们来看个例子，如何用 Word2vec 寻找相似词：

对于一句话：『她们 夸 吴彦祖 帅 到 没朋友』，如果输入 x 是『吴彦祖』，那么 y 可以是『她们』、『夸』、『帅』、『没朋友』这些词
现有另一句话：『她们 夸 我 帅 到 没朋友』，如果输入 x 是『我』，那么不难发现，这里的上下文 y 跟上面一句话一样
从而 f(吴彦祖) = f(我) = y，所以大数据告诉我们：我 = 吴彦祖（完美的结论）

3.2. Skip-gram 和 CBOW 模型
上面我们提到了语言模型

如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』
而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』

3.2.1 Skip-gram 和 CBOW 的简单情形
我们先来看个最简单的例子。上面说到， y 是 x 的上下文，所以 y 只取上下文里一个词语的时候，语言模型就变成：
用当前词 x 预测它的下一个词 y

但如上面所说，一般的数学模型只接受数值型输入，这里的 x 该怎么表示呢？ 显然不能用 Word2vec，因为这是我们训练完模型的产物，现在我们想要的是 x 的一个原始输入形式。

答案是：one-hot encoder

所谓 one-hot encoder，其思想跟特征工程里处理类别变量的 one-hot 一样（参考我的前作『数据挖掘比赛通用框架』、『深挖One-hot和Dummy背后的玄机』）。本质上是用一个只含一个 1、其他都是 0 的向量来唯一表示词语。

我举个例子，假设全世界所有的词语总共有 V 个，这 V 个词语有自己的先后顺序，假设『吴彦祖』这个词是第1个词，『我』这个单词是第2个词，那么『吴彦祖』就可以表示为一个 V 维全零向量、把第1个位置的0变成1，而『我』同样表示为 V 维全零向量、把第2个位置的0变成1。这样，每个词语都可以找到属于自己的唯一表示。

OK，那我们接下来就可以看看 Skip-gram 的网络结构了，x 就是上面提到的 one-hot encoder 形式的输入，y 是在这 V 个词上输出的概率，我们希望跟真实的 y 的 one-hot encoder 一样。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154955627342704968.jpg'/>

首先说明一点：隐层的激活函数其实是线性的，相当于没做任何处理（这也是 Word2vec 简化之前语言模型的独到之处），我们要训练这个神经网络，用反向传播算法，本质上是链式求导，在此不展开说明了，

当模型训练完后，最后得到的其实是神经网络的权重，比如现在输入一个 x 的 one-hot encoder: [1,0,0,…,0]，对应刚说的那个词语『吴彦祖』，则在输入层到隐含层的权重里，只有对应 1 这个位置的权重被激活，这些权重的个数，跟隐含层节点数是一致的，从而这些权重组成一个向量 vx 来表示x，而因为每个词语的 one-hot encoder 里面 1 的位置是不同的，所以，这个向量 vx 就可以用来唯一表示 x。

注意：上面这段话说的就是 Word2vec 的精髓！！

此外，我们刚说了，输出 y 也是用 V 个节点表示的，对应V个词语，所以其实，我们把输出节点置成 [1,0,0,…,0]，它也能表示『吴彦祖』这个单词，但是激活的是隐含层到输出层的权重，这些权重的个数，跟隐含层一样，也可以组成一个向量 vy，跟上面提到的 vx 维度一样，并且可以看做是词语『吴彦祖』的另一种词向量。而这两种词向量 vx 和 vy，正是 Mikolov 在论文里所提到的，『输入向量』和『输出向量』，一般我们用『输入向量』。

需要提到一点的是，这个词向量的维度（与隐含层节点数一致）一般情况下要远远小于词语总数 V 的大小，所以 Word2vec 本质上是一种降维操作——把词语从 one-hot encoder 形式的表示降维到 Word2vec 形式的表示。

3.2.2. Skip-gram 更一般的情形
上面讨论的是最简单情形，即 y 只有一个词，当 y 有多个词时，网络结构如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154955634355639125.jpg'/>

可以看成是 单个x->单个y 模型的并联，cost function 是单个 cost function 的累加（取log之后）
如果你想深入探究这些模型是如何并联、 cost function 的形式怎样，不妨仔细阅读参考资料4. 在此我们不展开。

3.2.3 CBOW 更一般的情形
跟 Skip-gram 相似，只不过:
Skip-gram 是预测一个词的上下文，而 CBOW 是用上下文预测这个词
网络结构如下
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154955642339133552.jpg'/>

更 Skip-gram 的模型并联不同，这里是输入变成了多个单词，所以要对输入处理下（一般是求和然后平均），输出的 cost function 不变，在此依然不展开，建议你阅读参考资料4.

3.3. Word2vec 的训练trick
相信很多初次踩坑的同学，会跟我一样陷入 Mikolov 那篇论文（参考资料1.）里提到的 hierarchical softmax 和 negative sampling 里不能自拔，但其实，它们并不是 Word2vec 的精髓，只是它的训练技巧，但也不是它独有的训练技巧。 Hierarchical softmax 只是 softmax 的一种近似形式（详见参考资料7.），而 negative sampling 也是从其他方法借鉴而来。

为什么要用训练技巧呢？ 如我们刚提到的，Word2vec 本质上是一个语言模型，它的输出节点数是 V 个，对应了 V 个词语，本质上是一个多分类问题，但实际当中，词语的个数非常非常多，会给计算造成很大困难，所以需要用技巧来加速训练。

这里我总结了一下这两个 trick 的本质，有助于大家更好地理解，在此也不做过多展开，有兴趣的同学可以深入阅读参考资料1.~7.

hierarchical softmax
本质是把 N 分类问题变成 log(N)次二分类

negative sampling
本质是预测总体类别的一个子集

3.4. 扩展
很多时候，当我们面对林林总总的模型、方法时，我们总希望总结出一些本质的、共性的东西，以构建我们的知识体系，比如我在前作『分类和回归的本质』里，原创性地梳理了分类模型和回归模型的本质联系，比如在词嵌入领域，除了 Word2vec之外，还有基于共现矩阵分解的 GloVe 等等词嵌入方法。

深入进去我们会发现，神经网络形式表示的模型（如 Word2vec），跟共现矩阵分解模型（如 GloVe），有理论上的相通性，这里我推荐大家阅读参考资料5. ——来斯惟博士在它的博士论文附录部分，证明了 Skip-gram 模型和 GloVe 的 cost fucntion 本质上是一样的。是不是一个很有意思的结论？ 所以在实际应用当中，这两者的差别并不算很大，尤其在很多 high-level 的 NLP 任务（如句子表示、命名体识别、文档表示）当中，经常把词向量作为原始输入，而到了 high-level 层面，差别就更小了。

鉴于词语是 NLP 里最细粒度的表达，所以词向量的应用很广泛，既可以执行词语层面的任务，也可以作为很多模型的输入，执行 high-level 如句子、文档层面的任务，包括但不限于：

计算相似度
寻找相似词
信息检索

作为 SVM/LSTM 等模型的输入
中文分词
命名体识别

句子表示
情感分析

文档表示
文档主题判别

4. 实战
上面讲了这么多理论细节，其实在真正应用的时候，只需要调用 Gensim （一个 Python 第三方库）的接口就可以。但对理论的探究仍然有必要，你能更好地知道参数的意义、模型结果受哪些因素影响，以及举一反三地应用到其他问题当中，甚至更改源码以实现自己定制化的需求。

这里我们将使用 Gensim 和 NLTK 这两个库，来完成对生物领域的相似词挖掘，将涉及：

解读 Gensim 里 Word2vec 模型的参数含义
基于相应语料训练 Word2vec 模型，并评估结果
对模型结果调优
语料我已经放出来了，可以关注我的公众号『数据挖掘机养成记』，并回复 Sherlocked 获取语料，包含5000行生物医学领域相关文献的摘要(英文)


我将在下一篇文章里详细讲解实战步骤，敬请关注本人公众号。友情建议：请先自行安装 Gensim 和 NLTK 两个库，并建议使用 jupyter notebook 作为代码运行环境
## 什么是词嵌入word embedding?
Embedding在数学上表示一个maping, f: X -> Y， 也就是一个function，其中该函数是injective（就是我们所说的单射函数，每个Y只有唯一的X对应，反之亦然）和structure-preserving (结构保存，比如在X所属的空间上X1 < X2,那么映射后在Y所属空间上同理 Y1 < Y2)。那么对于word embedding，就是将单词word映射到另外一个空间，其中这个映射具有injective和structure-preserving的特点。

通俗的翻译可以认为是单词嵌入，就是把X所属空间的单词映射为到Y空间的多维向量，那么该多维向量相当于嵌入到Y所属空间中，一个萝卜一个坑。

word embedding，就是找到一个映射或者函数，生成在一个新的空间上的表达，该表达就是word representation。
推广开来，还有image embedding, video embedding, 都是一种将源数据映射到另外一个空间。

更多解释见：https://www.zhihu.com/question/32275069#130283448
## 了解NLP神经网络的发展历史么？
本文主要的内容如下：
2001 – 神经语言模型
2008 – 多任务学习
2013 – 词嵌入
2013 – NLP 神经网络
2014 – sequence-to-sequence 模型
2015 – 注意力机制
2015 – 基于记忆的网络
2018 – 预训练语言模型
其他的里程碑事件

传统算法里程碑事件
2001 – 神经语言模型
语言建模任务指的是给定前一个单词去预测文本中的下一个单词。它可能是比较简单的语言处理任务，具体的实际应用场景包括 智能键盘 、电子邮件回复建议（Kannan 等人， 2016）、拼写自动更正等。正如很多人所知，语言建模有着丰富的历史。其中比较经典的方法基于 n-grams ，并使用平滑处理不可见的 n-grams（Kneser & Ney, 1995）。

第一个神经语言模型是 Bengio 等人在 2001 年提出的前馈神经网络，如图 1 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728399726069727.png'/>图 1：前馈神经网络语言模型（Bengio 等，2001；2003）

这个模型将从表 C 中查找到的 n 个单词作为输入向量表征。这种向量被现在的学者们称做“词嵌入”。这些词嵌入级联后被输入到一个隐藏层中，该隐藏层的输出又被输入到 softmax 层。更多关于模型的信息，请看 这篇文章 。

最近，在语言建模技术方面，前馈神经网络被循环神经网络（RNNs；Mikolov 等人，2010）和长短时记忆网络（LSTMs；格雷夫斯，2013）所取代。尽管近年来提出了许多扩展经典 LSTM 的新语言模型（请参阅 本页 以获得概述），经典的 LSTM 仍然作为一个强大的基线存在着（Melis 等人， 2018）。甚至 Bengio 等人的经典前馈神经网络在某些情况下也可以与更复杂的模型一较高下，因为这些模型通常只会考虑距离较近的单词（Daniluk 等人， 2017）。因此，如何更好地理解这种语言模型所捕获的信息也是一个比较热门的研究领域（Kuncoro 等人， 2018；布莱文斯等人，2018 年）。

语言建模通常是应用 RNN 时的第一步，对于这一点大家已经形成了共识。许多人是通过 Andrej 的博客 文章第一次接触到语言建模的。语言建模是一种非监督学习形式，Yann LeCun 也将其称为预测性学习，并将其作为获得基础常识的先决条件（ 参见 NIPS 2016 年的幻灯片）。语言建模最引人关注的一点是，尽管它很简单，但却是本文后面讨论的许多技术发展的核心：
词嵌入：word2vec 的目标是简化语言建模。
sequence-to-sequence 模型：这种模型通过一次预测一个单词生成一个输出序列。
预训练语言模型：这些方法使用来自语言模型的表述进行迁移学习。

反过来讲，这意味着近年来 NLP 的许多重要进展都可以归结为某些形式的语言建模。为了“真正”理解自然语言，仅仅从文本的原始形式中学习是不够的。我们需要新的方法和模型。


2008 – 多任务学习
多任务学习是在多个任务上训练的模型之间共享参数的一种通用方法。在神经网络中，可以通过给不同层施以不同的权重，来很容易地实现多任务学习。多任务学习的概念最初由 Rich Caruana 在 1993 年提出，并被应用于道路跟踪和肺炎预测（Caruana, 1998）。直观地说，多任务学习鼓励模型学习对许多任务有用的表述。这对于学习一般的、低级的表述形式、集中模型的注意力或在训练数据有限的环境中特别有用。想要更全面地了解多任务学习，请看 这篇文章 。

在 2008 年，Collobert 和 Weston 将多任务学习首次应用于 NLP 的神经网络。在他们的模型中，查询表（或单词嵌入矩阵）在两个接受不同任务训练的模型之间共享，如下面的图 2 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728400924094710.png'/>图 2：单词嵌入矩阵的共享（Collobert & Weston, 2008 年；Collobert 等人，2011）

词嵌入的共享，使得模型能够在词嵌入矩阵中协作和共享一般的低级信息，而这些低级信息所占的参数量往往是模型中最大的一部分。2008 年，Collobert 和 Weston 共同撰写的论文对多任务学习之外的其他应用还产生了一定的影响。它率先提出了一些想法，如对文字嵌入进行预训练以及使用卷积神经网络（CNNs）来处理文本数据。它获得了 ICML 2018 年的经典论文奖 （ 参见 本文的经典论文奖演讲）。

多任务学习现在被广泛地用于 NLP 任务。充分利用现有的或“人造”的任务进行训练，可以更好的提高 NLP 效率。有关不同辅助任务的概述，请看 这篇文章 。虽然参数的共享通常是预定义的，但是在优化过程中也可以学习到不同的共享模式（Ruder 等人， 2017）。随着对多任务模型泛化能力的评估，多任务学习越来越重要，最近还提出了多任务学习的专用标准（Wang 等人， 2018；McCann 等人，2018 年）。


2013 – 词嵌入
用稀疏向量表示文本，即所谓的 词袋模型 在 NLP 有着悠久的历史。正如上文中介绍的，早在 2001 年就开始使用密集向量表示词或词嵌入。Mikolov 等人在 2013 年提出的创新技术是通过去除隐藏层，逼近目标，进而使这些单词嵌入的训练更加高效。虽然这些技术变更本质上很简单，但它们与高效的 word2vec 配合使用，便能使大规模的词嵌入训练成为可能。

Word2vec 有两种风格，如下面的图 3 所示：连续字袋（CBOW）和 skip-gram。不过他们的目标不同：一个是根据周围的单词预测中心单词，而另一个则相反。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728401749507875.png'/>图 3：连续字袋和 skip-gram 架构（Mikolov 等人， 2013a；2013 b）

虽然这些嵌入在概念上与使用前馈神经网络学习的嵌入在概念上没有区别，但是在一个非常大的语料库上训练之后，它们就能够捕获诸如性别、动词时态和国家 – 首都关系等单词之间的特定关系，如下图 4 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728402413484256.png'/>图 4：word2vec（Mikolov 等人， 2013a；2013 b）

这些词语间关系的获得及其背后的意义引发了人们对嵌入技术的兴趣——人们开始大量研究这些线性关系形成的原理（Arora 等人， 2016；Mimno & Thompson, 2017；Antoniak & Mimno，2018 年；Wendlandt 等人，2018 年）。然而，推动词嵌入技术成为当前 NLP 的主流技术的却不是这些原理，而是在初始化时使用预训练的嵌入，因为这样做可以提高下游任务的性能。

虽然 word2vec 捕捉到的词间关系很直观、质量高得甚至有些神奇，但后来的研究表明，word2vec 本身并没有什幺特别之处：词嵌入也可以通过矩阵分解来学习（Pennington 等人，2014）；通过适当的调优，SVD 和 LSA 等经典的矩阵分解方法也得到了类似的结果（Levy 等人， 2015）。

从那以后，人们开始投入大量的精力去探索词嵌入的各个方面（从 原文引用的惊人数量 可以看出）。通过 这篇文章 ，我们可以看出一些趋势和未来的方向。尽管有许多发展进步，但到现在为止，word2vec 仍然是大众的首选。对 Word2vec 的使用范围已经不限于单词级别了：基于局部上下文学习嵌入的简单目标——带负抽样的 skip-gram 已被用于学习句子表示（Mikolov & Le, 2014；Kiros 等人，2015）。Word2vec 甚至还在网络（Grover & Leskovec, 2016）和生物序列（Asgari & Mofrad, 2015）等其他应用场景中发挥了作用。

一个比较有研究价值的技术方向是将不同语言的词嵌入到同一个空间中，以实现（零样本）跨语言迁移。以一种完全不受监督的方式（至少对于类似的语言来说）学习数据以实现一个好的推测效果变得越来越有可能（Conneau 等人，2018 年；Artetxe 等人，2018 年；Søgaard 等人，2018）。这种学习方式可被应用于语言资源缺乏的无监督机器翻译系统中（Lample 等人，2018;；Artetxe 等人，2018）。查看（Ruder 等人， 2018）以获得概述。


2013 – NLP 神经网络
2013 年和 2014 年是 NLP 问题开始引入神经网络模型的时期。使用最广泛的三种主要的神经网络是：循环神经网络、卷积神经网络和递归神经网络。

循环神经网络（RNNs）循环神经网络是处理 NLP 中普遍存在的动态输入序列的一个最佳的技术方案。Vanilla RNNs（Elman, 1990）很快被经典的长 – 短期记忆网络（Hochreiter & Schmidhuber，1997）所取代，它被证明 对消失和爆炸梯度问题更有弹性 。在 2013 年之前，RNN 仍被认为很难训练； Ilya Sutskever 的博士论文 为改变这种现状提供了一个关键性的例子。下面的图 5 对 LSTM 单元进行了可视化显示。双向 LSTM（Graves 等人， 2013）通常用于处理左右两边的上下文。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728403223620244.png'/>图 5：LSTM 网络（来源：Chris Olah）

卷积神经网络（CNNs） 卷积神经网络本来是广泛应用于计算机视觉领域的技术，现在也开始应用于语言（Kalchbrenner 等人， 2014；Kim 等人，2014）。文本的卷积神经网络只在两个维度上工作，其中滤波器（卷积核）只需要沿着时间维度移动。下面的图 6 显示了 NLP 中使用的典型 CNN。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728404093234745.png'/>图 6：文本卷积神经网络（Kim, 2014）

卷积神经网络的一个优点是它们比 RNN 更可并行化，因为其在每个时间步长的状态只依赖于本地上下文（通过卷积运算），而不是像 RNN 那样依赖过去所有的状态。使用膨胀卷积，可以扩大 CNN 的感受野，使网络有能力捕获更长的上下文（Kalchbrenner 等人， 2016）。CNN 和 LSTM 可以组合和叠加（Wang 等人， 2016），卷积也可以用来加速 LSTM（Bradbury 等人， 2017）。

递归神经网络RNN 和 CNN 都将语言视为一个序列。然而，从语言学的角度来看，语言本质上是 层次化的 ：单词被组合成高阶短语和从句，这些短语和从句本身可以根据一组生产规则递归地组合。将句子视为树而不是序列的语言学启发思想产生了递归神经网络（Socher 等人， 2013），如下图 7 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728404734327836.png'/>图 7：递归神经网络（Socher 等人， 2013）

递归神经网络从下到上构建序列的表示，这一点不同于从左到右或从右到左处理句子的 RNN。在树的每个节点上，通过组合子节点的结果来计算新的结果。由于树也可以被视为在 RNN 上强加不同的处理顺序，所以 LSTM 自然地也被扩展到树上（Tai 等人， 2015）。

RNN 和 LSTM 可以扩展到使用层次结构。单词嵌入不仅可以在本地学习，还可以在语法语境中学习（Levy & Goldberg, 2014）；语言模型可以基于句法堆栈生成单词（Dyer 等人， 2016）；图卷积神经网络可以基于树结构运行（Bastings 等人， 2017）。


2014 – sequence-to-sequence 模型
2014 年，Sutskever 等人提出了 sequence-to-sequence 模型。这是一个使用神经网络将一个序列映射到另一个序列的通用框架。在该框架中，编码器神经网络逐符号处理一个句子，并将其压缩为一个向量表示；然后，一个解码器神经网络根据编码器状态逐符号输出预测值，并将之前预测的符号作为每一步的输入，如下图 8 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728405469075559.png'/>图 8：sequence-to-sequence 模型（Sutskever 等人， 2014）

机器翻译是对这个框架比较成功的应用。2016 年，谷歌宣布将开始用神经 MT 模型取代基于单片短语的 MT 模型（Wu 等人， 2016）。 根据 Jeff Dean 的说法 ，这意味着用 500 行神经网络模型替换 50 万行基于短语的 MT 代码。

由于其灵活性，这个框架现在是自然语言生成任务的首选框架，其中不同的模型承担了编码器和解码器的角色。重要的是，解码器模型不仅可以解码一个序列，而且可以解码任意表征。例如，可以基于图像生成标题（Vinyals 等人， 2015）（如下图 9 所示）、基于表生成文本（Lebret 等人， 2016）和基于应用程序中源代码更改描述（Loyola 等人， 2017）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728406157980794.png'/>图 9：基于图像生成标题（Vinyals 等人，2015）

sequence-to-sequence 学习甚至可以应用于 NLP 中输出具有特定结构的结构化预测任务。为了简单起见，输出被线性化，如下面的图 10 所示，用于进行选区解析。神经网络已经证明了在有足够数量的训练数据进行选区分析（Vinyals 等人，2015）和命名实体识别（Gillick 等人， 2016）的情况下，直接学习可以产生这种线性化输出的能力。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728407015071412.png'/>图 10：线性化选区解析树（Vinyals 等人，2015）

序列和解码器的编码器通常基于 RNN，但可以使用其他模型类型。新的体系结构主要来源于 MT 的贡献，它是 sequence-to-sequence 模型体系结构的主要开发者。最新的模型有 deep LSTMs（Wu 等人，2016；tional encoders 、Kalchbrenner 等人，2016；Gehring 等人， Transformer 、Vaswani 等人，2017）和 LSTM 与 Transformer 的结合体（Chen 等人， 2018）。


2015 – 注意力机制
注意力机制（Bahdanau 等人， 2015）是神经网络机器翻译（NMT）的核心创新之一，也是使 NMT 模型胜过经典的基于短语的 MT 系统的关键思想。sequence-to-sequence 模型的主要瓶颈是需要将源序列的全部内容压缩为一个固定大小的向量。注意力机制通过允许解码器回头查看源序列隐藏状态来缓解这一问题，然后将其加权平均作为额外输入提供给解码器，如下面的图 11 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728407872231977.png'/>
图 11：Attention（Bahdanau 等人， 2015）

注意力机制有很多不同的形式（Luong 等人，2015）。 这里  有一个简短的概述。注意力机制广泛适用于任何需要根据输入的特定部分做出决策的任务，并且效果不错。它已被应用于一致性解析（Vinyals 等人，2015）、阅读理解（Hermann 等人，2015）和一次性学习（Vinyals 等人，2016）等诸多领域。输入甚至不需要是一个序列，即可以包含其他表示，如图像字幕（Xu 等人， 2015），如下图 12 所示。注意力机制的一个额外的功能是，它提供了一种少见的功能，我们可以通过检查输入的哪些部分与基于注意力权重的特定输出相关来了解模型的内部工作方式。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728409185148843.png'/>图 12：图像字幕模型中的视觉注意力，预测模型在生成“飞盘”时所关注的内容。（Xu 等人， 2015）

注意力机制也不仅仅局限于观察输入序列；可以使用 self-attention 查看句子或文档中的周围单词，以获得更佳的上下文敏感的词表示。Transformer 架构的核心是多层次的自我关注（Vaswani 等人， 2017），这是目前 NMT 最先进的模型。


2015 – 基于记忆的网络
注意力机制可以看作是模糊记忆的一种形式。记忆由模型的隐藏状态组成，模型选择从记忆中检索内容。想要更详细地了解注意力及其与记忆的联系，请看 这篇文章 。研究者们提出了许多具有更明确记忆的模型。这些模型有不同的变体，如神经图灵机（Graves 等 ，2014）、记忆网络（Weston 等 ，2015）和端到端记忆网络（Sukhbaatar 等，2015）、动态记忆网络（Kumar 等 ，2015）、神经微分计算机（Graves 等，2016）和循环实体网络（Henaff 等，2017）。

记忆的访问通常基于与当前状态的相似度，类似于注意力，通常可以写入和读取。模型在如何实现和利用内存方面有所不同。例如，端到端记忆网络多次处理输入，并更新记忆以实现多个推理步骤。神经图灵机也有一个基于位置的寻址，这允许他们学习简单的计算机程序，如排序。基于记忆的模型通常应用于一些特定任务中，如语言建模和阅读理解。在这些任务中，长时间保存信息应该很有用。记忆的概念是非常通用的：知识库或表可以充当记忆，而记忆也可以根据整个输入或它的特定部分填充。


2018 – 预训练语言模
预训练的词嵌入与上下文无关，仅用于初始化模型中的第一层。最近几个月，一系列监督型任务被用于神经网络的预训练（Conneau 等人，2017；McCann 等人，2017；Subramanian 等人，2018 年）。相反，语言模型只需要无标签的文本；因此，训练可以扩展到数十亿个令牌、新域和新语言。预训练语言模型于 2015 年被首次提出（Dai & Le, 2015）；直到最近，它们才被证明在各种任务中效果还是不错的。语言模型嵌入可以作为目标模型中的特征（Peters 等人，2018 年），或者使用语言模型对目标任务数据进行微调（Ramachandran 等人，2017 年；霍华德 & 鲁德出版社，2018 年）。添加语言模型嵌入可以在许多不同的任务中提供比最先进的技术更大的改进，如下面的图 13 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728409864861426.png'/>图 13：嵌入到最先进的语言模型中的改进（Peters 等人，2018）

预训练的语言模型已经被证明可以用更少的数据进行学习。由于语言模型只需要无标记的数据，因此对于标记数据稀缺的低资源语言尤其有用。有关预训练语言模型潜力的更多信息， 请参阅本文 。

其他里程碑事件
其他一些技术发展没有上面提到的那样流行，但仍然有广泛的影响。

基于字符的表示在字符上使用 CNN 或 LSTM 以获得基于字符的词表示的做法现在相当普遍，特别是对于形态信息重要或有许多未知单词的丰富的语言和任务，效果更加明显。据我所知，序列标签使用基于字符的表示（Lample 等人，2016；普兰克等人，2016），可以减轻在计算成本增加的情况下必须处理固定词汇表的需要，并支持完全基于字符的 NMT（Ling 等人， 2016；Lee 等人，2017）。

对抗学习对抗学习方法已经在 ML 领域掀起了风暴，在 NLP 中也有不同形式的应用。对抗性的例子越来越被广泛使用，它不仅是作为一种工具来探究模型和理解它们的失败案例，而且也使自身更加鲁棒（Jia & Liang， 2017）。（虚拟）对抗性训练，即最坏情况扰动（Miyato 等人，2017）和领域对抗性损失（Ganin 等人， 2016；Kim 等人，2017），同样可以使模型更加鲁棒。生成对抗网络（GANs）对于自然语言生成还不是很有效（Semeniuta 等人， 2018），但在匹配分布时很有用（Conneau 等人， 2018）。

强化学习强化学习已被证明对具有时间依赖性的任务有效，例如在训练期间选择数据（Fang 等人， 2017；Wu 等人， 2018）和建模对话（Liu 等人， 2018）。RL 对于直接优化不可微的末端度量（如 ROUGE 或 BLEU）也有效，反而在汇总中优化替代损失（如交叉熵）（Paulus 等人， 2018；Celikyilmaz 等人，2018）和机器翻译场景效果就不明显了（Ranzato 等人，2016）。类似地，逆向强化学习在过于复杂而无法指定数据的情况下也很有用，比看图说话任务（Wang 等人， 2018）。

非神经网络算法的里程碑事件
在 1998 年和接下来的几年里， FrameNet 项目诞生了（Baker 等人， 1998），这指导了 语义角色 标注 的任务。这是一种浅语义解析的形式，至今仍在积极研究开发中。在本世纪初，与自然语言学习会议（CoNLL）一起组织的共享任务促进了核心 NLP 任务的研究，如组块（Tjong Kim Sang 等人， 2000）、命名实体识别（Tjong Kim Sang 等人， 2003）和依赖解析（Buchholz 等人， 2006）等。许多 CoNLL 共享任务数据集现在仍然被用作评估的标准。

2001 年，条件随机场（CRF；Lafferty 等人， 2001）成为了最具影响力的序列标注方法类别之一，获得了 ICML 2011 的最佳论文奖 。CRF 层是当前最先进的序列标注问题模型的核心部分，这些模型具有标签间的相互依赖性，如命名实体识别（Lample 等，2016）。

2002 年，双语互译质量评估辅助工具（BLEU；Papineni 等人，2002）给出了双语互译质量度量标准，这使得 MT 系统得以扩展。其现在仍然是 MT 评估的标准度量标准。同年，结构感知机（Collins，2002）问世，为结构化感知工作奠定了基础。在同一次会议上，情感分析也成了最受欢迎和广泛研究的 NLP 任务之一（Pang 等人， 2002）。这三篇论文都获得了 2018 年 NAACL 最佳论文奖 。

2003 年引入了潜在狄利克雷分配（LDA；Blei 等人，2003），这是机器学习中应用最广泛的技术之一，至今仍是主题建模的标准方法。在 2004 年，有学者提出了比 SVM 更适合于捕获结构化数据中的相关性的新最大边缘模型（Taskar 等人， 2004a；2004b）。

2006 年，OntoNotes（Hovy 等人， 2006）介绍了一个具有多个注释和高注释协议的大型多语言语料库。OntoNotes 已被用于训练和评估各种任务，如依赖解析和引用解析。Milne 和 Witten（2008）在 2008 年介绍了利用维基百科丰富机器学习方法的方案。到目前为止，Wikipedia 是训练 ML 方法最有用的资源之一，无论是用于实体链接和消除歧义、语言建模、知识库还是其他各种任务。

2009 年，提出了远程监督的概念（Mintz 等人， 2009）。远程监督利用启发式或现有知识库中的信息生成带有噪声的模式，可用于从大型语料库中自动提取示例。远程监督现已被广泛应用，并且已经是关系提取、信息提取、情感分析等领域的常用技术。

英文原文： A Review of the Neural History of Natural Language Processing
## 判别式（discriminative）模型和生成式(generative)模型的核心区别是什么？
本题解析来源：https://www.zhihu.com/question/35866596/answer/236886066

在监督学习下，模型可以分为判别式模型与生成式模型。

根据经验，A批模型（神经网络模型、SVM、perceptron、LR、DT……）与B批模型（NB、LDA……），有啥区别不？（这个问题需要一些模型使用经验）应该是这样的：
1. A批模型是这么工作的，他们直接将数据的Y（或者label），根据所提供的features，学习，最后画出了一个明显或者比较明显的边界（具体怎么做到的？通过复杂的函数映射，或者决策叠加等等mechanism），这一点线性LR、线性SVM应该很明显吧。 

2. B批模型是这么工作的，他们先从训练样本数据中，将所有的数据的分布情况摸透，然后最终确定一个分布，来作为我的所有的输入数据的分布，并且他是一个联合分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035990526332970.svg'/>   (注意<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035991554313137.svg'/>包含所有的特征<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035992531381823.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035993717483536.svg'/>包含所有的label)。然后我来了新的样本数据（inference），好，通过学习来的模型的联合分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035995664047739.svg'/>，再结合新样本给的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035997142504104.svg'/>，通过条件概率就能出来<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036000361353863.svg'/>：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036001491062177.svg'/>

1. 判别式模型
那么A批模型对应了判别式模型。根据上面的两句话的区别，可以知道判别模型的特征了，所以有句话说：判别模型是直接对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036003810017449.svg'/>建模，就是说，直接根据X特征来对Y建模训练。

具体地，我的训练过程是确定构件  模型里面“复杂映射关系”中的参数，完了再去inference一批新的sample。

所以判别式模型的特征总结如下：
1) 对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036005749731553.svg'/>建模
2) 对所有的样本只构建一个模型，确认总体判别边界
3) 观测到输入什么特征，就预测最可能的label
4) 另外，判别式的优点是：对数据量要求没生成式的严格，速度也会快，小数据量下准确率也会好些。

2. 生成式模型
同样，B批模型对应了生成式模型。并且需要注意的是，在模型训练中，我学习到的是X与Y的联合模型<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036009086309819.svg'/>，也就是说，我在训练阶段是只对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036010615518794.svg'/>建模，我需要确定维护这个联合概率分布的所有的信息参数。完了之后在inference再对新的sample计算<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036015019525080.svg'/>，导出<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036016480767790.svg'/> ,但这已经不属于建模阶段了。

结合NB过一遍生成式模型的工作流程。学习阶段，建模：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415703601835560797.svg'/>（当然，NB具体流程去隔壁参考）,然后<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036022766630471.svg'/>。
另外，LDA也是这样，只是他更过分，需要确定很多个概率分布，而且建模抽样都蛮复杂的。

所以生成式总结下有如下特点：
① 对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036024830103287.svg'/>建模
② 这里我们主要讲分类问题，所以是要对每个label（<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036026167421345.svg'/>）都需要建模，最终选择最优概率的label为结果，所以没有什么判别边界。（对于序列标注问题，那只需要构件一个model）
③ 中间生成联合分布，并可生成采样数据。
④ 生成式模型的优点在于，所包含的信息非常齐全，我称之为“上帝信息”，所以不仅可以用来输入label，还可以干其他的事情。生成式模型关注结果是如何产生的。但是生成式模型需要非常充足的数据量以保证采样到了数据本来的面目，所以速度相比之下，慢。

这一点明白后，后面讲到的HMM与CRF的区别也会非常清晰。
最后identity the picture below：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036028335909703.jpg'/>
## 如何通俗理解隐马尔可夫模型HMM？
作者：Yang Eninala​，杜克大学 生物化学博士
链接：https://www.zhihu.com/question/20962240

隐马尔可夫（HMM）好讲，简单易懂不好讲。我认为 @者也的回答没什么错误，不过我想说个更通俗易懂的例子。我希望我的读者不是专家，而是对这个问题感兴趣的入门者，所以我会多阐述数学思想，少写公式。霍金曾经说过，你多写一个公式，就会少一半的读者。所以时间简史这本关于物理的书和麦当娜关于性的书卖的一样好。我会效仿这一做法，写最通俗易懂的答案。

还是用最经典的例子，掷骰子。

假设我手里有三个不同的骰子。
第一个骰子是我们平常见的骰子（称这个骰子为D6），6个面，每个面（1，2，3，4，5，6）出现的概率是1/6。
第二个骰子是个四面体（称这个骰子为D4），每个面（1，2，3，4）出现的概率是1/4。
第三个骰子有八个面（称这个骰子为D8），每个面（1，2，3，4，5，6，7，8）出现的概率是1/8。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415496212405785727.jpg'/>

假设我们开始掷骰子，我们先从三个骰子里挑一个，挑到每一个骰子的概率都是1/3。然后我们掷骰子，得到一个数字，1，2，3，4，5，6，7，8中的一个。不停的重复上述过程，我们会得到一串数字，每个数字都是1，2，3，4，5，6，7，8中的一个。例如我们可能得到这么一串数字（掷骰子10次）：1 6 3 5 2 7 3 5 2 4

这串数字叫做可见状态链。但是在隐马尔可夫模型中，我们不仅仅有这么一串可见状态链，还有一串隐含状态链。在这个例子里，这串隐含状态链就是你用的骰子的序列。比如，隐含状态链有可能是：D6 D8 D8 D6 D4 D8 D6 D6 D4 D8

一般来说，HMM中说到的马尔可夫链其实是指隐含状态链，因为隐含状态（骰子）之间存在转换概率（transition probability）。在我们这个例子里，D6的下一个状态是D4，D6，D8的概率都是1/3。D4，D8的下一个状态是D4，D6，D8的转换概率也都一样是1/3。这样设定是为了最开始容易说清楚，但是我们其实是可以随意设定转换概率的。比如，我们可以这样定义，D6后面不能接D4，D6后面是D6的概率是0.9，是D8的概率是0.1。这样就是一个新的HMM。

同样的，尽管可见状态之间没有转换概率，但是隐含状态和可见状态之间有一个概率叫做输出概率（emission probability）。就我们的例子来说，六面骰（D6）产生1的输出概率是1/6。产生2，3，4，5，6的概率也都是1/6。我们同样可以对输出概率进行其他定义。比如，我有一个被赌场动过手脚的六面骰子，掷出来是1的概率更大，是1/2，掷出来是2，3，4，5，6的概率是1/10。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415496317172103652.jpeg'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963177265294242.jpeg'/>

其实对于HMM来说，如果提前知道所有隐含状态之间的转换概率和所有隐含状态到所有可见状态之间的输出概率，做模拟是相当容易的。但是应用HMM模型时候呢，往往是缺失了一部分信息的，有时候你知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；有时候你只是看到了很多次掷骰子的结果，剩下的什么都不知道。如果应用算法去估计这些缺失的信息，就成了一个很重要的问题。这些算法我会在下面详细讲。

回到正题，和HMM模型相关的算法主要分为三类，分别解决三种问题：
1）知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道每次掷出来的都是哪种骰子（隐含状态链）。这个问题呢，在语音识别领域呢，叫做解码问题。这个问题其实有两种解法，会给出两个不同的答案。每个答案都对，只不过这些答案的意义不一样。
第一种解法求最大似然状态路径，说通俗点呢，就是我求一串骰子序列，这串骰子序列产生观测结果的概率最大。
第二种解法呢，就不是求一组骰子序列了，而是求每次掷出的骰子分别是某种骰子的概率。比如说我看到结果后，我可以求得第一次掷骰子是D4的概率是0.5，D6的概率是0.3，D8的概率是0.2。
第一种解法我会在下面说到，但是第二种解法我就不写在这里了，如果大家有兴趣，我们另开一个问题继续写吧。

2）还是知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道掷出这个结果的概率。看似这个问题意义不大，因为你掷出来的结果很多时候都对应了一个比较大的概率。问这个问题的目的呢，其实是检测观察到的结果和已知的模型是否吻合。如果很多次结果都对应了比较小的概率，那么就说明我们已知的模型很有可能是错的，有人偷偷把我们的骰子給换了。

3）知道骰子有几种（隐含状态数量），不知道每种骰子是什么（转换概率），观测到很多次掷骰子的结果（可见状态链），我想反推出每种骰子是什么（转换概率）。这个问题很重要，因为这是最常见的情况。很多时候我们只有可见结果，不知道HMM模型里的参数，我们需要从可见结果估计出这些参数，这是建模的一个必要步骤。

问题阐述完了，下面就开始说解法。（0号问题在上面没有提，只是作为解决上述问题的一个辅助）

0.一个简单问题
其实这个问题实用价值不高。由于对下面较难的问题有帮助，所以先在这里提一下。

知道骰子有几种，每种骰子是什么，每次掷的都是什么骰子，根据掷骰子掷出的结果，求产生这个结果的概率。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963191446207141.jpeg'/>

解法无非就是概率相乘：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963195175430992.png'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963195950738756.png'/>

1.看见不可见的，破解骰子序列
这里我说的是第一种解法，解最大似然路径问题。

举例来说，我知道我有三个骰子，六面骰，四面骰，八面骰。我也知道我掷了十次的结果（1 6 3 5 2 7 3 5 2 4），我不知道每次用了那种骰子，我想知道最有可能的骰子序列。

其实最简单而暴力的方法就是穷举所有可能的骰子序列，然后依照第零个问题的解法把每个序列对应的概率算出来。然后我们从里面把对应最大概率的序列挑出来就行了。如果马尔可夫链不长，当然可行。如果长的话，穷举的数量太大，就很难完成了。

另外一种很有名的算法叫做Viterbi algorithm. 要理解这个算法，我们先看几个简单的列子。
首先，如果我们只掷一次骰子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963210314993492.jpeg'/>

看到结果为1.对应的最大概率骰子序列就是D4，因为D4产生1的概率是1/4，高于1/6和1/8.
把这个情况拓展，我们掷两次骰子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963215041149237.jpeg'/>

结果为1，6.这时问题变得复杂起来，我们要计算三个值，分别是第二个骰子是D6，D4，D8的最大概率。显然，要取到最大概率，第一个骰子必须为D4。这时，第二个骰子取到D6的最大概率是
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963245064702512.png'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963247286074053.png'/>

同上，我们可以计算第三个骰子是D6或D8时的最大概率。我们发现，第三个骰子取到D4的概率最大。而使这个概率最大时，第二个骰子为D6，第一个骰子为D4。所以最大概率骰子序列就是D4 D6 D4。

写到这里，大家应该看出点规律了。既然掷骰子一二三次可以算，掷多少次都可以以此类推。我们发现，我们要求最大概率骰子序列时要做这么几件事情。首先，不管序列多长，要从序列长度为1算起，算序列长度为1时取到每个骰子的最大概率。然后，逐渐增加长度，每增加一次长度，重新算一遍在这个长度下最后一个位置取到每个骰子的最大概率。因为上一个长度下的取到每个骰子的最大概率都算过了，重新计算的话其实不难。当我们算到最后一位时，就知道最后一位是哪个骰子的概率最大了。然后，我们要把对应这个最大概率的序列从后往前推出来。

2.谁动了我的骰子？
比如说你怀疑自己的六面骰被赌场动过手脚了，有可能被换成另一种六面骰，这种六面骰掷出来是1的概率更大，是1/2，掷出来是2，3，4，5，6的概率是1/10。你怎么办么？答案很简单，算一算正常的三个骰子掷出一段序列的概率，再算一算不正常的六面骰和另外两个正常骰子掷出这段序列的概率。如果前者比后者小，你就要小心了。

比如说掷骰子的结果是：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415496325727916489.jpeg'/>

要算用正常的三个骰子掷出这个结果的概率，其实就是将所有可能情况的概率进行加和计算。同样，简单而暴力的方法就是把穷举所有的骰子序列，还是计算每个骰子序列对应的概率，但是这回，我们不挑最大值了，而是把所有算出来的概率相加，得到的总概率就是我们要求的结果。这个方法依然不能应用于太长的骰子序列（马尔可夫链）。

我们会应用一个和前一个问题类似的解法，只不过前一个问题关心的是概率最大值，这个问题关心的是概率之和。解决这个问题的算法叫做前向算法（forward algorithm）。

首先，如果我们只掷一次骰子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963266094334650.jpeg'/>

看到结果为1.产生这个结果的总概率可以按照如下计算，总概率为0.18：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963269969259385.jpeg'/>

把这个情况拓展，我们掷两次骰子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963278953070319.jpeg'/>

看到结果为1，6.产生这个结果的总概率可以按照如下计算，总概率为0.05：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415496328485517326.jpeg'/>

继续拓展，我们掷三次骰子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963294735647584.jpeg'/>

看到结果为1，6，3.产生这个结果的总概率可以按照如下计算，总概率为0.03：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963298847740924.jpeg'/>

同样的，我们一步一步的算，有多长算多长，再长的马尔可夫链总能算出来的。用同样的方法，也可以算出不正常的六面骰和另外两个正常骰子掷出这段序列的概率，然后我们比较一下这两个概率大小，就能知道你的骰子是不是被人换了。

3.掷一串骰子出来，让我猜猜你是谁
这个算法就是著名的EM算法，详见：https://www.julyedu.com/question/big/kp_id/23/ques_id/1007

上述算法呢，其实用到了递归，逆向推导，循环这些方法，我只不过用很直白的语言写出来了。如果你们去看专业书籍呢，会发现更加严谨和专业的描述。毕竟，我只做了会其意，要知其形，还是要看书的。
## 预训练方法 BERT和OpenAI GPT有什么区别？
1.GPT在BooksCorpus(800M单词)训练；BERT在BooksCorpus(800M单词)和维基百科(2,500M单词)训练
2.GPT使用一种句子分隔符([SEP])和分类符词块([CLS])，它们仅在微调时引入；BERT在预训练期间学习[SEP]，[CLS]和句子A/B嵌入
3.GPT用一个批量32,000单词训练1M步；BERT用一个批量128,000单词训练1M步
4.GPT对所有微调实验使用的5e-5相同学习率；BERT选择特定于任务的微调学习率，在开发集表现最佳
## 了解词嵌入的来龙去脉么？
本题解析来源：@Scofield_Phil，https://blog.csdn.net/Scotfield_msn/article/details/69075227 

Indexing:
〇、序
一、DeepNLP的核心关键：语言表示（Representation）
二、NLP词的表示方法类型
   1、词的独热表示one-hot representation
   2、词的分布式表示distributed representation
三、NLP语言模型

四、词的分布式表示
    1. 基于矩阵的分布表示
    2. 基于聚类的分布表示
    3. 基于神经网络的分布表示，词嵌入（ word embedding）
五、词嵌入（ word embedding）
    1、概念
    2、理解
六、神经网络语言模型与word2vec
    1、神经网络语言模型
    2.word2vec与CBOW、Skip-gram
    3.个人对word embedding的理解


〇、序
之前一段时间，在结合深度学习做NLP的时候一直有思考一些问题，其中有一个问题算是最核心一个：究竟深度网络是怎么做到让各种NLP任务解决地如何完美呢？到底我的数据在NN中发什么了什么呢？

并且，不少的terms like： 词向量、word embedding、分布式表示、word2vec、glove等等，这一锅粥的名词术语分别代表什么，他们具体的关系是什么，他们是否处于平级关系？

出于对知识结构追求完整梳理的强迫症的老毛病，于是不停地查资料、思考、keep revolving……

然后就感觉有一点小进展了。想到，不如将个人对其的理解，无论对错，先拿出来跟peer分享下，或许能交换出更有意义的东西呢？

整篇文章的构架是按照属于概念在逻辑上的先后大小顺序，一层一层一级一级地往下剖析、比较、说明。

另外说明下，here整篇文字内容相对是比较入门，甚至有的点可能描述的不太客观正确，限于当前的认知水平……还请您海涵，希望您在评论中指正！

一、DeepNLP的核心关键：语言表示（Representation）
最近有一个新名词：Deep Learning + NLP =  DeepNLP。当常规的机器学习Machine Learning升级发展到了一定的阶段后，慢慢的被后起的深度学习Deep Learning夺势而去，并如火如荼地引领了一波新高潮，因为Deep Learning有machine learning过而不及之处！那当Deep Learning进入NLP领域，自然是要横扫ACL一批paper才是。事实也是这样的。

先提下数据特征表示问题。数据表示是机器学习的核心问题，在过去的Machine Learning阶段，大量兴起特征工程，人工设计大量的特征解决数据的有效表示问题。而到了Deep Learning，想都别想，end-2-end，一步到位，hyper-parameter自动帮你选择寻找关键的特征参数。

那么，Deep Learning如何能在NLP中发挥出应有的real power呢？很明显，先不提如何设计出很强势的网络结构，不提如何在NLP中引入基于NN的解决例如情感分析、实体识别、机器翻译、文本生成这些高级任务，咱们首先得把语言表示这一关过了——如何让语言表示成为NN能够处理的数据类型。

我们看看图像和语音是怎么表示数据的：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211539066450273.jpg'/>

在语音中，用音频频谱序列向量所构成的matrix作为前端输入喂给NN进行处理，good；在图像中，用图片的像素构成的matrix展平成vector后组成的vector序列喂给NN进行处理，good；那在自然语言处理中呢？噢你可能知道或者不知道，将每一个词用一个向量表示出来！想法是挺简单的，对，事实上就是这么简单，然而真有这么简单吗？可能没这么简单。

有人提到，图像、语音属于比较自然地低级数据表示形式，在图像和语音领域，最基本的数据是信号数据，我们可以通过一些距离度量，判断信号是否相似，在判断两幅图片是否相似时，只需通过观察图片本身就能给出回答。而语言作为人类在进化了几百万年所产生的一种高层的抽象的思维信息表达的工具，其具有高度抽象的特征，文本是符号数据，两个词只要字面不同，就难以刻画它们之间的联系，即使是“麦克风”和“话筒”这样的同义词，从字面上也难以看出这两者意思相同（语义鸿沟现象），可能并不是简单地一加一那么简单就能表示出来，而判断两个词是否相似时，还需要更多的背景知识才能做出回答。

那么据上是不是可以自信地下一个结论呢：如何有效地表示出语言句子是决定NN能发挥出强大拟合计算能力的关键前提！


二、NLP词的表示方法类型
接下来将按照上面的思路，引出各种词的表示方法。按照现今目前的发展，词的表示分为独热表示one-hot、分布式表示distributed。

1、词的独热表示one-hot representation
NLP 中最直观，也是到目前为止最常用的词表示方法是 One-hot Representation，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。关于one-hot编码的资料很多，街货，这里简单举个栗子说明：

                “话筒”表示为 [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ...]
                “麦克”表示为 [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ...]

每个词都是茫茫 0 海中的一个 1。这种 One-hot Representation 如果采用稀疏方式存储，会是非常的简洁：也就是给每个词分配一个数字 ID。比如刚才的例子中，话筒记为 3，麦克记为 8（假设从 0 开始记）。如果要编程实现的话，用 Hash 表给每个词分配一个编号就可以了。这么简洁的表示方法配合上最大熵、SVM、CRF 等等算法已经很好地完成了 NLP 领域的各种主流任务。

现在我们分析他的不当处。1、向量的维度会随着句子的词的数量类型增大而增大；2、任意两个词之间都是孤立的，根本无法表示出在语义层面上词语词之间的相关信息，而这一点是致命的。

2、词的分布式表示distributed representation
传统的独热表示（ one-hot representation）仅仅将词符号化，不包含任何语义信息。如何将语义融入到词表示中？Harris 在 1954 年提出的分布假说（ distributional hypothesis）为这一设想提供了理论基础：上下文相似的词，其语义也相似。Firth 在 1957 年对分布假说进行了进一步阐述和明确：词的语义由其上下文决定（ a word is characterized by thecompany it keeps）。

到目前为止，基于分布假说的词表示方法，根据建模的不同，主要可以分为三类：基于矩阵的分布表示、基于聚类的分布表示和基于神经网络的分布表示。尽管这些不同的分布表示方法使用了不同的技术手段获取词表示，但由于这些方法均基于分布假说，它们的核心思想也都由两部分组成：一、选择一种方式描述上下文；二、选择一种模型刻画某个词（下文称“目标词”）与其上下文之间的关系。


三、NLP语言模型
在详细介绍词的分布式表示之前，需要将NLP中的一个关键概念描述清楚：语言模型。语言模型包括文法语言模型和统计语言模型。一般我们指的是统计语言模型。之所以要将语言模型摆在词表示方法之前，是因为后面的表示方法马上要用到这一概念。

统计语言模型： 统计语言模型把语言（词的序列）看作一个随机事件，并赋予相应的概率来描述其属于某种语言集合的可能性。给定一个词汇集合 V，对于一个由 V 中的词构成的序列S = ⟨w1, · · · , wT ⟩ ∈ Vn，统计语言模型赋予这个序列一个概率P(S)，来衡量S 符合自然语言的语法和语义规则的置信度。

用一句简单的话说，就语言模型就是计算一个句子的概率大小的这种模型。有什么意义呢？一个句子的打分概率越高，越说明他是更合乎人说出来的自然句子。

就是这么简单。常见的统计语言模型有N元文法模型（N-gram Model），最常见的是unigram model、bigram model、trigram model等等。形式化讲，统计语言模型的作用是为一个长度为 m 的字符串确定一个概率分布 P(w1; w2; :::; wm)，表示其存在的可能性，其中 w1 到 wm 依次表示这段文本中的各个词。一般在实际求解过程中，通常采用下式计算其概率值：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211544519075211.png'/>
 
同时通过这些方法均也可以保留住一定的词序信息，这样就能把一个词的上下文信息capture住。
具体的语言模型详情属于街货，详细请自行搜索。


四、词的分布式表示
1. 基于矩阵的分布表示
基于矩阵的分布表示通常又称为分布语义模型，在这种表示下，矩阵中的一行，就成为了对应词的表示，这种表示描述了该词的上下文的分布。由于分布假说认为上下文相似的词，其语义也相似，因此在这种表示下，两个词的语义相似度可以直接转化为两个向量的空间距离。

常见到的Global Vector 模型（ GloVe模型）是一种对“词-词”矩阵进行分解从而得到词表示的方法，属于基于矩阵的分布表示。

2. 基于聚类的分布表示
基于聚类的分布表示我也还不是太清楚，所以就不做具体描述。

3. 基于神经网络的分布表示，词嵌入（ word embedding）
基于神经网络的分布表示一般称为词向量、词嵌入（ word embedding）或分布式表示（ distributed representation）。这正是我们的主角today。

神经网络词向量表示技术通过神经网络技术对上下文，以及上下文与目标词之间的关系进行建模。由于神经网络较为灵活，这类方法的最大优势在于可以表示复杂的上下文。在前面基于矩阵的分布表示方法中，最常用的上下文是词。如果使用包含词序信息的 n-gram 作为上下文，当 n 增加时， n-gram 的总数会呈指数级增长，此时会遇到维数灾难问题。而神经网络在表示 n-gram 时，可以通过一些组合方式对 n 个词进行组合，参数个数仅以线性速度增长。有了这一优势，神经网络模型可以对更复杂的上下文进行建模，在词向量中包含更丰富的语义信息。


五、词嵌入（ word embedding）
1、概念
基于神经网络的分布表示又称为词向量、词嵌入，神经网络词向量模型与其它分布表示方法一样，均基于分布假说，核心依然是上下文的表示以及上下文与目标词之间的关系的建模。

前面提到过，为了选择一种模型刻画某个词（下文称“目标词”）与其上下文之间的关系，我们需要在词向量中capture到一个词的上下文信息。同时，上面我们恰巧提到了统计语言模型正好具有捕捉上下文信息的能力。那么构建上下文与目标词之间的关系，最自然的一种思路就是使用语言模型。从历史上看，早期的词向量只是神经网络语言模型的副产品。

2001年， Bengio 等人正式提出神经网络语言模型（ Neural Network Language Model ，NNLM），该模型在学习语言模型的同时，也得到了词向量。所以请注意一点：词向量可以认为是神经网络训练语言模型的副产品。

2、理解
前面提过，one-hot表示法具有维度过大的缺点，那么现在将vector做一些改进：
1、将vector每一个元素由整形改为浮点型，变为整个实数范围的表示；
2、将原来稀疏的巨大维度压缩嵌入到一个更小维度的空间。

如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211548737052650.png'/>
 
这也是词向量又名词嵌入的缘由了。


六、神经网络语言模型与word2vec
好了，到目前为止我们已经对的分布式表示以及词嵌入的概念的层级关系有了个理性的认识了，那这跟word2vec有什么联系？

1、神经网络语言模型
上面说，通过神经网络训练语言模型可以得到词向量，那么，究竟有哪些类型的神经网络语言模型呢？个人所知，大致有这么些个：

a) Neural Network Language Model ，NNLM
b) Log-Bilinear Language Model， LBL
c) Recurrent Neural Network based Language Model，RNNLM
d) Collobert 和 Weston 在2008 年提出的 C&W 模型
e) Mikolov 等人提出了 CBOW（ Continuous Bagof-Words）和 Skip-gram 模型

到这，估计有人看到了两个熟悉的term：CBOW、skip-gram，有看过word2vec的同学应该对此有所了解。我们继续。


2.word2vec与CBOW、Skip-gram
现在我们正式引出最火热的另一个term：word2vec。

上面提到的5个神经网络语言模型，只是个在逻辑概念上的东西，那么具体我们得通过设计将其实现出来，而实现CBOW（ Continuous Bagof-Words）和 Skip-gram 语言模型的工具正是well-known word2vec！另外，C&W 模型的实现工具是SENNA。

所以说，分布式词向量并不是word2vec的作者发明的，他只是提出了一种更快更好的方式来训练语言模型罢了。分别是：连续词袋模型Continous Bag of Words Model(CBOW)和Skip-Gram Model，这两种都是可以训练出词向量的方法，再具体代码操作中可以只选择其一，不过据论文说CBOW要更快一些。

顺便说说这两个语言模型。统计语言模型statistical language model就是给你几个词，在这几个词出现的前提下来计算某个词出现的（事后）概率。CBOW也是统计语言模型的一种，顾名思义就是根据某个词前面的C个词或者前后C个连续的词，来计算某个词出现的概率。Skip-Gram Model相反，是根据某个词，然后分别计算它前后出现某几个词的各个概率。

以“我爱北京天安门”这句话为例。假设我们现在关注的词是“爱”，C＝2时它的上下文分别是“我”，“北京天安门”。CBOW模型就是把“我” “北京天安门” 的one hot表示方式作为输入，也就是C个1xV的向量，分别跟同一个VxN的大小的系数矩阵W1相乘得到C个1xN的隐藏层hidden layer，然后C个取平均所以只算一个隐藏层。这个过程也被称为线性激活函数(这也算激活函数？分明就是没有激活函数了)。

然后再跟另一个NxV大小的系数矩阵W2相乘得到1xV的输出层，这个输出层每个元素代表的就是词库里每个词的事后概率。输出层需要跟ground truth也就是“爱”的one hot形式做比较计算loss。

这里需要注意的就是V通常是一个很大的数比如几百万，计算起来相当费时间，除了“爱”那个位置的元素肯定要算在loss里面，word2vec就用基于huffman编码的Hierarchical softmax筛选掉了一部分不可能的词，然后又用nagetive samping再去掉了一些负样本的词所以时间复杂度就从O(V)变成了O(logV)。Skip gram训练过程类似，只不过输入输出刚好相反。

补充下，Word embedding的训练方法大致可以分为两类：一类是无监督或弱监督的预训练；一类是端对端（end to end）的有监督训练。无监督或弱监督的预训练以word2vec和auto-encoder为代表。这一类模型的特点是，不需要大量的人工标记样本就可以得到质量还不错的embedding向量。不过因为缺少了任务导向，可能和我们要解决的问题还有一定的距离。因此，我们往往会在得到预训练的embedding向量后，用少量人工标注的样本去fine-tune整个模型。

相比之下，端对端的有监督模型在最近几年里越来越受到人们的关注。与无监督模型相比，端对端的模型在结构上往往更加复杂。同时，也因为有着明确的任务导向，端对端模型学习到的embedding向量也往往更加准确。例如，通过一个embedding层和若干个卷积层连接而成的深度神经网络以实现对句子的情感分类，可以学习到语义更丰富的词向量表达。

3.个人对word embedding的理解
现在，词向量既能够降低维度，又能够capture到当前词在本句子中上下文的信息（表现为前后距离关系），那么我们对其用来表示语言句子词语作为NN的输入是非常自信与满意的。

另外一点很实用的建议，在你做某一项具体的NLP任务时如你要用到词向量，那么我建议你：
要么1、选择使用别人训练好的词向量，注意，得使用相同语料内容领域的词向量；
要么2、自己训练自己的词向量。我建议是前者，因为……坑太多了。


References：
《How to Generate a Good Word Embedding?》,Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao
《基于神经网络的词和文档语义向量表示方法研究》，来斯惟
《面向自然语言处理的分布式表示学习》，邱锡鹏
《Deep Learning 实战之 word2vec》
http://www.cnblogs.com/iloveai/p/word2vec.html

http://www.hankcs.com/nlp/word2vec.html
http://licstar.net/archives/328
https://zhuanlan.zhihu.com/p/22477976
http://blog.csdn.net/itplus/article/details/37969519
http://www.tuicool.com/articles/fmuyamf
http://licstar.net/archives/620#comment-1542
http://blog.csdn.net/ycheng_sjtu/article/details/48520293
## 如何通俗理解深度学习中的注意力机制
本题解析来源：https://blog.csdn.net/qq_40027052/article/details/78421155

最近两年，注意力模型（Attention Model）被广泛使用在自然语言处理、图像识别及语音识别等各种不同类型的深度学习任务中，是深度学习技术中最值得关注与深入了解的核心技术之一。本文以机器翻译为例，深入浅出地介绍了深度学习中注意力机制的原理及关键计算机制，同时也抽象出其本质思想，并介绍了注意力模型在图像及语音等领域的典型应用场景。

注意力模型最近几年在深度学习各个领域被广泛使用，无论是图像处理、语音识别还是自然语言处理的各种不同类型的任务中，都很容易遇到注意力模型的身影。所以，了解注意力机制的工作原理对于关注深度学习技术发展的技术人员来说有很大的必要。

人类的视觉注意力
从注意力模型的命名方式看，很明显其借鉴了人类的注意力机制，因此，我们首先简单介绍人类视觉的选择性注意力机制。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211742747672925.1'/>
图1 人类的视觉注意力

视觉注意力机制是人类视觉所特有的大脑信号处理机制。人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段，是人类在长期进化中形成的一种生存机制，人类视觉注意力机制极大地提高了视觉信息处理的效率与准确性。

图1形象化展示了人类在看到一副图像时是如何高效分配有限的注意力资源的，其中红色区域表明视觉系统更关注的目标，很明显对于图1所示的场景，人们会把注意力更多投入到人的脸部，文本的标题以及文章首句等位置。

深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。

Encoder-Decoder框架
要了解深度学习中的注意力模型，就不得不先谈Encoder-Decoder框架，因为目前大多数注意力模型附着在Encoder-Decoder框架下，当然，其实注意力模型可以看作一种通用的思想，本身并不依赖于特定框架，这点需要注意。

Encoder-Decoder框架可以看作是一种深度学习领域的研究模式，应用场景异常广泛。图2是文本处理领域里常用的Encoder-Decoder框架最抽象的一种表示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211744751311155.2'/>
图2 抽象的文本处理领域的Encoder-Decoder框架

文本处理领域的Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对< Source,Target >，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。Source和Target可以是同一种语言，也可以是两种不同的语言。而Source和Target分别由各自的单词序列构成：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415521174572929437.3'/>

Encoder顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211747814699010.4'/>

对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息y1,y2……yi-1来生成i时刻要生成的单词yi：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211748555662960.5'/>

每个yi都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target。
如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题的Encoder-Decoder框架；
如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要的Encoder-Decoder框架；
如果Source是一句问句，Target是一句回答，那么这是问答系统或者对话机器人的Encoder-Decoder框架。
由此可见，在文本处理领域，Encoder-Decoder的应用领域相当广泛。

Encoder-Decoder框架不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用。
比如对于语音识别来说，图2所示的框架完全适用，区别无非是Encoder部分的输入是语音流，输出是对应的文本信息；
而对于“图像描述”任务来说，Encoder部分的输入是一副图片，Decoder的输出是能够描述图片语义内容的一句描述语。
一般而言，文本处理和语音识别的Encoder部分通常采用RNN模型，图像处理的Encoder一般采用CNN模型。

Attention模型
本节先以机器翻译作为例子讲解最常见的Soft Attention模型的基本原理，之后抛离Encoder-Decoder框架抽象出了注意力机制的本质思想，然后简单介绍最近广为使用的Self Attention的基本思路。

Soft Attention模型
图2中展示的Encoder-Decoder框架是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Target中每个单词的生成过程如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211749984814036.6'/>

其中f是Decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的，没有任何区别。而语义编码C是由句子Source的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词，y1，y2还是y3，其实句子Source中任意单词对生成某个目标单词yi来说影响力都是相同的，这是为何说这个模型没有体现出注意力的缘由。这类似于人类看到眼前的画面，但是眼中却没有注意焦点一样。

如果拿机器翻译来解释这个分心模型的Encoder-Decoder框架更好理解，比如输入的是英文句子：Tom chase Jerry，Encoder-Decoder框架逐步生成中文单词：“汤姆”，“追逐”，“杰瑞”。在翻译“杰瑞”这个中文单词的时候，分心模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要，但是分心模型是无法体现这一点的，这就是为何说它没有引入注意力的原因。没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。

上面的例子中，如果引入Attention模型的话，应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：

（Tom,0.3）(Chase,0.2) (Jerry,0.5)

每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词yi的时候，原先都是相同的中间语义表示C会被替换成根据当前生成单词而不断变化的Ci。

理解Attention模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。增加了注意力模型的Encoder-Decoder框架理解起来如图3所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415521175116518819.7'/>
图3 引入注意力模型的Encoder-Decoder框架

即生成目标句子单词的过程成了下面的形式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211752485511920.8'/>

而每个Ci可能对应着不同的源语句子单词的注意力分配概率分布，比如对于上面的英汉翻译来说，其对应的信息可能如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211753566542139.9'/>

其中，f2函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值；g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和，即下列公式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211754138153314.10'/>

其中，Lx代表输入句子Source的长度，aij代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数，而hj则是Source输入句子中第j个单词的语义编码。假设Ci下标i就是上面例子所说的“ 汤姆” ，那么Lx就是3，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)分别是输入句子每个单词的语义编码，对应的注意力模型权值则分别是0.6,0.2,0.2，所以g函数本质上就是个加权求和函数。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的中间语义表示Ci的形成过程类似图4。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415521175545482914.11'/>
图4 Attention的形成过程

这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，如何知道Attention模型所需要的输入句子单词注意力分配概率分布值呢？就是说“汤姆”对应的输入句子Source中各个单词的概率分布：(Tom,0.6)(Chase,0.2) (Jerry,0.2) 是如何得到的呢？

为了便于说明，我们假设对图2的非Attention模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图2的框架转换为图5。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211756860639756.12'/>
图5 RNN作为具体模型的Encoder-Decoder框架

那么用图6可以较为便捷地说明注意力分配概率分布值的通用计算过程。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211758070033907.13'/>
图6 注意力分配概率计算

对于采用RNN的Decoder来说，在时刻i，如果要生成yi单词，我们是可以知道Target在生成yi之前的时刻i-1时，隐层节点i-1时刻的输出值Hi-1的，而我们的目的是要计算生成yi时输入句子中的单词“Tom”、“Chase”、“Jerry”对yi来说的注意力分配概率分布，那么可以用Target输出句子i-1时刻的隐层节点状态Hi-1去一一和输入句子Source中每个单词对应的RNN隐层节点状态hj进行对比，即通过函数F(hj,Hi-1)来获得目标单词yi和每个输入单词对应的对齐可能性，这个F函数在不同论文里可能会采取不同的方法，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。绝大多数Attention模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在F的定义上可能有所不同。图7可视化地展示了在英语-德语翻译系统中加入Attention机制后，Source和Target两个句子每个单词对应的注意力分配概率分布。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415521175955491524.14'/>
图7 英语-德语翻译的注意力概率分布

上述内容就是经典的Soft Attention模型的基本思想，那么怎么理解Attention模型的物理含义呢？一般在自然语言处理应用里会把Attention模型看作是输出Target句子中某个单词和输入Source句子每个单词的对齐模型，这是非常有道理的。目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤，而注意力模型其实起的是相同的作用。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211760936424713.15'/>
图8 Google 神经网络机器翻译系统结构图

图8所示即为Google于2016年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了60%，其架构就是上文所述的加上Attention机制的Encoder-Decoder框架，主要区别无非是其Encoder和Decoder使用了8层叠加的LSTM模型。

Attention机制的本质思想
如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211762137451769.16'/>
图9 Attention机制的本质思想

我们可以这样来看待Attention机制（参考图9）：将Source中的构成元素想象成是由一系列的< Key,Value >数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415521176349162958.17'/>

其中， Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。

当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。从图9可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。

至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211765427968810.18'/>
图10 三阶段计算Attention过程

在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个Keyi，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211766341219640.19'/>

第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211766949656168.20'/>

第二阶段的计算结果ai即为Valuei对应的权重系数，然后进行加权求和即可得到Attention数值：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211768186014632.21'/>

通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。

Self Attention模型
通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。Self Attention也经常被称为intra Attention（内部Attention），最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型。在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。而Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。

如果是常规的Target不等于Source情形下的注意力计算，其物理含义正如上文所讲，比如对于机器翻译来说，本质上是目标语单词和源语单词之间的一种单词对齐机制。那么如果是Self Attention机制，一个很自然的问题是：通过Self Attention到底学到了哪些规律或者抽取出了哪些特征呢？或者说引入Self Attention有什么增益或者好处呢？我们仍然以机器翻译中的Self Attention来说明，图11和图12是可视化地表示Self Attention在同一个英语句子内单词间产生的联系。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211769243433697.22'/>
图11 可视化Self Attention实例

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211770048194399.23'/>
图12 可视化Self Attention实例

从两张图（图11、图12）可以看出，Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）。很明显，引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。

Attention机制的应用
前文有述，Attention机制在深度学习的各种应用领域都有广泛的使用场景。上文在介绍过程中我们主要以自然语言处理中的机器翻译任务作为例子，下面分别再从图像处理领域和语音识别选择典型应用实例来对其应用做简单说明。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211771159077828.24'/>
图13 图片-描述任务的Encoder-Decoder框架

图片描述（Image-Caption）是一种典型的图文结合的深度学习应用，输入一张图片，人工智能系统输出一句描述句子，语义等价地描述图片所示内容。很明显这种应用场景也可以使用Encoder-Decoder框架来解决任务目标，此时Encoder输入部分是一张图片，一般会用CNN来对图片进行特征抽取，Decoder部分使用RNN或者LSTM来输出自然语言句子（参考图13）。此时如果加入Attention机制能够明显改善系统输出效果，Attention模型在这里起到了类似人类视觉选择性注意的机制，在输出某个实体单词的时候会将注意力焦点聚焦在图片中相应的区域上。图14给出了根据给定图片生成句子“A person is standing on a beach with a surfboard.”过程时每个单词对应图片中的注意力聚焦区域。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211772324247639.25'/>
图14 图片生成句子中每个单词时的注意力聚焦区域

图15给出了另外四个例子形象地展示了这种过程，每个例子上方左侧是输入的原图，下方句子是人工智能系统自动产生的描述语句，上方右侧图展示了当AI系统产生语句中划横线单词的时候，对应图片中聚焦的位置区域。比如当输出单词dog的时候，AI系统会将注意力更多地分配给图片中小狗对应的位置。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211773028957634.26'/>
图15 图像描述任务中Attention机制的聚焦作用

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211775024135448.27'/>
图16 语音识别中音频序列和输出字符之间的Attention

语音识别的任务目标是将语音流信号转换成文字，所以也是Encoder-Decoder的典型应用场景。Encoder部分的Source输入是语音流信号，Decoder部分输出语音对应的字符串流。图16可视化地展示了在Encoder-Decoder框架中加入Attention机制后，当用户用语音说句子 how much would a woodchuck chuck 时，输入部分的声音特征信号和输出字符之间的注意力分配概率分布情况，颜色越深代表分配到的注意力概率越高。从图中可以看出，在这个场景下，Attention机制起到了将输出字符和输入语音信号进行对齐的功能。

上述内容仅仅选取了不同AI领域的几个典型Attention机制应用实例，Encoder-Decoder加Attention架构由于其卓越的实际效果，目前在深度学习领域里得到了广泛的使用，了解并熟练使用这一架构对于解决实际问题会有极大帮助。
## 请详细说说Transformer （超详细图解，一图胜千言）
本文解析来源：https://blog.csdn.net/longxinchen_ml/article/details/86533005，原英文链接：https://jalammar.github.io/illustrated-transformer/

编者按：前一段时间谷歌推出的BERT模型在11项NLP任务中夺得SOTA结果，引爆了整个NLP界。而BERT取得成功的一个关键因素是Transformer的强大作用。谷歌的Transformer模型最早是用于机器翻译任务，当时达到了SOTA效果。Transformer改进了RNN最被人诟病的训练慢的缺点，利用self-attention机制实现快速并行。并且Transformer可以增加到非常深的深度，充分发掘DNN模型的特性，提升模型准确率。在本文中，我们将研究Transformer模型，把它掰开揉碎，理解它的工作原理。

正文：
Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。

在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。
Attention is All You Need：https://arxiv.org/abs/1706.03762

从宏观的视角开始
首先将这个模型看成是一个黑箱操作。在机器翻译中，就是输入一种语言，输出另一种语言。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845827584775818.png'/>

那么拆开这个黑箱，我们可以看到它是由编码组件、解码组件和它们之间的连接组成。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845828062714956.png'/>

编码组件部分由一堆编码器（encoder）构成（论文中是将6个编码器叠在一起——数字6没有什么神奇之处，你也可以尝试其他数字）。解码组件部分也是由相同数量（与编码器对应）的解码器（decoder）组成的。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845829057658997.png'/>

所有的编码器在结构上都是相同的，但它们没有共享参数。每个解码器都可以分解成两个子层。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845842850978652.jpg'/>

从编码器输入的句子首先会经过一个自注意力（self-attention）层，这层帮助编码器在对每个单词编码时关注输入句子的其他单词。我们将在稍后的文章中更深入地研究自注意力。

自注意力层的输出会传递到前馈（feed-forward）神经网络中。每个位置的单词对应的前馈神经网络都完全一样（译注：另一种解读就是一层窗口为一个单词的一维卷积神经网络）。

解码器中也有编码器的自注意力（self-attention）层和前馈（feed-forward）层。除此之外，这两个层之间还有一个注意力层，用来关注输入句子的相关部分（和seq2seq模型的注意力作用相似）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845844714514848.jpg'/>

将张量引入图景
我们已经了解了模型的主要部分，接下来我们看一下各种向量或张量（译注：张量概念是矢量概念的推广，可以简单理解矢量是一阶张量、矩阵是二阶张量。）是怎样在模型的不同部分中，将输入转化为输出的。

像大部分NLP应用一样，我们首先将每个输入单词通过词嵌入算法转换为词向量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845833046907804.png'/>

每个单词都被嵌入为512维的向量，我们用这些简单的方框来表示这些向量。

词嵌入过程只发生在最底层的编码器中。所有的编码器都有一个相同的特点，即它们接收一个向量列表，列表中的每个向量大小为512维。在底层（最开始）编码器中它就是词向量，但是在其他编码器中，它就是下一层编码器的输出（也是一个向量列表）。向量列表大小是我们可以设置的超参数——一般是我们训练集中最长句子的长度。

将输入序列进行词嵌入之后，每个单词都会流经编码器中的两个子层。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845847673986901.jpg'/>

接下来我们看看Transformer的一个核心特性，在这里输入序列中每个位置的单词都有自己独特的路径流入编码器。在自注意力层中，这些路径之间存在依赖关系。而前馈（feed-forward）层没有这些依赖关系。因此在前馈（feed-forward）层时可以并行执行各种路径。

然后我们将以一个更短的句子为例，看看编码器的每个子层中发生了什么。

现在我们开始“编码”
如上述已经提到的，一个编码器接收向量列表作为输入，接着将向量列表中的向量传递到自注意力层进行处理，然后传递到前馈神经网络层中，将输出结果传递到下一个编码器中。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684584927375001.jpg'/>

输入序列的每个单词都经过自编码过程。然后，他们各自通过前向传播神经网络——完全相同的网络，而每个向量都分别通过它。

从宏观视角看自注意力机制
不要被我用自注意力这个词弄迷糊了，好像每个人都应该熟悉这个概念。其实我之也没有见过这个概念，直到读到Attention is All You Need 这篇论文时才恍然大悟。让我们精炼一下它的工作原理。

例如，下列句子是我们想要翻译的输入句子：

The animal didn’t cross the street because it was too tired

这个“it”在这个句子是指什么呢？它指的是street还是这个animal呢？这对于人类来说是一个简单的问题，但是对于算法则不是。

当模型处理这个单词“it”的时候，自注意力机制会允许“it”与“animal”建立联系。

随着模型处理输入序列的每个单词，自注意力会关注整个输入序列的所有单词，帮助模型对本单词更好地进行编码。

如果你熟悉RNN（循环神经网络），回忆一下它是如何维持隐藏层的。RNN会将它已经处理过的前面的所有单词/向量的表示与它正在处理的当前单词/向量结合起来。而自注意力机制会将所有相关单词的理解融入到我们正在处理的单词中。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845850836579196.png'/>

当我们在编码器#5（栈中最上层编码器）中编码“it”这个单词的时，注意力机制的部分会去关注“The Animal”，将它的表示的一部分编入“it”的编码中。

请务必检查Tensor2Tensor notebook ，在里面你可以下载一个Transformer模型，并用交互式可视化的方式来检验。

从微观视角看自注意力机制
首先我们了解一下如何使用向量来计算自注意力，然后来看它实怎样用矩阵来实现。

计算自注意力的第一步就是从每个编码器的输入向量（每个单词的词向量）中生成三个向量。也就是说对于每个单词，我们创造一个查询向量、一个键向量和一个值向量。这三个向量是通过词嵌入与三个权重矩阵后相乘创建的。

可以发现这些新向量在维度上比词嵌入向量更低。他们的维度是64，而词嵌入和编码器的输入/输出向量的维度是512. 但实际上不强求维度更小，这只是一种基于架构上的选择，它可以使多头注意力（multiheaded attention）的大部分计算保持不变。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845853367578284.jpg'/>

X1与WQ权重矩阵相乘得到q1, 就是与这个单词相关的查询向量。最终使得输入序列的每个单词的创建一个查询向量、一个键向量和一个值向量。

什么是查询向量、键向量和值向量向量？

它们都是有助于计算和理解注意力机制的抽象概念。请继续阅读下文的内容，你就会知道每个向量在计算注意力机制中到底扮演什么样的角色。

计算自注意力的第二步是计算得分。假设我们在为这个例子中的第一个词“Thinking”计算自注意力向量，我们需要拿输入句子中的每个单词对“Thinking”打分。这些分数决定了在编码单词“Thinking”的过程中有多重视句子的其它部分。

这些分数是通过打分单词（所有输入句子的单词）的键向量与“Thinking”的查询向量相点积来计算的。所以如果我们是处理位置最靠前的词的自注意力的话，第一个分数是q1和k1的点积，第二个分数是q1和k2的点积。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845856798161316.jpg'/>

第三步和第四步是将分数除以8(8是论文中使用的键向量的维数64的平方根，这会让梯度更稳定。这里也可以使用其它值，8只是默认值)，然后通过softmax传递结果。softmax的作用是使所有单词的分数归一化，得到的分数都是正值且和为1。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845858096672999.jpg'/>

这个softmax分数决定了每个单词对编码当下位置（“Thinking”）的贡献。显然，已经在这个位置上的单词将获得最高的softmax分数，但有时关注另一个与当前单词相关的单词也会有帮助。

第五步是将每个值向量乘以softmax分数(这是为了准备之后将它们求和)。这里的直觉是希望关注语义上相关的单词，并弱化不相关的单词(例如，让它们乘以0.001这样的小数)。

第六步是对加权值向量求和（译注：自注意力的另一种解释就是在编码某个单词时，就是将所有单词的表示（值向量）进行加权求和，而权重是通过该词的表示（键向量）与被编码词表示（查询向量）的点积并通过softmax得到。），然后即得到自注意力层在该位置的输出(在我们的例子中是对于第一个单词)。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845866637300867.jpg'/>

这样自自注意力的计算就完成了。得到的向量就可以传给前馈神经网络。然而实际中，这些计算是以矩阵形式完成的，以便算得更快。那我们接下来就看看如何用矩阵实现的。

通过矩阵运算实现自注意力机制
第一步是计算查询矩阵、键矩阵和值矩阵。为此，我们将将输入句子的词嵌入装进矩阵X中，将其乘以我们训练的权重矩阵(WQ，WK，WV)。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641568458696369372.png'/>

x矩阵中的每一行对应于输入句子中的一个单词。我们再次看到词嵌入向量 (512，或图中的4个格子)和q/k/v向量(64，或图中的3个格子)的大小差异。

最后，由于我们处理的是矩阵，我们可以将步骤2到步骤6合并为一个公式来计算自注意力层的输出。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845871490011733.png'/>

自注意力的矩阵运算形式

“大战多头怪”
通过增加一种叫做“多头”注意力（“multi-headed” attention）的机制，论文进一步完善了自注意力层，并在两方面提高了注意力层的性能：

1.它扩展了模型专注于不同位置的能力。在上面的例子中，虽然每个编码都在z1中有或多或少的体现，但是它可能被实际的单词本身所支配。如果我们翻译一个句子，比如“The animal didn’t cross the street because it was too tired”，我们会想知道“it”指的是哪个词，这时模型的“多头”注意机制会起到作用。

2.它给出了注意力层的多个“表示子空间”（representation subspaces）。接下来我们将看到，对于“多头”注意机制，我们有多个查询/键/值权重矩阵集(Transformer使用八个注意力头，因此我们对于每个编码器/解码器有八个矩阵集合)。这些集合中的每一个都是随机初始化的，在训练之后，每个集合都被用来将输入词嵌入(或来自较低编码器/解码器的向量)投影到不同的表示子空间中。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684587581642812.jpg'/>

在“多头”注意机制下，我们为每个头保持独立的查询/键/值权重矩阵，从而产生不同的查询/键/值矩阵。和之前一样，我们拿X乘以WQ/WK/WV矩阵来产生查询/键/值矩阵。

如果我们做与上述相同的自注意力计算，只需八次不同的权重矩阵运算，我们就会得到八个不同的Z矩阵。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845876557715677.png'/>

这给我们带来了一点挑战。前馈层不需要8个矩阵，它只需要一个矩阵(由每一个单词的表示向量组成)。所以我们需要一种方法把这八个矩阵压缩成一个矩阵。那该怎么做？其实可以直接把这些矩阵拼接在一起，然后用一个附加的权重矩阵WO与它们相乘。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845878591825568.jpg'/>

这几乎就是多头自注意力的全部。这确实有好多矩阵，我们试着把它们集中在一个图片中，这样可以一眼看清。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845879611656792.jpg'/>

既然我们已经摸到了注意力机制的这么多“头”，那么让我们重温之前的例子，看看我们在例句中编码“it”一词时，不同的注意力“头”集中在哪里：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845880564049405.png'/>

当我们编码“it”一词时，一个注意力头集中在“animal”上，而另一个则集中在“tired”上，从某种意义上说，模型对“it”一词的表达在某种程度上是“animal”和“tired”的代表。

然而，如果我们把所有的attention都加到图示里，事情就更难解释了：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845881721069536.png'/>

使用位置编码表示序列的顺序
到目前为止，我们对模型的描述缺少了一种理解输入单词顺序的方法。

为了解决这个问题，Transformer为每个输入的词嵌入添加了一个向量。这些向量遵循模型学习到的特定模式，这有助于确定每个单词的位置，或序列中不同单词之间的距离。这里的直觉是，将位置向量添加到词嵌入中使得它们在接下来的运算中，能够更好地表达的词与词之间的距离。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845883192685943.jpg'/>

为了让模型理解单词的顺序，我们添加了位置编码向量，这些向量的值遵循特定的模式。

如果我们假设词嵌入的维数为4，则实际的位置编码如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845884574651402.jpg'/>

尺寸为4的迷你词嵌入位置编码实例

这个模式会是什么样子？

在下图中，每一行对应一个词向量的位置编码，所以第一行对应着输入序列的第一个词。每行包含512个值，每个值介于1和-1之间。我们已经对它们进行了颜色编码，所以图案是可见的。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845885532148322.png'/>

20字(行)的位置编码实例，词嵌入大小为512(列)。你可以看到它从中间分裂成两半。这是因为左半部分的值由一个函数(使用正弦)生成，而右半部分由另一个函数(使用余弦)生成。然后将它们拼在一起而得到每一个位置编码向量。

原始论文里描述了位置编码的公式(第3.5节)。你可以在 get_timing_signal_1d()中看到生成位置编码的代码。这不是唯一可能的位置编码方法。然而，它的优点是能够扩展到未知的序列长度(例如，当我们训练出的模型需要翻译远比训练集里的句子更长的句子时)。

残差模块
在继续进行下去之前，我们需要提到一个编码器架构中的细节：在每个编码器中的每个子层（自注意力、前馈网络）的周围都有一个残差连接，并且都跟随着一个“层-归一化”步骤。

层-归一化步骤：https://arxiv.org/abs/1607.06450 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845887369372446.jpg'/>

如果我们去可视化这些向量以及这个和自注意力相关联的层-归一化操作，那么看起来就像下面这张图描述一样：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845889917117177.jpg'/>

解码器的子层也是这样样的。如果我们想象一个2 层编码-解码结构的transformer，它看起来会像下面这张图一样：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845897927214300.jpg'/>

解码组件
既然我们已经谈到了大部分编码器的概念，那么我们基本上也就知道解码器是如何工作的了。但最好还是看看解码器的细节。

编码器通过处理输入序列开启工作。顶端编码器的输出之后会变转化为一个包含向量K（键向量）和V（值向量）的注意力向量集 。这些向量将被每个解码器用于自身的“编码-解码注意力层”，而这些层可以帮助解码器关注输入序列哪些位置合适：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156846894583861613.gif'/>
在完成编码阶段后，则开始解码阶段。解码阶段的每个步骤都会输出一个输出序列（在这个例子里，是英语翻译的句子）的元素。

接下来的步骤重复了这个过程，直到到达一个特殊的终止符号，它表示transformer的解码器已经完成了它的输出。每个步骤的输出在下一个时间步被提供给底端解码器，并且就像编码器之前做的那样，这些解码器会输出它们的解码结果 。另外，就像我们对编码器的输入所做的那样，我们会嵌入并添加位置编码给那些解码器，来表示每个单词的位置。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156846899939997439.gif'/>
而那些解码器中的自注意力层表现的模式与编码器不同：在解码器中，自注意力层只被允许处理输出序列中更靠前的那些位置。在softmax步骤前，它会把后面的位置给隐去（把它们设为-inf）。

这个“编码-解码注意力层”工作方式基本就像多头自注意力层一样，只不过它是通过在它下面的层来创造查询矩阵，并且从编码器的输出中取得键/值矩阵。

最终的线性变换和Softmax层
解码组件最后会输出一个实数向量。我们如何把浮点数变成一个单词？这便是线性变换层要做的工作，它之后就是Softmax层。

线性变换层是一个简单的全连接神经网络，它可以把解码组件产生的向量投射到一个比它大得多的、被称作对数几率（logits）的向量里。

不妨假设我们的模型从训练集中学习一万个不同的英语单词（我们模型的“输出词表”）。因此对数几率向量为一万个单元格长度的向量——每个单元格对应某一个单词的分数。

接下来的Softmax 层便会把那些分数变成概率（都为正数、上限1.0）。概率最高的单元格被选中，并且它对应的单词被作为这个时间步的输出。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845944079222297.jpg'/>

这张图片从底部以解码器组件产生的输出向量开始。之后它会转化出一个输出单词。

训练部分总结
既然我们已经过了一遍完整的transformer的前向传播过程，那我们就可以直观感受一下它的训练过程。

在训练过程中，一个未经训练的模型会通过一个完全一样的前向传播。但因为我们用有标记的训练集来训练它，所以我们可以用它的输出去与真实的输出做比较。

为了把这个流程可视化，不妨假设我们的输出词汇仅仅包含六个单词：“a”, “am”, “i”, “thanks”, “student”以及 “”（end of sentence的缩写形式）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845946021955055.png'/>

我们模型的输出词表在我们训练之前的预处理流程中就被设定好。

一旦我们定义了我们的输出词表，我们可以使用一个相同宽度的向量来表示我们词汇表中的每一个单词。这也被认为是一个one-hot 编码。所以，我们可以用下面这个向量来表示单词“am”：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684595265463151.jpg'/>

例子：对我们输出词表的one-hot 编码

接下来我们讨论模型的损失函数——这是我们用来在训练过程中优化的标准。通过它可以训练得到一个结果尽量准确的模型。

损失函数
比如说我们正在训练模型，现在是第一步，一个简单的例子——把“merci”翻译为“thanks”。

这意味着我们想要一个表示单词“thanks”概率分布的输出。但是因为这个模型还没被训练好，所以不太可能现在就出现这个结果。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845954355490127.jpg'/>

因为模型的参数（权重）都被随机的生成，（未经训练的）模型产生的概率分布在每个单元格/单词里都赋予了随机的数值。我们可以用真实的输出来比较它，然后用反向传播算法来略微调整所有模型的权重，生成更接近结果的输出。

你会如何比较两个概率分布呢？我们可以简单地用其中一个减去另一个。更多细节请参考交叉熵和KL散度。

交叉熵：https://colah.github.io/posts/2015-09-Visual-Information/
KL散度：https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained

但注意到这是一个过于简化的例子。更现实的情况是处理一个句子。例如，输入“je suis étudiant”并期望输出是“i am a student”。那我们就希望我们的模型能够成功地在这些情况下输出概率分布：

每个概率分布被一个以词表大小（我们的例子里是6，但现实情况通常是3000或10000）为宽度的向量所代表。

第一个概率分布在与“i”关联的单元格有最高的概率

第二个概率分布在与“am”关联的单元格有最高的概率

以此类推，第五个输出的分布表示“”关联的单元格有最高的概率
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845957766480817.jpg'/>

依据例子训练模型得到的目标概率分布

在一个足够大的数据集上充分训练后，我们希望模型输出的概率分布看起来像这个样子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845959033771441.jpg'/>

我们期望训练过后，模型会输出正确的翻译。当然如果这段话完全来自训练集，它并不是一个很好的评估指标（参考：交叉验证，链接https://www.youtube.com/watch?v=TIgfjmp-4BA）。注意到每个位置（词）都得到了一点概率，即使它不太可能成为那个时间步的输出——这是softmax的一个很有用的性质，它可以帮助模型训练。

因为这个模型一次只产生一个输出，不妨假设这个模型只选择概率最高的单词，并把剩下的词抛弃。这是其中一种方法（叫贪心解码）。另一个完成这个任务的方法是留住概率最靠高的两个单词（例如I和a），那么在下一步里，跑模型两次：其中一次假设第一个位置输出是单词“I”，而另一次假设第一个位置输出是单词“me”，并且无论哪个版本产生更少的误差，都保留概率最高的两个翻译结果。然后我们为第二和第三个位置重复这一步骤。这个方法被称作集束搜索（beam search）。在我们的例子中，集束宽度是2（因为保留了2个集束的结果，如第一和第二个位置），并且最终也返回两个集束的结果（top_beams也是2）。这些都是可以提前设定的参数。

再进一步
我希望通过上文已经让你们了解到Transformer的主要概念了。如果你想在这个领域深入，我建议可以走以下几步：阅读Attention Is All You Need，Transformer博客和Tensor2Tensor announcement，以及看看Łukasz Kaiser的介绍，了解模型和细节。

Attention Is All You Need：https://arxiv.org/abs/1706.03762
Transformer博客：https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html
Tensor2Tensor announcement：https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html
Łukasz Kaiser的介绍：https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb

接下来可以研究的工作：
Depthwise Separable Convolutions for Neural Machine Translation
https://arxiv.org/abs/1706.03059

One Model To Learn Them All
https://arxiv.org/abs/1706.05137

Discrete Autoencoders for Sequence Models
https://arxiv.org/abs/1801.09797

Generating Wikipedia by Summarizing Long Sequences
https://arxiv.org/abs/1801.10198

Image Transformer
https://arxiv.org/abs/1802.05751

Training Tips for the Transformer Model
https://arxiv.org/abs/1804.00247

Self-Attention with Relative Position Representations
https://arxiv.org/abs/1803.02155

Fast Decoding in Sequence Models using Discrete Latent Variables
https://arxiv.org/abs/1803.03382

Adafactor: Adaptive Learning Rates with Sublinear Memory Cost
https://arxiv.org/abs/1804.04235
## 如何通俗理解LDA主题模型
本题解析来源于July在CSDN上超过20万阅读量的LDA笔记《通俗理解LDA主题模型》，特原封不动的转至于此。


0 前言
    印象中，最开始听说“LDA”这个名词，是缘于rickjin在2013年3月写的一个LDA科普系列，叫LDA数学八卦，我当时一直想看来着，记得还打印过一次，但不知是因为这篇文档的前序铺垫太长（现在才意识到这些“铺垫”都是深刻理解LDA 的基础，但如果没有人帮助初学者提纲挈领、把握主次、理清思路，则很容易陷入LDA的细枝末节之中），还是因为其中的数学推导细节太多，导致一直没有完整看完过。

    2013年12月，在我组织的Machine Learning读书会第8期上，@夏粉_百度 讲机器学习中排序学习的理论和算法研究，@沈醉2011 则讲主题模型的理解。又一次碰到了主题模型，当时貌似只记得沈博讲了一个汪峰写歌词的例子，依然没有理解LDA到底是怎样一个东西（但理解了LDA之后，再看沈博主题模型的PPT会很赞）。

    直到昨日下午，机器学习班 第12次课上，邹讲完LDA之后，才真正明白LDA原来是那么一个东东！上完课后，趁热打铁，再次看LDA数学八卦，发现以前看不下去的文档再看时竟然一路都比较顺畅，一口气看完大部。看完大部后，思路清晰了，知道理解LDA，可以分为下述5个步骤：

一个函数：gamma函数
四个分布：二项分布、多项分布、beta分布、Dirichlet分布
一个概念和一个理念：共轭先验和贝叶斯框架
两个模型：pLSA、LDA（在本文第4 部分阐述）
一个采样：Gibbs采样
    本文便按照上述5个步骤来阐述，希望读者看完本文后，能对LDA有个尽量清晰完整的了解。同时，本文基于邹讲LDA的PPT、rickjin的LDA数学八卦及其它参考资料写就，可以定义为一篇学习笔记或课程笔记，当然，后续不断加入了很多自己的理解。若有任何问题，欢迎随时于本文评论下指出，thanks。


1 gamma函数
1.0 整体把握LDA
    关于LDA有两种含义，一种是线性判别分析（Linear Discriminant Analysis），一种是概率主题模型：隐含狄利克雷分布（Latent Dirichlet Allocation，简称LDA），本文讲后者。

    另外，我先简单说下LDA的整体思想，不然我怕你看了半天，铺了太长的前奏，却依然因没见到LDA的影子而显得“心浮气躁”，导致不想再继续看下去。所以，先给你吃一颗定心丸，明白整体框架后，咱们再一步步抽丝剥茧，展开来论述。

    按照wiki上的介绍，LDA由Blei, David M.、Ng, Andrew Y.、Jordan于2003年提出，是一种主题模型，它可以将文档集 中每篇文档的主题以概率分布的形式给出，从而通过分析一些文档抽取出它们的主题（分布）出来后，便可以根据主题（分布）进行主题聚类或文本分类。同时，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的关系。

    此外，一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。

    人类是怎么生成文档的呢？LDA的这三位作者在原始论文中给了一个简单的例子。比如假设事先给定了这几个主题：Arts、Budgets、Children、Education，然后通过学习训练，获取每个主题Topic对应的词语。如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223030724224382.1'/>

    然后以一定的概率选取上述某个主题，再以一定的概率选取那个主题下的某个单词，不断的重复这两步，最终生成如下图所示的一篇文章（其中不同颜色的词语分别对应上图中不同主题下的词）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223031682919494.2'/>

    而当我们看到一篇文章后，往往喜欢推测这篇文章是如何生成的，我们可能会认为作者先确定这篇文章的几个主题，然后围绕这几个主题遣词造句，表达成文。

    LDA就是要干这事：根据给定的一篇文档，反推其主题分布。

    通俗来说，可以假定认为人类是根据上述文档生成过程写成了各种各样的文章，现在某小撮人想让计算机利用LDA干一件事：你计算机给我推测分析网络上各篇文章分别都写了些啥主题，且各篇文章中各个主题出现的概率大小（主题分布）是啥。

    然，就是这么一个看似普通的LDA，一度吓退了不少想深入探究其内部原理的初学者。难在哪呢，难就难在LDA内部涉及到的数学知识点太多了。

    在LDA模型中，一篇文档生成的方式如下：

    ①从狄利克雷分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223041580429184.3'/>中取样生成文档 i 的主题分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223042353367220.4'/>
    ②从主题的多项式分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223043433138956.5'/>中取样生成文档i第 j 个词的主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223044046358704.6'/>
    ③从狄利克雷分布中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223047393316093.7'/>取样生成主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223048937470750.8'/>对应的词语分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223049586264826.9'/>
    ④从词语的多项式分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223050433074241.10'/>中采样最终生成词语<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223050950540711.11'/>

    其中，类似Beta分布是二项式分布的共轭先验概率分布，而狄利克雷分布（Dirichlet分布）是多项式分布的共轭先验概率分布。

    此外，LDA的图模型结构如下图所示（类似贝叶斯网络结构）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223057373957026.12'/>

    恩，不错，短短6句话整体概括了整个LDA的主体思想！但也就是上面短短6句话，却接连不断或重复出现了二项分布、多项式分布、beta分布、狄利克雷分布（Dirichlet分布）、共轭先验概率分布、取样，那么请问，这些都是啥呢？

    这里先简单解释下二项分布、多项分布、beta分布、Dirichlet 分布这4个分布。

 二项分布（Binomial distribution）。
    二项分布是从伯努利分布推进的。伯努利分布，又称两点分布或0-1分布，是一个离散型的随机分布，其中的随机变量只有两类取值，非正即负{+，-}。而二项分布即重复n次的伯努利试验，记为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415522306648555491.13'/>。简言之，只做一次实验，是伯努利分布，重复做了n次，是二项分布。二项分布的概率密度函数为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223067794562016.14'/>

    对于k = 0, 1, 2, ..., n，其中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223069542303939.15'/>是二项式系数（这就是二项分布的名称的由来），又记为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223070680695592.16'/>。回想起高中所学的那丁点概率知识了么：想必你当年一定死记过这个二项式系数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415522307199634074.17'/>就是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415522307255987551.18'/>。

多项分布，是二项分布扩展到多维的情况。
    多项分布是指单次试验中的随机变量的取值不再是0-1的，而是有多种离散值可能（1,2,3...,k）。比如投掷6个面的骰子实验，N次实验结果服从K=6的多项分布。其中
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223074876480664.19'/>

    多项分布的概率密度函数为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223075581464852.20'/>

Beta分布，二项分布的共轭先验分布。
    给定参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415522307904324346.21'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223080157054835.22'/>，取值范围为[0,1]的随机变量 x 的概率密度函数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415522308114994355.23'/>

    其中：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223082621142936.24'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223083618227062.25'/>

   注：便是所谓的gamma函数，下文会具体阐述。

Dirichlet分布，是beta分布在高维度上的推广。
    Dirichlet分布的的密度函数形式跟beta分布的密度函数如出一辙：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223112271512633.31'/>

    其中
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223112796786823.32'/>

    至此，我们可以看到二项分布和多项分布很相似，Beta分布和Dirichlet 分布很相似，而至于“Beta分布是二项式分布的共轭先验概率分布，而狄利克雷分布（Dirichlet分布）是多项式分布的共轭先验概率分布”这点在下文中说明。

    OK，接下来，咱们就按照本文开头所说的思路：“一个函数：gamma函数，四个分布：二项分布、多项分布、beta分布、Dirichlet分布，外加一个概念和一个理念：共轭先验和贝叶斯框架，两个模型：pLSA、LDA（文档-主题，主题-词语），一个采样：Gibbs采样”一步步详细阐述，争取给读者一个尽量清晰完整的LDA。

    （当然，如果你不想深究背后的细节原理，只想整体把握LDA的主体思想，可直接跳到本文第4 部分，看完第4部分后，若还是想深究背后的细节原理，可再回到此处开始看）

1.1 gamma函数
    咱们先来考虑一个问题（此问题1包括下文的问题2-问题4皆取材自LDA数学八卦）：

问题1 随机变量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223090075485218.26'/>
把这n 个随机变量排序后得到顺序统计量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223090967158461.27'/>
然后请问<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223091925303405.28'/>的分布是什么。

    为解决这个问题，可以尝试计算<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223093225086067.29'/>落在区间[x,x+Δx]的概率。即求下述式子的值：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223095916322370.30'/>

    首先，把 [0,1] 区间分成三段 [0,x)，[x,x+Δx]，(x+Δx,1]，然后考虑下简单的情形：即假设n 个数中只有1个落在了区间 [x,x+Δx]内，由于这个区间内的数X(k)是第k大的，所以[0,x)中应该有 k−1 个数，(x+Δx,1] 这个区间中应该有n−k 个数。如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223121355760865.33'/>

    从而问题转换为下述事件E：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223122063787678.34'/>

    对于上述事件E，有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223122755489443.35'/>

    其中，o(Δx)表示Δx的高阶无穷小。显然，由于不同的排列组合，即n个数中有一个落在 [x,x+Δx]区间的有n种取法，余下n−1个数中有k−1个落在[0,x)的有<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223127124703337.36'/>种组合，所以和事件E等价的事件一共有<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223127986924382.37'/>个。

    如果有2个数落在区间[x,x+Δx]呢？如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223128658913270.38'/>

    类似于事件E，对于2个数落在区间[x,x+Δx]的事件E’：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223130769986750.39'/>

    有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223131599874421.40'/>

   从上述的事件E、事件E‘中，可以看出，只要落在[x,x+Δx]内的数字超过一个，则对应的事件的概率就是 o(Δx)。于是乎有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223149499756040.41'/>

    从而得到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223153365411196.42'/>的概率密度函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223154232926834.43'/>为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641552231548865276.44'/>

    至此，本节开头提出的问题得到解决。然仔细观察<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223157626755370.45'/>的概率密度函数，发现式子的最终结果有阶乘，联想到阶乘在实数上的推广函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223159439752741.46'/>：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223160618449295.47'/>

    两者结合是否会产生奇妙的效果呢？考虑到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223163335912349.48'/>具有如下性质：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223164012353331.49'/>

    故将<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223166328509447.50'/>代入到的概率密度函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223167028426475.51'/>中，可得：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223168292466822.52'/>

    然后取<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223171484240359.53'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223172045156108.54'/>，转换<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223172857689698.55'/>得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223173788644765.56'/>

    如果熟悉beta分布的朋友，可能会惊呼：哇，竟然推出了beta分布！


2 beta分布
2.1 beta分布
    在概率论中，beta是指一组定义在（0,1）区间的连续概率分布，有两个参数α和β，且α,β＞0。

    beta分布的概率密度函数是：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231207547653659.1'/>

  其中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231204010221212.3'/>便是函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231204560600459.4'/>：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231214890576308.1'/>

随机变量X服从参数为的beta分布通常写作：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229714554930785.png'/>。

2.2 Beta-Binomial 共轭
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229718746243542.png'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231219212539276.2'/>

熟悉贝叶斯方法（不熟悉的没事，参见此文第一部分）的朋友心里估计又犯“嘀咕”了，这不就是贝叶斯式的思考过程么？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229722237463745.png'/>

  回顾下贝叶斯派思考问题的固定模式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229725615314415.png'/>

上述思考模式意味着，新观察到的样本信息将修正人们以前对事物的认知。换言之，在得到新的样本信息之前，人们对的认知是先验分布π(θ)，在得到新的样本信息X后，人们对θ的认知为π(θ|X)。

    类比到现在这个问题上，我们也可以试着写下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415523122275489906.3'/>

   其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231227566978481.4'/>对应的是二项分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231228693653066.5'/>的计数。

    更一般的，对于非负实数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231229667562070.6'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231230168992630.7'/>，我们有如下关系
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231231826118016.8'/>

    针对于这种观测到的数据符合二项分布，参数的先验分布和后验分布都是Beta分布的情况，就是Beta-Binomial共轭。换言之，Beta分布是二项式分布的共轭先验概率分布。

    二项分布和Beta分布是共轭分布意味着，如果我们为二项分布的参数p选取的先验分布是Beta分布，那么以p为参数的二项分布用贝叶斯估计得到的后验分布仍然服从Beta分布。

    此外，如何理解参数α和β所表达的意义呢？α、β可以认为形状参数，通俗但不严格的理解是，α和β共同控制Beta分布的函数“长的样子”：形状千奇百怪，高低胖瘦，如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229756384798734.png'/>


2.3 共轭先验分布
    什么又是共轭呢？轭的意思是束缚、控制，共轭从字面上理解，则是共同约束，或互相约束。

    在贝叶斯概率理论中，如果后验概率P(θ|x)和先验概率p(θ)满足同样的分布律，那么，先验分布和后验分布被叫做共轭分布，同时，先验分布叫做似然函数的共轭先验分布。

    比如，某观测数据服从概率分布P(θ)时，当观测到新的X数据时，我们一般会遇到如下问题：

·可否根据新观测数据X，更新参数θ？
·根据新观测数据可以在多大程度上改变参数θ，即
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231235822904796.9'/>

·当重新估计θ的时候，给出新参数值θ的新概率分布，即P(θ|x)。
    事实上，根据根据贝叶斯公式可知：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231236453266717.10'/>

      其中，P(x|θ)表示以预估θ为参数的x概率分布，可以直接求得，P(θ)是已有原始的θ概率分布。
    所以，如果我们选取P(x|θ)的共轭先验作为P(θ)的分布，那么P(x|θ)乘以P(θ)，然后归一化的结果P(θ|x)跟和P(θ)的形式一样。换句话说，先验分布是P(θ)，后验分布是P(θ|x)，先验分布跟后验分布同属于一个分布族，故称该分布族是θ的共轭先验分布（族）。

    举个例子。投掷一个非均匀硬币，可以使用参数为θ的伯努利模型，θ为硬币为正面的概率，那么结果x的分布形式为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415523123964620658.11'/>

其共轭先验为beta分布，具有两个参数α和β，称为超参数（hyperparameters）。且这两个参数决定了θ参数，其Beta分布形式为
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231241271314136.12'/>

然后计算后验概率
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231242356335304.13'/>

归一化这个等式后会得到另一个Beta分布，从而证明了Beta分布确实是伯努利分布的共轭先验分布。

2.4 从beta分布推广到Dirichlet 分布
    接下来，咱们来考察beta分布的一个性质。

    如果<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229779552176762.png'/>，则有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231263691991014.14'/>

注意到上式最后结果的右边积分
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231264996810441.15'/>

   其类似于概率分布，而对于这个分布有
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231267979918048.16'/>

    从而求得
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231299175015654.21'/>

    的结果为
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231299718546134.22'/>

 最后将此结果带入E（P）的计算式，得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231300527577258.23'/>

最后的这个结果意味着对于Beta 分布的随机变量，其均值（期望）可以用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231311169628707.24'/>来估计。此外，狄利克雷Dirichlet 分布也有类似的结论，即如果<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231312235227334.25'/>，同样可以证明有下述结论成立：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231312887026070.26'/>

那什么是Dirichlet 分布呢？简单的理解Dirichlet 分布就是一组连续多变量概率分布，是多变量普遍化的beta分布。为了纪念德国数学家约翰·彼得·古斯塔夫·勒热纳·狄利克雷（Peter Gustav Lejeune Dirichlet）而命名。狄利克雷分布常作为贝叶斯统计的先验概率。


3 Dirichlet 分布
3.1 Dirichlet 分布
    根据wikipedia上的介绍，维度K ≥ 2（x1,x2…xK-1维，共K个）的狄利克雷分布在参数α1, ..., αK > 0上、基于欧几里得空间RK-1里的勒贝格测度有个概率密度函数，定义为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240414332330366.27'/>

其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240419717553514.30'/>相当于是多项beta函数
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641552404182645672.29'/>

    且<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240422584074653.31'/>

    此外，x1+x2+…+xK-1+xK=1，x1,x2…xK-1>0，且在(K-1)维的单纯形上，其他区域的概率密度为0。
    当然，也可以如下定义Dirichlet 分布
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240427272704404.33'/>

    其中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240429788762649.34'/>称为Dirichlet 分布的归一化系数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240431290405971.35'/>

   且根据Dirichlet分布的积分为1（概率的基本性质），可以得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240433938538250.36'/>

3.2 Dirichlet-Multinomial 共轭
    下面，在2.2节问题2的基础上继续深入，引出问题3。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265255074788118.1'/>，
排序后对应的顺序统计量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265256651524124.2'/>，
问<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265257489886146.3'/>的联合分布是什么？

    为了简化计算，取x3满足x1+x2+x3=1,但只有x1,x2是变量，如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265261057547216.4'/>

    从而有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265262453871123.5'/>

    于是我们得到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265267979274685.6'/>的联合分布为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415526526886519885.7'/>

    观察上述式子的最终结果，可以看出上面这个分布其实就是3维形式的 Dirichlet 分布
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265272189207353.8'/>

    令<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265272917507727.9'/>，于是分布密度可以写为
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265277366683357.10'/>

    这个就是一般形式的3维 Dirichlet 分布，即便<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265290079110244.11'/>延拓到非负实数集合，以上概率分布也是良定义的。

    将Dirichlet分布的概率密度函数取对数，绘制对称Dirichlet分布的图像如下图所示（截取自wikipedia上）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265290955067777.gif'/>

    上图中，取K=3，也就是有两个独立参数x1,x2，分别对应图中的两个坐标轴，第三个参数始终满足x3=1-x1-x2且α1=α2=α3=α，图中反映的是参数α从α=(0.3, 0.3, 0.3)变化到(2.0, 2.0, 2.0)时的概率对数值的变化情况。

    为了论证Dirichlet分布是多项式分布的共轭先验概率分布，下面咱们继续在上述问题3的基础上再进一步，提出问题4。

① 问题4 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265317275584434.13'/>，排序后对应的顺序统计量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265318230535687.14'/>
② 令<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265319252084598.15'/>,<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265319879551800.16'/>,③<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265320814461814.17'/>（此处的p3非变量，只是为了表达方便），现在要猜测<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265321669607942.18'/>；
③<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265332219024969.19'/>，Yi中落到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265333267422880.20'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265333853453858.21'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265335340907058.22'/>三个区间的个数分别为 m1,m2,m3，m=m1+m2+m3；
④问后验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265338581905745.23'/>的分布是什么。

   为了方便讨论，记<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265346181281799.24'/>，及<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265348350761128.25'/>，根据已知条件“<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265350124385705.26'/>，Yi中落到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265351269767948.27'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265351725235046.28'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265352645486574.29'/>三个区间的个数分别为 m1,m2”，可得<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265358817258273.30'/>、<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415526535953785854.31'/>分别是这m+n个数中第<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265360948108935.32'/>大、第<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265361659953478.33'/>大的数。于是，后验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265362898411793.34'/>应该为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265363849875110.35'/>，即一般化的形式表示为：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265364434267239.36'/>。

    同样的，按照贝叶斯推理的逻辑，可将上述过程整理如下：

①我们要猜测参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265390663085151.1'/>，其先验分布为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265391457469294.2'/>；
②数据Yi落到三个区间<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265392382060271.3'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265393093165149.4'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265393692361971.5'/>的个数分别为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265400758288001.6'/>，所以<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415526540295154371.7'/>服从多项分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265405353911281.8'/>
③在给定了来自数据提供的知识<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415526540662937979.9'/>后，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265407534466969.10'/>的后验分布变为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265408067825039.11'/>

    上述贝叶斯分析过程的直观表述为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265412656626144.12'/>

    令<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265415634287599.13'/>，可把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265416576979635.14'/>从整数集合延拓到实数集合，从而得到更一般的表达式如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265418255923943.15'/>

    针对于这种观测到的数据符合多项分布，参数的先验分布和后验分布都是Dirichlet 分布的情况，就是Dirichlet-Multinomial 共轭。换言之，至此已经证明了Dirichlet分布的确就是多项式分布的共轭先验概率分布。

    意味着，如果我们为多项分布的参数p选取的先验分布是Dirichlet分布，那么以p为参数的多项分布用贝叶斯估计得到的后验分布仍然服从Dirichlet分布。

    进一步，一般形式的Dirichlet 分布定义如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265423696979493.16'/>

    而对于给定的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265424520230749.17'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265430564190441.18'/>，其多项分布为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265429665098453.19'/>

    结论是：Dirichlet分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265429077652257.20'/>和多项分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265428574395056.21'/>是共轭关系。

4 主题模型LDA
 在开始下面的旅程之前，先来总结下我们目前所得到的最主要的几个收获：
通过上文的第2.2节，我们知道beta分布是二项式分布的共轭先验概率分布：
“对于非负实数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535172306510478.jpg'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351721212669411.png'/>，我们有如下关系
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351725130307553.png'/>
其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351776279339979.png'/>对应的是二项分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351777836965389.png'/>的计数。针对于这种观测到的数据符合二项分布，参数的先验分布和后验分布都是Beta分布的情况，就是Beta-Binomial 共轭。”

通过上文的3.2节，我们知道狄利克雷分布（Dirichlet分布）是多项式分布的共轭先验概率分布：
“ 把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351782973597967.png'/>从整数集合延拓到实数集合，从而得到更一般的表达式如下：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351786241796470.png'/>
 针对于这种观测到的数据符合多项分布，参数的先验分布和后验分布都是Dirichlet 分布的情况，就是 Dirichlet-Multinomial 共轭。 ”

以及贝叶斯派思考问题的固定模式：
先验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351796643995114.png'/>+ 样本信息<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351798095493072.png'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351798691125902.png'/>后验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351799854779787.png'/>

    上述思考模式意味着，新观察到的样本信息将修正人们以前对事物的认知。换言之，在得到新的样本信息之前，人们对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351804248040092.png'/>的认知是先验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351805677607437.png'/>，在得到新的样本信息<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351806970884361.png'/>后，人们对的认知为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351807977911931.png'/>。
顺便提下频率派与贝叶斯派各自不同的思考方式：
频率派把需要推断的参数θ看做是固定的未知常数，即概率虽然是未知的，但最起码是确定的一个值，同时，样本X 是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本X 的分布；
而贝叶斯派的观点则截然相反，他们认为待估计的参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351831867203184.png'/>是随机变量，服从一定的分布，而样本X 是固定的，由于样本是固定的，所以他们重点研究的是参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351834420951277.png'/>的分布。

  OK，在杀到终极boss——LDA模型之前，再循序渐进理解基础模型：Unigram model、mixture of unigrams model，以及跟LDA最为接近的pLSA模型。

   为了方便描述，首先定义一些变量：
1. <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535185486898741.png'/>表示词，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351858832767877.png'/>表示所有单词的个数（固定值）
2 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351862485749542.png'/>表示主题，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351865867996166.png'/>是主题的个数（预先给定，固定值）
3 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351869938370218.png'/>表示语料库，其中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535187439814166.png'/>是语料库中的文档数（固定值）
4 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351878044686506.png'/>表示文档，其中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351885884888045.png'/>表示一个文档中的词数（随机变量）

4.1 各个基础模型
4.1.1 Unigram model
对于文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415536018804693961.png'/>，用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351898332010957.png'/>表示词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351901293720939.png'/>的先验概率，生成文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351908620697437.png'/>的概率为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351912492396105.png'/>
 
其图模型为（图中被涂色的w表示可观测变量，N表示一篇文档中总共N个单词，M表示M篇文档）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351917291347195.png'/>
 
 或为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351920664655666.png'/>
 
unigram model假设文本中的词服从Multinomial分布，而我们已经知道Multinomial分布的先验分布为Dirichlet分布。
 上图中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351925688181359.png'/>表示在文本中观察到的第n个词，n∈[1,N]表示该文本中一共有N个单词。加上方框表示重复，即一共有N个这样的随机变量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351932891807587.png'/>。其中，p和α是隐含未知变量：

p是词服从的Multinomial分布的参数
α是Dirichlet分布（即Multinomial分布的先验分布）的参数。

一般α由经验事先给定，p由观察到的文本中出现的词学习得到，表示文本中出现每个词的概率。

4.1.2 Mixture of unigrams model
    该模型的生成过程是：给某个文档先选择一个主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351952988658030.png'/>,再根据该主题生成文档，该文档中的所有词都来自一个主题。假设主题有<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351960621703324.png'/>，生成文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351965060615535.png'/>的概率为：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351970337977230.png'/>

其图模型为（图中被涂色的w表示可观测变量，未被涂色的z表示未知的隐变量，N表示一篇文档中总共N个单词，M表示M篇文档）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351974151443443.png'/>

4.2 PLSA模型
    啊哈，长征两万五，经过前面这么长的铺垫，终于快要接近LDA模型了！因为跟LDA模型最为接近的便是下面要阐述的这个pLSA模型，理解了pLSA模型后，到LDA模型也就一步之遥——给pLSA加上贝叶斯框架，便是LDA。

4.2.1 pLSA模型下生成文档
    OK，在上面的Mixture of unigrams model中，我们假定一篇文档只有一个主题生成，可实际中，一篇文章往往有多个主题，只是这多个主题各自在文档中出现的概率大小不一样。比如介绍一个国家的文档中，往往会分别从教育、经济、交通等多个主题进行介绍。那么在pLSA中，文档是怎样被生成的呢？

    假设你要写M篇文档，由于一篇文档由各个不同的词组成，所以你需要确定每篇文档里每个位置上的词。

    再假定你一共有K个可选的主题，有V个可选的词，咱们来玩一个扔骰子的游戏。

1. 假设你每写一篇文档会制作一颗K面的“文档-主题”骰子（扔此骰子能得到K个主题中的任意一个），和K个V面的“主题-词项” 骰子（每个骰子对应一个主题，K个骰子对应之前的K个主题，且骰子的每一面对应要选择的词项，V个面对应着V个可选的词）。

比如可令K=3，即制作1个含有3个主题的“文档-主题”骰子，这3个主题可以是：教育、经济、交通。然后令V = 3，制作3个有着3面的“主题-词项”骰子，其中，教育主题骰子的3个面上的词可以是：大学、老师、课程，经济主题骰子的3个面上的词可以是：市场、企业、金融，交通主题骰子的3个面上的词可以是：高铁、汽车、飞机。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351982937840324.png'/>

2. 每写一个词，先扔该“文档-主题”骰子选择主题，得到主题的结果后，使用和主题结果对应的那颗“主题-词项”骰子，扔该骰子选择要写的词。

 先扔“文档-主题”的骰子，假设（以一定的概率）得到的主题是教育，所以下一步便是扔教育主题筛子，（以一定的概率）得到教育主题筛子对应的某个词：大学。

 上面这个投骰子产生词的过程简化下便是：“先以一定的概率选取主题，再以一定的概率选取词”。事实上，一开始可供选择的主题有3个：教育、经济、交通，那为何偏偏选取教育这个主题呢？其实是随机选取的，只是这个随机遵循一定的概率分布。比如可能选取教育主题的概率是0.5，选取经济主题的概率是0.3，选取交通主题的概率是0.2，那么这3个主题的概率分布便是{教育：0.5，经济：0.3，交通：0.2}，我们把各个主题z在文档d中出现的概率分布称之为主题分布，且是一个多项分布。

同样的，从主题分布中随机抽取出教育主题后，依然面对着3个词：大学、老师、课程，这3个词都可能被选中，但它们被选中的概率也是不一样的。比如大学这个词被选中的概率是0.5，老师这个词被选中的概率是0.3，课程被选中的概率是0.2，那么这3个词的概率分布便是{大学：0.5，老师：0.3，课程：0.2}，我们把各个词语w在主题z下出现的概率分布称之为词分布，这个词分布也是一个多项分布。

所以，选主题和选词都是两个随机的过程，先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后从该教育主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。

3. 最后，你不停的重复扔“文档-主题”骰子和”主题-词项“骰子，重复N次（产生N个词），完成一篇文档，重复这产生一篇文档的方法M次，则完成M篇文档。
    
上述过程抽象出来即是PLSA的文档生成模型。在这个过程中，我们并未关注词和词之间的出现顺序，所以pLSA是一种词袋方法。具体说来，该模型假设一组共现(co-occurrence)词项关联着一个隐含的主题类别<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351998213382297.png'/>。同时定义：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352002150071997.png'/>表示海量文档中某篇文档被选中的概率。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352006183724999.png'/>表示词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352009676765455.png'/>在给定文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352013258884194.png'/>中出现的概率。

怎么计算得到呢？针对海量文档，对所有文档进行分词后，得到一个词汇列表，这样每篇文档就是一个词语的集合。对于每个词语，用它在文档中出现的次数除以文档中词语总的数目便是它在文档中出现的概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352019545721319.png'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352024270338597.png'/>表示具体某个主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352029443891821.png'/>在给定文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352033453911092.png'/>下出现的概率。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352039853393421.png'/>表示具体某个词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352043684085659.png'/>在给定主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352047799253493.png'/>下出现的概率，与主题关系越密切的词，其条件概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535205203254211.png'/>越大。
 
利用上述的第1、3、4个概率，我们便可以按照如下的步骤得到“文档-词项”的生成模型：

1.按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352062899758278.png'/>选择一篇文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352064564521102.png'/>
2.选定文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535206908447580.png'/>后，从主题分布中按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155360862879631619.png'/>选择一个隐含的主题类别<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352074435875593.png'/>
3.选定<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352077286470568.png'/>后，从词分布中按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352080024544288.png'/>
选择一个词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352082182451662.png'/>
  
所以pLSA中生成文档的整个过程便是选定文档生成主题，确定主题生成词。
  

4.2.1 根据文档反推其主题分布
反过来，既然文档已经产生，那么如何根据已经产生好的文档反推其主题呢？这个利用看到的文档推断其隐藏的主题（分布）的过程（其实也就是产生文档的逆过程），便是主题建模的目的：自动地发现文档集中的主题（分布）。

    换言之，人类根据文档生成模型写成了各类文章，然后丢给了计算机，相当于计算机看到的是一篇篇已经写好的文章。现在计算机需要根据一篇篇文章中看到的一系列词归纳出当篇文章的主题，进而得出各个主题各自不同的出现概率：主题分布。即文档d和单词w是可被观察到的，但主题z却是隐藏的。

    如下图所示（图中被涂色的d、w表示可观测变量，未被涂色的z表示未知的隐变量，N表示一篇文档中总共N个单词，M表示M篇文档）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352089953160215.png'/>

上图中，文档d和词w是我们得到的样本（样本随机，参数虽未知但固定，所以pLSA属于频率派思想。区别于下文要介绍的LDA中：样本固定，参数未知但不固定，是个随机变量，服从一定的分布，所以LDA属于贝叶斯派思想），可观测得到，所以对于任意一篇文档，其<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352102141399349.png'/>是已知的。
 
从而可以根据大量已知的文档-词项信息<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352105911187250.png'/>，训练出文档-主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352109183646967.png'/>和主题-词项<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352112551770813.png'/>，如下公式所示：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352116356129118.png'/>

 故得到文档中每个词的生成概率为：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352119519512651.png'/>

由于<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352126947332565.png'/>可事先计算求出，而<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352131773110152.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535213557938318.png'/>未知，所以<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352138829704316.png'/>就是我们要估计的参数（值），通俗点说，就是要最大化这个θ。

  用什么方法进行估计呢，常用的参数估计方法有极大似然估计MLE、最大后验证估计MAP、贝叶斯估计等等。因为该待估计的参数中含有隐变量z，所以我们可以考虑EM算法。

4.2.1.1 EM算法的简单介绍
    EM算法，全称为Expectation-maximization algorithm，为期望最大算法，其基本思想是：首先随机选取一个值去初始化待估计的值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352147339279118.png'/>，然后不断迭代寻找更优的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352150834369608.png'/>使得其似然函数likelihood<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352154639134153.png'/>比原来的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352157815713092.png'/>要大。换言之，假定现在得到了<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352161771937404.png'/>，想求<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155360898816678576.png'/>，使得
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352173565268236.png'/>
 
EM的关键便是要找到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352178841278499.png'/>的一个下界<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352185797295694.png'/>（注：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352189266615936.png'/>，其中，X表示已经观察到的随机变量），然后不断最大化这个下界，通过不断求解下界<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535219387766440.png'/>的极大化，从而逼近要求解的似然函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352197891781163.png'/>。
 
所以EM算法的一般步骤为：
1. 随机选取或者根据先验知识初始化<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352204413956527.png'/>；
2. 不断迭代下述两步
①给出当前的参数估计<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352209568676129.png'/>，计算似然函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352211731064424.png'/>的下界<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535221606467251.png'/>
②重新估计参数θ，即求<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535222196165359.png'/>，使得<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535222814050175.png'/>
3. 上述第二步后，如果<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352230788002509.png'/>收敛（即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352233865245342.png'/>收敛）则退出算法，否则继续回到第二步。

上述过程好比在二维平面上，有两条不相交的曲线，一条曲线在上（简称上曲线<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352239311710575.png'/>），一条曲线在下（简称下曲线<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352243013947692.png'/>），下曲线为上曲线的下界。现在对上曲线未知，只已知下曲线，为了求解上曲线的最高点，我们试着不断增大下曲线，使得下曲线不断逼近上曲线，下曲线在某一个点达到局部最大值并与上曲线在这点的值相等，记录下这个值，然后继续增大下曲线，寻找下曲线上与上曲线上相等的值，迭代到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352246021459246.png'/>收敛（即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352250118457515.png'/>收敛）停止，从而利用当前下曲线上的局部最大值当作上曲线的全局最大值（换言之，EM算法不保证一定能找到全局最优值）。如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352254171513644.png'/>

  以下是详细介绍。

  假定有训练集<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352259269706536.png'/>，包含m个独立样本，希望从中找到该组数据的模型p(x,z)的参数。   

  然后通过极大似然估计建立目标函数--对数似然函数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352263149431888.png'/>

  这里，z是隐随机变量，直接找到参数的估计是很困难的。我们的策略是建立<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352267759665504.png'/>的下界，并且求该下界的最大值；重复这个过程，直到收敛到局部最大值。

    令Qi是z的某一个分布，Qi≥0，且结合Jensen不等式，有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352271697349975.png'/>

  为了寻找尽量紧的下界，我们可以让使上述等号成立，而若要让等号成立的条件则是：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352274895011896.png'/>
 
 换言之，有以下式子成立：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535227756859447.png'/>，且由于有：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535228017333416.png'/>

  所以可得：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352283565659113.png'/>
 最终得到EM算法的整体框架如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352286271375942.png'/>

OK，EM算法还会在本博客后面的博文中具体阐述。接下来，回到pLSA参数的估计问题上。

4.2.1.2 EM算法估计pLSA的两未知参数
    首先尝试从矩阵的角度来描述待估计的两个未知变量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352291631971718.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352292434633816.png'/>。
假定用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352297030254289.png'/>表示词表<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352301189888408.png'/>在主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352302913692625.png'/>上的一个多项分布，则<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352304964410693.png'/>可以表示成一个向量，每个元素<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352308442000767.png'/>表示词项<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352311344378316.png'/>出现在主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352313382645402.png'/>中的概率，即
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352317021943030.png'/>

用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352320939984026.png'/>表示所有主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352324316569651.png'/>在文档上的一个多项分布，则<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352333393092979.png'/>可以表示成一个向量，每个元素<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352336563166032.png'/>表示主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352338527941490.png'/>出现在文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352340930640951.png'/>中的概率，即
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535234352433311.png'/>
   
  这样，巧妙的把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352348486881632.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352350645765621.png'/>转换成了两个矩阵。换言之，最终我们要求解的参数是这两个矩阵：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352353073972994.png'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352355184296622.png'/>

 由于词和词之间是相互独立的，所以整篇文档N个词的分布为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352357788773508.png'/>
  
  再由于文档和文档之间也是相互独立的，所以整个语料库中词的分布为（整个语料库M篇文档，每篇文档N个词）：
  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352361858829873.png'/>

  其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352364935525345.png'/>表示词项<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535236667245054.png'/>在文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352368213803366.png'/>中的词频，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352371760112539.png'/>表示文档di中词的总数，显然有<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352374898103137.png'/>。
从而得到整个语料库的词分布的对数似然函数（下述公式中有个小错误，正确的应该是：N为M，M为
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352379054242048.png'/>

 现在，我们需要最大化上述这个对数似然函数来求解参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352381489436443.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352385566987101.png'/>。对于这种含有隐变量的最大似然估计，可以使用EM算法。EM算法，分为两个步骤：先E-step，后M-step。

E-step：假定参数已知，计算此时隐变量的后验概率。
    利用贝叶斯法则，可以得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352389044058754.png'/>

M-step：带入隐变量的后验概率，最大化样本分布的对数似然函数，求解相应的参数。

 观察之前得到的对数似然函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352394396615182.png'/>的结果，由于文档长度<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352396955817109.png'/>可以单独计算，所以去掉它不影响最大化似然函数。此外，根据E-step的计算结果，把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352399941382410.png'/>，于是我们只要最大化下面这个函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535240327201289.png'/>即可（下述公式中有个小错误，正确的应该是：N为M，M为N）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352406233499612.png'/>
 
这是一个多元函数求极值问题，并且已知有如下约束条件（下述公式中有个小错误，正确的应该是：M为N）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352409455110165.png'/>

熟悉凸优化的朋友应该知道，一般处理这种带有约束条件的极值问题，常用的方法便是拉格朗日乘数法，即通过引入拉格朗日乘子将约束条件和多元（目标）函数融合到一起，转化为无约束条件的极值问题。

 这里我们引入两个拉格朗日乘子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352413738698728.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352416366512507.png'/>，从而写出拉格朗日函数（下述公式中有个小错误，正确的应该是：N为M，M为N）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352420359093828.png'/>

因为我们要求解的参数是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352422618590365.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352424574090968.png'/>，所以分别对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352426669008186.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352427787047989.png'/>求偏导，然后令偏导结果等于0，得到（下述公式中有个小错误，正确的应该是：N为M，M为N）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535243064050167.png'/>

  消去拉格朗日乘子，最终可估计出参数和（下述公式中有个小错误，正确的应该是：N为M，M为N）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352434487426261.png'/>

综上，在pLSA中：
1.由于<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352440312429474.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352442333506579.png'/>未知，所以我们用EM算法去估计<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352445692171967.png'/>这个参数的值。
2.而后，用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352449631508144.png'/>表示词项<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535245186332266.png'/>出现在主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352453435224270.png'/>中的概率，即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352456134811955.png'/>，用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352457840200732.png'/>表示主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352459424484384.png'/>出现在文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352460946637132.png'/>中的概率，即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352463463822127.png'/>，从而把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535246568171006.png'/>转换成了“主题-词项”矩阵Φ（主题生成词），把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352467848466871.png'/>转换成了“文档-主题”矩阵Θ（文档生成主题）。
3.最终求解出<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352471759175481.png'/>、<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352473630066744.png'/>。

4.3 LDA模型
    
事实上，理解了pLSA模型，也就差不多快理解了LDA模型，因为LDA就是在pLSA的基础上加层贝叶斯框架，即LDA就是pLSA的贝叶斯版本（正因为LDA被贝叶斯化了，所以才需要考虑历史先验知识，才加的两个先验参数）。

4.3.1 pLSA跟LDA的对比：生成文档与参数估计

    在pLSA模型中，我们按照如下的步骤得到“文档-词项”的生成模型：
1.按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359322481186453.png'/>选择一篇文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359323551097493.png'/>
2.选定文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359325085150489.png'/>后，确定文章的主题分布
3.从主题分布中按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359328613609280.png'/>选择一个隐含的主题类别<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359329788656813.png'/>
4.选定<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359331176906780.png'/>后，确定主题下的词分布
5.从词分布中按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535933335424985.png'/>选择一个词 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359334640009226.png'/>”

下面，咱们对比下本文开头所述的LDA模型中一篇文档生成的方式是怎样的：

1.按照先验概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359337712851481.png'/>选择一篇文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359338815076728.png'/>
2.从狄利克雷分布（即Dirichlet分布）<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359341151542134.png'/>中取样生成文档 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359342523720499.png'/>的主题分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359344162706964.png'/>，换言之，主题分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359345336716218.png'/>由超参数为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359347046693480.png'/>的Dirichlet分布生成
3.从主题的多项式分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359349648689986.png'/>中取样生成文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359351216238589.png'/>第 j 个词的主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359354911479692.png'/>
4.从狄利克雷分布（即Dirichlet分布）<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359359147094570.png'/>中取样生成主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359360763058204.png'/>对应的词语分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359363613691817.png'/>，换言之，词语分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359364720601037.png'/>由参数为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359366134741704.png'/>的Dirichlet分布生成
5.从词语的多项式分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359368732723273.png'/>中采样最终生成词语<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359371844169016.png'/>
 
从上面两个过程可以看出，LDA在PLSA的基础上，为主题分布和词分布分别加了两个Dirichlet先验。

    继续拿之前讲解PLSA的例子进行具体说明。如前所述，在PLSA中，选主题和选词都是两个随机的过程，先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后从该主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359376162963529.png'/>

而在LDA中，选主题和选词依然都是两个随机的过程，依然可能是先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后再从该主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。

    那PLSA跟LDA的区别在于什么地方呢？区别就在于：

PLSA中，主题分布和词分布是唯一确定的，能明确的指出主题分布可能就是{教育：0.5，经济：0.3，交通：0.2}，词分布可能就是{大学：0.5，老师：0.3，课程：0.2}。
但在LDA中，主题分布和词分布不再唯一确定不变，即无法确切给出。例如主题分布可能是{教育：0.5，经济：0.3，交通：0.2}，也可能是{教育：0.6，经济：0.2，交通：0.2}，到底是哪个我们不再确定（即不知道），因为它是随机的可变化的。但再怎么变化，也依然服从一定的分布，即主题分布跟词分布由Dirichlet先验随机确定。
   
看到这，你可能凌乱了，你说面对多个主题或词，各个主题或词被抽中的概率不一样，所以抽取主题或词是随机抽取，还好理解。但现在你说主题分布和词分布本身也都是不确定的，这是怎么回事？没办法，谁叫Blei等人“强行”给PLSA安了个贝叶斯框架呢，正因为LDA是PLSA的贝叶斯版本，所以主题分布跟词分布本身由先验知识随机给定。

    进一步，你会发现：
pLSA中，主题分布和词分布确定后，以一定的概率（<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359387526978616.png'/>、<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359390838572878.png'/>）分别选取具体的主题和词项，生成好文档。而后根据生成好的文档反推其主题分布、词分布时，最终用EM算法（极大似然估计思想）求解出了两个未知但固定的参数的值：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641553593940883943.png'/>（由<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359396128718352.png'/>转换而来）和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359399361339549.png'/>（由<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359400856737205.png'/>转换而来）。

文档d产生主题z的概率，主题z产生单词w的概率都是两个固定的值。

举个文档d产生主题z的例子。给定一篇文档d，主题分布是一定的，比如{ P(zi|d), i = 1,2,3 }可能就是{0.4,0.5,0.1}，表示z1、z2、z3，这3个主题被文档d选中的概率都是个固定的值：P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1，如下图所示（图截取自沈博PPT上）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359406378585932.png'/>

但在贝叶斯框架下的LDA中，我们不再认为主题分布（各个主题在文档中出现的概率分布）和词分布（各个词语在某个主题下出现的概率分布）是唯一确定的（而是随机变量），而是有很多种可能。但一篇文档总得对应一个主题分布和一个词分布吧，怎么办呢？LDA为它们弄了两个Dirichlet先验参数，这个Dirichlet先验为某篇文档随机抽取出某个主题分布和词分布。
文档d产生主题z（准确的说，其实是Dirichlet先验为文档d生成主题分布Θ，然后根据主题分布Θ产生主题z）的概率，主题z产生单词w的概率都不再是某两个确定的值，而是随机变量。
还是再次举下文档d具体产生主题z的例子。给定一篇文档d，现在有多个主题z1、z2、z3，它们的主题分布{ P(zi|d), i = 1,2,3 }可能是{0.4,0.5,0.1}，也可能是{0.2,0.2,0.6}，即这些主题被d选中的概率都不再认为是确定的值，可能是P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1，也有可能是P(z1|d) = 0.2、P(z2|d) = 0.2、P(z3|d) = 0.6等等，而主题分布到底是哪个取值集合我们不确定（为什么？这就是贝叶斯派的核心思想，把未知参数当作是随机变量，不再认为是某一个确定的值），但其先验分布是dirichlet 分布，所以可以从无穷多个主题分布中按照dirichlet 先验随机抽取出某个主题分布出来。如下图所示（图截取自沈博PPT上）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359413518476943.png'/>

换言之，LDA在pLSA的基础上给这两参数（<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359415874805605.png'/>、<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359417519361279.png'/>加了两个先验分布的参数（贝叶斯化）：一个主题分布的先验分布Dirichlet分布
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359419613790061.png'/>，和一个词语分布的先验分布Dirichlet分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359421690163061.png'/>。

    综上，LDA真的只是pLSA的贝叶斯版本，文档生成后，两者都要根据文档去推断其主题分布和词语分布（即两者本质都是为了估计给定文档生成主题，给定主题生成词语的概率），只是用的参数推断方法不同，在pLSA中用极大似然估计的思想去推断两未知的固定参数，而LDA则把这两参数弄成随机变量，且加入dirichlet先验。

    所以，pLSA跟LDA的本质区别就在于它们去估计未知参数所采用的思想不同，前者用的是频率派思想，后者用的是贝叶斯派思想。
    好比，我去一朋友家：

按照频率派的思想，我估计他在家的概率是1/2，不在家的概率也是1/2，是个定值。
而按照贝叶斯派的思想，他在家不在家的概率不再认为是个定值1/2，而是随机变量。比如按照我们的经验（比如当天周末），猜测他在家的概率是0.6，但这个0.6不是说就是完全确定的，也有可能是0.7。如此，贝叶斯派没法确切给出参数的确定值（0.3,0.4，0.6,0.7，0.8,0.9都有可能），但至少明白在哪个范围或哪些取值（0.6,0.7，0.8,0.9）更有可能，哪个范围或哪些取值（0.3,0.4） 不太可能。进一步，贝叶斯估计中，参数的多个估计值服从一定的先验分布，而后根据实践获得的数据（例如周末不断跑他家），不断修正之前的参数估计，从先验分布慢慢过渡到后验分布。
    
OK，相信已经解释清楚了。如果是在机器学习班上face-to-face，更好解释和沟通。

4.3.2 LDA生成文档过程的进一步理解

    上面说，LDA中，主题分布 —— 比如{ P(zi), i =1,2,3 }等于{0.4,0.5,0.1}或{0.2,0.2,0.6} —— 是由dirichlet先验给定的，不是根据文档产生的。所以，LDA生成文档的过程中，先从dirichlet先验中“随机”抽取出主题分布，然后从主题分布中“随机”抽取出主题，最后从确定后的主题对应的词分布中“随机”抽取出词。

    那么，dirichlet先验到底是如何“随机”抽取主题分布的呢？

    事实上，从dirichlet分布中随机抽取主题分布，这个过程不是完全随机的。为了说清楚这个问题，咱们得回顾下dirichlet分布。事实上，如果我们取3个事件的话，可以建立一个三维坐标系，类似xyz三维坐标系，这里，我们把3个坐标轴弄为p1、p2、p3，如下图所示：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359431634073119.png'/>

在这个三维坐标轴所划分的空间里，每一个坐标点(p1,p2,p3)就对应着一个主题分布，且某一个点(p1,p2,p3)的大小表示3个主题z1、z2、z3出现的概率大小（因为各个主题出现的概率和为1，所以p1+p2+p3 = 1，且p1、p2、p3这3个点最大取值为1）。比如(p1,p2,p3) = (0.4,0.5,0.1)便对应着主题分布{ P(zi), i =1,2,3 } = {0.4,0.5,0.1}。

    可以想象到，空间里有很多这样的点(p1,p2,p3)，意味着有很多的主题分布可供选择，那dirichlet分布如何选择主题分布呢？把上面的斜三角形放倒，映射到底面的平面上，便得到如下所示的一些彩图（3个彩图中，每一个点对应一个主题分布，高度代表某个主题分布被dirichlet分布选中的概率，且选不同的，dirichlet 分布会偏向不同的主题分布）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359435897199172.png'/>

我们来看上图中左边这个图，高度就是代表dirichlet分布选取某个坐标点(p1,p2,p3)（这个点就是一个主题分布）的概率大小。如下图所示，平面投影三角形上的三个顶点上的点：A=(0.9,0.05,0.05)、B=(0.05,0.9,0.05)、C=(0.05,0.05,0.9)各自对应的主题分布被dirichlet分布选中的概率值很大，而平面三角形内部的两个点：D、E对应的主题分布被dirichlet分布选中的概率值很小。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359441826947541.png'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359442527123381.png'/>
 
所以虽然说dirichlet分布是随机选取任意一个主题分布的，但依然存在着P(A) = P(B) = P(C) >> P(D) = P(E)，即dirichlet分布还是“偏爱”某些主题分布的。至于dirichlet分布的参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359444870976857.png'/>是如何决定dirichlet分布的形状的，可以从dirichlet分布的定义和公式思考。

此外，就算说“随机”选主题也是根据主题分布来“随机”选取，这里的随机不是完全随机的意思，而是根据各个主题出现的概率值大小来抽取。比如当dirichlet先验为文档d生成的主题分布{ P(zi), i =1,2,3 }是{0.4,0.5,0.1}时，那么主题z2在文档d中出现的概率便是0.5。所以，从主题分布中抽取主题，这个过程也不是完全随机的，而是按照各个主题出现的概率值大小进行抽取。

4.3.3 pLSA跟LDA的概率图对比

    接下来，对比下LDA跟pLSA的概率模型图模型，左图是pLSA，右图是LDA（右图不太规范，z跟w都得是小写， 其中，阴影圆圈表示可观测的变量，非阴影圆圈表示隐变量，箭头表示两变量间的条件依赖性conditional dependency，方框表示重复抽样，方框右下角的数字代表重复抽样的次数）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359454253285997.png'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359454968917172.png'/>

对应到上面右图的LDA，只有W / w是观察到的变量，其他都是隐变量或者参数，其中，Φ表示词分布，Θ表示主题分布，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359458595491102.png'/>是主题分布Θ的先验分布（即Dirichlet 分布）的参数，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535946061014763.png'/>是词分布Φ的先验分布（即Dirichlet 分布）的参数，N表示文档的单词总数，M表示文档的总数。

    所以，对于一篇文档d中的每一个单词，LDA根据先验知识<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359463185900462.png'/>确定某篇文档的主题分布θ，然后从该文档所对应的多项分布（主题分布）θ中抽取一个主题z，接着根据先验知识<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535946517204555.png'/>确定当前主题的词语分布ϕ，然后从主题z所对应的多项分布（词分布）ϕ中抽取一个单词w。然后将这个过程重复N次，就产生了文档d。

    换言之：
1.假定语料库中共有M篇文章，每篇文章下的Topic的主题分布是一个从参数为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359468293917039.png'/>的Dirichlet先验分布中采样得到的Multinomial分布，每个Topic下的词分布是一个从参数为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359470361659360.png'/>的Dirichlet先验分布中采样得到的Multinomial分布。
2.对于某篇文章中的第n个词，首先从该文章中出现的每个主题的Multinomial分布（主题分布）中选择或采样一个主题，然后再在这个主题对应的词的Multinomial分布（词分布）中选择或采样一个词。不断重复这个随机生成过程，直到M篇文章全部生成完成。

    综上，M 篇文档会对应于 M 个独立的 Dirichlet-Multinomial 共轭结构，K 个 topic 会对应于 K 个独立的 Dirichlet-Multinomial 共轭结构。

其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359475913343454.png'/>→θ→z 表示生成文档中的所有词对应的主题，显然<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535947715124992.png'/>→θ 对应的是Dirichlet 分布，θ→z 对应的是 Multinomial 分布，所以整体是一个 Dirichlet-Multinomial 共轭结构，如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359480762738464.png'/>

类似的，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359482582107994.png'/>→φ→w，容易看出， 此时β→φ对应的是 Dirichlet 分布， φ→w 对应的是 Multinomial 分布， 所以整体也是一个Dirichlet-Multinomial 共轭结构，如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359486495256002.png'/>

4.3.4 pLSA跟LDA参数估计方法的对比

    上面对比了pLSA跟LDA生成文档的不同过程，下面，咱们反过来，假定文档已经产生，反推其主题分布。那么，它们估计未知参数所采用的方法又有什么不同呢？

在pLSA中，我们使用EM算法去估计“主题-词项”矩阵Φ（由<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359492797010796.png'/>转换得到）和“文档-主题”矩阵Θ（由<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359494649312679.png'/>转换得到）这两个参数，而且这两参数都是个固定的值，只是未知，使用的思想其实就是极大似然估计MLE。

而在LDA中，估计Φ、Θ这两未知参数可以用变分(Variational inference)-EM算法，也可以用gibbs采样，前者的思想是最大后验估计MAP（MAP与MLE类似，都把未知参数当作固定的值），后者的思想是贝叶斯估计。贝叶斯估计是对MAP的扩展，但它与MAP有着本质的不同，即贝叶斯估计把待估计的参数看作是服从某种先验分布的随机变量。
关于贝叶斯估计再举个例子。假设中国的大学只有两种：理工科和文科，这两种学校数量的比例是1:1，其中，理工科男女比例7:1，文科男女比例1:7。某天你被外星人随机扔到一个校园，问你该学校可能的男女比例是多少？然后，你实际到该校园里逛了一圈，看到的5个人全是男的，这时候再次问你这个校园的男女比例是多少？

1.因为刚开始时，有先验知识，所以该学校的男女比例要么是7:1，要么是1:7，即P(比例为7:1) = 1/2，P(比例为1:7) = 1/2。
2.然后看到5个男生后重新估计男女比例，其实就是求P(比例7:1|5个男生）= ？，P(比例1:7|5个男生) = ？
3.用贝叶斯公式<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359514141108834.png'/>可得：P(比例7:1|5个男生) = P(比例7:1)*P(5个男生|比例7:1) / P(5个男生)，P(5个男生)是5个男生的先验概率，与学校无关，所以是个常数；类似的，P(比例1:7|5个男生) = P((比例1:7)*P(5个男生|比例1:7)/P(5个男生)。
4.最后将上述两个等式比一下，可得：P(比例7:1|5个男生)/P(比例1:7|5个男生) = {P((比例7:1)*P(5个男生|比例7:1)} / { P(比例1:7)*P(5个男生|比例1:7)}。

由于LDA把要估计的主题分布和词分布看作是其先验分布是Dirichlet分布的随机变量，所以，在LDA这个估计主题分布、词分布的过程中，它们的先验分布（即Dirichlet分布）事先由人为给定，那么LDA就是要去求它们的后验分布（LDA中可用gibbs采样去求解它们的后验分布，得到期望<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359522940867967.png'/>、<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359527491936344.png'/>）！

  此外，不厌其烦的再插一句，在LDA中，主题分布和词分布本身都是多项分布，而由上文3.2节可知“Dirichlet分布是多项式分布的共轭先验概率分布”，因此选择Dirichlet 分布作为它们的共轭先验分布。意味着为多项分布的参数p选取的先验分布是Dirichlet分布，那么以p为参数的多项分布用贝叶斯估计得到的后验分布仍然是Dirichlet分布。

4.3.5 LDA参数估计：Gibbs采样

    理清了LDA中的物理过程，下面咱们来看下如何学习估计。

    类似于pLSA，LDA的原始论文中是用的变分-EM算法估计未知参数，后来发现另一种估计LDA未知参数的方法更好，这种方法就是：Gibbs Sampling，有时叫Gibbs采样或Gibbs抽样，都一个意思。Gibbs抽样是马尔可夫链蒙特卡尔理论（MCMC）中用来获取一系列近似等于指定多维概率分布（比如2个或者多个随机变量的联合概率分布）观察样本的算法。

    OK，给定一个文档集合，w是可以观察到的已知变量，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359532295718453.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359533165831980.png'/>是根据经验给定的先验参数，其他的变量z，θ和φ都是未知的隐含变量，需要根据观察到的变量来学习估计的。根据LDA的图模型，可以写出所有变量的联合分布：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359536591356059.png'/>

注：上述公式中及下文中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359542072756136.png'/>等价上文中定义的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359543426315732.png'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359546885021084.png'/>等价于上文中定义的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359548691118426.png'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359551783463011.png'/>等价于上文中定义的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359553751938575.png'/>，等价于上文中定义的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359555428029823.png'/>。

因为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359557999852117.png'/>产生主题分布θ，主题分布θ确定具体主题，且<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359559851337359.png'/>产生词分布φ、词分布φ确定具体词，所以上述式子等价于下述式子所表达的联合概率分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359563427448219.png'/>：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359565643480339.png'/>

其中，第一项因子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359570837415527.png'/>表示的是根据确定的主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359573958414593.png'/>和词分布的先验分布参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359576639474534.png'/>
采样词的过程，第二项因子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359580311650615.png'/>是根据主题分布的先验分布参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359582066245634.png'/>采样主题的过程，这两项因子是需要计算的两个未知参数。

    由于这两个过程是独立的，所以下面可以分别处理，各个击破。

第一个因子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359585480164890.png'/>，可以根据确定的主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359586943966272.png'/>和从先验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359588773349861.png'/>取样得到的词分布Φ产生：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359590962076577.png'/>

   由于样本中的词服从参数为主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359618289654499.png'/>的独立多项分布，这意味着可以把上面对词的乘积分解成分别对主题和对词的两层乘积：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359593565890753.png'/>

其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359596066791425.png'/>是词 t 在主题 k 中出现的次数。

    回到第一个因子上来。目标分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359597966127928.png'/>需要对词分布Φ积分，且结合我们之前在3.1节定义的Dirichlet 分布的归一化系数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359611537715401.png'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359613776670830.png'/>

  可得：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359622645015940.png'/>

这个结果可以看作K个Dirichlet-Multinomial模型的乘积。
现在开始求第二个因子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359625338257227.png'/>。类似于<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359627036094269.png'/>的步骤，先写出条件分布，然后分解成两部分的乘积：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535962945646534.png'/>

其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359631142393470.png'/>表示的单词 i 所属的文档，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359633983813018.png'/>是主题 k 在文章 m 中出现的次数。

    对主题分布Θ积分可得：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359636767049375.png'/>

 综合第一个因子和第二个因子的结果，得到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359639459843092.png'/>的联合分布结果为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359644266955465.png'/>

接下来，有了联合分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359647047163864.png'/>，咱们便可以通过联合分布来计算在给定可观测变量 w 下的隐变量 z 的条件分布（后验分布）<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155361082156196393.png'/>来进行贝叶斯分析。

换言之，有了这个联合分布后，要求解第m篇文档中的第n个词（下标为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359654142524587.png'/>的词）的全部条件概率就好求了。

 先定义几个变量。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535965893140882.png'/>表示除去<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535966138089608.png'/>的词，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359664740690253.png'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359666565248711.png'/>

然后，排除当前词的主题分配，即根据其他词的主题分配和观察到的单词来计算当前词主题的概率公式为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359668959930366.png'/>

勘误：考虑到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359671922591402.png'/>，所以上述公式的第二行的分子，非p(w,z) *p(z)，而是p(w|z)*p(z)。

    且有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359674945226332.png'/>

    最后一步，便是根据Markov链的状态获取主题分布的参数Θ和词分布的参数Φ。

    换言之根据贝叶斯法则和Dirichlet先验，以及上文中得到的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359681388661477.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359683243973640.png'/>自被分解成两部分乘积的结果，可以计算得到每个文档上Topic的后验分布和每个Topic下的词的后验分布分别如下（据上文可知：其后验分布跟它们的先验分布一样，也都是Dirichlet 分布）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359685989285980.png'/>

其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359691132878833.png'/>是构成文档m的主题数向量，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359693536235563.png'/>是构成主题k的词项数向量。

  此外，别忘了上文中2.4节所述的Dirichlet的一个性质，如下：
“ 如果<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359696734950792.png'/>，同样可以证明有下述结论成立：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359699468234589.png'/>

即：如果<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359702552311065.png'/>，则<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359706273738100.png'/>中的任一元素<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359707051302300.png'/>的期望是：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359709377025673.png'/>

可以看出，超参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359713397744124.png'/>的直观意义就是事件先验的伪计数(prior pseudo-count)。 ”
    所以，最终求解的Dirichlet 分布期望为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359715726876809.png'/>

然后将<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359717666161966.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359724431736088.png'/>的结果代入之前得到的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359727179740137.png'/>的结果中，可得：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359729581510899.png'/>

    仔细观察上述结果，可以发现，式子的右半部分便是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359731996365266.png'/>，这个概率的值对应着<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359734227139103.png'/>的路径概率。如此，K 个topic 对应着K条路径，Gibbs Sampling 便在这K 条路径中进行采样，如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359739411606863.png'/>

何等奇妙，就这样，Gibbs Sampling通过求解出主题分布和词分布的后验分布，从而成功解决主题分布和词分布这两参数未知的问题。

5 读者微评
 

    本文发表后，部分热心的读者在微博上分享了他们自己理解LDA的心得，也欢迎更多朋友分享你的理解心得（比如评论在本文下，或评论在微博上），从而在分享、讨论的过程中让更多人可以更好的理解：

@SiNZeRo：lda 如果用em就是 map估计了. lda本意是要去找后验分布 然后拿后验分布做bayesian分析. 比如theta的期望 . 而不是把先验作为正则化引入。最后一点gibbs sampling其实不是求解的过程 是去explore后验分布 去采样 用于求期望.
@研究者July：好问题好建议，这几天我陆续完善下！//@帅广应s：LDA这个东西该怎么用？可以用在哪些地方？还有就是Gibbs抽样的原理是什么？代码怎么实现？如果用EM来做，代码怎么实现？ LDA模型的变形和优化有哪些？LDA不适用于解决哪类的问题？总之，不明白怎么用，参数怎么调优？ 

@xiangnanhe：写的很好，4.1.3节中的那两个图很赞，非常直观的理解了LDA模型加了先验之后在学参数的时候要比PLSI更灵活；PLSI在学参数的过程中比较容易陷入local minimum然后overfitting。

@asker2：无论是pLSA中，还是LDA中，主题分布和词分布本身是固定的存在，但都未知。pLSA跟LDA的区别在于，去探索这两个未知参数的方法或思想不一样。pLSA是求到一个能拟合文本最好的参数（分布），这个值就认为是真实的参数。但LDA认为，其实我们没法去完全求解出主题分布、词分布到底是什么参数，我们只能把它们当成随机变量，通过缩小其方差（变化度）来尽量让这个随机变量变得更“确切”。换言之，我们不再求主题分布、词分布的具体值，而是通过这些分布生成的观测值（即实际文本）来反推分布的参数的范围，即在什么范围比较可能，在什么范围不太可能。所以，其实这就是一种贝叶斯分析的思想，虽然无法给出真实值具体是多少，但可以按照经验给一个相对合理的真实值服从的先验分布，然后从先验出发求解其后验分布。
..
 

6 参考文献与推荐阅读
1.Blei, David M.; Ng, Andrew Y.; Jordan, Michael I. Latent Dirichlet allocation（LDA原始论文）：http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf。
2 Blei. Probabilistic Topic Models：http://www.cs.princeton.edu/~blei/papers/Blei2012.pdf，一网友的翻译：http://www.cnblogs.com/siegfang/archive/2013/01/30/2882391.html；
3 一堆wikipedia，比如隐含狄利克雷分布LDA的wiki：http://zh.wikipedia.org/wiki/éå«çå©åé·åå¸，狄利克雷分布的wiki：http://zh.wikipedia.org/wiki/çå©åé·åå¸；
4.从贝叶斯方法谈到贝叶斯网络 ；
5.rickjin的LDA数学八卦（力荐，本文部分图片和公式来自于此文档）网页版：http://www.flickering.cn/tag/lda/，PDF版：http://emma.memect.com/t/9756da9a47744de993d8df13a26e04e38286c9bc1c5a0d2b259c4564c6613298/LDA；

6.Thomas Hofmann.Probabilistic Latent Semantic Indexing（pLSA原始论文）：http://cs.brown.edu/~th/papers/Hofmann-SIGIR99.pdf；
7.Gregor Heinrich.Parameter estimation for text analysis（关于Gibbs 采样最精准细致的论述）：http://www.arbylon.net/publications/text-est.pdf；
8.Probabilistic latent semantic analysis (pLSA)：http://blog.tomtung.com/2011/10/plsa/http://blog.tomtung.com/2011/10/plsa/。
9.《概率论与数理统计教程第二版 茆诗松等人著》，如果忘了相关统计分布，建议复习此书或此文第二部分；
10.《支持向量机通俗导论：理解SVM的三层境界》，第二部分关于拉格朗日函数的讨论；

11.机器学习班第11次课上，邹博讲EM & GMM的PPT：http://pan.baidu.com/s/1i3zgmzF；
12.机器学习班第12次课上，邹博讲主题模型LDA的PPT：http://pan.baidu.com/s/1jGghtQm；
13.主题模型之pLSA：http://blog.jqian.net/post/plsa.html；
14主题模型之LDA：http://blog.jqian.net/post/lda.html；
15.搜索背后的奥秘——浅谈语义主题计算：http://www.semgle.com/search-engine-algorithms-mystery-behind-
search-on-the-calculation-of-semantic-topic；

16.LDA的EM推导：http://www.cnblogs.com/hebin/archive/2013/04/25/3043575.html；
17.Machine Learning读书会第8期上，沈博讲主题模型的PPT：http://vdisk.weibo.com/s/zrFL6OXKgKMAf；
18.Latent Dirichlet Allocation （LDA）- David M.Blei：http://www.xperseverance.net/blogs/2012/03/17/；
19.用GibbsLDA做Topic Modeling：http://weblab.com.cityu.edu.hk/blog/luheng/2011/06/24/ç¨gibbsldaåtopic-modeling/#comment-87；
20.主题模型在文本挖掘中的应用：http://net.pku.edu.cn/~zhaoxin/Topic-model-xin-zhao-wayne.pdf；

21.二项分布和多项分布，beta分布的对比：http://www.cnblogs.com/wybang/p/3206719.html；
22.LDA简介：http://cos.name/2010/10/lda_topic_model/；
23.LDA的相关论文、工具库：http://site.douban.com/204776/widget/notes/12599608/note/287085506/；
24.一个网友学习LDA的心得：http://www.xuwenhao.com/2011/03/20/suggestions-for-programmers-to-learn-lda/；
25.http://blog.csdn.net/hxxiaopei/article/details/7617838；

26.主题模型LDA及其在微博推荐&广告算法中的应用：http://www.wbrecom.com/?p=136；
27.LDA发明人之一Blei 写的毕业论文：http://www.cs.princeton.edu/~blei/papers/Blei2004.pdf；
28.LDA的一个C实现：http://www.cs.princeton.edu/~blei/lda-c/index.html；
29.LDA的一些其它资料：http://www.xperseverance.net/blogs/2012/03/657/。
## Word2Vec中为什么使用负采样（negtive sample）？
解析一
如七月在线推荐就业班的专家讲师李老师所言
负采样这个点引入word2vec非常巧妙，两个作用：
1.加速了模型计算
2.保证了模型训练的效果，其一 模型每次只需要更新采样的词的权重，不用更新所有的权重，那样会很慢，其二 中心词其实只跟它周围的词有关系，位置离着很远的词没有关系，也没必要同时训练更新，作者这点非常聪明。

解析二
下述解析来源于：https://zhuanlan.zhihu.com/p/29488930
1. 随机梯度下降法有什么问题？
通过对代价函数求权重的梯度，我们可以一次性对所有的参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415527417807244433.svg'/>进行优化，但是如果每次等全部计算完成再优化升级，我们将等待很长时间（对于很大的语料库来说）。

所以我们采用随机梯度下降（ Stochastic Gradient Descent），也就是说每次完成一次计算就进行升级。

但是，还有两个问题导致目前的模型效率低下！
第一个问题，我们每次只对窗口中出现的几个单词进行升级，但是在计算梯度的过程中，我们是对整个参数矩阵进行运算，这样参数矩阵中的大部分值都是0。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274196765063064.jpg'/>

计算效率低下！

第二个问题：我们使用的目标函数是softmax函数   
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274198388176499.svg'/>

我们观察分母，分母需要把窗口中所有单词的“得分”都算出来再求和，效率低下！

2. 使用负采样
负采样的核心思想是：计算目标单词和窗口中的单词的真实单词对“得分”，再加一些“噪声”，即词表中的随机单词和目标单词的“得分”。

真实单词对“得分”和“噪声”作为代价函数。
每次优化参数，只关注代价函数中涉及的词向量。

下面给出公式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274230878559348.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274232155160337.svg'/>

采用上述公式解决了之前说的两个问题：

我们仅对K个参数进行采样
我们放弃softmax函数，采用sigmoid函数，这样就不存在先求一遍窗口中所有单词的‘“得分”的情况了。

3. 计算梯度
既然代价函数已经更新了，那么我们需要对梯度进行更新。

首先考虑一下，我们想要求导的目标，也就是对谁求导？

答案是，对我们想要优化的参数求导，前面说了，负采样的目的是不需要对整个向量矩阵 U或 V 进行优化，而是仅对求代价过程中涉及的词向量进行优化，因此，求导对象是目标向量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274269955206683.svg'/>,窗口中的其他词向量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641552742710818869.svg'/>和负采样时随机选取的词向量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274272164152465.svg'/> 。

此篇文章关注的问题不是求导的过程，因此下面直接给出梯度：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274273898566277.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274275076208119.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274276112212624.svg'/>
## 什么是TF-IDF算法？
TF-IDF(term frequency–inverse document frequency)是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。

TF-IDF有两层意思，一层是"词频"（Term Frequency，缩写为TF），另一层是"逆文档频率"（Inverse Document Frequency，缩写为IDF）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156630878613221494.jpg'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156630878961416696.jpg'/>

假设我们现在有一片长文叫做《量化系统架构设计》词频高在文章中往往是停用词，“的”，“是”，“了”等，这些在文档中最常见但对结果毫无帮助、需要过滤掉的词，用TF可以统计到这些停用词并把它们过滤。当高频词过滤后就只需考虑剩下的有实际意义的词。

但这样又会遇到了另一个问题，我们可能发现"量化"、"系统"、"架构"这三个词的出现次数一样多。这是不是意味着，作为关键词，它们的重要性是一样的？事实上系统应该在其他文章比较常见，所以在关键词排序上，“量化”和“架构”应该排在“系统”前面，这个时候就需要IDF，IDF会给常见的词较小的权重，它的大小与一个词的常见程度成反比。

当有TF(词频)和IDF(逆文档频率)后，将这两个词相乘，就能得到一个词的TF-IDF的值。某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由大到小排序，排在最前面的几个词，就是该文章的关键词。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156630881464111037.jpg'/>

TF-IDF算法步骤

第一步，计算词频：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156630882773571227.jpg'/>
考虑到文章有长短之分，为了便于不同文章的比较，进行"词频"标准化。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415718241246797631.jpg'/>
第二步，计算逆文档频率：

这时，需要一个语料库（corpus），用来模拟语言的使用环境。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157182414255715037.jpg'/>
如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数。

第三步，计算TF-IDF：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157182415370411571.jpg'/>
可以看到，TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。所以，自动提取关键词的算法就很清楚了，就是计算出文档的每个词的TF-IDF值，然后按降序排列，取排在最前面的几个词。

优缺点
TF-IDF的优点是简单快速，而且容易理解。缺点是有时候用词频来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。如果要体现词的上下文结构，那么你可能需要使用word2vec算法来支持。

示例代码
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156630890881435667.jpg'/>

本题解析来源：https://zhuanlan.zhihu.com/p/31197209
## 请说说word2vec的简要理解
本题解析来源：https://blog.csdn.net/lilong117194/article/details/81979522?tdsourcetag=s_pcqq_aiomsg

在自然语言处理领域中，本文向量化是文本表示的一种重要方式。在当前阶段，对文本的大部分研究都是通过词向量化实现的，但同时也有一部分研究将句子作为文本处理的基本单元，也就是doc2vec和str2vec技术。

1. word2vec简介
大家很熟悉的词袋(bag of words)模型是最早的以词语为基本处理单元的文本向量化算法，所谓的词袋模型就是借助于词典把文本转化为一组向量，下面是两个简单的文本示例：

john likes to watch movies, mary likes too.
john also likes to watch football games.

现假设词典如下： 
{"john":1,"likes":2,"to":3,"watch":4, "movies":5,"also":6,"football":7,"games":8,"mary":9 "too":10} 
在这个自己构建的词典中，每个单词都有一个唯一的索引，那么上述的两个文本就可以基于这个暂时的词典来构建其文本的向量表示，如下： 
[1,2,1,1,1,0,0,0,1,1] 
[1,1,1,1,0,1,1,1,0,0] 

由此可以看出此向量的构建是根据该词在词典出现的次数而构成的，比如第一条文本中的”likes”,这个词在文本中出现了2次，所以基于词袋的文本向量是根据词出现的次数构建的。但是此向量与文本中单词出现的顺序没有关系，只是一种频率的表示，该方法容易实现，但是有很大的问题：
a)维数灾难：假如词典包含10000个单词，那么每个文本需要使用10000维的向量表示，那么向量的很多位置必定是0，如此稀疏的高维向量会严重影响计算速度。
b)这样构成的向量无法保存词序信息，而词序对于自然语言处理又是那么的重要。
c)存在语义鸿沟

例如：关于数据稀疏的问题 
自然语言处理经常把字词转为离散的单独的符号，也就是One-Hot Encoder。
杭州 [0,0,0,0,0,0,0,1,0,……，0,0,0,0,0,0,0]
上海 [0,0,0,0,1,0,0,0,0,……，0,0,0,0,0,0,0]
宁波 [0,0,0,1,0,0,0,0,0,……，0,0,0,0,0,0,0]
北京 [0,0,0,0,0,0,0,0,0,……，1,0,0,0,0,0,0]

比如上面的这个例子，在语料库中，杭州、上海、宁波、北京各对应一个向量，向量中只有一个值为1，其余都为0。但是使用One-Hot Encoder有以下问题。一方面，城市编码是随机的，向量之间相互独立，看不出城市之间可能存在的关联关系。其次，向量维度的大小取决于语料库中字词的多少。如果将世界所有城市名称对应的向量合为一个矩阵的话，那这个矩阵过于稀疏，并且会造成维度灾难。

现在随着互联网的发展，大量的无标注数据产生，此时的word2vec技术即是利用神经网络从大量的无标注的文本中提取有用的信息而产生的。

为什么说word2vec能提取有用的信息呢？ 
我们知道词语是表达语义的基本单元，而词袋模型只是简单的将词语符号化，举个不太恰当的比喻就是：现在有”一麻袋”的词语，而我们要处理的文本就像是从一个麻袋中无序得（不分先后顺序）抽出麻袋中所有的词，再查看文本中出现的个数，注意这里的从麻袋中抽取词的过程是无序的，也就是只是简单的统计文本中有没有出现该词和该词出现了几次，所以对于词袋模型，文本的语序特征就丧失了，也就丧失了语义的信息。

此时我们需要一个模型就是能在使文本向量化的同时也保留了词序的信息。分布式假说的提出就是解决了语义信息的问题。该方法的思想是：上下文相似的词，其语义也相似，随后就有了基于上下文分布表示词义的方法，这就是“词空间模型“。Word2Vec可以将One-Hot Encoder转化为低维度的连续值，也就是稠密向量，并且其中意思相近的词将被映射到向量空间中相近的位置。而使用神经网络可以灵活的对上下文进行建模，也因此成为用的比较多的方法。

2. 模型简介
one-hot向量作为word2vec的输入，通过word2vec训练低维词向量（word embedding） 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156820904924499143.png'/>
输入层：One-Hot Vector 
隐藏层：没有激活函数，也就是线性的单元。 
输出层：维度跟输入层的维度一样，用的是Softmax回归。 
我们要获取的dense vector其实就是Hidden Layer的输出单元。有的地方定为Input Layer和Hidden Layer之间的权重，其实说的是一回事。

下面用具体的例子看下： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156820906914753774.png'/>
可以看出： 
输入层：5个神经元 
隐藏层：3个神经元 
所以权重矩阵是5x3的大小，可以看出权重矩阵中的[10,12,19]和前向传播后[10,12,19]是一样的。

3. CBOW模式
word2vec主要分为CBOW（Continuous Bag of Words）和Skip-Gram两种模式。CBOW是从原始语句推测目标字词；而Skip-Gram正好相反，是从目标字词推测出原始语句。CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。

CBOW模型的理解： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156820922755035768.png'/>
CBOW模型结构图
1 输入层：上下文单词的onehot. {假设单词向量空间dim为V，也就是词典的大小。上下文单词个数为C}。
2 所有onehot分别乘以共享的输入权重矩阵W(V*N矩阵，N为自己设定的数，N也是隐藏层的神经元个数，初始化权重矩阵W）。
3 所得的向量 {因为是onehot所以为向量} 相加求平均作为隐层向量, size为1*N。
4 乘以输出权重矩阵W′W′(N*V)。
5 得到向量 (1*V) ，激活函数处理得到V-dim概率分布，概率最大的index所指示的单词为预测出的中间词(target word)。
6 与true label的onehot做比较，误差越小越好。

所以，需要定义loss function（一般为交叉熵代价函数），采用梯度下降算法更新W和W′W′。训练完毕后，输入层的每个单词与矩阵W相乘得到的向量的就是我们想要的词向量（word embedding），这个矩阵（所有单词的word embedding）也叫做look up table（其实这个look up table就是矩阵W自身），也就是说，任何一个单词的onehot乘以这个矩阵都将得到自己的词向量。有了look up table就可以免去训练过程直接查表得到单词的词向量了。

案例： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156820926741308823.png'/>

5. Skip-Gram模式
从直观上理解，Skip-Gram是给定input word来预测上下文。

接下来我们来看看如何训练我们的神经网络。假如我们有一个句子“The dog barked at the mailman”。

首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；

有了input word以后，我们再定义一个叫做skip_window的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设置skip_window=2，那么我们最终获得窗口中的词（包括input word在内）就是[‘The’, ‘dog’，’barked’, ‘at’]。skip_window=2代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小span=2x2=4。

另一个参数叫num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当skip_window=2，num_skips=2时，我们将会得到两组 (input word, output word) 形式的训练数据，即 (‘dog’, ‘barked’)，(‘dog’, ‘the’)。

神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词是output word的可能性。这句话有点绕，我们来看个栗子。第二步中我们在设置skip_window和num_skips=2的情况下获得了两组训练数据。假如我们先拿一组数据 (‘dog’, ‘barked’) 来训练神经网络，那么模型通过学习这个训练样本，会告诉我们词汇表中每个单词是“barked”的概率大小。

模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。举个栗子，如果我们向神经网络模型中输入一个单词“中国“，那么最终模型的输出概率中，像“英国”， ”俄罗斯“这种相关词的概率将远高于像”苹果“，”蝈蝈“非相关词的概率。因为”英国“，”俄罗斯“在文本中更大可能在”中国“的窗口中出现。我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。

面的图中给出了一些我们的训练样本的例子。我们选定句子“The quick brown fox jumps over lazy dog”，设定我们的窗口大小为2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。Training Samples（输入， 输出） 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156820979228136457.png'/>
我们的模型将会从每对单词出现的次数中习得统计结果。例如，我们的神经网络可能会得到更多类似（“中国“，”英国“）这样的训练样本对，而对于（”英国“，”蝈蝈“）这样的组合却看到的很少。因此，当我们的模型完成训练后，给定一个单词”中国“作为输入，输出的结果中”英国“或者”俄罗斯“要比”蝈蝈“被赋予更高的概率。

再次提醒，最终我们需要的是训练出来的权重矩阵。

5. 训练优化
此时注意到，这个训练过程的参数规模非常巨大。 
假设语料库中有30000个不同的单词，hidden layer取128，word2vec两个权值矩阵维度都是[30000,128]，在使用SGD对庞大的神经网络进行学习时，将是十分缓慢的。而且，你需要大量的训练数据来调整许多权重，避免过度拟合。数以百万计的重量数十亿倍的训练样本意味着训练这个模型将是一个野兽。 
一般来说，有两种加速算法：Hierarchical Softmax、Negative Sampling等方式来解决。

参考： 
https://blog.csdn.net/mylove0414/article/details/61616617 
https://blog.csdn.net/free356/article/details/79445895
## 如何理解Word2vec 之 Skip-Gram 模型
本题解析来源：https://zhuanlan.zhihu.com/p/27234078

什么是Word2Vec和Embeddings？
Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，它被大量地用在自然语言处理（NLP）中。那么它是如何帮助我们做自然语言处理呢？

Word2Vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。
本质上，Embedding其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。

我们从直观角度上来理解一下，cat这个单词和kitten属于语义上很相近的词，而dog和kitten则不是那么相近，iphone这个单词和kitten的语义就差的更远了。通过对词汇表中单词进行这种数值表示方式的学习（也就是将单词转换为词向量），能够让我们基于这样的数值进行向量化的操作从而得到一些有趣的结论。

比如说，如果我们对词向量kitten、cat以及dog执行这样的操作：kitten - cat + dog，那么最终得到的嵌入向量（embedded vector）将与puppy这个词向量十分相近。

第一部分
模型
Word2Vec模型中，主要有CBOW和Skip-Gram两种模型，从直观上理解，CBOW是给定上下文来预测input word，而Skip-Gram是给定input word来预测上下文。本篇文章仅讲解Skip-Gram模型。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821561640625665.png'/>
Skip-Gram模型的基础形式非常简单，为了更清楚地解释模型，我们先从最一般的基础模型来看Word2Vec（下文中所有的Word2Vec都是指Skip-Gram模型）。

Word2Vec模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。基于训练数据建模的过程，我们给它一个名字叫“Fake Task”，意味着建模并不是我们最终的目的。

上面提到的这种方法实际上会在无监督特征学习（unsupervised feature learning）中见到，最常见的就是自编码器（auto-encoder）：通过在隐层将输入进行编码压缩，继而在输出层将数据解码恢复初始状态，训练完成后，我们会将输出层“砍掉”，仅保留隐层。

The Fake Task
我们在上面提到，训练模型的真正目的是获得模型基于训练数据学得的隐层权重。为了得到这些权重，我们首先要构建一个完整的神经网络作为我们的“Fake Task”，后面再返回来看通过“Fake Task”我们如何间接地得到这些词向量。

接下来我们来看看如何训练我们的神经网络。假如我们有一个句子“The dog barked at the mailman（狗对邮递员吠叫）”。

首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；
有了input word以后，我们再定义一个叫做skip_window的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设置<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821596247104635.svg'/>，那么我们最终获得窗口中的词（包括input word在内）就是[&#39;The&#39;, &#39;dog&#39;，&#39;barked&#39;, &#39;at&#39;]。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821612511990114.svg'/>代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821615820640129.svg'/>。

另一个参数叫num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821618970912257.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821619542372541.svg'/>时，我们将会得到两组 (input word, output word) 形式的训练数据，即 (&#39;dog&#39;, &#39;barked&#39;)，(&#39;dog&#39;, &#39;the&#39;)。

神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词是output word的可能性。这句话有点绕，我们来看个栗子。

第二步中我们在设置skip_window和num_skips=2的情况下获得了两组训练数据。假如我们先拿一组数据 (&#39;dog&#39;, &#39;barked&#39;) 来训练神经网络，那么模型通过学习这个训练样本，会告诉我们词汇表中每个单词是“barked”的概率大小。模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。

举个栗子，如果我们向神经网络模型中输入一个单词“Soviet“，那么最终模型的输出概率中，像“Union”， ”Russia“这种相关词的概率将远高于像”watermelon“，”kangaroo“非相关词的概率。因为”Union“，”Russia“在文本中更大可能在”Soviet“的窗口中出现。

我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。下面的图中给出了一些我们的训练样本的例子。我们选定句子“The quick brown fox jumps over lazy dog”，设定我们的窗口大小为2（<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821622585857030.svg'/>），也就是说我们仅选输入词前后各两个词和输入词进行组合。

下图中，蓝色代表input word，方框内代表位于窗口内的单词。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821625740244978.png'/>
我们的模型将会从每对单词出现的次数中习得统计结果。例如，我们的神经网络可能会得到更多类似（“Soviet“，”Union“）这样的训练样本对，而对于（”Soviet“，”Sasquatch“）这样的组合却看到的很少。

因此，当我们的模型完成训练后，给定一个单词”Soviet“作为输入，输出的结果中”Union“或者”Russia“要比”Sasquatch“被赋予更高的概率。

模型细节
我们如何来表示这些单词呢？

首先，我们都知道神经网络只能接受数值输入，我们不可能把一个单词字符串作为输入，因此我们得想个办法来表示这些单词。最常用的办法就是基于训练文档来构建我们自己的词汇表（vocabulary）再对单词进行one-hot编码。

假设从我们的训练文档中抽取出10000个唯一不重复的单词组成词汇表。我们对这10000个单词进行one-hot编码，得到的每个单词都是一个10000维的向量，向量每个维度的值只有0或者1，假如单词ants在词汇表中的出现位置为第3个，那么ants的向量就是一个第三维度取值为1，其他维都为0的10000维的向量（<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821632794376147.svg'/>）。

还是上面的例子，“The dog barked at the mailman”，那么我们基于这个句子，可以构建一个大小为5的词汇表（忽略大小写和标点符号）：("the", "dog", "barked", "at", "mailman")，我们对这个词汇表的单词进行编号0-4。那么”dog“就可以被表示为一个5维向量[0, 1, 0, 0, 0]。

模型的输入如果为一个10000维的向量，那么输出也是一个10000维度（词汇表的大小）的向量，它包含了10000个概率，每一个概率代表着当前词是输入样本中output word的概率大小。

下图是我们神经网络的结构：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821635134131103.png'/>
隐层没有使用任何激活函数，但是输出层使用了sotfmax。

我们基于成对的单词来对神经网络进行训练，训练样本是 ( input word, output word ) 这样的单词对，input word和output word都是one-hot编码的向量。最终模型的输出是一个概率分布。

隐层

说完单词的编码和训练样本的选取，我们来看下我们的隐层。如果我们现在想用300个特征来表示一个单词（即每个词可以被表示为300维的向量）。那么隐层的权重矩阵应该为10000行，300列（隐层有300个结点）。
Google在最新发布的基于Google news数据集训练的模型中使用的就是300个特征的词向量。词向量的维度是一个可以调节的超参数（在Python的gensim包中封装的Word2Vec接口默认的词向量大小为100， window_size为5）。

看下面的图片，左右两张图分别从不同角度代表了输入层-隐层的权重矩阵。左图中每一列代表一个10000维的词向量和隐层单个神经元连接的权重向量。从右边的图来看，每一行实际上代表了每个单词的词向量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821637292037709.png'/>
所以我们最终的目标就是学习这个隐层的权重矩阵。
我们现在回来接着通过模型的定义来训练我们的这个模型。上面我们提到，input word和output word都会被我们进行one-hot编码。仔细想一下，我们的输入被one-hot编码以后大多数维度上都是0（实际上仅有一个位置为1），所以这个向量相当稀疏，那么会造成什么结果呢。如果我们将一个1 x 10000的向量和10000 x 300的矩阵相乘，它会消耗相当大的计算资源，为了高效计算，它仅仅会选择矩阵中对应的向量中维度值为1的索引行（这句话很绕），看图就明白。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821638986334887.png'/>
我们来看一下上图中的矩阵运算，左边分别是1 x 5和5 x 3的矩阵，结果应该是1 x 3的矩阵，按照矩阵乘法的规则，结果的第一行第一列元素为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821640674382966.svg'/>，同理可得其余两个元素为12，19。如果10000个维度的矩阵采用这样的计算方式是十分低效的。

为了有效地进行计算，这种稀疏状态下不会进行矩阵乘法计算，可以看到矩阵的计算的结果实际上是矩阵对应的向量中值为1的索引，上面的例子中，左边向量中取值为1的对应维度为3（下标从0开始），那么计算结果就是矩阵的第3行（下标从0开始）—— [10, 12, 19]，这样模型中的隐层权重矩阵便成了一个”查找表“（lookup table），进行矩阵计算时，直接去查输入向量中取值为1的维度下对应的那些权重值。隐层的输出就是每个输入单词的“嵌入词向量”。

输出层

经过神经网络隐层的计算，ants这个词会从一个1 x 10000的向量变成1 x 300的向量，再被输入到输出层。输出层是一个softmax回归分类器，它的每个结点将会输出一个0-1之间的值（概率），这些所有输出层神经元结点的概率之和为1。

下面是一个例子，训练样本为 (input word: “ants”， output word: “car”) 的计算示意图。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821642423711066.png'/>
直觉上的理解

下面我们将通过直觉来进行一些思考。

如果两个不同的单词有着非常相似的“上下文”（也就是窗口单词很相似，比如“Kitty climbed the tree”和“Cat climbed the tree”），那么通过我们的模型训练，这两个单词的嵌入向量将非常相似。

那么两个单词拥有相似的“上下文”到底是什么含义呢？比如对于同义词“intelligent”和“smart”，我们觉得这两个单词应该拥有相同的“上下文”。而例如”engine“和”transmission“这样相关的词语，可能也拥有着相似的上下文。

实际上，这种方法实际上也可以帮助你进行词干化（stemming），例如，神经网络对”ant“和”ants”两个单词会习得相似的词向量。词干化（stemming）就是去除词缀得到词根的过程。

第二部分
第一部分我们了解skip-gram的输入层、隐层、输出层。在第二部分，会继续深入讲如何在skip-gram模型上进行高效的训练。

在第一部分讲解完成后，我们会发现Word2Vec模型是一个超级大的神经网络（权重矩阵规模非常大）。

举个栗子，我们拥有10000个单词的词汇表，我们如果想嵌入300维的词向量，那么我们的输入-隐层权重矩阵和隐层-输出层的权重矩阵都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。百万数量级的权重矩阵和亿万数量级的训练样本意味着训练这个模型将会是个灾难（太凶残了）。

Word2Vec的作者在它的第二篇论文中强调了这些问题，下面是作者在第二篇论文中的三个创新：
1 将常见的单词组合（word pairs）或者词组作为单个“words”来处理。
2 对高频次单词进行抽样来减少训练样本的个数。
3 对优化目标采用“negative sampling”方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担。事实证明，对常用词抽样并且对优化目标采用“negative sampling”不仅降低了训练过程中的计算负担，还提高了训练的词向量的质量。

Word pairs and "phases"
论文的作者指出，一些单词组合（或者词组）的含义和拆开以后具有完全不同的意义。比如“Boston Globe”是一种报刊的名字，而单独的“Boston”和“Globe”这样单个的单词却表达不出这样的含义。因此，在文章中只要出现“Boston Globe”，我们就应该把它作为一个单独的词来生成其词向量，而不是将其拆开。同样的例子还有“New York”，“United Stated”等。

在Google发布的模型中，它本身的训练样本中有来自Google News数据集中的1000亿的单词，但是除了单个单词以外，单词组合（或词组）又有3百万之多。

如果你对模型的词汇表感兴趣，可以点击这里，你还可以直接浏览这个词汇表。

如果想了解这个模型如何进行文档中的词组抽取，可以看论文中“Learning Phrases”这一章，对应的代码word2phrase.c被发布在这里。

对高频词抽样

在第一部分的讲解中，我们展示了训练样本是如何从原始文档中生成出来的，这里我再重复一次。我们的原始文本为“The quick brown fox jumps over the laze dog”，如果我使用大小为2的窗口，那么我们可以得到图中展示的那些训练样本。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821649048207731.png'/>
但是对于“the”这种常用高频单词，这样的处理方式会存在下面两个问题：
①当我们得到成对的单词训练样本时，("fox", "the") 这样的训练样本并不会给我们提供关于“fox”更多的语义信息，因为“the”在每个单词的上下文中几乎都会出现。
②由于在文本中“the”这样的常用词出现概率很大，因此我们将会有大量的（”the“，...）这样的训练样本，而这些样本数量远远超过了我们学习“the”这个词向量所需的训练样本数。

Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。

如果我们设置窗口大小<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821654393333842.svg'/>（即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821654950932640.svg'/>），并且从我们的文本中删除所有的“the”，那么会有下面的结果：
i)由于我们删除了文本中所有的“the”，那么在我们的训练样本中，“the”这个词永远也不会出现在我们的上下文窗口中。
ii)当“the”作为input word时，我们的训练样本数至少会减少10个。

这句话应该这么理解，假如我们的文本中仅出现了一个“the”，那么当这个“the”作为input word时，我们设置span=10，此时会得到10个训练样本 ("the", ...) ，如果删掉这个“the”，我们就会减少10个训练样本。实际中我们的文本中不止一个“the”，因此当“the”作为input word的时候，至少会减少10个训练样本。上面提到的这两个影响结果实际上就帮助我们解决了高频词带来的问题。

抽样率

word2vec的C语言代码实现了一个计算在词汇表中保留某个词概率的公式。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821660035913093.svg'/>是一个单词，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821660960057105.svg'/>是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415682166216556831.svg'/>这个单词在所有语料中出现的频次。举个栗子，如果单词“peanut”在10亿规模大小的语料中出现了1000次，那么<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821664260389439.svg'/>。

在代码中还有一个参数叫“sample”，这个参数代表一个阈值，默认值为0.001（在gensim包中的Word2Vec类说明中，这个参数默认为0.001，文档中对这个参数的解释为“ threshold for configuring which higher-frequency words are randomly downsampled”）。这个值越小意味着这个单词被保留下来的概率越小（即有越大的概率被我们删除）。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821667218406991.svg'/>代表着保留某个单词的概率：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821667767554195.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821669356397987.png'/>
图中x轴代表着<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415682167169523557.svg'/>，即单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821672898807262.svg'/>在语料中出现频率，y轴代表某个单词被保留的概率。对于一个庞大的语料来说，单个单词的出现频率不会很大，即使是常用词，也不可能特别大。

从这个图中，我们可以看到，随着单词出现频率的增高，它被采样保留的概率越来越小，我们还可以看到一些有趣的结论：
当时<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821677797780491.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821678357190252.svg'/>。当单词在语料中出现的频率小于0.0026时，它是100%被保留的，这意味着只有那些在语料中出现频率超过0.26%的单词才会被采样。
当时<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821679039763958.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821679552009052.svg'/>，意味着这一部分的单词有50%的概率被保留。
当时<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821680087665003.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821680494439768.svg'/>，意味着这部分单词以3.3%的概率被保留。如果你去看那篇论文的话，你会发现作者在论文中对函数公式的定义和在C语言代码的实现上有一些差别，但我认为C语言代码的公式实现是更权威的一个版本。

负采样（negative sampling）

训练一个神经网络意味着要输入训练样本并且不断调整神经元的权重，从而不断提高对目标的准确预测。每当神经网络经过一个训练样本的训练，它的权重就会进行一次调整。

正如我们上面所讨论的，vocabulary的大小决定了我们的Skip-Gram神经网络将会拥有大规模的权重矩阵，所有的这些权重需要通过我们数以亿计的训练样本来进行调整，这是非常消耗计算资源的，并且实际中训练起来会非常慢。

负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。

当我们用训练样本 ( input word: "fox"，output word: "quick") 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的vocabulary大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为“negative” word。

当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。
在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。

回忆一下我们的隐层-输出层拥有300 x 10000的权重矩阵。如果使用了负采样的方法我们仅仅去更新我们的positive word-“quick”的和我们选择的其他5个negative words的结点对应的权重，共计6个输出神经元，相当于每次只更新<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821683822681342.svg'/>个权重。对于3百万的权重来说，相当于只计算了0.06%的权重，这样计算效率就大幅度提高。

如何选择negative words

我们使用“一元模型分布（unigram distribution）”来选择“negative words”。

要注意的一点是，一个单词被选作negative sample的概率跟它出现的频次有关，出现频次越高的单词越容易被选作negative words。

在word2vec的C语言实现中，你可以看到对于这个概率的实现公式。每个单词被选为“negative words”的概率计算公式与其出现的频次有关。

代码中的公式实现如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821686197707259.svg'/>
每个单词被赋予一个权重，即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821687325457026.svg'/>， 它代表着单词出现的频次。

公式中开3/4的根号完全是基于经验的，论文中提到这个公式的效果要比其它公式更加出色。你可以在google的搜索栏中输入“plot y = x^(3/4) and y = x”，然后看到这两幅图（如下图），仔细观察x在[0,1]区间内时y的取值，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821691465707304.svg'/>有一小段弧形，取值在<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821692590826333.svg'/>函数之上。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821694160224328.png'/>
负采样的C语言实现非常的有趣。unigram table有一个包含了一亿个元素的数组，这个数组是由词汇表中每个单词的索引号填充的，并且这个数组中有重复，也就是说有些单词会出现多次。那么每个单词的索引在这个数组中出现的次数该如何决定呢，有公式<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415682169593130473.svg'/>，也就是说计算出的负采样概率*1亿=单词在表中出现的次数。

有了这张表以后，每次去我们进行负采样时，只需要在0-1亿范围内生成一个随机数，然后选择表中索引号为这个随机数的那个单词作为我们的negative word即可。一个单词的负采样概率越大，那么它在这个表中出现的次数就越多，它被选中的概率就越大。

到目前为止，Word2Vec中的Skip-Gram模型就讲完了，对于里面具体的数学公式推导细节这里并没有深入。这篇文章只是对于实现细节上的一些思想进行了阐述。
## 请用图形象的解释word2vec（一图胜千言）
本题解析来源：https://blog.csdn.net/longxinchen_ml/article/details/89077048#commentBox，英文原文：https://jalammar.github.io/illustrated-word2vec/

嵌入（embedding）是机器学习中最迷人的想法之一。 如果你曾经使用Siri、Google Assistant、Alexa、Google翻译，甚至智能手机键盘进行下一词预测，那么你很有可能从这个已经成为自然语言处理模型核心的想法中受益。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844468755940872.png'/>
在过去的几十年中，嵌入技术用于神经网络模型已有相当大的发展。尤其是最近，其发展包括导致BERT和GPT2等尖端模型的语境化嵌入。

BERT：
https://jalammar.github.io/illustrated-bert/

Word2vec是一种有效创建词嵌入的方法，它自2013年以来就一直存在。但除了作为词嵌入的方法之外，它的一些概念已经被证明可以有效地创建推荐引擎和理解时序数据。在商业的、非语言的任务中。像Airbnb、阿里巴巴、Spotify这样的公司都从NLP领域中提取灵感并用于产品中，从而为新型推荐引擎提供支持。

在这篇文章中，我们将讨论嵌入的概念，以及使用word2vec生成嵌入的机制。让我们从一个例子开始，熟悉使用向量来表示事物。你是否知道你的个性可以仅被五个数字的列表（向量）表示？

个性嵌入：你是什么样的人？
如何用0到100的范围来表示你是多么内向/外向（其中0是最内向的，100是最外向的）？ 你有没有做过像MBTI那样的人格测试，或者五大人格特质测试？ 如果你还没有，这些测试会问你一系列的问题，然后在很多维度给你打分，内向/外向就是其中之一。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684451535247366.png'/>

五大人格特质测试测试结果示例。它可以真正告诉你很多关于你自己的事情，并且在学术、人格和职业成功方面都具有预测能力。此处可以找到测试结果。

假设我的内向/外向得分为38/100。 我们可以用这种方式绘图：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844516154531694.png'/>

让我们把范围收缩到-1到1:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844520232134004.png'/>
当你只知道这一条信息的时候，你觉得你有多了解这个人？了解不多。人很复杂，让我们添加另一测试的得分作为新维度。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844520941566848.png'/>

我们可以将两个维度表示为图形上的一个点，或者作为从原点到该点的向量。我们拥有很棒的工具来处理即将上场的向量们。

我已经隐藏了我们正在绘制的人格特征，这样你会渐渐习惯于在不知道每个维度代表什么的情况下，从一个人格的向量表示中获得价值信息。

我们现在可以说这个向量部分地代表了我的人格。当你想要将另外两个人与我进行比较时，这种表示法就有用了。假设我被公共汽车撞了，我需要被性格相似的人替换，那在下图中，两个人中哪一个更像我？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844521531016577.png'/>

处理向量时，计算相似度得分的常用方法是余弦相似度：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844522551918807.png'/>

1号替身在性格上与我更相似。指向相同方向的向量（长度也起作用）具有更高的余弦相似度。

再一次，两个维度还不足以捕获有关不同人群的足够信息。心理学已经研究出了五个主要人格特征（以及大量的子特征），所以让我们使用所有五个维度进行比较：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844523345738278.png'/>

使用五个维度的问题是我们不能在二维平面绘制整齐小箭头了。这是机器学习中的常见问题，我们经常需要在更高维度的空间中思考。 但好在余弦相似度仍然有效，它适用于任意维度：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844524467774046.png'/>

余弦相似度适用于任意数量的维度。这些得分比上次的得分要更好，因为它们是根据被比较事物的更高维度算出的。

在本节的最后，我希望提出两个中心思想：
1.我们可以将人和事物表示为代数向量（这对机器来说很棒！）。
2.我们可以很容易地计算出相似的向量之间的相互关系。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844525211426089.png'/>

词嵌入
通过上文的理解，我们继续看看训练好的词向量实例（也被称为词嵌入）并探索它们的一些有趣属性。

这是一个单词“king”的词嵌入（在维基百科上训练的GloVe向量）：
[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]

这是一个包含50个数字的列表。通过观察数值我们看不出什么，但是让我们稍微给它可视化，以便比较其它词向量。我们把所有这些数字放在一行：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844527173787486.png'/>

让我们根据它们的值对单元格进行颜色编码（如果它们接近2则为红色，接近0则为白色，接近-2则为蓝色）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844527990576253.png'/>

我们将忽略数字并仅查看颜色以指示单元格的值。现在让我们将“king”与其它单词进行比较：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844529089103826.png'/>

看看“Man”和“Woman”彼此之间是如何比它们任一一个单词与“King”相比更相似的？ 这暗示你一些事情。这些向量图示很好的展现了这些单词的信息/含义/关联。

这是另一个示例列表（通过垂直扫描列来查找具有相似颜色的列）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844530092331232.png'/>

有几个要点需要指出：
1.所有这些不同的单词都有一条直的红色列。 它们在这个维度上是相似的（虽然我们不知道每个维度是什么）
2.你可以看到“woman”和“girl”在很多地方是相似的，“man”和“boy”也是一样
3.“boy”和“girl”也有彼此相似的地方，但这些地方却与“woman”或“man”不同。这些是否可以总结出一个模糊的“youth”概念？可能吧。
4.除了最后一个单词，所有单词都是代表人。 我添加了一个对象“water”来显示类别之间的差异。你可以看到蓝色列一直向下并在 “water”的词嵌入之前停下了。
5.“king”和“queen”彼此之间相似，但它们与其它单词都不同。这些是否可以总结出一个模糊的“royalty”概念？

类比
展现嵌入奇妙属性的著名例子是类比。我们可以添加、减去词嵌入并得到有趣的结果。一个著名例子是公式：“king”-“man”+“woman”：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844530810251399.png'/>

在python中使用Gensim库，我们可以添加和减去词向量，它会找到与结果向量最相似的单词。该图像显示了最相似的单词列表，每个单词都具有余弦相似性。

我们可以像之前一样可视化这个类比：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844531824302089.png'/>

由“king-man + woman”生成的向量并不完全等同于“queen”，但“queen”是我们在此集合中包含的400,000个字嵌入中最接近它的单词。

现在我们已经看过训练好的词嵌入，接下来让我们更多地了解训练过程。 但在我们开始使用word2vec之前，我们需要看一下词嵌入的父概念：神经语言模型。

语言模型
如果要举自然语言处理最典型的例子，那应该就是智能手机输入法中的下一单词预测功能。这是个被数十亿人每天使用上百次的功能。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684453231639228.png'/>

下一单词预测是一个可以通过语言模型实现的任务。语言模型会通过单词列表(比如说两个词)去尝试预测可能紧随其后的单词。

在上面这个手机截屏中，我们可以认为该模型接收到两个绿色单词(thou shalt)并推荐了一组单词(“not” 就是其中最有可能被选用的一个)：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844533326231028.png'/>

我们可以把这个模型想象为这个黑盒:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844534289421319.png'/>

但事实上，该模型不会只输出一个单词。实际上，它对所有它知道的单词(模型的词库，可能有几千到几百万个单词)的按可能性打分，输入法程序会选出其中分数最高的推荐给用户。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844535045920372.png'/>

自然语言模型的输出就是模型所知单词的概率评分，我们通常把概率按百分比表示，但是实际上，40%这样的分数在输出向量组是表示为0.4

自然语言模型(请参考Bengio 2003)在完成训练后，会按如下中所示法人三步完成预测：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844536035278081.png'/>

第一步与我们最相关，因为我们讨论的就是Embedding。模型在经过训练之后会生成一个映射单词表所有单词的矩阵。在进行预测的时候，我们的算法就是在这个映射矩阵中查询输入的单词，然后计算出预测值:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844536687698599.png'/>

现在让我们将重点放到模型训练上，来学习一下如何构建这个映射矩阵。

语言模型训练
相较于大多数其他机器学习模型，语言模型有一个很大有优势，那就是我们有丰富的文本来训练语言模型。所有我们的书籍、文章、维基百科、及各种类型的文本内容都可用。相比之下，许多其他机器学习的模型开发就需要手工设计数据或者专门采集数据。

我们通过找常出现在每个单词附近的词，就能获得它们的映射关系。机制如下：

1.先是获取大量文本数据(例如所有维基百科内容)
2. 然后我们建立一个可以沿文本滑动的窗(例如一个窗里包含三个单词)
3. 利用这样的滑动窗就能为训练模型生成大量样本数据。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684453784736340.png'/>

当这个窗口沿着文本滑动时，我们就能(真实地)生成一套用于模型训练的数据集。为了明确理解这个过程，我们看下滑动窗是如何处理这个短语的:

“Thou shalt not make a machine in the likeness of a human mind” ~Dune

在一开始的时候，窗口锁定在句子的前三个单词上:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844538544645411.png'/>

我们把前两个单词单做特征，第三个单词单做标签:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844539597558241.png'/>

这时我们就生产了数据集中的第一个样本，它会被用在我们后续的语言模型训练中。

接着，我们将窗口滑动到下一个位置并生产第二个样本:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844540444667243.png'/>

这时第二个样本也生成了。

不用多久，我们就能得到一个较大的数据集，从数据集中我们能看到在不同的单词组后面会出现的单词:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844541131034754.png'/>

在实际应用中，模型往往在我们滑动窗口时就被训练的。但是我觉得将生成数据集和训练模型分为两个阶段会显得更清晰易懂一些。除了使用神经网络建模之外，大家还常用一项名为N-gams的技术进行模型训练。

如果想了解现实产品从使用N-gams模型到使用神经模型的转变，可以看一下Swiftkey (我最喜欢的安卓输入法)在2015年的发表一篇博客，文中介绍了他们的自然语言模型及该模型与早期N-gams模型的对比。我很喜这个例子，因为这个它能告诉你如何在营销宣讲中把Embedding的算法属性解释清楚。

顾及两头
根据前面的信息进行填空:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844542392507445.png'/>

在空白前面，我提供的背景是五个单词(如果事先提及到‘bus’)，可以肯定，大多数人都会把bus填入空白中。但是如果我再给你一条信息——比如空白后的一个单词，那答案会有变吗？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844542995294732.png'/>

这下空白处改填的内容完全变了。这时’red’这个词最有可能适合这个位置。从这个例子中我们能学到，一个单词的前后词语都带信息价值。事实证明，我们需要考虑两个方向的单词(目标单词的左侧单词与右侧单词)。那我们该如何调整训练方式以满足这个要求呢，继续往下看。

Skipgram模型
我们不仅要考虑目标单词的前两个单词，还要考虑其后两个单词。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844544844737440.png'/>

如果这么做，我们实际上构建并训练的模型就如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844545454377802.png'/>

上述的这种架构被称为连续词袋(CBOW)，在一篇关于word2vec的论文中有阐述。

还有另一种架构，它不根据前后文(前后单词)来猜测目标单词，而是推测当前单词可能的前后单词。我们设想一下滑动窗在训练数据时如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844551639257097.png'/>

绿框中的词语是输入词，粉框则是可能的输出结果

这里粉框颜色深度呈现不同，是因为滑动窗给训练集产生了4个独立的样本:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844552333293804.png'/>

这种方式称为Skipgram架构。我们可以像下图这样将展示滑动窗的内容。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844553075616690.png'/>
这样就为数据集提供了4个样本:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844556060192397.png'/>
然后我们移动滑动窗到下一个位置:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844557654635876.png'/>

这样我们又产生了接下来4个样本:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844560054138247.png'/>

在移动几组位置之后，我们就能得到一批样本:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844576031016522.png'/>

重新审视训练过程
现在我们已经从现有的文本中获得了Skipgram模型的训练数据集，接下来让我们看看如何使用它来训练一个能预测相邻词汇的自然语言模型。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684457694057923.png'/>

从数据集中的第一个样本开始。我们将特征输入到未经训练的模型，让它预测一个可能的相邻单词。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844578099644532.png'/>

该模型会执行三个步骤并输入预测向量(对应于单词表中每个单词的概率)。因为模型未经训练，该阶段的预测肯定是错误的。但是没关系，我们知道应该猜出的是哪个单词——这个词就是我训练集数据中的输出标签:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844579274876441.png'/>

目标单词概率为1，其他所有单词概率为0，这样数值组成的向量就是“目标向量”。

模型的偏差有多少？将两个向量相减，就能得到偏差向量:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684457997052686.png'/>

现在这一误差向量可以被用于更新模型了，所以在下一轮预测中，如果用not作为输入，我们更有可能得到thou作为输出了。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844580832335579.png'/>

这其实就是训练的第一步了。我们接下来继续对数据集内下一份样本进行同样的操作，直到我们遍历所有的样本。这就是一轮（epoch）了。我们再多做几轮（epoch），得到训练过的模型，于是就可以从中提取嵌入矩阵来用于其他应用了。

以上确实有助于我们理解整个流程，但这依然不是word2vec真正训练的方法。我们错过了一些关键的想法。

负例采样
回想一下这个神经语言模型计算预测值的三个步骤：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844581688372943.png'/>

从计算的角度来看，第三步非常昂贵 - 尤其是当我们将需要在数据集中为每个训练样本都做一遍（很容易就多达数千万次）。我们需要寻找一些提高表现的方法。

一种方法是将目标分为两个步骤：
1.生成高质量的词嵌入（不要担心下一个单词预测）。
2.使用这些高质量的嵌入来训练语言模型（进行下一个单词预测）。

在本文中我们将专注于第1步（因为这篇文章专注于嵌入）。要使用高性能模型生成高质量嵌入，我们可以改变一下预测相邻单词这一任务：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844582543625632.png'/>

将其切换到一个提取输入与输出单词的模型，并输出一个表明它们是否是邻居的分数（0表示“不是邻居”，1表示“邻居”）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844585866655467.png'/>

这个简单的变换将我们需要的模型从神经网络改为逻辑回归模型——因此它变得更简单，计算速度更快。

这个开关要求我们切换数据集的结构——标签值现在是一个值为0或1的新列。它们将全部为1，因为我们添加的所有单词都是邻居。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684458643093615.png'/>

现在的计算速度可谓是神速啦——在几分钟内就能处理数百万个例子。但是我们还需要解决一个漏洞。如果所有的例子都是邻居（目标：1），我们这个”天才模型“可能会被训练得永远返回1——准确性是百分百了，但它什么东西都学不到，只会产生垃圾嵌入结果。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844594770863315.png'/>

为了解决这个问题，我们需要在数据集中引入负样本 - 不是邻居的单词样本。我们的模型需要为这些样本返回0。模型必须努力解决这个挑战——而且依然必须保持高速。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844595838706243.png'/>

对于我们数据集中的每个样本，我们添加了负面示例。它们具有相同的输入字词，标签为0。

但是我们作为输出词填写什么呢？我们从词汇表中随机抽取单词
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844596819454389.png'/>

这个想法的灵感来自噪声对比估计。我们将实际信号（相邻单词的正例）与噪声（随机选择的不是邻居的单词）进行对比。这导致了计算和统计效率的巨大折衷。

噪声对比估计
http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf

基于负例采样的Skipgram（SGNS）
我们现在已经介绍了word2vec中的两个（一对）核心思想：负例采样，以及skipgram。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684459775546353.png'/>

Word2vec训练流程
现在我们已经了解了skipgram和负例采样的两个中心思想，可以继续仔细研究实际的word2vec训练过程了。

在训练过程开始之前，我们预先处理我们正在训练模型的文本。在这一步中，我们确定一下词典的大小（我们称之为vocab_size，比如说10,000）以及哪些词被它包含在内。

在训练阶段的开始，我们创建两个矩阵——Embedding矩阵和Context矩阵。这两个矩阵在我们的词汇表中嵌入了每个单词（所以vocab_size是他们的维度之一）。第二个维度是我们希望每次嵌入的长度（embedding_size——300是一个常见值，但我们在前文也看过50的例子）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844599257221521.png'/>

在训练过程开始时，我们用随机值初始化这些矩阵。然后我们开始训练过程。在每个训练步骤中，我们采取一个相邻的例子及其相关的非相邻例子。我们来看看我们的第一组：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844599990371684.png'/>

现在我们有四个单词：输入单词not和输出/上下文单词: thou（实际邻居词），aaron和taco（负面例子）。我们继续查找它们的嵌入——对于输入词，我们查看Embedding矩阵。对于上下文单词，我们查看Context矩阵（即使两个矩阵都在我们的词汇表中嵌入了每个单词）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844600598212911.png'/>

然后，我们计算输入嵌入与每个上下文嵌入的点积。在每种情况下，结果都将是表示输入和上下文嵌入的相似性的数字。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684460147082011.png'/>

现在我们需要一种方法将这些分数转化为看起来像概率的东西——我们需要它们都是正值，并且 处于0到1之间。sigmoid这一逻辑函数转换正适合用来做这样的事情啦。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844602121676776.png'/>

现在我们可以将sigmoid操作的输出视为这些示例的模型输出。您可以看到taco得分最高，aaron最低，无论是sigmoid操作之前还是之后。

既然未经训练的模型已做出预测，而且我们确实拥有真实目标标签来作对比，那么让我们计算模型预测中的误差吧。为此我们只需从目标标签中减去sigmoid分数。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844602716214910.png'/>

error = target - sigmoid_scores

这是“机器学习”的“学习”部分。现在，我们可以利用这个错误分数来调整not、thou、aaron和taco的嵌入，使我们下一次做出这一计算时，结果会更接近目标分数。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844603139960024.png'/>

训练步骤到此结束。我们从中得到了这一步所使用词语更好一些的嵌入（not，thou，aaron和taco）。我们现在进行下一步（下一个相邻样本及其相关的非相邻样本），并再次执行相同的过程。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684460369423321.png'/>

当我们循环遍历整个数据集多次时，嵌入会继续得到改进。然后我们就可以停止训练过程，丢弃Context矩阵，并使用Embeddings矩阵作为下一项任务的已被训练好的嵌入。

窗口大小和负样本数量
word2vec训练过程中的两个关键超参数是窗口大小和负样本的数量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844605320678823.png'/>

不同的任务适合不同的窗口大小。一种启发式方法是，使用较小的窗口大小（2-15）会得到这样的嵌入：两个嵌入之间的高相似性得分表明这些单词是可互换的（注意，如果我们只查看附近距离很近的单词，反义词通常可以互换——例如，好的和坏的经常出现在类似的语境中）。使用较大的窗口大小（15-50，甚至更多）会得到相似性更能指示单词相关性的嵌入。在实际操作中，你通常需要对嵌入过程提供指导以帮助读者得到相似的”语感“。Gensim默认窗口大小为5（除了输入字本身以外还包括输入字之前与之后的两个字）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844605973404618.png'/>

负样本的数量是训练训练过程的另一个因素。原始论文认为5-20个负样本是比较理想的数量。它还指出，当你拥有足够大的数据集时，2-5个似乎就已经足够了。Gensim默认为5个负样本。

结论
我希望您现在对词嵌入和word2vec算法有所了解。我也希望现在当你读到一篇提到“带有负例采样的skipgram”（SGNS）的论文（如顶部的推荐系统论文）时，你已经对这些概念有了更好的认识。
## 请简要说说word2vec的来龙去脉/前世今生？
本题解析来源：https://www.cnblogs.com/iloveai/p/word2vec.html，July和助教远根特给此文补充了相关例子和表格，以为更通俗形象、一目了然。

2013年，Google开源了一款用于词向量计算的工具——word2vec，引起了工业界和学术界的关注。首先，word2vec可以在百万数量级的词典和上亿的数据集上进行高效地训练；其次，该工具得到的训练结果——词向量（word embedding），可以很好地度量词与词之间的相似性。

随着深度学习（Deep Learning）在自然语言处理中应用的普及，很多人误以为word2vec是一种深度学习算法。其实word2vec算法的背后是一个浅层神经网络，是一个计算word vector的开源工具。所以，当我们在说word2vec算法或模型的时候，其实指的是其背后用于计算word vector的CBoW模型和Skip-gram模型。

接下来，本文将从统计语言模型出发，尽可能详细地介绍word2vec工具背后的算法模型的来龙去脉。

Statistical Language Model
在深入word2vec算法的细节之前，我们首先回顾一下自然语言处理中的一个基本问题：如何计算一段文本序列在某种语言下出现的概率？之所为称其为一个基本问题，是因为它在很多NLP任务中都扮演着重要的角色。

例如，在机器翻译的问题中，如果我们知道了目标语言中每句话的概率，就可以从候选集合中挑选出最合理的句子做为翻译结果返回。统计语言模型给出了这一类问题的一个基本解决框架。对于一段文本序列S=w1,w2,...,wT，它的概率可以表示为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904223641520182.png'/>
即将序列的联合概率转化为一系列条件概率的乘积。

问题变成了如何去预测这些给定previous words下的条件概率：p(wt|w1,w2,...,wt−1)

由于其巨大的参数空间，这样一个原始的模型在实际中并没有什么卵用。我们更多的是采用其简化版本——Ngram模型：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904231571943173.png'/>

常见的如bigram模型（N=2N=2）和trigram模型（N=3N=3）。事实上，由于模型复杂度和预测精度的限制，我们很少会考虑N>3N>3的模型。

我们可以用最大似然法去求解Ngram模型的参数——等价于去统计每个Ngram的条件词频。

为了避免统计中出现的零概率问题（一段从未在训练集中出现过的Ngram片段会使得整个序列的概率为0），人们基于原始的Ngram模型进一步发展出了back-off trigram模型（用低阶的bigram和unigram代替零概率的trigram），和interpolated trigram模型（将条件概率表示为unigram、bigram、trigram三者的线性函数）。此处不再赘述。感兴趣者可进一步阅读相关的文献[3]。

Distributed Representation
不过，Ngram模型仍有其局限性。首先，由于参数空间的爆炸式增长，它无法处理更长程的context（N>3N>3）。

其次，它没有考虑词与词之间内在的联系性。例如，考虑"the cat is walking in the bedroom"这句话。如果我们在训练语料中看到了很多类似“the dog is walking in the bedroom”或是“the cat is running in the bedroom”这样的句子，那么，即使我们没有见过这句话，也可以从“cat”和“dog”（“walking”和“running”）之间的相似性，推测出这句话的概率[3]。
然而， Ngram模型做不到。这是因为，Ngram本质上是将词当做一个个孤立的原子单元（atomic unit）去处理的。这种处理方式对应到数学上的形式是一个个离散的one-hot向量（除了一个词典索引的下标对应的方向上是1，其余方向上都是0）。

例如，对于一个大小为5的词典：
{"I", "love", "nature", "luaguage", "processing"}，
“nature”对应的one-hot向量为：
[0,0,1,0,0]。

显然，one-hot向量的维度等于词典的大小。这在动辄上万甚至百万词典的实际应用中，面临着巨大的维度灾难问题（the curse of dimensionality）。

于是，人们就自然而然地想到，能否用一个连续的稠密向量去刻画一个word的特征呢？这样，我们不仅可以直接刻画词与词之间的相似度，还可以建立一个从向量到概率的平滑函数模型，使得相似的词向量可以映射到相近的概率空间上。这个稠密连续向量也被称为word的distributed representation[3]。

事实上，这个概念在信息检索（Information Retrieval）领域早就已经被广泛地使用了。只不过，在IR领域里，这个概念被称为向量空间模型（Vector Space Model，以下简称VSM）。

VSM是基于一种Statistical Semantics Hypothesis[4]：语言的统计特征隐藏着语义的信息（Statistical pattern of human word usage can be used to figure out what people mean）。例如，两篇具有相似词分布的文档可以被认为是有着相近的主题。

这个Hypothesis有很多衍生版本。其中，比较广为人知的两个版本是Bag of Words Hypothesis和Distributional Hypothesis。前者是说，一篇文档的词频（而不是词序）代表了文档的主题；后者是说，上下文环境相似的两个词有着相近的语义。后面我们会看到，word2vec算法也是基于Distributional的假设。

那么，VSM是如何将稀疏离散的one-hot词向量映射为稠密连续的distributional representation的呢？

简单来说，基于Bag of Words Hypothesis，我们可以构造一个term-document矩阵A：
矩阵的行Ai对应着词典里的一个word；
矩阵的列Aj对应着训练语料里的一篇文档；
矩阵里的元素Aij代表着word wi在文档Dj中出现的次数（或频率）。
那么，我们就可以提取行向量做为word的语义向量（不过，在实际应用中，我们更多的是用列向量做为文档的主题向量）。

恩，屏幕前的你现在是不是想问，有没例子呀？当然有的，我们基于这两个相似语句，例如
语句A：the dog is walking in the bedrom
语句B：the cat is running in the bedroom
构造term-document矩阵A，用于表示每一个词分别语句A和语句B中出现的次数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156905711095687650.png'/>
则提取行向量作为word的语义向量，思考：这是基于词频的，也可以基于此得到主题分布，但是缺少语义的考虑即上下文context。

类似地，我们可以基于Distributional Hypothesis构造一个word-context的矩阵。此时，矩阵的列变成了context里的word，矩阵的元素也变成了一个context窗口里word的共现次数。

举个例子：通过分析如下三个相似语句
语句A：I like deep learning.
语句B：I like NLP.
语句C：I enjoy flying.
构造word-content矩阵（设置窗口的大小为1），表示每个词与这三个语句中其他词相邻的次数（这个时候，必须上表格，便可瞬间清晰、一目了然）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156905680876441470.png'/>

注意，这两类矩阵的行向量所计算的相似度有着细微的差异：
term-document矩阵会给经常出现在同一篇document里的两个word赋予更高的相似度；
而word-context矩阵会给那些有着相同context的两个word赋予更高的相似度。

后者相对于前者是一种更高阶的相似度，因此在传统的信息检索领域中得到了更加广泛的应用。不过，这种co-occurrence矩阵仍然存在着数据稀疏性和维度灾难的问题。为此，人们提出了一系列对矩阵进行降维的方法（如LSI／LSA等）。这些方法大都是基于SVD的思想，将原始的稀疏矩阵分解为两个低秩矩阵乘积的形式。

关于VSM更多的介绍，可以进一步阅读文末的参考文献[4]。

Neural Network Language Model
接下来，让我们回到对统计语言模型的讨论。鉴于Ngram等模型的不足，2003年，Bengio等人发表了一篇开创性的文章：A neural probabilistic language model[3]。在这篇文章里，他们总结出了一套用神经网络建立统计语言模型的框架（Neural Network Language Model，以下简称NNLM），并首次提出了word embedding的概念（虽然没有叫这个名字），从而奠定了包括word2vec在内后续研究word representation learning的基础。

NNLM模型的基本思想可以概括如下：
①假定词表中的每一个word都对应着一个连续的特征向量；
②假定一个连续平滑的概率模型，输入一段词向量的序列，可以输出这段序列的联合概率；
③同时学习词向量的权重和概率模型里的参数。
值得注意的一点是，这里的词向量也是要学习的参数。

在03年的论文里，Bengio等人采用了一个简单的前向反馈神经网络f(wt−n+1,...,wt)来拟合一个词序列的条件概率p(wt|w1,w2,...,wt−1)。
整个模型的网络结构见下图：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904095373575974.png'/>

我们可以将整个模型拆分成两部分加以理解：
    i)首先是一个线性的embedding层。它将输入的N−1个one-hot词向量，通过一个共享的D×V的矩阵C，映射为N−1个分布式的词向量（distributed vector）。其中，V是词典的大小，D是embedding向量的维度（一个先验参数）。C矩阵里存储了要学习的word vector。
    ii)其次是一个简单的前向反馈神经网络g。它由一个tanh隐层和一个softmax输出层组成。

通过将embedding层输出的N−1N−1个词向量映射为一个长度为VV的概率分布向量，从而对词典中的word在输入context下的条件概率做出预估：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904307614253337.png'/>

我们可以通过最小化一个cross-entropy的正则化损失函数来调整模型的参数θ：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904311795942433.png'/>

其中，模型的参数θ包括了embedding层矩阵C的元素，和前向反馈神经网络模型g里的权重。这是一个巨大的参数空间。不过，在用SGD学习更新模型的参数时，并不是所有的参数都需要调整（例如未在输入的context中出现的词对应的词向量）。计算的瓶颈主要是在softmax层的归一化函数上（需要对词典中所有的word计算一遍条件概率）。

然而，抛却复杂的参数空间，我们不禁要问，为什么这样一个简单的模型会取得巨大的成功呢？仔细观察这个模型就会发现，它其实在同时解决两个问题：一个是统计语言模型里关注的条件概率p(wt|context)的计算；一个是向量空间模型里关注的词向量的表达。而这两个问题本质上并不独立。通过引入连续的词向量和平滑的概率模型，我们就可以在一个连续空间里对序列概率进行建模，从而从根本上缓解数据稀疏性和维度灾难的问题。

另一方面，以条件概率p(wt|context)为学习目标去更新词向量的权重，具有更强的导向性，同时也与VSM里的Distributional Hypothesis不谋而合。CBoW & Skip-gram Model铺垫了这么多，终于要轮到主角出场了。

不过在主角正式登场前，我们先看一下NNLM存在的几个问题。一个问题是，同Ngram模型一样，NNLM模型只能处理定长的序列。在03年的论文里，Bengio等人将模型能够一次处理的序列长度N提高到了5，虽然相比bigram和trigram已经是很大的提升，但依然缺少灵活性。因此，Mikolov等人在2010年提出了一种RNNLM模型[7]，用递归神经网络代替原始模型里的前向反馈神经网络，并将embedding层与RNN里的隐藏层合并，从而解决了变长序列的问题。

另一个问题就比较严重了。NNLM的训练太慢了。即便是在百万量级的数据集上，即便是借助了40个CPU进行训练，NNLM也需要耗时数周才能给出一个稍微靠谱的解来。显然，对于现在动辄上千万甚至上亿的真实语料库，训练一个NNLM模型几乎是一个impossible mission。这时候，还是那个Mikolov站了出来。他注意到，原始的NNLM模型的训练其实可以拆分成两个步骤：
用一个简单模型训练出连续的词向量；
基于词向量的表达，训练一个连续的Ngram神经网络模型。
而NNLM模型的计算瓶颈主要是在第二步。

如果我们只是想得到word的连续特征向量，是不是可以对第二步里的神经网络模型进行简化呢？Mikolov是这么想的，也是这么做的。他在2013年一口气推出了两篇paper，并开源了一款计算词向量的工具——至此，word2vec横空出世，主角闪亮登场。

下面，我将带领大家简单剖析下word2vec算法的原理。有了前文的基础，理解word2vec算法就变得很简单了。首先，我们对原始的NNLM模型做如下改造：
1 移除前向反馈神经网络中非线性的hidden layer，直接将中间层的embedding layer与输出层的softmax layer连接；
2 忽略上下文环境的序列信息：输入的所有词向量均汇总到同一个embedding layer；
3 将future words纳入上下文环境

得到的模型称之为CBoW模型（Continuous Bag-of-Words Model），也是word2vec算法的第一个模型：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904097679110071.png'/>

从数学上看，CBoW模型等价于一个词袋模型的向量乘以一个embedding矩阵，从而得到一个连续的embedding向量。这也是CBoW模型名称的由来。CBoW模型依然是从context对target word的预测中学习到词向量的表达。反过来，我们能否从target word对context的预测中学习到word vector呢？答案显然是可以的：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904099658168993.png'/>
这个模型被称为Skip-gram模型（名称源于该模型在训练时会对上下文环境里的word进行采样）。如果将Skip-gram模型的前向计算过程写成数学形式，我们得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904107834080692.png'/>
其中，Vi是embedding层矩阵里的列向量，也被称为wi的input vector。Uj是softmax层矩阵里的行向量，也被称为wj的output vector。

因此，Skip-gram模型的本质是计算输入word的input vector与目标word的output vector之间的余弦相似度，并进行softmax归一化。我们要学习的模型参数正是这两类词向量。

然而，直接对词典里的V个词计算相似度并归一化，显然是一件极其耗时的impossible mission。为此，Mikolov引入了两种优化算法：层次Softmax（Hierarchical Softmax）和负采样（Negative Sampling）。

Hierarchical Softmax[5]
层次Softmax的方法最早由Bengio在05年引入到语言模型中。它的基本思想是将复杂的归一化概率分解为一系列条件概率乘积的形式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904345491578560.png'/>

其中，每一层条件概率对应一个二分类问题，可以通过一个简单的逻辑回归函数去拟合。这样，我们将对V个词的概率归一化问题，转化成了对logV个词的概率拟合问题。

我们可以通过构造一颗分类二叉树来直观地理解这个过程。首先，我们将原始字典D划分为两个子集D1、D2，并假设在给定context下，target word属于子集D1的概率p(wt∈D1|context)服从logistical function的形式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904122895000025.png'/>
其中，UDroot和Vwt都是模型的参数。

接下来，我们可以对子集D1和D2进一步划分。重复这一过程，直到集合里只剩下一个word。这样，我们就将原始大小为V的字典DD转换成了一颗深度为logV的二叉树。树的叶子节点与原始字典里的word一一对应；非叶节点则对应着某一类word的集合。显然，从根节点出发到任意一个叶子节点都只有一条唯一路径——这条路径也编码了这个叶子节点所属的类别。

同时，从根节点出发到叶子节点也是一个随机游走的过程。因此，我们可以基于这颗二叉树对叶子节点出现的似然概率进行计算。例如，对于训练样本里的一个target word wt，假设其对应的二叉树编码为{1,0,1,...,1}，则我们构造的似然函数为：
p(wt|context)=p(D1=1|context)p(D2=0|D1=1)…p(wt|Dk=1)

乘积中的每一项都是一个逻辑回归的函数。

我们可以通过最大化这个似然函数来求解二叉树上的参数——非叶节点上的向量，用来计算游走到某一个子节点的概率。层次Softmax是一个很巧妙的模型。它通过构造一颗二叉树，将目标概率的计算复杂度从最初的V降低到了logV的量级。不过付出的代价是人为增强了词与词之间的耦合性。例如，一个word出现的条件概率的变化，会影响到其路径上所有非叶节点的概率变化，间接地对其他word出现的条件概率带来不同程度的影响。因此，构造一颗有意义的二叉树就显得十分重要。实践证明，在实际的应用中，基于Huffman编码的二叉树可以满足大部分应用场景的需求。

Negative Sampling[6]
负采样的思想最初来源于一种叫做Noise-Contrastive Estimation的算法[6]，原本是为了解决那些无法归一化的概率模型的参数预估问题。与改造模型输出概率的层次Softmax算法不同，NCE算法改造的是模型的似然函数。以Skip-gram模型为例，其原始的似然函数对应着一个Multinomial的分布。在用最大似然法求解这个似然函数时，我们得到一个cross-entropy的损失函数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904138341967126.png'/>

式中的p(wt+j|wt)是一个在整个字典上归一化了的概率。而在NCE算法中，我们构造了这样一个问题：对于一组训练样本，我们想知道，target word的出现，是来自于context的驱动，还是一个事先假定的背景噪声的驱动？显然，我们可以用一个逻辑回归的函数来回答这个问题：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904147178192128.png'/>
这个式子给出了一个target word w来自于context驱动的概率。其中，k是一个先验参数，表明噪声的采样频率。p(w|context)是一个非归一化的概率分布，这里采用softmax归一化函数中的分子部分。pn(w)pn(w)则是背景噪声的词分布。通常采用word的unigram分布。

通过对噪声分布的kk采样，我们得到一个新的数据集：。其中，label标记了数据的来源（真实数据分布还是背景噪声分布？）。在这个新的数据集上，我们就可以用最大化上式中逻辑回归的似然函数来求解模型的参数。而Mikolov在2013年的论文里提出的负采样算法， 是NCE的一个简化版本。在这个算法里，Mikolov抛弃了NCE似然函数中对噪声分布的依赖，直接用原始softmax函数里的分子定义了逻辑回归的函数，进一步简化了计算：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904153361440918.png'/>
此时，模型相应的目标函数变为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641569041599475384.png'/>
J(θ)=logσ(Uo⋅Vi)+∑j=1kEwj∼pn(w)[logσ(−Uj⋅Vi)]J(θ)=log⁡σ(Uo⋅Vi)+∑j=1kEwj∼pn(w)[log⁡σ(−Uj⋅Vi)]除了这里介绍的层次Softmax和负采样的优化算法，Mikolov在13年的论文里还介绍了另一个trick：下采样（subsampling）。其基本思想是在训练时依概率随机丢弃掉那些高频的词：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904166546122186.png'/>
其中，tt=是一个先验参数，一般取为10^5。f(w)是w在语料中出现的频率。实验证明，这种下采样技术可以显著提高低频词的词向量的准确度。

Beyond the Word Vector
介绍完word2vec模型的算法和原理，我们来讨论一些轻松点的话题——模型的应用。13年word2vec模型横空出世后，人们最津津乐道的是它学到的向量在语义和语法相似性上的应用——尤其是这种相似性居然对数学上的加减操作有意义[8]！

最经典的一个例子是，v("King")−v("Man")+v("Woman")=v("Queen")。然而，这种例子似乎并没有太多实际的用途。

除此之外，word2vec模型还被应用于机器翻译和推荐系统领域。Machine Translation[9]与后来提出的在sentence level上进行机器翻译的RNN模型不同，word2vec模型主要是用于词粒度上的机器翻译。具体来说，我们首先从大量的单语种语料中学习到每种语言的word2vec表达，再借助一个小的双语语料库学习到两种语言word2vec表达的线性映射关系WW。构造的损失函数为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904177816033839.png'/>
在翻译的过程中，我们首先将源语言的word2vec向量通过矩阵WW映射到目标语言的向量空间上；再在目标语言的向量空间中找出与投影向量距离最近的word做为翻译的结果返回。

其原理是，不同语言学习到的word2vec向量空间在几何上具有一定的同构性。映射矩阵W本质上是一种空间对齐的线性变换。

Item2Vec[11]
本质上，word2vec模型是在word-context的co-occurrence矩阵基础上建立起来的。因此，任何基于co-occurrence矩阵的算法模型，都可以套用word2vec算法的思路加以改进。比如，推荐系统领域的协同过滤算法。协同过滤算法是建立在一个user-item的co-occurrence矩阵的基础上，通过行向量或列向量的相似性进行推荐。如果我们将同一个user购买的item视为一个context，就可以建立一个item-context的矩阵。进一步的，可以在这个矩阵上借鉴CBoW模型或Skip-gram模型计算出item的向量表达，在更高阶上计算item间的相似度。关于word2vec更多应用的介绍，可以进一步参考这篇文献[10]。

Word Embedding
最后，我想简单阐述下我对word embedding的几点思考。不一定正确，也欢迎大家提出不同的意见。

Word embedding最早出现于Bengio在03年发表的开创性文章中[3]。通过嵌入一个线性的投影矩阵（projection matrix），将原始的one-hot向量映射为一个稠密的连续向量，并通过一个语言模型的任务去学习这个向量的权重。这一思想后来被广泛应用于包括word2vec在内的各种NLP模型中。

Word embedding的训练方法大致可以分为两类：
一类是无监督或弱监督的预训练；
一类是端对端（end to end）的有监督训练。

无监督或弱监督的预训练以word2vec和auto-encoder为代表。这一类模型的特点是，不需要大量的人工标记样本就可以得到质量还不错的embedding向量。不过因为缺少了任务导向，可能和我们要解决的问题还有一定的距离。因此，我们往往会在得到预训练的embedding向量后，用少量人工标注的样本去fine-tune整个模型。

相比之下，端对端的有监督模型在最近几年里越来越受到人们的关注。与无监督模型相比，端对端的模型在结构上往往更加复杂。同时，也因为有着明确的任务导向，端对端模型学习到的embedding向量也往往更加准确。例如，通过一个embedding层和若干个卷积层连接而成的深度神经网络以实现对句子的情感分类，可以学习到语义更丰富的词向量表达。

Word embedding的另一个研究方向是在更高层次上对sentence的embedding向量进行建模。我们知道，word是sentence的基本组成单位。一个最简单也是最直接得到sentence embedding的方法是将组成sentence的所有word的embedding向量全部加起来——类似于CBoW模型。显然，这种简单粗暴的方法会丢失很多信息。
另一种方法借鉴了word2vec的思想——将sentence或是paragraph视为一个特殊的word，然后用CBoW模型或是Skip-gram进行训练[12]。

这种方法的问题在于，对于一篇新文章，总是需要重新训练一个新的sentence2vec。此外，同word2vec一样，这个模型缺少有监督的训练导向。个人感觉比较靠谱的是第三种方法——基于word embedding的端对端的训练。Sentence本质上是word的序列。因此，在word embedding的基础上，我们可以连接多个RNN模型或是卷积神经网络，对word embedding序列进行编码，从而得到sentence embedding。这方面的工作已有很多。有机会，我会再写一篇关于sentence embedding的综述。

References
[1]: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013, January 17). Efficient Estimation of Word Representations in Vector Space. arXiv.org.
[2]: Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013, October 17). Distributed Representations of Words and Phrases and their Compositionality. arXiv.org.
[3]: Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). A neural probabilistic language model. The Journal of Machine Learning Research, 3, 1137–1155.
[4]: Turney, P. D., & Pantel, P. (2010). From frequency to meaning: vector space models of semantics. Journal of Artificial Intelligence Research, 37(1).

[5]: Morin, F., & Bengio, Y. (2005). Hierarchical Probabilistic Neural Network Language Model. Aistats.
[6]: Mnih, A., & Kavukcuoglu, K. (2013). Learning word embeddings efficiently with noise-contrastive estimation, 2265–2273.
[7]: Mikolov, T., Karafiát, M., Burget, L., & Cernocký, J. (2010). Recurrent neural network based language model. Interspeech.
[8]: Mikolov, T., Yih, W., & Zweig, G. (2013). Linguistic Regularities in Continuous Space Word Representations. Hlt-Naacl.

[9]: Mikolov, T., Le, Q. V., & Sutskever, I. (2013, September 17). Exploiting Similarities among Languages for Machine Translation. arXiv.org.
[10]: Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011). Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12(Aug), 2493–2537.
[11]: Barkan, O., & Koenigstein, N. (2016, March 14). Item2Vec: Neural Item Embedding for Collaborative Filtering. arXiv.org.
[12]: Le, Q. V., & Mikolov, T. (2014, May 16). Distributed Representations of Sentences and Documents. arXiv.org
.

## 了解什么是正则表达式么？
本题解析来源：https://www.liaoxuefeng.com/wiki/897692888725344/923056128128864，配图则来自七月在线自然语言处理课程。

字符串是编程时涉及到的最多的一种数据结构，对字符串进行操作的需求几乎无处不在。比如判断一个字符串是否是合法的Email地址，虽然可以编程提取@前后的子串，再分别判断是否是单词和域名，但这样做不但麻烦，而且代码难以复用。

正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。

所以我们判断一个字符串是否是合法的Email的方法是：
1 创建一个匹配Email的正则表达式；
2 用该正则表达式去匹配用户的输入来判断是否合法。

因为正则表达式也是用字符串表示的，所以，我们要首先了解如何用字符来描述字符。在正则表达式中，如果直接给出字符，就是精确匹配。用\d可以匹配一个数字，\w可以匹配一个字母或数字，所以：
  &#39;00\d&#39;可以匹配&#39;007&#39;，但无法匹配&#39;00A&#39;；
  &#39;\d\d\d&#39;可以匹配&#39;010&#39;；
  &#39;\w\w\d&#39;可以匹配&#39;py3&#39;；
.可以匹配任意字符，
所以：
&#39;py.&#39;可以匹配&#39;pyc&#39;、&#39;pyo&#39;、&#39;py!&#39;等等。

要匹配变长的字符，在正则表达式中，用*表示任意个字符（包括0个），用+表示至少一个字符，用?表示0个或1个字符，用{n}表示n个字符，用{n,m}表示n-m个字符：

来看一个复杂的例子：\d{3}\s+\d{3,8}。我们来从左到右解读一下：
  \d{3}表示匹配3个数字，例如&#39;010&#39;；
  \s可以匹配一个空格（也包括Tab等空白符），所以\s+表示至少有一个空格，例如匹配&#39; &#39;，&#39; &#39;等；
  \d{3,8}表示3-8个数字，例如&#39;1234567&#39;。
综合起来，上面的正则表达式可以匹配以任意个空格隔开的带区号的电话号码。

如果要匹配&#39;010-12345&#39;这样的号码呢？由于&#39;-&#39;是特殊字符，在正则表达式中，要用&#39;\&#39;转义，所以，上面的正则是\d{3}\-\d{3,8}。

但是，仍然无法匹配&#39;010 - 12345&#39;，因为带有空格。所以我们需要更复杂的匹配方式。

进阶
要做更精确地匹配，可以用[]表示范围，比如：
  [0-9a-zA-Z\_]可以匹配一个数字、字母或者下划线；
  [0-9a-zA-Z\_]+可以匹配至少由一个数字、字母或者下划线组成的字符串，比如&#39;a100&#39;，&#39;0_Z&#39;，&#39;Py3000&#39;等等；
  [a-zA-Z\_][0-9a-zA-Z\_]*可以匹配由字母或下划线开头，后接任意个由一个数字、字母或者下划线组成的字符串，也就是Python合法的变量；
  [a-zA-Z\_][0-9a-zA-Z\_]{0, 19}更精确地限制了变量的长度是1-20个字符（前面1个字符+后面最多19个字符）。

A|B可以匹配A或B，所以(P|p)ython可以匹配&#39;Python&#39;或者&#39;python&#39;。
  ^表示行的开头，^\d表示必须以数字开头。
  $表示行的结束，\d$表示必须以数字结束。
你可能注意到了，py也可以匹配&#39;python&#39;，但是加上^py$就变成了整行匹配，就只能匹配&#39;py&#39;了。

re模块
有了准备知识，我们就可以在Python中使用正则表达式了。Python提供re模块，包含所有正则表达式的功能。由于Python的字符串本身也用\\u8f6c义，所以要特别注意：
s = &#39;ABC\\-001&#39; # Python的字符串
# 对应的正则表达式字符串变成：
# &#39;ABC\-001&#39;
因此我们强烈建议使用Python的r前缀，就不用考虑转义的问题了：
s = r&#39;ABC\-001&#39; 
# Python的字符串
# 对应的正则表达式字符串不变：
# &#39;ABC\-001&#39;

先看看如何判断正则表达式是否匹配：
>>> import re
>>> re.match(r&#39;^\d{3}\-\d{3,8}$&#39;, &#39;010-12345&#39;)
>>> re.match(r&#39;^\d{3}\-\d{3,8}$&#39;, &#39;010 12345&#39;)
>>>

match()方法判断是否匹配，如果匹配成功，返回一个Match对象，否则返回None。常见的判断方法就是：
test = &#39;用户输入的字符串
&#39;if re.match(r&#39;正则表达式&#39;, test):
    print &#39;ok&#39;
else:
    print &#39;failed&#39;

切分字符串用
正则表达式切分字符串比用固定的字符更灵活，请看正常的切分代码：
>>> &#39;a b   c&#39;.split(&#39; &#39;)
[&#39;a&#39;, &#39;b&#39;, &#39;&#39;, &#39;&#39;, &#39;c&#39;]

嗯，无法识别连续的空格，用正则表达式试试：
>>> re.split(r&#39;\s+&#39;, &#39;a b   c&#39;)
[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]

无论多少个空格都可以正常分割。加入,试试：
>>> re.split(r&#39;[\s\,]+&#39;, &#39;a,b, c  d&#39;)
[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]

再加入;试试：
>>> re.split(r&#39;[\s\,\;]+&#39;, &#39;a,b;; c  d&#39;)
[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]

如果用户输入了一组标签，下次记得用正则表达式来把不规范的输入转化成正确的数组。

分组
除了简单地判断是否匹配之外，正则表达式还有提取子串的强大功能。用()表示的就是要提取的分组（Group）。比如：
^(\d{3})-(\d{3,8})$分别定义了两个组，可以直接从匹配的字符串中提取出区号和本地号码：
>>> m = re.match(r&#39;^(\d{3})-(\d{3,8})$&#39;, &#39;010-12345&#39;)
>>> m

>>> m.group(0)
&#39;010-12345&#39;
>>> m.group(1)
&#39;010&#39;
>>> m.group(2)
&#39;12345&#39;

如果正则表达式中定义了组，就可以在Match对象上用group()方法提取出子串来。

注意到group(0)永远是原始字符串，group(1)、group(2)……表示第1、2、……个子串。提取子串非常有用。

来看一个更凶残的例子：
>>> t = &#39;19:05:30&#39;
>>> m = re.match(r&#39;^(0[0-9]|1[0-9]|2[0-3]|[0-9])\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])$&#39;, t)>>> m.groups()
(&#39;19&#39;, &#39;05&#39;, &#39;30&#39;)

这个正则表达式可以直接识别合法的时间。但是有些时候，用正则表达式也无法做到完全验证，比如识别日期：
&#39;^(0[1-9]|1[0-2]|[0-9])-(0[1-9]|1[0-9]|2[0-9]|3[0-1]|[0-9])$&#39;

对于&#39;2-30&#39;，&#39;4-31&#39;这样的非法日期，用正则还是识别不了，或者说写出来非常困难，这时就需要程序配合识别了。

贪婪匹配
最后需要特别指出的是，正则匹配默认是贪婪匹配，也就是匹配尽可能多的字符。举例如下，匹配出数字后面的0：
>>> re.match(r&#39;^(\d+)(0*)$&#39;, &#39;102300&#39;).groups()
(&#39;102300&#39;, &#39;&#39;)

由于\d+采用贪婪匹配，直接把后面的0全部匹配了，结果0*只能匹配空字符串了。

必须让\d+采用非贪婪匹配（也就是尽可能少匹配），才能把后面的0匹配出来，加个?就可以让\d+采用非贪婪匹配：
>>> re.match(r&#39;^(\d+?)(0*)$&#39;, &#39;102300&#39;).groups()
(&#39;1023&#39;, &#39;00&#39;)

编译
当我们在Python中使用正则表达式时，re模块内部会干两件事情：
  i)编译正则表达式，如果正则表达式的字符串本身不合法，会报错；
  ii)用编译后的正则表达式去匹配字符串。

如果一个正则表达式要重复使用几千次，出于效率的考虑，我们可以预编译该正则表达式，接下来重复使用时就不需要编译这个步骤了，直接匹配：
>>> import re
# 编译:
>>> re_telephone = re.compile(r&#39;^(\d{3})-(\d{3,8})$&#39;)
# 使用：
>>> re_telephone.match(&#39;010-12345&#39;).groups()
(&#39;010&#39;, &#39;12345&#39;)
>>> re_telephone.match(&#39;010-8086&#39;).groups()
(&#39;010&#39;, &#39;8086&#39;)

编译后生成Regular Expression对象，由于该对象自己包含了正则表达式，所以调用对应的方法时不用给出正则字符串。

练习与总结
请尝试写一个验证Email地址的正则表达式。
版本一应该可以验证出类似的Email：
someone@gmail.com
bill.gates@microsoft.com

版本二可以验证并提取出带名字的Email地址： tom@voyager.org

最后为方便快速查阅，附一张正则表达式的快速查阅图
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156912802151684336.png'/>
## 如何理解NNLM（Neural Network Language Model）模型？
本题解析来源：https://blog.csdn.net/qq_39422642/article/details/78658309?tdsourcetag=s_pcqq_aiomsg，参考：https://shomy.top/2017/07/28/word2vec-all/

1 基本概念
传统的机器翻译，自然语言处理多是基于规则的，现在更多的是基于模型，规则隐含的参数里。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415692507657682586.png'/>

词编码 
每个词都用不同的含义，而要被机器所识别，就必须要把词进行编码，同时词编码时要保证词的相似性。图像识别的时候，对图像在RGB三个颜色通道中看他们的相似性就可以了，但是，无论中文还是英文，词都太多了，他是人造的，很难保持像图片这样的信息，所以我们希望能对词进行编码，保持它所包含的信息量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925077029995836.png'/>
因此，我们希望能有一个对应关系，如图，这些数字在空间中的表示能有一个对应关系。这不就是和机器学习差不多吗？很多机器学习的预测都是寻找一个对应关系，也就是数据（X）和预测的东西（Y）的对应。机器翻译其实原理也差不多。 


向量空间子结构 
我们希望找到这样一个关系，可以作为机器学习/深度学习的输入.
VKing−VQueen+VWomen=VMan
VKing−VQueen+VWomen=VMan


这个有没有感觉呢？其实换成这样，你可能更好理解:
VKing−VQueen=VMan−VWomen
VKing−VQueen=VMan−VWomen
我们就是希望找到King,QueenKing,Queen之间的差异，隐含的关系，然后通过一个dense vector表示。

One-Hot 
最简单的一种想法，就是对一句话用one-hot编码:比如对于这句话：
John likes to watch movies,Mary likes too.
John also likes to watch football games.

"John":1,"likes":2,"to":3,"watch":4,"movies":5,"also":6,"football":7,"games":8,"Mary":9,"too":10

用one-hot可以表示为：
John:[1,0,0,0,0,0,0,0,0,0]
likes:[0,1,0,0,0,0,0,0,0,0]
...
too:[0,0,0,0,0,0,0,0,0,1]

但事实是，这样做耗费的资源太多，而且不能很好的表征每句话的特性。

Bag of words(词袋模型) 
另一种方法则是词袋模型，它相当于一个词袋，不考虑词/句之间的相关性，只要出现了该词，就会记为1，再次出现就会+1。比如前面的那句话：
John likes to watch movies,Mary likes too.

可以表示为
[1,2,1,1,1,0,0,0,1,1]

与其相似的是binary weighting,它就是看一下每个词是否出现，出现记为1，没出现记为0
[1,1,1,1,1,0,0,0,1,1]

但他们都有一个缺点，不能将每个单词所代表的中心表示出来，比如John 和to,watch他们都是1，怎么直到那个词更重要呢？

TF-IDF 
因此，就有tf-idf解决这个问题，它的主要思路就是有两方面： 
A—第一就是如果这个词在我们当前文档出现的频率非常高，说明它在当前文档应该是比较重要的。 
B-但如果它在所有的文档中出现的频次都非常好，大家可能就会觉得这个词应该不是那么重要的。

比如中文的“的“，或者我们上面那两个句子中的to. 
因此，tf-idf就是一个在当前文档和所有文档中权衡他们的重要性，然后计算出每个词的重要度的方法。

语言模型 作为Word Embedding的背景, 语言模型(Language Model)也是很有必要简要介绍一下。
统计语言模型就是用来计算一个句子的概率分布简单来说，就是计算一个句子的概率, 语言模型用处很广泛,比如机器翻译中, 如何挑选一个概率尽可能大的句子也就是尽量靠谱的句子返回. 假设一个长度为m的句子,包含词:[(w1,w2,w3,..,wm), 
那么这个句子的概率,也就是这m个词共现的概率:<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925419251452802.png'/>

一般情况, 语言模型都是为了使得条件概率: P(wt|w1,w2,..,wt−1)最大化, 不过考虑到近因效应, 当前词与距离它比较近的n个词更加相关，而非前面所有的词都有关, 因此上述公式可以近似为:<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925443879348816.png'/>
上述便是经典的n-gram模型的近似表示方式。

语言模型可以考虑到词之间的前后关系，缺点就是n-gram随着预料的增多，离散化越严重，最终导致数据稀疏的问题。

分布式表示 
如果数据的维度过高，经常需要用到分布式来表示，它是个什么东西呢？ 
比如有这么一个例子：
红色的大型卡车，黄色的中型轿车，蓝色的小型电动车

可以发现，这几个词之间都有一定的模式存在，我们可以通过这些模式，把表达的空间压缩的三个维度：颜色，车型，那个品牌的车。然后他们做一个笛卡尔积就可以有:
需要记忆的单元数=颜色 X 型号 X 车型

其中，在现代统计自然语言处理中，有一个非常有洞见的想法，就是：
能否用一个词附近的其他词来表示该词？

就像你认识一个新认识的朋友，想直到他的收入，你可以看一下他周围10个朋友的收入大致是多少，来推断他的收入是多少一样。

共现矩阵 
基于前面那个想法，就有了这样一个局域窗口，这个窗口的主要作用就是看一下周围的几个词才好，窗口的长度一般设为5-10。就像你看你朋友的收入一样，你是看周边的五个人还是10个人呢？

那他具体怎么表示呢？ 
假设为三句话： 
I like deep learning,
I like NLP,
I enjoy flying

假设局域窗口为1，可以得到这样的对称矩阵 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925104327717944.png'/>

可以发现I 的前后一共出现了2次like，所以第一行第二列是2。
其实共现矩阵的本质代表着词与词之间的连接，且不考虑顺序。因为他是对称的，所以一般会取行向量或者列向量来表示词向量。

其中面临的问题就是向量的维数随着词典的大小呈先行增长，且对于模型有着严重的稀疏性问题。

2.NNLM（Neural Network Language Model）神经网络语言模型
NNLM的基本思想
在一开始的时候，做自然语言处理可以发现很多问题，比如很多情况下，要做平滑处理等，因此深度学习慢慢开始火了之后，就有人说，不然咱来试一下神经网络来做这个吧。


他的本质就是：直接从语言模型出发，将模型最优化的过程转换为求词向量表示的过程。 
最优的方向是这个目标函数：用“我爱自然语言处理“为例,窗长度为n-1：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925110098567176.png'/>
这个是n=3的通俗表达。 

概率p满足归一化条件，因为词典中的每个词都有一个概率:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925108395493047.png'/>

NNLM原理 
神经语言模型(NNLM)最初由Bengio提出的A Neural Probabilistic Language Mode最为经典， word2vec便是从其中简化训练而来. Bengio通过下面的一个三层神经网络来计算P(wt|wt−1,wt−2...wt−n(+1))。
其原理图如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925110832083434.png'/>
首先第一层输入就是前n−1个词wt−(n+1),...,wt−1 去预测第t个词是wt 的概率. 这里面的矩阵C∈|V|×d维护着词汇表中所有词的词向量, 其中|V|是词汇表中词的个数, d是词向量的维度. 然后根据输入的前n−1个词, 在C中找到它们对应的词向量, 然后直接串联起来成为一个维度为(n−1)d的向量x 作为接下来三层神经网络的输入，后面就是普通神经网络了。

需要说明的是,因为我们那要预测概率最大的wt, 因此最后输出层的神经元应该与词汇表大小同样为|V|, 这里使用使用softmax函数归一化输出层的值到[0,1], 代表可能的每个词的概率. 此外在原文中, 存在一些直连边, 也就是上图中的虚线, 从输入层直接到输出层，是一个线性变换。Bingo在文中表示, 直连边的存在大幅降低迭代次数, 但对语言模型效果无提升, 随着计算能力的提高, 后续的工作基本都去掉了直连边。

整个模型可以解决类似这样的问题：假设我们有一分文本，我们可以通过他的前N−1个词预测他的第N个词应该是什么？

projection layer 
举个例子，有这样一个句子：“我爱自然语言处理“，拆开就是‘我’，’爱’，’自然’，’语言’，一共四个词，预测一下，下一个词是什么？ 
如果每个词都给一个索引表示，’我’为0，’爱’为1。

C矩阵为投影矩阵，其中词典的维数为v，假设v=10000。 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925113218237384.png'/>
那么‘我‘和‘爱‘的one-hot向量表示为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415692511387257335.png'/>

每个列向量都有D行，那么一个词对应的列向量乘以这个矩阵C就可以得到
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925115965306031.png'/>

这样的乘法相当于把一个句子中的一个词取出来了，将每个词的结果进行拼接，就可以得到一句话，这句话的向量表示为
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925115378871189.png'/>的列向量。

总结一下这一层做的事：主要是把一句话用one-hot向量表示，通过一个权重矩阵，得到表示这一句话的词向量。

hidden layer 
这一部分主要做的就是将上一层的输出作为输入，进行全连接，然后一般会有个tanh，来处理这些数据。

SoftMax层 
隐层出来之后，接一个SoftMax分类器，预测一下，在这10000个词的词表中，出现每个单词出现概率有多大。因此拿到的是一个10000X1 的概率向量。 

因为我们有标准答案，就是”我爱自然语言”的第五个词应该是“处理“，如果预测出来的不准确，就可以通过定义一个交叉熵损失函数来计算损失，通过BP算法来调整参数C。
## 请详细推导下word2vec(Xin Rong牛论文的解读)
本题解析来源：https://shomy.top/2017/07/28/word2vec-all/ 

目前需要做Network Embedding方面的课题，而复杂网络本身就经常借鉴NLP的一些算法模型, Embedding也不例外. 因此先从Word Embedding入手。之前对Word Embedding(暂且翻译为词嵌入或者词向量)的理解就是将单词根据某种特征转为数值向量，再来做其他工作比如文本分类的工作。而word2vec则是word embedding的一种模型，也是目前使用最广的词向量模型, 由Google的Mikolov团队2013年提出。之前仅仅能够使用第三方库来训练直接使用, 对其中原理并没有多少理解, 这篇博客则比较完整的从背景知识到原理，参数训练等方面整理一下word2Vec。

Mikolov的两篇文章中涉及word2vec的细节甚少. 有不少人都对此模型作出了更加详细的解释, 本文主要沿着Rong, X.word2vec Parameter Learning Explained这篇文章的思路来整理一下，很多公式参考于这篇文章。

参考:
Mikolov, T.(2013). Distributed Representations of Words and Phrases and their Compositionality.
Mikolov, T.(2013). Efficient Estimation of Word Representations in Vector Space.
Rong, X. (2014). word2vec Parameter Learning Explained.

背景
神经网络
引入word2vec之前需要先对神经网络的知识有一定了解, 这里只贴一张图说明一个简单三层神经网络，如下图, x=[x1,x2,...xK]是模型的输入, 中间经过与权重矩阵<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932500382113185.png'/>运算, 矩阵运算结果再经过非线性激活函数得到隐层的结果h, 从隐层到输出层同理. 这样从输入层到输出层既有线性变换,又有非线性变换, 因此可以更好刻画出输入变量的特征。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932503956394300.png'/>
神经语言模型
作为Word Embedding的背景, 语言模型(Language Model)也是很有必要简要介绍一下.

统计语言模型就是用来计算一个句子的概率分布

简单来说，就是计算一个句子的概率, 语言模型用处很广泛,比如机器翻译中, 如何挑选一个概率尽可能大的句子也就是尽量靠谱的句子返回. 假设一个长度为m的句子,包含词:[(w1,w2,w3,..,wm)，那么这个句子的概率,也就是这m个词共现的概率:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932513663330365.png'/>

一般情况, 语言模型都是为了使得条件概率: P(wt|w1,w2,..,wt−1)最大化, 不过考虑到近因效应, 当前词与距离它比较近的n个词更加相关(一般n不超过5),而非前面所有的词都有关, 因此上述公式可以近似为:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415693252738649414.png'/>

上述便是经典的n-gram模型的近似表示方式. 下面需要介绍一下神经语言模型(NNLM), 最初由Bengio提出的A Neural Probabilistic Language Mode最为经典, word2vec便是从其中简化训练而来. Bengio通过下面的一个三层神经网络来计算P(wt|wt−1,wt−2...wt−n(+1))：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932530668515851.png'/>
首先第一层输入就是前n−1个词wt−(n+1),...,wt−1wt−(n+1),...,wt−1 去预测第t个词是wt的概率. 这里面的矩阵C∈|V|×d维护着词汇表中所有词的词向量, 其中|V|是词汇表中词的个数, d是词向量的维度. 然后根据输入的前n−1个词, 在C中找到它们对应的词向量, 然后直接串联起来成为一个维度为(n−1)d的向量x 作为接下来三层神经网络的输入, 后面就是普通神经网络了。

需要说明的是,因为我们那要预测概率最大的wt, 因此最后输出层的神经元应该与词汇表大小同样为|V|, 这里使用使用softmax函数归一化输出层的值到[0,1], 代表可能的每个词的概率. 此外在原文中, 存在一些直连边, 也就是上图中的虚线, 从输入层直接到输出层, 是一个线性变换, Bingo在文中表示, 直连边的存在大幅降低迭代次数, 但对语言模型效果无提升, 随着计算能力的提高, 后续的工作基本都去掉了直连边.

神经语言模型构建完成之后,就是训练参数了. 这里的参数包括词向量矩阵C, 以及三层神经网络的权重, 偏置等参数. 训练数据就是大堆大堆的预料库. 训练结束之后, 语言模型得到了, 词向量也得到了. 换言之, 词向量是这个语言模型的副产品. 但是这个模型的缺点就是速度问题, 因为词汇表往往很大,几十万几百王, 训练起来就很耗时, Bengo仅仅训练5个epoch就花了3周, 这还是40个CPU并行训练的结果. 因此才会有了后续好多的优化工作, word2vec便是其中一个.

Word2Vec
简介
背景介绍完毕, 终于到主角了. word2vec是google于2013年的Distributed Representations ofWords and Phrases and their Compositionality 以及后续的Distributed Representations ofWords and Phrases and their Compositionality 两篇文章中提出的一种高效训练词向量的模型, 基本出发点是上下文相似的两个词,它们的词向量也应该相似, 比如香蕉和梨在句子中可能经常出现在相同的上下文中，因此这两个词的表示向量应该就比较相似.

word2vec模型中比较重要的概念是词汇的上下文, 说白了就是一个词周围的词, 比如wt的范围为1的上下文就是wt−1和wt+1. 在word2vec中提出两个模型(假设上下文窗口为3)

CBOW(Continuous Bag-of-Word): 以上下文词汇预测当前词: wt−1,wt+1去预测 wt
SkipGram: 以当前词预测其上下文词汇: wt 去预测wt−1,wt+1

两个模型图示如下
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932542888568931.png'/>
下面将会从最简单的上下文只有一个词的情形入手, 然后扩展到CBOW以及Skip-gram, 介绍原理以及参数训练过程. 关于word2vec的训练这里将会从完全的BP神经网络的过程来介绍。

One-Word Model
首先先看简化版入手: 输入输出都只有一个词, 如下图示:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933399243251430.png'/>
首先说明符号:
V: 词汇表长度; N: 隐层神经元个数, 同时也是词向量维度
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933416211290600.png'/>：输入层到隐层的权重矩阵, 其实就是词向量矩阵,其中每一行代表一个词的词向量
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933423931286960.png'/>：隐层到输出层的权重矩阵, 其中每一列也可以看作额外的一种词向量

下面从神经网络的前向过程开始介绍:
我们需要做的是用输入的词去预测输出的词. 其中 输入层的单词wI使用one-hot来表示的, 即在上图中x1,x2,x3,...,xV只有xk为1, 其余为0, 其中k可以是输入的词在词汇表中的索引下标。之后就是经过词向量矩阵W连接输入层和隐层. 其中由于X中只有一个1, 因此经过与W相乘, 相当于取出W中的第k行，实际也就是输入单词的wI的N维的词向量，使用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933436784247204.png'/>表示，来作为隐层的值，注意word2vec的隐层并没有激活函数:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933441348742956.png'/>

然后考虑从隐层的h到输出层Y, 同样h经过矩阵W′相乘，得到一个V×1的向量u:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415693345038428885.png'/>
其中u中的每个元素<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933461833468244.png'/>就是W′的第j列：用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933466565509846.png'/>表示，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933466565509846.png'/>与h做内积得到: <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933470292881474.png'/>，含义就是词汇表中第j个词的分数，我们的目的就是要根据输入词wI去预测输出的词，因此预测的词就取分数最高的即可。

这里为了方便概率表示，使用softmax将u归一化到[0,1]之间, 从而作为输出词的概率, 其实是一个多项分布, 也就是上图中的y:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933478828014034.png'/>

其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933486213632671.png'/>与<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933486591104120.png'/>都称为词w的词向量，一般使用前者作为词向量，而非后者，原因后续会解释。至此前向过程完成，就是给定一个词作为输入，来预测它的上下文词，还是比较简单的，属于简化版的神经语言模型。这个过程中需要用到的参数有两个词向量矩阵W,W′，下面就是重点了，介绍如何根据语料库来训练模型，更新参数，得到最终的词向量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933532510144847.png'/>
到此为止， 一个训练样本的反向传播训练过程就为止了。 我们可以看到，对于输入层到隐层的矩阵W，我们每次训练只需要更新一行向量即可，而对于隐层到输出层的矩阵W′的所有N×V个元素都需要更新一遍，这里的计算量还是很大的。这一节主要比较细致的介绍了最简单的输入输出只有一个单词的情况的推理和训练的过程，后面的CBOW(上下文预测单词)以及SG(单词预测上下文)均基于这一节扩展开来。

CBOW Model
这一部分讲word2vec的第一个形式: Continurous Bag-Of-Word，模型图示如下:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933536021583652.png'/>
跟上一个模型唯一的不同就是输入不再是一个词wI, 而是多个词，上图中一共有C个单词: x1k,x2k,...,xCk，每个x都是one-hot表示。 这样隐层的h的计算就会不同了: 之前一个单词的模型是直接取出W的一行<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933544723147001.png'/>作为h的值，在CBOW中则是取出W中输入的所有C个单词的词向量，然后直接取平均，如下:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933540692252654.png'/>

后面隐层到输出层的过程与One-Word Model 一模一样，包括目标函数定义， 反向传播训练等。将W′的更新公式照抄下来如下,依旧是每次都需要更新所有的行:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933556266255795.png'/>

隐层神经元的梯度也相同:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933561236111035.png'/>

下面考虑输入层到隐层稍微有些不同，在One-Word Model里面因为输入只有一个词，因此每次训练只更新这个词对应到W的那一行，但是在CBOW里面有多个词，这里采取的策略是将hh的梯度均摊到每个词上，因此每次训练会更新W中的C行，如下:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933564744121022.png'/>

到此为止 CBOW 的推理和训练过程也介绍完毕，基本跟One-Word Model 一样。

SkipGram Model
现在开始介绍word2vec的第二种形式: SkipGram(根据单词预测上下文)，这个模型与One-Word Model不同的地方在于，SG的输出有多个词，而非One-Word 中输出只有一个词，这样输出层就不是一个多项分布了，而是C个多项分布了，

模型图示如下：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933585910070681.png'/>

因此从输入层到隐层部分与One-Word Model 相同，隐层神经元的计算方式如下:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933591963138753.png'/>

因为输出层是有C个单词， 因此有C个多项分布: y1,y2...yC, 因此前向计算的过程也需要分开计算，如下公式，用来计算第c个输出单词的预测的多项分布中第j项，相比One-Word Model 多了一个c参数:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933595936280024.png'/>

需要主要的是这C个输出向量是相互独立的，可以当做是独立的C个One-Word Model 中的输出向量，相互之间没有影响，并且从图中也可以看出，连接隐层与C个输出层的参数矩阵W′是共享的，于是便有: <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933602574149387.png'/>

这里的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415693360812860055.png'/>的含义与One Word Model 中相同，都代表W′的第j列，同时也是词汇表中第j个单词的一种词向量(虽然实际中不用)。从前向后 根据上述公式计算出C个输出向量之后，在每个V维向量中选取概率最大的作为输出的单词，这样根据输出单词wI就得到了C个输出单词，也就达到了根据单词预测上下文的目的。

下面开始介绍SG的反向传播训练的过程，这个跟前面的有些许的不同, 首先是损失函数:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933612843278322.png'/>

前面说过输出的C个词是相互独立，因此<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933616694598396.png'/>, 此外<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933621342815638.png'/>的含义同One-Word Model 中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933624099921471.png'/>一样，都代表训练的真实的输出单词在词汇表的下标。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933629761306020.png'/>

优化
复杂度
前面的CBOW与SG模型是标准的word2vec模型，或者说是神经语言模型的简化版，去掉了隐层的激活函数，其余的变化不大，因此训练效率还是很低的。

我们分析下训练的复杂度。首先明确需要学习的两个词向量矩阵W,W′，从前面的推导中知道对于每一个训练样本，CBOW更新W的C行，SG更新W其中一行，也就是每次更新有限个词的词向量。但是对于W′则不同了，正如前面一直提到的，无论是CBOW还是SG，对每个训练样本(或者Mini Batch)从梯度更新中需要对W′的所有V×N个元素，也就是词汇表中所有V个单词都需要更新词向量，考虑现实任务词汇表一般是几十万，上百万千万级别的， 这个计算成本是巨大的。

关于计算成本大的原因，除了上面提到的训练部分，还有就是在每次前向计算的时候，隐层到输出层的softmax函数计算输出层V个元素，计算量也是很大，这样整个模型现实意义不大。

考虑到计算量大的部分都是在隐层到输出层上，尤其是W′的更新。因此word2vec使用了两种优化策略: Hierarchical Softmax 和 Negative Sampling。二者的出发点一致，就是在每个训练样本中，不再完全计算或者更新W′这个矩阵。二者都不再显示使用W′这个矩阵。

因此这也就解释了前面说的为什么不用W′作为最终词向量。在多一句，其实上述训练和推理的复杂度很大的根本原因是softmax的分母上的∑，因此在求梯度的时候，就会有V次的计算。因此下面的两种方法其实是对softmax的优化，不仅仅限制在word2vec.两种优化方式使得word2vec的训练速度大大提升，并且词向量的质量几乎没有下降，这也是word2vec在NLP领域如此流行的原因。

这里只介绍其中一种优化算法：Hierarchical SoftMax。

Hierarchical SoftMax
首先Hierarchical SoftMax(HS)并不是word2vec提出来的, 而是之前Bengio在2005年最早提出来专门为了加速计算神经语言模型中的softmax的一种方式, 这里介绍如何在word2vec中使用. 

HS主要基于哈夫曼树(一种二叉数)将计算量大的部分变为了一种二分类的问题. 先看下面的图，原来的模型在隐层之后通过W′连接输出层, 现在HS则去掉了W′, 隐层h直接与下面的二叉树的root节点相连：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932079953438540.png'/>
其中图中白色的叶子节点表示词汇表中所有的|V|个词, 黑色节点表示非叶子节点, 每一个叶子节点也就是每一个单词, 都对应唯一的一条从root节点出发的路径。而我们的目的是使的w=wO这条路径的概率最大，即: P(w=wO|wI)最大, 此时每一个分支都代表一个选择, 向左转还是向右转. 所以如何判断向左还是向右呢? 

我们用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932096590847829.png'/>表示从root到叶子节点w的路径上的第j个非叶子节点, 并且每个非叶子节点都对应一个向量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932102811228403.png'/>，维度与h相同, 然后使用一个sigmod函数: <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932131541126578.png'/>, 结合向量的内积, 来判断该向左还是向右, 如下, 第n个节点向左 以及向右的概率定义：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415693213649981981.png'/>

有了上述的概率, 我们可以重新定义P(wO|wi)了：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932160782034408.png'/>

其中I()是指示函数, 条件成立值为1, 反之为-1. 而L(w)表示整条路径的长度, 这样整个概率就是从root节点到叶子节点这条路径的概率, 这样我们在训练的时候, 通过训练样本来更新非叶子节点的参数v′w.

举个例子, 比如上图中的加粗的黑色路径: (n(w2,1),n(w2,2),n(w2,3),w2(n(w2,1),n(w2,2),n(w2,3),w2 , 就是说假设有一个训练样本是(wI,w2), 我们需要使得P(wO=w2|wI)概率最大:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932197471243141.png'/>

并且在一个非叶子节点处, 向左向右的概率和为1, 因此一直分裂下去,最后的和肯定还是1. 因此可以很容易得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932202684423110.png'/>

这一点的证明是有必要的, 因为在原始的softmax本身保证了所有单词的概率和是1, 而通过上式也知道了通过HS得到的输出层仍然是一个概率多项分布, 输出所有的单词概率和为1.

讲完了从前向后的如何得到输出单词的概率的过程, 下面开始后向传播的训练过程.。

首先需要明确的是训练的参数: 输入层与隐层的词向量矩阵W, 以及二叉树的非叶子节点对应的向量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932216066412765.png'/>。

为了书写方便,下面简化一部分的符号: 用[I]表示前面的指示函数I(n(w,j+1)==left), 使用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932228316836686.png'/>表示<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641569322289906486.png'/>。

对于一组训练数据, 损失函数的定义与前面相同, 最大似然(注意这里以One-Word Model为例，CBOW与Skip-Gram按照上述的思路完全一样)：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932238064727751.png'/>

之后便可以逐项求梯度了, 先考虑<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415693224974527821.png'/>, 注意<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932250552539245.png'/>：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932251433225928.png'/>

之后对[I]分情况讨论, 可得:<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932256676898368.png'/>

这里如果[I]=1, 那么tj=1, 否则 tj=0, 这个公式与前面的yj−tj很类似, 可以理解为预测值与实际值的差别。

有了上述的梯度,就可以很简单的求出<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932281131851034.png'/>的梯度了：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932278177987766.png'/>

有了梯度,便可以更新了, 具体公式还是梯度下降：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932286990541555.png'/>

也就是说对于一个训练样本， 我们只需要更新L(w)−1个向量就好了， 而未优化的版本需要更新V个， 相当于时间复杂度从O(V)O(V)降到了O(logV), 这个提升还是非常大的。
虽然在考察空间复杂度方面，HS的二叉树的非叶子节点有V−1个，也就是我们需要V−1存储<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932298814615118.png'/>,优化之前则是V个， 空间复杂度相同， 但总体而言，时间复杂度大大降低了。

然后考虑隐层h的梯度，因为我们的优化目标都是在隐层到输出层，因此前面的几乎不变， 跟One-Word Model 一样，路径上的非叶子节点的表达式都含有hh，因此需要对梯度求和：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932307786980718.png'/>
其实跟前面很一样了， 只需要替代下导数就好了， 后面就不再赘述了。整个Hierarchical Softmax的优化算法介绍完了，隐层到输出层的计算量从O(V), 利用二叉树(更加具体来说是哈夫曼树）降为了O(logV)。

## word2vec 相比之前的 Word Embedding 方法好在什么地方？
本题解析来源：https://www.zhihu.com/question/53011711

解析一
@邱锡鹏：Word2vec训练方面采用的HSoftmax以及负采样确实可以认为是创新不大。但Word2vec流行的主要原因也不在于此。主要原因在于以下3点：
1. 极快的训练速度。以前的语言模型优化的目标是MLE，只能说词向量是其副产品。Mikolov应该是第一个提出抛弃MLE（和困惑度）指标，就是要学习一个好的词嵌入。如果不追求MLE，模型就可以大幅简化，去除隐藏层。再利用HSoftmax以及负采样的加速方法，可以使得训练在小时级别完成。而原来的语言模型可能需要几周时间。
2. 一个很酷炫的man-woman=king-queen的示例。这个示例使得人们发现词嵌入还可以这么玩，并促使词嵌入学习成为了一个研究方向，而不再仅仅是神经网络中的一些参数。
3. word2vec里有大量的tricks，比如噪声分布如何选？如何采样？如何负采样？等等。这些tricks虽然摆不上台面，但是对于得到一个好的词向量至关重要。

举一个生活中的例子，语言模型和word2vec的关系可以类比于单反相机和美颜手机，它们的受众不一样。就照片质量（MLE）而言，单反肯定好。但如果更关心美颜（词嵌入）和便携性（训练速度），美颜手机就更受欢迎。

更多的资料可以参考：
https://nndl.github.io/ch12.pdf 

解析二
@吴海波：最近正好写了篇相关的文章，原文如下：

引子
此时再谈Word2Vec，特别是ELMo、Bert相继大火后，有点炒冷饭的意味。但有一个点，很多同学可能好奇过却没有深究：自然语言处理里面应用Distribution representation做Embedding，word2vec不是第一个，对比Matrix Fatorication的方法，比如SVD、LSA，为何Word2Vec出来后，Emebedding变的这么火？

面试的时候，如果聊得时间有多，会顺便问下这个问题，大多数同学都没有想过这个问题，能提到Word2Vec比如SVD之类的更容易训练更多的数据，已经寥寥无几。

其实这个问题学术界相关的研究很多，但不熟悉的同学往往不知道该用什么关键词，而Word2Vec的相关的文献不计其数，导致真正有价值的信息被埋没了，比如上面这个问题，你如果搜why word2vec so good，出来的大多是描述其原理的（知乎上就有很好的讨论帖[2][3]），需要搜的key是word2vec matrix fatorication，这就需要你了解一些相关的背景。准确的定义问题的描述，往往比问题的答案更重要。

本文参考了一些相关论文，尽量少引入公式（主要是我懒，公式编辑太麻烦了），尝试解答上述问题。

背景知识
Word2Vec
详尽的原理，网上有很多非常好的资料，这里不再赘述。Word2Vec和Deep Learing的关系并不深，至少一点也不Deep。13、14年那段时间，很多学者都在尝试解释它到底学到了什么。无论是CBOW还是Skip-Gram，本质还是要基于word和context做文章，即可以理解为模型在学习word和context的co-occurrence。

参考[1]本文重点关注Skip-Gram with Negative Sample，简称为SGNS，让我们来回顾下它的目标函数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157034184239331650.jpg'/>

再介绍论文[1]如何将它一步步变成和Matrix Factorization等价前，我们先来了解下MF的背景。

Matrix Factorization
MF即矩阵分解，典型的算法有SVD、LSA，是一种常见的技术，在推荐、NLP都有应用。据资料显示，百度在2010年之前，就做过大规模分布式的plsa算法。简单来讲，MF就是将一个矩阵分解成多个矩阵的乘积。让我们回顾下word2vec，最终每一个word都会有两个向量：V_word和V_context，假设存在一个矩阵W = V_word * V_context，word2vec也可以理解成是对矩阵W的矩阵分解。那么，问题就变成这个matrix W是什么？

Pointwise Mutual Information（PMI）
论文[1]中有将SGNS的目标函数，一步步的变换，变成下面的形式（具体见论文）
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157034185955190239.jpg'/>

上述公式中，左边的部分就是大名鼎鼎的PMI，即w和c同时出现的次数/(w出现的次数 + c出现的次数)。和MF对比，就多了个常量logk。PMI是dense matrix，对模型优化带来很多困难，所以一般会用sparse matrix PPMI（positive PMI）来替代它。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157034187556666755.jpg'/>

到此，可以说明word2vec和MF并没有本质的区别，是一种implicit的MF，正如论文[1]的标题。既然其本质差别不大，是否MF的测试效果也能和word2vec一致呢。首先，在word similar上，SVD和Word2vec差距并不大，但在word analogies上，word2vec要明显好于SVD。

Why
训练大规模语料的能力
这个当然是核心优点，虽然SVD有SMF的版本去处理大数据，LSA也有大数据的方案，但谁都没有word2vec的简单易实现。google一开始的开源代码，根本不需要分布式就能跑以前不能跑的数据规模。

Simple is Power！

举个不恰当的例子：很多人怀疑牛顿是先发明了微积分，后面才做出了很多重要的工作，为了装逼在他的著作里面用传统的数学方法重写了一遍，各种奇技淫巧。当然这个逼装的有点过，导致后面和莱布尼茨关于微积分的发明权吵了一辈子。

很多论文，因为word2vec的成果，有了方向后再去回溯传统，其价值自然远不如开创者。数据量的问题，在之前可能都不是核心问题，再加上LSA模型稳定，在小数据集上效果挺好，数据规模增长一点不一定能看出收益，而不像word2vec，在数据集小的时候，可能还不如CountVectorizer，必须往大数据去。

再一次，定义问题是什么，比找到方法难。

MF suffer from unobserved value
在word-context matrix中，这是个常见的问题。另外，对matrix中的value做不同的weight也不是件容易的事情。最重要的是，MF类的算法在analogies task上有明显劣势。即当初那个经典的word2vec展示case：king - queen = man - woman，即词向量可以相互做加减，具有实际意义。

这个case有点特殊，并不是所有的词向量都成立，但确实比MF类的模型总体上好。比较酷炫，其原理我还没有找到特别好的解释，如果有童鞋知道，求告知。

融合二者
Glove是其中代表，即结合了global matrix factorization and local context window methods。论文[5]中给出了不错的实验数据，但也有不少人质疑它，比如论文[6]。另外，word2vec比glove简单多了，也更容易被大家接受。我们自己实践来看，glove也没有明显增量。

最后
这篇是好奇心驱动的产物，实用价值并不高，只为答疑解惑。前期在搜索过程中看了不少无效的资料，浪费了不少时间，希望本文能给感兴趣的同学一些帮助。

参考文献
[1] Neural Word Embedding as Implicit Matrix Factorization
[2] word2vec 相比之前的 Word Embedding 方法好在什么地方？
[3] 词向量，LDA，word2vec三者的关系是什么
[4] What is the connection between PCA and Word2Vec in terms of word embedding? Is there an empiric superiority between the two?
[5]GloVe: Global Vectors for Word Representation 

[6]Improving Distributional Similarity with Lessons Learned from Word Embeddings
[7]Linguistic Regularities in Sparse and Explicit Word Representations
[8]ALL-BUT-THE-TOP: SIMPLE AND EFFECTIVE POSTPROCESSING FOR WORD REPRESENTATIONS
[9]Enriching Word Vectors with Subword Information

解析三
@李韶华：词嵌入模型效果好不好的关键之一，是用上下文词预测当前词的formulation，即采用的回归函数。

Hinton等07年和08年的log bilinear language model之前的工作都采用的是 softmax([上下文词向量,当前词向量]的线性变换) 的形式，softmax里边可以简化认为是一些向量的线性和。但几个向量的线性和不能很好的抓住这几个向量在隐空间一些维度上取值接近的特点，所以效果并不好。

07年的Three New Graphical Models for Statistical Language Modelling里，三个模型之一是log bilinear language model (LBL), 题目中08年的论文扩展了这个方法，得到Hierarchical Log Bilinear Language model。

为了叙述简单，下面把这两种方法统称为LBL。LBL使用了 softmax(上下文词向量的线性变换 * 当前词向量) 的形式，点乘在抓两个向量在一些维度上取值接近方面，比相加要好得多，这是词向量模型发展的一个重大突破。

word2vec使用的也是LBL。那么和之前的方法有什么区别呢？08年的Hierarchical LBL里，用的是这样的回归函数:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157034315393488470.svg'/>

这里的Ci都是矩阵，不同位置对应不同的矩阵。
word2vec的CBOW用的是（skip-gram我觉得和CBOW基本是等价的，效果也类似，但CBOW的概率解释好些，所以拿它来比较）:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157034316295567998.svg'/>

可见它移除了变换矩阵Ci和偏移量bw. 实际上我们事后诸葛来看，变换矩阵Ci也的确是多余的，因为 两个词经常一块出现两个词在某方面有相似语义两个向量在某些维度取值类似，那么直接点乘就可以了，用Ci变换一下，反而有可能把本来相似的维度变得不同，从而让学出来的向量不能很好满足“相似词在有些维度上取值相近”的训练效果。

显而易见，移除Ci会极大的提高运算速度，使word2vec在大语料上训练非常可行。两个LBL模型训练语料都是1000w单词左右，而word2vec即使用wikipedia这样>20亿词规模的语料也只需几小时，大语料下得出的词向量当然会抓住更多的语法语义规律，从而更准确。

所以我觉得，word2vec的成功，印证了一句话：Less is more.
## 说说NLP中的预训练技术发展史：从Word Embedding到Bert模型
本题解析的作者：张俊林，链接：https://zhuanlan.zhihu.com/p/49271699

July注：本文是了解bert历史的最佳文章，把整个来龙去脉介绍的高屋建瓴、通俗细致，全文主要有这些内容：预训练在图像领域的应用、从语言模型到Word Embedding、从Word Embedding到ELMO、从Word Embedding到GPT、Bert的诞生。

当然，在此之前，建议先通过此文了解word2vec：https://blog.csdn.net/v_JULY_v/article/details/102708459，这是理解bert的关键，其次则是Transformer，关于Transformer，推荐此文 https://www.julyedu.com/question/big/kp_id/30/ques_id/2912。

因为我也是这么过来的，之前bert刚火起来的时候，就看到俊林老师这篇文章，当时看的不甚了解，及至后来先学习word2vec和Transformer之后，再看此文，你会觉得真是高屋建瓴，甚至醍醐灌顶，是关于bert中文介绍少有的好文章。
话休絮烦，以下即为张俊林老师所写的正文。

Bert最近很火，应该是最近最火爆的AI进展，网上的评价很高，那么Bert值得这么高的评价吗？我个人判断是值得。那为什么会有这么高的评价呢？是因为它有重大的理论或者模型创新吗？

其实并没有，从模型创新角度看一般，创新不算大。但是架不住效果太好了，基本刷新了很多NLP的任务的最好性能，有些任务还被刷爆了，这个才是关键。另外一点是Bert具备广泛的通用性，就是说绝大部分NLP任务都可以采用类似的两阶段模式直接去提升效果，这个第二关键。客观的说，把Bert当做最近两年NLP重大进展的集大成者更符合事实。

本文的主题是自然语言处理中的预训练过程，会大致说下NLP中的预训练技术是一步一步如何发展到Bert模型的，从中可以很自然地看到Bert的思路是如何逐渐形成的，Bert的历史沿革是什么，继承了什么，创新了什么，为什么效果那么好，主要原因是什么，以及为何说模型创新不算太大，为何说Bert是近年来NLP重大进展的集大成者。

我们一步一步来讲，而串起来这个故事的脉络就是自然语言的预训练过程，但是落脚点还是在Bert身上。要讲自然语言的预训练，得先从图像领域的预训练说起。

图像领域的预训练自从深度学习火起来后，预训练过程就是做图像或者视频领域的一种比较常规的做法，有比较长的历史了，而且这种做法很有效，能明显促进应用的效果。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156040081288495.png'/>

那么图像领域怎么做预训练呢，上图展示了这个过程，我们设计好网络结构以后，对于图像来说一般是CNN的多层叠加网络结构，可以先用某个训练集合比如训练集合A或者训练集合B对这个网络进行预先训练，在A任务上或者B任务上学会网络参数，然后存起来以备后用。假设我们面临第三个任务C，网络结构采取相同的网络结构，在比较浅的几层CNN结构，网络参数初始化的时候可以加载A任务或者B任务学习好的参数，其它CNN高层参数仍然随机初始化。

之后我们用C任务的训练数据来训练网络，此时有两种做法，
一种是浅层加载的参数在训练C任务过程中不动，这种方法被称为“Frozen”;
另外一种是底层网络参数尽管被初始化了，在C任务训练过程中仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”，顾名思义，就是更好地把参数进行调整使得更适应当前的C任务。

一般图像或者视频领域要做预训练一般都这么做。这么做有几个好处，首先，如果手头任务C的训练集合数据量较少的话，现阶段的好用的CNN比如Resnet/Densenet/Inception等网络结构层数很深，几百万上千万参数量算起步价，上亿参数的也很常见，训练数据少很难很好地训练这么复杂的网络，但是如果其中大量参数通过大的训练集合比如ImageNet预先训练好直接拿来初始化大部分网络结构参数，然后再用C任务手头比较可怜的数据量上Fine-tuning过程去调整参数让它们更适合解决C任务，那事情就好办多了。

这样原先训练不了的任务就能解决了，即使手头任务训练数据也不少，加个预训练过程也能极大加快任务训练的收敛速度，所以这种预训练方式是老少皆宜的解决方案，另外疗效又好，所以在做图像处理领域很快就流行开来。

那么新的问题来了，为什么这种预训练的思路是可行的？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154534918309172.jpg'/>

目前我们已经知道，对于层级的CNN结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构，如上图所示，如果我们手头是个人脸识别任务，训练好网络后，把每层神经元学习到的特征可视化肉眼看一看每层学到了啥特征，你会看到最底层的神经元学到的是线段等特征，图示的第二个隐层学到的是人脸五官的轮廓，第三层学到的是人脸的轮廓，通过三步形成了特征的层级结构，越是底层的特征越是所有不论什么领域的图像都会具备的比如边角线弧线等底层基础特征，越往上抽取出的特征越与手头任务相关。

正因为此，所以预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务网络参数的原因。而高层特征跟任务关联较大，实际可以不用使用，或者采用Fine-tuning用新数据集合清洗掉高层无关的特征抽取器。

一般我们喜欢用ImageNet来做网络的预训练，主要有两点，一方面ImageNet是图像领域里有超多事先标注好训练数据的数据集合，分量足是个很大的优势，量越大训练出的参数越靠谱；另外一方面因为ImageNet有1000类，类别多，算是通用的图像数据，跟领域没太大关系，所以通用性好，预训练完后哪哪都能用，是个万金油。分量足的万金油当然老少通吃，人人喜爱。

听完上述话，如果你是具备研究素质的人，也就是说具备好奇心，你一定会问下面这个问题：”既然图像领域预训练这么好用，那干嘛自然语言处理不做这个事情呢？是不是搞NLP的人比搞CV的傻啊？就算你傻，你看见人家这么做，有样学样不就行了吗？这不就是创新吗，也许能成，万一成了，你看，你的成功来得就是这么突然!”

嗯，好问题，其实搞NLP的人一点都不比你傻，早就有人尝试过了，不过总体而言不太成功而已。听说过word embedding吗？2003年出品，陈年技术，馥郁芳香。word embedding其实就是NLP里的早期预训练技术。当然也不能说word embedding不成功，一般加到下游任务里，都能有1到2个点的性能提升，只是没有那么耀眼的成功而已。没听过？那下面就把这段陈年老账讲给你听听。

Word Embedding考古史这块大致讲讲Word Embedding的故事，很粗略，因为网上关于这个技术讲的文章太多了，汗牛冲动，我不属牛，此刻更没有流汗，所以其实丝毫没有想讲Word Embedding的冲动和激情，但是要说预训练又得从这开始，那就粗略地讲讲，主要是引出后面更精彩的部分。在说Word Embedding之前，先更粗略地说下语言模型，因为一般NLP里面做预训练一般的选择是用语言模型任务来做。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157155009414062228.jpg'/>

什么是语言模型？其实看上面这张PPT上扣下来的图就明白了，为了能够量化地衡量哪个句子更像一句人话，可以设计如上图所示函数，核心函数P的思想是根据句子里面前面的一系列前导单词预测后面跟哪个单词的概率大小（理论上除了上文之外，也可以引入单词的下文联合起来预测单词出现概率）。句子里面每个单词都有个根据上文预测自己的过程，把所有这些单词的产生概率乘起来，数值越大代表这越像一句人话。

语言模型压下暂且不表，我隐约预感到我这么讲你可能还是不太会明白，但是大概这个意思，不懂的可以去网上找，资料多得一样地汗牛冲动（July注：关于语言模型还可以看看这篇 https://www.julyedu.com/question/big/kp_id/30/ques_id/2984）。

假设现在让你设计一个神经网络结构，去做这个语言模型的任务，就是说给你很多语料做这个事情，训练好一个神经网络，训练好之后，以后输入一句话的前面几个单词，要求这个网络输出后面紧跟的单词应该是哪个，你会怎么做？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154560273799310.jpg'/>

你可以像上图这么设计这个网络结构，这其实就是大名鼎鼎的中文人称“神经网络语言模型”，英文小名NNLM的网络结构，用来做语言模型。这个工作有年头了，是个陈年老工作，是Bengio 在2003年发表在JMLR上的论文。它生于2003，火于2013，以后是否会不朽暂且不知，但是不幸的是出生后应该没有引起太大反响，沉寂十年终于时来运转沉冤得雪，在2013年又被NLP考古工作者从海底湿淋淋地捞出来了祭入神殿。

为什么会发生这种技术奇遇记？你要想想2013年是什么年头，是深度学习开始渗透NLP领域的光辉时刻，万里长征第一步，而NNLM可以算是南昌起义第一枪。在深度学习火起来之前，极少有人用神经网络做NLP问题，如果你10年前坚持用神经网络做NLP，估计别人会认为你这人神经有问题。所谓红尘滚滚，谁也挡不住历史发展趋势的车轮，这就是个很好的例子。

上面是闲话，闲言碎语不要讲，我们回来讲一讲NNLM的思路。先说训练过程，现在看其实很简单，见过RNN、LSTM、CNN后的你们回头再看这个网络甚至显得有些简陋。学习任务是输入某个句中单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154570412977637.svg'/>前面句子的t-1个单词，要求网络正确预测单词Bert，即最大化：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154572430609079.svg'/>

前面任意单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154574554305681.svg'/>用Onehot编码（比如：0001000）作为原始单词输入，之后乘以矩阵Q后获得向量 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154576598939623.svg'/>，每个单词的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715457745240528.svg'/>拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。

这个<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154578489621011.svg'/>是什么？这其实就是单词对应的Word Embedding值，那个矩阵Q包含V行，V代表词典大小，每一行内容代表对应单词的Word embedding值。只不过Q的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵Q，当这个网络训练好之后，矩阵Q的内容被正确赋值，每一行代表一个单词对应的Word embedding值。

所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵Q，这就是单词的Word Embedding是被如何学会的。2013年最火的用语言模型做Word Embedding的工具是Word2Vec，后来又出了Glove。

Word2Vec是怎么工作的呢？看下图。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154582635057288.jpg'/>

Word2Vec的网络结构其实和NNLM是基本类似的，只是这个图长得清晰度差了点，看上去不像，其实它们是亲兄弟。不过这里需要指出：尽管网络结构相近，而且也是做语言模型任务，但是其训练方法不太一样。

Word2Vec有两种训练方法，一种叫CBOW，核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词；
第二种叫做Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。

而你回头看看，NNLM是怎么训练的？是输入一个单词的上文，去预测这个单词。这是有显著差异的。为什么Word2Vec这么处理？原因很简单，因为Word2Vec和NNLM不一样，NNLM的主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而word embedding只是无心插柳的一个副产品。

但是Word2Vec目标不一样，它单纯就是要word embedding的，这是主产品，所以它完全可以随性地这么去训练网络。为什么要讲Word2Vec呢？这里主要是要引出CBOW的训练方法，BERT其实跟它有关系，后面会讲它们之间是如何的关系，当然它们的关系BERT作者没说，是我猜的，至于我猜的对不对，后面你看后自己判断。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154605888272540.jpg'/>

使用Word2Vec或者Glove，通过做语言模型任务，就可以获得每个单词的Word Embedding，那么这种方法的效果如何呢？上图给了网上找的几个例子，可以看出有些例子效果还是很不错的，一个单词表达成Word Embedding后，很容易找出语义相近的其它词汇。

我们的主题是预训练，那么问题是Word Embedding这种做法能算是预训练吗？这其实就是标准的预训练过程。要理解这一点要看看学会Word Embedding后下游任务是怎么用它的。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156048711181137.png'/>

假设如上图所示，我们有个NLP的下游任务，比如QA，就是问答问题，所谓问答问题，指的是给定一个问题X，给定另外一个句子Y,要判断句子Y是否是问题X的正确答案。问答问题假设设计的网络结构如上图所示，这里不展开讲了，懂得自然懂，不懂的也没关系，因为这点对于本文主旨来说不关键，关键是网络如何使用训练好的Word Embedding的。

它的使用方法其实和前面讲的NNLM是一样的，句子中每个单词以Onehot形式作为输入，然后乘以学好的Word Embedding矩阵Q，就直接取出单词对应的Word Embedding了。

这乍看上去好像是个查表操作，不像是预训练的做法是吧？其实不然，那个Word Embedding矩阵Q其实就是网络Onehot层到embedding层映射的网络参数矩阵。所以你看到了，使用Word Embedding等价于什么？等价于把Onehot层到embedding层的网络用预训练好的参数矩阵Q初始化了。这跟前面讲的图像领域的低层预训练过程其实是一样的，区别无非Word Embedding只能初始化第一层网络参数，再高层的参数就无能为力了。

下游NLP任务在使用Word Embedding的时候也类似图像有两种做法，一种是Frozen，就是Word Embedding那层网络参数固定不动；另外一种是Fine-Tuning，就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新掉。

上面这种做法就是18年之前NLP领域里面采用预训练的典型做法，之前说过，Word Embedding其实对于很多下游NLP任务是有帮助的，只是帮助没有大到闪瞎忘记戴墨镜的围观群众的双眼而已。

那么新问题来了，为什么这样训练及使用Word Embedding的效果没有期待中那么好呢？答案很简单，因为Word Embedding有问题呗。这貌似是个比较弱智的答案，关键是Word Embedding存在什么问题？这其实是个好问题。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715605432899758.png'/>

这片在Word Embedding头上笼罩了好几年的乌云是什么？是多义词问题。我们知道，多义词是自然语言中经常出现的现象，也是语言灵活性和高效性的一种体现。多义词对Word Embedding来说有什么负面影响？

如上图所示，比如多义词Bank，有两个常用含义，但是Word Embedding在对bank这个单词进行编码的时候，是区分不开这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。

所以word embedding无法区分多义词的不同语义，这就是它的一个比较严重的问题。你可能觉得自己很聪明，说这可以解决啊，确实也有很多研究人员提出很多方法试图解决这个问题，但是从今天往回看，这些方法看上去都成本太高或者太繁琐了，有没有简单优美的解决方案呢？ELMO提供了一种简洁优雅的解决方案。

从Word Embedding到ELMO
ELMO是“Embedding from Language Models”的简称，其实这个名字并没有反应它的本质思想，提出ELMO的论文题目：“Deep contextualized word representation”更能体现其精髓，而精髓在哪里？在deep contextualized这个短语，一个是deep，一个是context，其中context更关键。

在此之前的Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变，所以对于比如Bank这个词，它事先学好的Word Embedding中混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含money等词）明显可以看出它代表的是“银行”的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。这是为何说它是静态的，这也是问题所在。

ELMO的本质思想是：我事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。

所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715606044100672.png'/>

ELMO采用了典型的两阶段过程，第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。

上图展示的是其预训练过程，它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715463197090044.svg'/>的上下文去正确预测单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154632618450065.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154633882353118.svg'/>之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。

图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154634360481822.svg'/>的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM叠加。

这个网络结构其实在NLP中是很常用的。使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154642594810124.svg'/>，句子中每个单词都能得到对应的三个Embedding：
最底层是单词的Word Embedding；
往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；
再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。

也就是说，ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156022749639923.png'/>

上面介绍的是ELMO的第一阶段：预训练阶段。那么预训练好网络结构后，如何给下游任务使用呢？上图展示了下游任务的使用过程，比如我们的下游任务仍然是QA问题，此时对于问句X，我们可以先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding，之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。

然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。对于上图所示下游任务QA中的回答句子Y来说也是如此处理。因为ELMO给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。至于为何这么做能够达到区分多义词的效果，你可以想一想，其实比较容易想明白原因。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156065876593589.png'/>

上面这个图是TagLM采用类似ELMO的思路做命名实体识别任务的过程，其步骤基本如上述ELMO的思路，所以此处不展开说了。TagLM的论文发表在2017年的ACL会议上，作者就是AllenAI里做ELMO的那些人，所以可以将TagLM看做ELMO的一个前导工作。

前几天这个PPT发出去后有人质疑说FastAI的在18年4月提出的ULMFiT才是抛弃传统Word Embedding引入新模式的开山之作，我深不以为然。
首先TagLM出现的更早而且模式基本就是ELMO的思路；
另外ULMFiT使用的是三阶段模式，在通用语言模型训练之后，加入了一个领域语言模型预训练过程，而且论文重点工作在这块，方法还相对比较繁杂，这并不是一个特别好的主意，因为领域语言模型的限制是它的规模往往不可能特别大，精力放在这里不太合适，放在通用语言模型上感觉更合理；
再者，尽管ULFMiT实验做了6个任务，但是都集中在分类问题相对比较窄，不如ELMO验证的问题领域广，我觉得这就是因为第二步那个领域语言模型带来的限制。
所以综合看，尽管ULFMiT也是个不错的工作，但是重要性跟ELMO比至少还是要差一档，当然这是我个人看法。

每个人的学术审美口味不同，我个人一直比较赞赏要么简洁有效体现问题本质，要么思想特别游离现有框架脑洞开得异常大的工作，所以ULFMiT我看论文的时候就感觉看着有点难受，觉得这工作没抓住重点而且特别麻烦，但是看ELMO论文感觉就赏心悦目，觉得思路特别清晰顺畅，看完暗暗点赞，心里说这样的文章获得NAACL2018最佳论文当之无愧，比ACL很多最佳论文也好得不是一点半点，这就是好工作带给一个有经验人士的一种在读论文时候就能产生的本能的感觉，也就是所谓的这道菜对上了食客的审美口味。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154658139852177.jpg'/>

前面我们提到静态Word Embedding无法解决多义词的问题，那么ELMO引入上下文动态调整单词的embedding后多义词问题解决了吗？解决了，而且比我们期待的解决得还要好。

上图给了个例子，对于Glove训练出的Word Embedding来说，多义词比如play，根据它的embedding找出的最接近的其它单词大多数集中在体育领域，这很明显是因为训练数据中包含play的句子中体育领域的数量明显占优导致；而使用ELMO，根据上下文动态调整后的embedding不仅能够找出对应的“演出”的相同语义的句子，而且还可以保证找出的句子中的play对应的词性也是相同的，这是超出期待之处。之所以会这样，是因为我们上面提到过，第一层LSTM编码了很多句法信息，这在这里起到了重要作用。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154672595377892.jpg'/>

ELMO经过这般操作，效果如何呢？实验效果见上图，6个NLP任务中性能都有幅度不同的提升，最高的提升达到25%左右，而且这6个任务的覆盖范围比较广，包含句子语义关系判断，分类任务，阅读理解等多个领域，这说明其适用范围是非常广的，普适性强，这是一个非常好的优点。

那么站在现在这个时间节点看，ELMO有什么值得改进的缺点呢？
首先，一个非常明显的缺点在特征抽取器选择方面，ELMO使用了LSTM而不是新贵Transformer，Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响，很多研究已经证明了Transformer提取特征的能力是要远强于LSTM的。如果ELMO采取Transformer作为特征提取器，那么估计Bert的反响远不如现在的这种火爆场面。
另外一点，ELMO采取双向拼接这种融合特征的能力可能比Bert一体化的融合特征方式弱，但是，这只是一种从道理推断产生的怀疑，目前并没有具体实验说明这一点。我们如果把ELMO这种预训练方法和图像领域的预训练方法对比，发现两者模式看上去还是有很大差异的。

除了以ELMO为代表的这种基于特征融合的预训练方法外，NLP里还有一种典型做法，这种做法和图像领域的方式就是看上去一致的了，一般将这种方法称为“基于Fine-tuning的模式”，而GPT就是这一模式的典型开创者。

从Word Embedding到GPT
GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156074526382168.png'/>

GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：
首先，特征抽取器不是用的RNN，而是用的Transformer，上面提到过它的特征抽取能力要强于RNN，这个选择很明显是很明智的；
其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，所谓“单向”的含义是指：语言模型训练的任务目标是根据<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154689157643717.svg'/>单词的上下文去正确预测单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154689157643717.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154689157643717.svg'/>之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。

ELMO在做语言模型预训练的时候，预测单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154689157643717.svg'/>同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文。这个选择现在看不是个太好的选择，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到Word Embedding中，是很吃亏的，白白丢掉了很多信息。

这里强行插入一段简单提下Transformer，尽管上面提到了，但是说的还不完整，补充两句。首先，Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器，注意力这个机制在此被发扬光大，从任务的配角不断抢戏，直到Transformer一跃成为踢开RNN和CNN传统特征提取器，荣升头牌，大红大紫。

你问了：什么是注意力机制？这里再插个广告，对注意力不了解的可以参考鄙人16年出品17年修正的文章：“深度学习中的注意力模型”：https://www.julyedu.com/question/big/kp_id/30/ques_id/2911，补充下相关基础知识，如果不了解注意力机制你肯定会落后时代的发展。

而介绍Transformer比较好的文章可以参考以下两篇文章：一个是Jay Alammar可视化地介绍Transformer的博客文章The Illustrated Transformer（July注，其中文题库版见：https://www.julyedu.com/question/big/kp_id/30/ques_id/2912），非常容易理解整个机制，建议先从这篇看起；然后可以参考哈佛大学NLP研究组写的“The Annotated Transformer. ”：http://nlp.seas.harvard.edu/2018/04/03/attention.html，代码原理双管齐下，讲得非常清楚。我相信上面两个文章足以让你了解Transformer了，所以这里不展开介绍。

其次，我的判断是Transformer在未来会逐渐替代掉RNN成为主流的NLP工具，RNN一直受困于其并行计算能力，这是因为它本身结构的序列性依赖导致的，尽管很多人在试图通过修正RNN结构来修正这一点，但是我不看好这种模式，因为给马车换轮胎不如把它升级到汽车，这个道理很好懂，更何况目前汽车的雏形已经出现了，干嘛还要执着在换轮胎这个事情呢？是吧？

再说CNN，CNN在NLP里一直没有形成主流，CNN的最大优点是易于做并行计算，所以速度快，但是在捕获NLP的序列关系尤其是长距离特征方面天然有缺陷，不是做不到而是做不好，目前也有很多改进模型，但是特别成功的不多。综合各方面情况，很明显Transformer同时具备并行性好，又适合捕获长距离特征，没有理由不在赛跑比赛中跑不过RNN和CNN。

好了，题外话结束，我们再回到主题，接着说GPT。上面讲的是GPT如何进行第一阶段的预训练，那么假设预训练好了网络模型，后面下游任务怎么用？它有自己的个性，和ELMO的方式大有不同。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156078381376885.png'/>

上图展示了GPT在第二阶段如何使用。

首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的。
然后，在做下游任务的时候，利用第一步预训练好的参数初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了，这是个非常好的事情。
再次，你可以用手头的任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决手头的问题。就是这样。

看到了么？这有没有让你想起最开始提到的图像领域如何做预训练的过程（请参考上图那句非常容易暴露年龄的歌词）？对，这跟那个模式是一模一样的。这里引入了一个新问题：对于NLP各种花样的不同任务，怎么改造才能靠近GPT的网络结构呢？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715472108350789.jpg'/>

GPT论文给了一个改造施工图如上，其实也很简单：对于分类问题，不用怎么动，加上一个起始和终结符号即可；对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可；对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154730583116276.jpg'/>

GPT的效果是非常令人惊艳的，在12个任务里，9个达到了最好的效果，有些任务性能提升非常明显。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154732491204502.jpg'/>

那么站在现在的时间节点看，GPT有什么值得改进的地方呢？其实最主要的就是那个单向语言模型，如果改造成双向的语言模型任务估计也没有Bert太多事了。当然，即使如此GPT也是非常非常好的一个工作，跟Bert比，其作者炒作能力亟待提升。

Bert的诞生
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154737146796255.jpg'/>

我们经过跋山涉水，终于到了目的地Bert模型了。Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型，当然另外一点是语言模型的数据规模要比GPT大。所以这里Bert的预训练过程不必多讲了。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156085031331286.png'/>

第二阶段，Fine-Tuning阶段，这个阶段的做法和GPT是一样的。当然，它也面临着下游任务网络结构改造的问题，在改造任务方面Bert和GPT有些不同，下面简单介绍一下。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715474627832825.jpg'/>

在介绍Bert如何改造下游任务之前，先大致说下NLP的几类问题，说这个是为了强调Bert的普适性有多强。通常而言，绝大部分NLP问题可以归入上图所示的四类任务中：
一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。
第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。
第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；
第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156088092600404.png'/>

对于种类如此繁多而且各具特点的下游NLP任务，Bert如何改造输入输出部分使得大部分NLP任务都可以使用Bert预训练好的模型参数呢？
上图给出示例，对于句子关系类任务，很简单，和GPT类似，加上一个起始和终结符号，句子之间加个分隔符即可。
对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。
对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；
对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。

从这里可以看出，上面列出的NLP四大任务里面，除了生成类任务外，Bert其它都覆盖到了，而且改造起来很简单直观。尽管Bert论文没有提，但是稍微动动脑子就可以想到，其实对于机器翻译或者文本摘要，聊天机器人这种生成式任务，同样可以稍作改造即可引入Bert的预训练成果。只需要附着在S2S结构上，encoder部分是个深度Transformer结构，decoder部分也是个深度Transformer结构。根据任务选择不同的预训练数据初始化encoder和decoder即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个Transformer结构上加装隐层产生输出也是可以的。

不论如何，从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154759012126361.jpg'/>

Bert采用这种两阶段方式解决各种NLP任务效果如何？在11个各种类型的NLP任务中达到目前最好的效果，某些任务性能有极大的提升。一个新模型好不好，效果才是王道。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156091744887247.png'/>

到这里我们可以再梳理下几个模型之间的演进关系。从上图可见，Bert其实和ELMO及GPT存在千丝万缕的关系，
比如如果我们把GPT预训练阶段换成双向语言模型，那么就得到了Bert；
而如果我们把ELMO的特征抽取器换成Transformer，那么我们也会得到Bert。

所以你可以看出：Bert最关键两点，一点是特征抽取器采用Transformer；第二点是预训练的时候采用双向语言模型。那么新问题来了：对于Transformer来说，怎么才能在这个结构上做双向语言模型任务呢？

乍一看上去好像不太好搞。我觉得吧，其实有一种很直观的思路，怎么办？看看ELMO的网络结构图，只需要把两个LSTM替换成两个Transformer，一个负责正向，一个负责反向特征提取，其实应该就可以。当然这是我自己的改造，Bert没这么做。

那么Bert是怎么做的呢？我们前面不是提过Word2Vec吗？我前面肯定不是漫无目的地提到它，提它是为了在这里引出那个CBOW训练方法，所谓写作时候埋伏笔的“草蛇灰线，伏脉千里”，大概就是这个意思吧？

前面提到了CBOW方法，它的核心思想是：在做语言模型任务的时候，我把要预测的单词抠掉，然后根据它的上文Context-Before和下文Context-after去预测单词。其实Bert怎么做的？Bert就是这么做的。从这里可以看到方法间的继承关系。当然Bert作者没提Word2Vec及CBOW方法，这是我的判断，Bert作者说是受到完形填空任务的启发，这也很可能，但是我觉得他们要是没想到过CBOW估计是不太可能的。

从这里可以看出，在文章开始我说过Bert在模型方面其实没有太大创新，更像一个最近几年NLP重要技术的集大成者，原因在于此，当然我不确定你怎么看，是否认同这种看法，而且我也不关心你怎么看。其实Bert本身的效果好和普适性强才是最大的亮点。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156095587160714.png'/>

那么Bert本身在模型和方法角度有什么创新呢？就是论文中指出的Masked 语言模型和Next Sentence Prediction。而Masked语言模型上面讲了，本质思想其实是CBOW，但是细节方面有改进。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154773536828846.jpg'/>

Masked双向语言模型向上图展示这么做：随机选择语料中15%的单词，把它抠掉，也就是用[Mask]掩码代替原始单词，然后要求模型去正确预测被抠掉的单词。但是这里有个问题：训练过程大量看到[mask]标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对[mask]这个标记的，但是实际使用又见不到这个标记，这自然会有问题。

为了避免这个问题，Bert改造了一下，15%的被上天选中要执行[mask]替身这项光荣任务的单词中，只有80%真正被替换成[mask]标记，10%被狸猫换太子随机替换成另外一个单词，10%情况这个单词还待在原地不做改动。这就是Masked双向语音模型的具体做法。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154778357827261.jpg'/>

至于说“Next Sentence Prediction”，指的是做语言模型预训练的时候，分两种情况选择两个句子，一种是选择语料中真正顺序相连的两个句子；另外一种是第二个句子从语料库中抛色子，随机选择一个拼到第一个句子后面。我们要求模型除了做上述的Masked语言模型任务外，附带再做个句子关系预测，判断第二个句子是不是真的是第一个句子的后续句子。

之所以这么做，是考虑到很多NLP任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。所以可以看到，它的预训练是个多任务过程。这也是Bert的一个创新。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154781152753624.jpg'/>

上面这个图给出了一个我们此前利用微博数据和开源的Bert做预训练时随机抽出的一个中文训练实例，从中可以体会下上面讲的masked语言模型和下句预测任务。训练数据就长这种样子。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154783471696873.jpg'/>

顺带讲解下Bert的输入部分，也算是有些特色。它的输入部分是个线性序列，两个句子通过分隔符分割，最前面和最后增加两个标识符号。每个单词有三个embedding：
位置信息embedding，这是因为NLP中单词顺序是很重要的特征，需要在这里对位置信息进行编码；
单词embedding,这个就是我们之前一直提到的单词embedding；
第三个是句子embedding，因为前面提到训练数据都是由两个句子构成的，那么每个句子有个句子整体的embedding项对应给每个单词。

把单词对应的三个embedding叠加，就形成了Bert的输入。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157157778492670494.png'/>

至于Bert在预训练的输出部分如何组织，可以参考上图的注释。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154788099931636.jpg'/>

我们说过Bert效果特别好，那么到底是什么因素起作用呢？如上图所示，对比试验可以证明，跟GPT相比，双向语言模型起到了最主要的作用，对于那些需要看到下文的任务来说尤其如此。而预测下个句子来说对整体性能来说影响不算太大，跟具体任务关联度比较高。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154789784307560.jpg'/>

最后，我讲讲我对Bert的评价和看法，我觉得Bert是NLP里里程碑式的工作，对于后面NLP的研究和工业应用会产生长久的影响，这点毫无疑问。但是从上文介绍也可以看出，从模型或者方法角度看，Bert借鉴了ELMO，GPT及CBOW，主要提出了Masked 语言模型及Next Sentence Prediction，但是这里Next Sentence Prediction基本不影响大局，而Masked LM明显借鉴了CBOW的思想。所以说Bert的模型没什么大的创新，更像最近几年NLP重要进展的集大成者，这点如果你看懂了上文估计也没有太大异议，如果你有大的异议，杠精这个大帽子我随时准备戴给你。

如果归纳一下这些进展就是：首先是两阶段模型，第一阶段双向语言模型预训练，这里注意要用双向而不是单向，第二阶段采用具体任务Fine-tuning或者做特征集成；第二是特征抽取要用Transformer作为特征提取器而不是RNN或者CNN；第三，双向语言模型可以采取CBOW的方法去做（当然我觉得这个是个细节问题，不算太关键，前两个因素比较关键）。

Bert最大的亮点在于效果好及普适性强，几乎所有NLP任务都可以套用Bert这种两阶段解决思路，而且效果应该会有明显提升。可以预见的是，未来一段时间在NLP应用领域，Transformer将占据主导地位，而且这种两阶段预训练方法也会主导各种应用。

另外，我们应该弄清楚预训练这个过程本质上是在做什么事情，本质上预训练是通过设计好一个网络结构来做语言模型任务，然后把大量甚至是无穷尽的无标注的自然语言文本利用起来，预训练任务把大量语言学知识抽取出来编码到网络结构中，当手头任务带有标注信息的数据有限时，这些先验的语言学特征当然会对手头任务有极大的特征补充作用，因为当数据有限的时候，很多语言学现象是覆盖不到的，泛化能力就弱，集成尽量通用的语言学知识自然会加强模型的泛化能力。

如何引入先验的语言学知识其实一直是NLP尤其是深度学习场景下的NLP的主要目标之一，不过一直没有太好的解决办法，而ELMO/GPT/Bert的这种两阶段模式看起来无疑是解决这个问题自然又简洁的方法，这也是这些方法的主要价值所在。

对于当前NLP的发展方向，我个人觉得有两点非常重要，一个是需要更强的特征抽取器，目前看Transformer会逐渐担当大任，但是肯定还是不够强的，需要发展更强的特征抽取器；
第二个就是如何优雅地引入大量无监督数据中包含的语言学知识，注意我这里强调地是优雅，而不是引入，此前相当多的工作试图做各种语言学知识的嫁接或者引入，但是很多方法看着让人牙疼，就是我说的不优雅。目前看预训练这种两阶段方法还是很有效的，也非常简洁，当然后面肯定还会有更好的模型出现。

完了，这就是自然语言模型预训练的发展史。

PS，July注：上文PPT下载地址：http://ccl.pku.edu.cn/doubtfire/NLP/Deep_Learning/%E4%BB%8EWord%20Embedding%E5%88%B0Bert%20ppt%20%E5%BC%A0%E4%BF%8A%E6%9E%97.pdf

## 如何理解Seq2Seq Attention模型：图解Seq2Seq Attention
本题解析来源：https://zhuanlan.zhihu.com/p/40920384 ，和https://blog.csdn.net/Irving_zhang/article/details/78889364

seq2seq是一个Encoder–Decoder结构的网络，它的输入是一个序列，输出也是一个序列，Encoder中将一个可变长度的信号序列变为固定长度的向量表达，Decoder将这个固定长度的向量变成可变长度的目标的信号序列。

首先，我们使用x={x1，x2，…，xn}代表输入的语句，y={y1,y2,…,yn}代表输出的语句，yt代表当前输出词。在理解seq2seq的过程中，我们要牢记我们的目标是：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820189856707261.jpg'/>
即输出的yt不仅依赖之前的输出{y1,y2,…,yt−1}，还依赖输入语句x，模型再怎么变化都是在上述公式的约束之下。

seq2seq最初模型
最早由bengio等人发表在computerscience上的论文：LearningPhraseRepresentationsusingRNNEncoder–Decoder 
forStatisticalMachineTranslation。

对于RNN来说，x={x1，x2，…，xt}代表输入，在每个时间步t，RNN的隐藏状态ht由下述公式更新：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820210449024097.png'/>
其中，f代表一个非线性函数。这时ht就是一个rnn_size的隐含状态。然后需要通过一个矩阵W将其转成一个symbol_size的输出，并通过softmax函数将其转化为概率，然后筛选出概率最大的symbol为输出symbol。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820217792527635.jpg'/>

以上是rnn的基本原理，接下来介绍论文中的seq2seq模型： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820221847717693.jpg'/>
模型包括encoder和decoder两个部分。
首先在encoder部分，将输入传到encoder部分，得到最后一个时间步长t的隐藏状态C，这就是RNNcell的基本功能。

其次是decoder部分，从上述模型的箭头中可以看出，decoder的隐藏状态ht就由ht−1，yt−1和C三部分构成。即： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820259193663983.jpg'/>
由此我们的到了decoder的隐藏状态，那么最后的输出yt从图中也可以看得出来由三部分得到，yt从图中也可以看得出来由三部分得到，ht−1，yt−1和C，即：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820266236752971.jpg'/>
到现在为止，我们就实现了上文最开始的目标。

seq2seq的改进模型
改进模型介绍2014年发表的论文SequencetoSequenceLearningwithNeuralNetworks。模型图： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820278511576601.jpg'/>
可以看到，该模型和第一个模型主要的区别在于从输入到输出有一条完整的流：ABC为encoder的输入，WXYZ为decoder的输入。将encoder最后得到的隐藏层的状态ht输入到decoder的第一个cell里，就不用像第一个模型一样，每一个decoder的cell都需要ht，因此从整体上看，从输入到输出像是一条“线性的数据流”。

本文的论文也提出来，ABC翻译为XYZ，将encoder的input变为“CBA”效果更好。即A和X的距离更近了，更有利于seq2seq模型的交流。

具体来说，encoder的过程如下图。这和我们之前的encoder都一样。 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820293919476479.jpg'/>
不同的是decoder的阶段：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820297935765869.jpg'/>

得到了encoderrepresention，即encoder的最后一个时间步长的隐层ht以后，输入到decoder的第一个cell里，然后通过一个激活函数和softmax层，得到候选的symbols，筛选出概率最大的symbol，然后作为下一个时间步长的输入，传到cell中。这样，我们就得到了我们最开始的目标。

seq2seq with attention
我们前面提到，距离decoder的第一个cell越近的输入单词，对decoder的影响越大。但这并不符合常理，这时就提出了attention机制，对于输出的每一个cell，都检测输入的sequence里每个单词的重要性，即论文Neural Machine Translation by Jointly Learning to Align and Translate。attention在NMT基于seq2seq的改进模型再进行改进。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820423226415848.jpg'/>
上图中，encoder和decoder都发生了变化。首先说encoder，使用了双向RNN，因为希望不仅能得到前向的词的顺序，还希望能够得到反向的词的顺序。使用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820447266351394.png'/>代表hj前向的隐层状态，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820448261022537.png'/>代表hj的反向隐层状态，hj的最终状态为将两者连接(concat)起来，即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820449073286618.png'/>。

再说decoder。我们再来回顾一下我们最开始的目标公式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820189856707261.jpg'/>
对于加入attention机制的seq2seq，每一个输出为公式如下。即对于时间步i的输出yi，由时间步i的隐藏状态si，由attention计算得到的输入内容ci和上一个输出yi-1得到。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820454744511338.jpg'/>

其中si是对于时间步i的隐藏状态，由下述公式计算。即对于时间步i的隐藏状态，由时间步i-1的隐藏状态si-1，由attention计算得到的输入内容ci和上一个输出yi-1得到。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820459313937216.jpg'/>
通过以上公式可以看出，加入attention的seq2seq比之前的seq2seq多了一个输入内容向量ci，那么这个ci是怎么得来的呢？和输入内容以及attention有什么关系呢？我们接着看下述公式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820464935278728.jpg'/>
即，对于decoder的时间步长i的隐藏状态si，ci等于Tx个输入向量[1,Tx]与其权重<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820483584567741.png'/>相乘求和。这个权重<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820483584567741.png'/>由这个公式得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820476762319690.jpg'/>
其中，eij由下面公式得到
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820479476665219.jpg'/>
总结一下，对于时间步i的隐藏状态si，可以通过求时间步i-1的隐藏状态si-1、输入内容的编码向量ci和上一个输出yi-1得到。输入内容编码ci是新加入的内容，可以通过计算输入句子中每个单词的权重，然后加权求和得到ci。
直观解释这个权重：对于decoder的si和encoder的hj的权重<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820483584567741.png'/>，就是上一个时间步长的隐藏状态si-1与encoder的hj通过非线性函数得到的。这样就把输入内容加入到解码的过程中，这和我们人类翻译的过程也是类似的，即对于当前输出的词，每一个输入给与的注意力是不一样的。

上面一堆公式是不是看懵了。好了别管了，接下来开始刷图吧。
大框架
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819613655535774.jpg'/>
想象一下翻译任务，input是一段英文，output是一段中文。
公式（直接跳过看图最佳）
输入： <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819666441379406.svg'/>
输出： <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415781966719389889.svg'/>
(1)  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819667877685337.svg'/>,Encoder方面接受的是每一个单词wordembedding，和上一个时间点的hiddenstate。输出的是这个时间点的hiddenstate。
(2)  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819668613974666.svg'/>，Decoder方面接受的是目标句子里单词的wordembedding，和上一个时间点的hiddenstate。
(3)  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819669118649057.svg'/>,contextvector是一个对于encoder输出的hiddenstates的一个加权平均。
(4) <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819670368675621.svg'/>,每一个encoder的hiddenstates对应的权重。
(5)  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819671335923896.svg'/>,通过decoder的hiddenstates加上encoder的hiddenstates来计算一个分数，用于计算权重(4)
(6)<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819672177541328.svg'/> ,将contextvector和decoder的hiddenstates串起来。
(7) <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819672968590209.svg'/>，计算最后的输出概率。

详细图
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819617176492580.jpg'/>
左侧为Encoder+输入，右侧为Decoder+输出。中间为Attention。

(1)<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819677645031312.svg'/> ,Encoder方面接受的是每一个单词wordembedding，和上一个时间点的hiddenstate。输出的是这个时间点的hiddenstate。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819621681817468.jpg'/>
从左边Encoder开始，输入转换为wordembedding,进入LSTM。LSTM会在每一个时间点上输出hiddenstates。如图中的h1,h2,...,h8。

(2)<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819678688484058.svg'/>，Decoder方面接受的是目标句子里单词的wordembedding，和上一个时间点的hiddenstate。
接下来进入右侧Decoder，输入为(1)句首符号，原始contextvector(为0)，以及从encoder最后一个hiddenstate:h8。LSTM的是输出是一个hiddenstate。（当然还有cellstate，这里没用到，不提。）
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819623777338747.jpg'/>

(3)  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819679648091265.svg'/>,contextvector是一个对于encoder输出的hiddenstates的一个加权平均。
(4) <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819680210877215.svg'/> ,每一个encoder的hiddenstates对应的权重。
(5) <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819680637877193.svg'/> ,通过decoder的hiddenstates加上encoder的hiddenstates来计算一个分数，用于计算权重(4)
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819627541167161.jpg'/>
Decoder的hiddenstate与Encoder所有的hiddenstates作为输入，放入Attention模块开始计算一个contextvector。之后会介绍attention的计算方法。

下一个时间点
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819628860823770.jpg'/>
来到时间点2，之前的contextvector可以作为输入和目标的单词串起来作为lstm的输入。之后又回到一个hiddnstate。以此循环。

(6)<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819683848695318.svg'/> ,将contextvector和decoder的hiddenstates串起来。
(7) <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819684211239337.svg'/>，计算最后的输出概率。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819631334788367.jpg'/>
另一方面，contextvector和decoder的hiddenstate合起来通过一系列非线性转换以及softmax最后计算出概率。

在luong中提到了三种score的计算方法。这里图解前两种：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819633397841742.jpg'/>
Attentionscorefunction: dot

输入是encoder的所有hiddenstatesH:大小为(hiddim,sequencelength)。decoder在一个时间点上的hiddenstate，s：大小为（hiddim,1）。
第一步：旋转H为（sequencelength,hiddim)与s做点乘得到一个大小为(sequencelength,1)的分数。
第二步：对分数做softmax得到一个合为1的权重。
第三步：将H与第二步得到的权重做点乘得到一个大小为(hiddim,1)的contextvector。
Attentionscorefunction: general
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819635740854076.jpg'/>
输入是encoder的所有hiddenstatesH:大小为(hiddim1,sequencelength)。decoder在一个时间点上的hiddenstate，s：大小为（hiddim2,1）。此处两个hiddenstate的纬度并不一样。
第一步：旋转H为（sequencelength,hiddim1)与 Wa[大小为hiddim1,hiddim2)] 做点乘，再和s做点乘得到一个大小为(sequencelength,1)的分数。
第二步：对分数做softmax得到一个合为1的权重。
第三步：将H与第二步得到的权重做点乘得到一个大小为(hiddim,1)的contextvector。

完结
看懂一个模型的最好办法就是在心里想一遍从输入到模型到输出每一个步骤里，tensor是如何流动的。希望对大家有帮助～点个赞吧

后记
写完这篇看图解说快有一年了，看到评论里的一些关于细节的问题我想在这里统一回复：本文主要介绍Attention机制的一个思路，其实就是一个对于hiddenstates的weightedaverage。至于怎么去用c，s，怎么去算，方法五花八门。如果你大概念不理解，你可以看看这篇文章。如果你对细节有疑问，正确的方法还是去看开源的代码。毕竟最后还是全靠实战。

# NLP
## 1.了解Google最新的模型bert么？
BERT (Bidirectional Encoder Representations from Transformers)

10月11日，Google AI Language 发布了论文
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding
提出的 BERT 模型在 11 个 NLP 任务上的表现刷新了记录，包括问答 Question Answering (SQuAD v1.1)，推理 Natural Language Inference (MNLI) 等：

GLUE ：General Language Understanding Evaluation
MNLI ：Multi-Genre Natural Language Inference
SQuAD v1.1 ：The Standford Question Answering Dataset
QQP ： Quora Question Pairs 
QNLI ： Question Natural Language Inference

SST-2 ：The Stanford Sentiment Treebank
CoLA ：The Corpus of Linguistic Acceptability 
STS-B ：The Semantic Textual Similarity Benchmark
MRPC ：Microsoft Research Paraphrase Corpus
RTE ：Recognizing Textual Entailment 
WNLI ：Winograd NLI
SWAG ：The Situations With Adversarial Generations

让我们先来看一下 BERT 在 Stanford Question Answering Dataset (SQuAD) 上面的排行榜吧：
https://rajpurkar.github.io/SQuAD-explorer/
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155721663860330387.png'/>
BERT 可以用来干什么？

BERT 可以用于问答系统，情感分析，垃圾邮件过滤，命名实体识别，文档聚类等任务中，作为这些任务的基础设施即语言模型，

BERT 的代码也已经开源：
https://github.com/google-research/bert
我们可以对其进行微调，将它应用于我们的目标任务中，BERT 的微调训练也是快而且简单的。

例如在 NER 问题上，BERT 语言模型已经经过 100 多种语言的预训练，这个是 top 100 语言的列表：
https://github.com/google-research/bert/blob/master/multilingual.md

只要在这 100 种语言中，如果有 NER 数据，就可以很快地训练 NER。

BERT 原理简述
BERT 的创新点在于它将双向 Transformer 用于语言模型，
之前的模型是从左向右输入一个文本序列，或者将 left-to-right 和 right-to-left 的训练结合起来。

实验的结果表明，双向训练的语言模型对语境的理解会比单向的语言模型更深刻，论文中介绍了一种新技术叫做 Masked LM（MLM），在这个技术出现之前是无法进行双向语言模型训练的。

BERT 利用了 Transformer 的 encoder 部分。
Transformer 是一种注意力机制，可以学习文本中单词之间的上下文关系的。
Transformer 的原型包括两个独立的机制，一个 encoder 负责接收文本作为输入，一个 decoder 负责预测任务的结果。
BERT 的目标是生成语言模型，所以只需要 encoder 机制。

Transformer 的 encoder 是一次性读取整个文本序列，而不是从左到右或从右到左地按顺序读取，
这个特征使得模型能够基于单词的两侧学习，相当于是一个双向的功能。

下图是 Transformer 的 encoder 部分，输入是一个 token 序列，先对其进行 embedding 称为向量，然后输入给神经网络，输出是大小为 H 的向量序列，每个向量对应着具有相同索引的 token。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155721672283337082.png'/>
图片 by Rani Horev

当我们在训练语言模型时，有一个挑战就是要定义一个预测目标，很多模型在一个序列中预测下一个单词，
“The child came home from ___”
双向的方法在这样的任务中是有限制的，为了克服这个问题，BERT 使用两个策略:

1. Masked LM (MLM)
在将单词序列输入给 BERT 之前，每个序列中有 15％ 的单词被 [MASK] token 替换。 然后模型尝试基于序列中其他未被 mask 的单词的上下文来预测被掩盖的原单词。

这样就需要：
i)在 encoder 的输出上添加一个分类层
ii)用嵌入矩阵乘以输出向量，将其转换为词汇的维度
iii)用 softmax 计算词汇表中每个单词的概率

BERT 的损失函数只考虑了 mask 的预测值，忽略了没有掩蔽的字的预测。这样的话，模型要比单向模型收敛得慢，不过结果的情境意识增加了。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155721677852106261.png'/>
图片 by Rani Horev

2. Next Sentence Prediction (NSP)
在 BERT 的训练过程中，模型接收成对的句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子。
在训练期间，50％ 的输入对在原始文档中是前后关系，另外 50％ 中是从语料库中随机组成的，并且是与第一句断开的。

为了帮助模型区分开训练中的两个句子，输入在进入模型之前要按以下方式进行处理：

在第一个句子的开头插入 [CLS] 标记，在每个句子的末尾插入 [SEP] 标记。
将表示句子 A 或句子 B 的一个句子 embedding 添加到每个 token 上。
给每个 token 添加一个位置 embedding，来表示它在序列中的位置。
为了预测第二个句子是否是第一个句子的后续句子，用下面几个步骤来预测：

整个输入序列输入给 Transformer 模型
用一个简单的分类层将 [CLS] 标记的输出变换为 2×1 形状的向量
用 softmax 计算 IsNextSequence 的概率
在训练 BERT 模型时，Masked LM 和 Next Sentence Prediction 是一起训练的，目标就是要最小化两种策略的组合损失函数。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155721681290293736.png'/>
如何使用 BERT?
BERT 可以用于各种NLP任务，只需在核心模型中添加一个层，例如：
a)在分类任务中，例如情感分析等，只需要在 Transformer 的输出之上加一个分类层
b)在问答任务（例如SQUAD v1.1）中，问答系统需要接收有关文本序列的 question，并且需要在序列中标记 answer。 可以使用 BERT 学习两个标记 answer 开始和结尾的向量来训练Q＆A模型。
c)在命名实体识别（NER）中，系统需要接收文本序列，标记文本中的各种类型的实体（人员，组织，日期等）。 可以用 BERT 将每个 token 的输出向量送到预测 NER 标签的分类层。

在 fine-tuning 中，大多数超参数可以保持与 BERT 相同，在论文中还给出了需要调整的超参数的具体指导（第3.5节）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155721683728608771.png'/>
学习资料：
https://arxiv.org/pdf/1810.04805.pdf
https://www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/
https://medium.com/syncedreview/best-nlp-model-ever-google-bert-sets-new-standards-in-11-language-tasks-4a2a189bc155
## 2.了解文本嵌入么？
什么是NLP？
自然语言处理（NLP） 是计算机科学，人工智能和语言学的交叉领域。目标是让计算机处理或“理解”自然语言，以执行语言翻译和问题回答等任务。

随着语音接口和聊天机器人的兴起，NLP正在成为信息时代最重要的技术之一，同时它也是人工智能的关键部分。充分理解和表达语言的含义是一个非常困难的目标。为什么？因为人类的语言很特别。

人类语言有什么特别之处？
1.人类语言是专门为传达说话人的意图而构建的系统。这不仅仅是一个环境信号，更是一个有意识的交流。
2.人类语言大多是离散/符号的/分类的信号系统，大概是因为信号可靠性更高。
3.一种语言的分类符号可以用几种方式编码为通信信号：声音，手势，写作，图像等。人类语言只是其中的一种。
4.人类语言是不明确的（与编程和其他正式语言不同）。 因此，在表达、学习和使用语言/情境/情境/文字/视觉知识对人类语言方面存在高度复杂性。

NLP应用到哪里？
从NLP研究领域衍生出了一批快速增长的应用程序。以下是其中几个：
1.拼写检查，关键字搜索，查找同义词；
2.从网站提取信息，例如：产品价格，日期，地点，人员或公司名称；
3.分类：长文档的积极/消极情绪；
4.机器翻译；
5.口语对话系统；
6.复杂的问答系统；

事实上，这些应用程序已经在现实中大量使用，从搜索到在线广告匹配 ; 从自动/辅助翻译到营销或财务/交易的情绪分析 ; 从语音识别到chatbots /对话代理（自动化客户支持，控制设备，订购商品）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946313784534369.jpg'/>

深度学习
大部分NLP技术都是由深度学习提供技术支持。近几年，深度学习才开始发挥作用，主要是因为：
·大量的训练数据；
·更快的机器和多核CPU / GPU；
·性能高的新模型和算法：有效的端到端联合系统学习、有效的使用上下文和任务间转换的学习方法，以及正则化优化方法。

在深度学习中，表示学习试图自动学习来自原始输入的良好特征或表示。而在机器学习中手动设计的特征通常过多且不完整，需要花费很长时间进行设计和验证。而且深度学习提供了一个非常灵活、通用且可学习的框架，用于呈现视觉和语言信息的世界。

最初，它在语音识别和计算机视觉等领域取得突破。最近，深度学习方法在许多不同的NLP任务中表现出了非常高的性能。这些模型通常可以通过单一的端到端模型进行训练，并且不需要传统的，特定于任务的特征工程。

下面简单介绍下文本嵌入（Text Embeddings）。

在传统的NLP中，我们将单词视为离散符号，然后可以用one-hot向量表示。向量的维度是整个词汇表中单词的数量。单词作为离散符号的问题在于，对于one-hot向量来说，没有自然的相似性概念。

因此，另一种方法是学习在向量本身中编码相似性。核心思想是一个词的含义是由经常出现在其旁边的单词给出的。

文本嵌入是字符串的实值向量表示。我们为每个单词建立一个密集的向量，选择它以便类似于类似上下文中出现的单词的向量。对于大多数NLP任务而言，词嵌入被认为是一个很好的起点。它们允许深度学习在较小的数据集上也是有效的，因为它们通常是深度学习体系的第一批输入，也是NLP中最流行的迁移学习方式。

在词嵌入中最流行的应该是Word2vec，它是由谷歌（Mikolov）开发的模型，另外一个是由斯坦福大学（彭宁顿，Socher和曼宁）开发的GloVe。

接着我们重点介绍这两种模型：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946332328697749.png'/>

在Word2vec中，我们有一个庞大的文本语料库，其中固定词汇表中的每个词都由一个向量表示。然后，我们通过文本中的每个位置t，其中有一个中心词c和上下文词o。

接下来，我们使用字向量的相似性Ç和Ò计算的概率ø给出Ç（或反之亦然）。我们不断调整单词向量来最大化这个概率。为了有效地训练Word2vec，我们可以从数据集中去除无意义的单词。这有助于提高模型的准确性。

Word2vec有两个变体值得一提：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946339451861219.png'/>

1.Skip-Gram：我们考虑一个包含k个连续项的上下文窗口。
然后，我们跳过其中一个单词，尝试学习一个神经网络，该网络可以获得除跳过的所有术语外的所有术语，并预测跳过的术语。
因此，如果两个单词在大语料库中反复共享相似的上下文，那么这些术语的嵌入向量将具有相似的向量。

2.Continuous Bag of Words：我们在一个大的语料库中获取大量的句子，每当我们看到一个词，我们就会联想到周围的词。
然后，我们将上下文单词输入到神经网络，并预测该上下文中心的单词。
当我们有数千个这样的上下文单词和中心单词时，我们就有了一个神经网络数据集的实例。我们训练神经网络，最后编码的隐藏层输出表示一个特定的词嵌入。

当我们通过大量的句子进行训练时，类似上下文中的单词会得到相似的向量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946350921665397.jpg'/>

对Skip-Gram和CBOW的一个吐槽就是它们都是基于窗口的模型，这意味着语料库的共现统计不能被有效使用，导致次优的嵌入（suboptimal embeddings）。

GloVe模型旨在通过捕捉一个字与整个观测语料库的结构嵌入的含义来解决这个问题。为此，该模型训练单词的全局共现次数，并通过最小化最小二乘误差来充分利用统计量，从而产生具有有意义子结构的单词向量空间。这样的做法足以保留单词与向量距离的相似性。

除了这两种文本嵌入外，还有许多最近开发的高级模型，包括FastText，Poincare嵌入，sense2vec，Skip-Thought，Adaptive Skip-Gram，我强烈建议你学习一下。

## 3.了解机器翻译中的NLP技术么？
机器翻译是语言理解的经典测试。它由语言分析和语言生成组成。大型机器翻译系统具有巨大的商业用途，给你一些值得注意的例子：

·谷歌翻译每天翻译1000亿字；
·Facebook使用机器翻译自动翻译帖子和评论中的文字，以打破语言障碍，让世界各地的人们相互交流；
·阿里巴巴使用机器翻译技术来实现跨境贸易，连接世界各地的买家和卖家；
·微软为Android、iOS和Amazon Fire上的最终用户和开发人员提供基于人工智能的翻译，无论他们是否可以访问互联网。

在传统的机器翻译系统中，我们必须使用平行语料库：一组文本，每个文本都被翻译成一种或多种不同于原文的其他语言。

例如，给定源语言f（例如法语）和目标语言e（例如英语），我们需要建立多个统计模型，包括使用贝叶斯规则的概率公式，训练的翻译模型p（f | e）平行语料库和语言模型p（e）在纯英文语料库上训练。这种方法跳过了数百个重要细节，需要大量的手工特征工程，整体而言它是一个非常复杂的系统。

神经机器翻译是通过一个称为递归神经网络（RNN）的大型人工神经网络对整个过程进行建模的方法。RNN是一个有状态的神经网络，它通过时间连接过去。神经元的信息不仅来自前一层，而且来自更前一层的信息。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946362363352968.png'/>

标准的神经机器翻译是一种端到端神经网络，其中，源语句由称为编码器的RNN 编码，目标词使用另一个称为解码器。RNN编码器一次读取一个源语句，然后在最后隐藏状态汇总整个源句子。RNN解码器使用反向传播学习这个汇总并返回翻译后的版本。

神经机器翻译从2014年的一项边缘研究领域发展到2016年广泛采用的领先机器翻译方式，那么，使用神经机器翻译的最大成功是什么？
1.端到端训练：NMT中的所有参数同时被优化，以最大限度地减少网络输出的损耗性能。
2.分布式表示的优势：NMT更好地利用单词和短语的相似性。
3.更好地探索上下文：NMT可以使用更多的上下文——源文本和部分目标文本以此进行更准确地翻译。
4.更流利的文本生成：深度学习文本生成质量高于平行语料库。

RNN的一个大问题是梯度消失（或爆炸）问题，其中取决于所使用的激活函数，随着时间的推移信息会迅速丢失。

直观地说，这不会成为一个很大问题，因为这些只是权重而不是神经元状态，但是时间的权重实际上是存储过去的信息的地方，如果权重达到0或1,000,000的值，那么以前的状态将不会提供很多信息。

因此，RNNs在记忆序列中的前几个单词时会表现的很困难，并且只能根据最近的单词进行预测。

长期/短期记忆（LSTM）网络试图通过引入门和明确定义的存储器单元来对抗梯度消失/爆炸问题。每个神经元都有一个存储单元和三个门：输入、输出和忘记。这些门的功能是通过停止或允许信息流来保护信息。
    ①输入门决定了来自上一层的多少信息存储在单元中；
    ②输出层在另一端获取任务，并确定下一层有多少单元知道该单元的状态。
    ③忘记门的作用起初看起来很奇怪，但有时候忘记门是个不错的设计：如果它正在学习一本书并开始新的一章，那么网络可能需要忘记前一章中的一些字符。

已经证明LSTM能够学习复杂的序列，例如像莎士比亚的写作或者创作原始音乐。请注意，这些门中的每一个都对前一个神经元中的一个单元具有权重，因此它们通常需要更多资源才能运行。

LSTM目前非常流行，并且在机器翻译中被广泛使用。除此之外，它是大多数序列标签任务的默认模型，其中有大量的数据。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946375621557650.jpg'/>

门控重复单元（GRU）是在LSTM的基础上变形得来的，也是神经机器翻译的扩展。它拥有更少的门，并且连接方式略有不同：它不是输入、输出和忘记门组成的，而是具有更新门。这个更新门决定了从最后一个状态开始保留多少信息以及从上一个层开始输入多少信息。

复位（reset）门的功能与LSTM的忘记（forget）门非常相似，但位置稍有不同。他们总是发出它们完整的状态因为他们没有输出门。在大多数情况下，它们的功能与LSTM非常相似，最大的不同之处在于GRUs稍快并且更容易运行（但表现力稍差）。

在实践中，这些往往会互相抵消，因为你需要一个更大的网络来重新获得一些表示能力，这反过来又抵消了性能的优势。在一些情况下，GRU可以胜过LSTM。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154946381391394604.png'/>

除了这三大体系结构之外，过去几年神经​​机器翻译系统还有进一步的改进。以下是最显着的发展：
    a)用神经网络进行序列学习的序列证明了LSTM在神经机器翻译中的有效性。它提出了序列学习的一种通用的端到端方法，对序列结构进行了最少的假设。该方法使用多层Long Short Term Memory（LSTM）将输入序列映射为固定维度的向量，然后使用另一个深度LSTM从向量解码目标序列。
    b)通过联合学习对齐和翻译的神经机器翻译引入了NLP中的注意机制（将在下一篇文章中介绍）。认识到使用固定长度矢量是提高NMT性能的瓶颈，作者建议通过允许模型自动（软）搜索与预测目标相关的源句子部分来扩展，而不必将这些部分明确地形成为一个固定的长度。
    c)用于神经机器翻译的循环编码器上的卷积利用附加的卷积层增强NMT中的标准RNN编码器，以在编码器输出中捕捉更广泛的上下文。
谷歌的神经机器翻译，它解决了准确性和部署方便性的问题。该模型由一个深度LSTM网络组成，该网络包含8个编码器和8个解码器层，使用残余连接以及从解码器网络到编码器的注意力连接。
    d)Facebook AI研究人员不使用递归神经网络，而是使用卷积神经网络序列对NMT中的学习任务进行排序。
## 4.了解情感分析中的NLP技术么？
人际交往不仅仅是文字和其明确的含义，而且它还是微妙且复杂的。即使在完全基于文本的对话中，你也可以根据单词选择和标点符号判断客户是否感到愤怒。你可以阅读产品在天猫平台的评论，并了解评论者是否喜欢或不喜欢它，即使他们从未直接说过。

为了使计算机真正理解人类每天的交流方式，他们需要理解的不仅仅是客观意义上的词语定义、而且他们需要了解我们的情绪。

情绪分析是通过较小元素的语义组成来解释较大文本单元（实体、描述性术语、事实、论据、故事）的含义的过程。

传统情感分析的方法是将句子视为一个词袋，并查阅“积极”和“消极”单词的策划列表，以确定该句子的情绪。这需要手工设计的特征来捕捉情绪，所有这是非常耗时和不可扩展的。

用于情感分析的现代深度学习方法可用于形态学、语法和逻辑语义，其中最有效的是递归神经网络。顾名思义，递归神经网络开发的主要假设递归是描述语言的自然方式。递归在消歧方面很有用，有助于某些任务引用特定的短语，并且对于使用语法树结构的任务非常有效。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728521912439982.png'/>
递归神经网络非常适合具有嵌套层次结构和内部递归结构的设置。语法的句法规则是高度递归的，因此，我们利用递归神经网络！

使用RNN对句子进行建模的另一个好处是，我们现在可以输入任意长度的句子，这对于在NLP中使用神经网络来说是一个巨大的难题，使用非常聪明的技巧使句子的输入向量具有相同的大小，尽管句子的长度不相等。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728522658917987.png'/>
标准RNN是一种递归神经网络的最基本的版本。它具有最大边距结构预测架构，可以在复杂的场景图像和句子中成功地运用这种结构。它用于为自然语言句子提供有竞争力的语法分析器比如说Penn Treebank。

作为参考，Penn Treebank是第一个大型树形数据集，由华尔街日报三年（WSJ）收集的24,799个故事组成，它广泛用于句法注释。此外，它优于语义场景分割、注释和分类的替代方法。

然而，标准RNN并不能捕获语法短语的完整语法。在语法上解开RNN，也被称为成分矢量语法（CVG），这个方法是解决这个问题的一个重大升级。它使用语法解开的递归神经网络来学习句法语义和组合向量表示。该模型能够像标准RNN一样快速地进行训练和实施。

另一个演变是Matrix-Vector RNN，它能够捕获更长短语的组成含义。该模型为解析树中的每个节点分配一个向量和一个矩阵：向量用于捕获成分的固有含义，而矩阵捕获它如何改变相邻单词或短语的含义。而且该矩阵向量RNN可以在命题逻辑和自然语言中学习运算符的含义。

该模型在三个不同的实验中获得过不错的表示：
· 预测副词-形容词对的细粒度情感分布；
· 对电影评论的情感标签进行分类；
· 使用它们之间的句法路径对名词之间的语义关系（例如因果关系）进行分类。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728523343039565.png'/>
迄今为止用于情感分析的最强大的RNN模型是递归神经张量网络，其在每个节点处具有神经网络的树结构。该模型可用于边界分割，以确定哪些词组是积极的，哪些是消极的。

在Sentiment Treebank上接受训练时，该模型在几个指标上的表现优于所有以前的方法。
## 5.了解问答系统中涉及的NLP技术么？
问答（QA）系统的想法是直接从文档、对话、在线搜索和其他地方提取信息，以满足用户的信息需求。QA系统不是让用户阅读整个文档，而是更喜欢简短而简洁的答案。如今，QA系统可以非常容易地与其他NLP系统结合使用，并且一些QA系统甚至超越了对文本文档的搜索，并且可以从图片集合中提取信息。

事实上，大多数NLP问题都可以被视为一个问题回答问题。范例很简单：我们发出查询指令，机器提供响应。通过阅读文档或一组指令，智能系统应该能够回答各种各样的问题。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728507426066624.png'/>
强大的深度学习架构（称为动态内存网络（DMN））已针对QA问题进行了专门开发和优化。给定输入序列（知识）和问题的训练集，它可以形成情节记忆，并使用它们来产生相关答案。该体系结构具有以下组件：

· 语义内存模块（类似于知识库）被用来创建从输入句子的嵌入字序列预先训练手套载体。
· 输入模块处理与问题有关的输入矢量称为事实。该模块使用门控循环单元实现，GRU使网络能够了解当前正在考虑的句子是否相关或与答案无关。

· 问题模块逐字处理疑问词，并且使用输出相同权重的GRU输入模块的向量。事实和问题都被编码为嵌入。
· 情景记忆模块接收从输入中提取和编码的嵌入事实和问题载体。这使用了一个受大脑海马体启发的想法，它可以检索由某些反应触发的时间状态，如景点或声音。
· 答案生成模块，通过适当的响应，情景记忆应该包含回答问题所需的所有信息。该模块使用另一个GRU，使用正确序列的交叉熵错误分类进行训练，然后可以将其转换回自然语言。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728508087672647.png'/>
DMN不仅在质量保证方面做得非常好，而且在情感分析和词性标注方面也优于其他架构。自开发以来，动态内存网络已经有了重大改进，进一步提高其在问答环境中的准确性，包括：
· 用于视觉和文本问题的动态存储网络问答应用基本上是将DMN应用于图像，其内存和输入模块已升级，以便能够回答视觉问题。该模型改进了许多基准Visual Question Answering数据集的现有技术水平，而不支持事实监督。

· 用于问题应答的动态Coattention网络解决了从对应于不正确答案的局部最大值恢复的问题。它首先融合了问题和文件的共同依赖表示，以便集中于两 者的相关部分。然后，动态指向解码器迭代潜在的答案跨度，该迭代过程使模型能够从对应于不正确答案的初始局部最大值中恢复。
## 6.了解文本摘要中的NLP技术么？
人类很难手动汇总大型文本文档。文本摘要是NLP为源文档创建简短、准确和流畅的摘要问题。随着推送通知和文章摘要获得越来越多的注意力，为长文本生成智能且准确摘要的任务每天都在增长。

通过首先计算整个文本文档的单词频率来自动汇总文本。

然后，存储和排序100个最常用的单词。然后根据它包含的高频词数对每个句子进行评分，更高频率的词，价值更大。

最后，根据它们在原始文本中的位置来获取和排序前X个句子。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728501064833759.png'/>
文本摘要有两种基本方法：提取和抽象。前者从原始文本中提取单词和单词短语以创建摘要。后者是学习内部语言表示以生成更像人类的摘要，解释原始文本的意图。

提取摘要的方法是通过选择子集来工作。这是通过从实际文章中提取短语或句子以形成摘要来完成的，LexRank和TextRank是众所周知的摘要总结，它们都使用了Google PageRank算法的变体。
· LexRank是一种无监督的基于图形的算法，它使用IDF修改的余弦作为两个句子之间的相似性度量。该相似度用作两个句子之间的图形边缘的权重。LexRank还采用了智能后处理步骤，确保为摘要选择的顶级句子彼此不太相似。
· TextRank是一种类似于LexRank的算法，具有一些增强功能，例如使用词形化而不是词干，结合词性标注和命名实体分辨率，从文章中提取关键短语，以及根据这些短语提取摘要句子。除了文章摘要外，TextRank还从文章中提取了有意义的关键短语。

抽象概括的模型属于深度学习。使用深度学习的文本摘要已经取得了一定的突破。以下是一些NLP领域最大公司最显着的公布结果：
· Facebook的神经注意是一种神经网络架构，它利用基于本地注意力的模型，能够根据输入句子生成摘要中的每个单词。
· Google Brain的Sequence-to-Sequence模型遵循编码器-解码器架构。编码器负责读取源文档并将其编码为内部表示，解码器是一种语言模型，负责使用源文档的编码表示在输出摘要中生成每个单词。
· IBM Watson使用类似的序列到序列模型，但具有注意力和双向递归神经网络功能。
## 7.了解注意力机制么？
神经网络中的注意力机制是基于人类的视觉注意机制。人类的视觉注意力虽然存在不同的模型，但它们都基本上归结为能够以“高分辨率”聚焦于图像的某个区域，同时以“低分辨率”感知周围的图像，然后随着时间的推移调整焦点。

想象一下，你正在阅读一篇完整的文章：不是按顺序浏览每个单词或字符，而是潜意识地关注一些信息密度最高的句子并过滤掉其余部分。你的注意力有效地以分层方式捕获上下文信息，这样就可以在减少开销的同时做出决策。

那为什么这很重要？诸如LSTM和GRU之类的模型依赖于读取完整的句子并将所有信息压缩为固定长度的矢量。这需要基于文本统计属性的复杂特征工程，用几个单词表示的数百个单词的句子肯定会导致信息丢失，翻译不足等。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728485442341045.png'/>
通过注意力机制，我们不再尝试将全文编码为固定长度的矢量。相反，我们允许解码器在输出生成的每个步骤处理源语句的不同部分。我们让模型根据输入句子以及它到目前为止产生的内容来学习要注意的内容。

根据上面从基于注意力的神经机器翻译的有效方法的图像，蓝色表示编码器，红色表示解码器，因此我们可以看到上下文向量将所有单元格的输出作为输入来计算每个单元格的源语言单词的概率分布。解码器想要生成单个字，通过利用该机制，解码器可以捕获全局信息而不是仅基于一个隐藏状态进行推断。

除了机器翻译之外，注意力模型还可以处理各种其他NLP任务。在Show，Attend和Tell：使用视觉注意生成神经图像标题，作者将注意力机制应用于生成图像描述的问题。他们使用卷积神经网络对图像进行编码，使用具有注意力机制的递归神经网络来生成描述。通过可视化注意力，他们可以在生成单词时解释模型正在查看的内容：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728486099757508.png'/>
在语法作为外语中，作者使用具有注意力机制的递归神经网络来生成句子解析的树。可视化的注意力矩阵可以深入了解网络如何生成这些树：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728486624127618.png'/>
在阅读和理解的教学机器中，作者使用回归神经网络来阅读文本，阅读问题，然后产生答案。通过可视化关注矩阵，它们可以在尝试查找问题答案时显示网络的外观：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728487292365440.png'/>
然而，注意力机制需要付出代价。我们需要计算输入和输出字的每个组合的注意力值。如果你有一个100字的输入序列并生成一个100字的输出序列，那将是10,000个注意力值。如果你进行字符级计算并处理由数百个令牌组成的序列，上述机制可能变得非常昂贵。

自然语言处理已经解决的障碍
值得注意的是，研究人员不得不处理各种障碍：算法的局限性、模型的可扩展性、对人类语言的模糊理解。好消息是，这个领域的发展似乎是一个巨大的开源项目：研究人员不断构建更好的模型来解决现有问题并与社区分享他们的结果。

由于最近的学术研究进展，以下是NLP中已经解决的主要障碍：
· 没有单一的模型架构，跨任务具有一致的最新结果。例如，在Question Answering中，我们有强监督的端到端内存网络 ; 在情感分析中，我们有Tree-LSTM ; 在序列标记中，我们有双向LSTM-CRF。我之前在问题回答部分中提到的动态内存网络以某种方式解决了这一挑战，因为它可以在多个域中一致地运行。

· 机器学习中一种强大的方法是多任务学习，它共享相关任务之间的表示，以使模型能够更好地概括原始任务。然而，相关的多任务学习很难，因为它通常仅限于较低层，仅在任务相关时才有用，并且在所提出的模型中具有相同的解码器/分类器。在联合多任务模型中：为多个NLP任务增长，作者预先定义了一个由几个NLP任务组成的分层架构，作为多任务学习的联合模型。该模型包括字符n-gram和短路以及最先进的纯前馈解析器，能够执行依赖解析，多句子任务和联合训练。

· 另一个挑战是重复字表示的问题，其中模型中编码器和解码器的不同编码导致重复的参数/含义。对此最简单的解决方案是将单词向量联系在一起并联合训练单个权重，如“绑定单词向量” 和“单词分类器：语言建模的损失框架”中所示。

· 另一个障碍是，与诸如卷积神经网络或前馈神经网络相比，任何Deep NLP技术的基本构建块Recurrent Neural Networks相当慢。准递归神经网络采用RNN和CNN的最佳部分来提高训练速度，使用卷积跨越时间的并行性和跨越信道的并行性的元素级门控递归。这种方法比语言建模和情感分析中的任何其他模型更好，更快。

· 最后，在NLP中，架构搜索使用机器学习自动化人工神经网络设计的过程 非常缓慢，因为传统的手动过程需要大量的专业知识。如果我们可以使用AI为任何问题找到合适的架构怎么办？使用Google Brain进行强化学习的神经架构搜索是迄今为止开发的最可行的解决方案。作者使用循环网络生成神经网络的模型描述，并使用强化学习训练此RNN，以最大化验证集上生成的体系结构的预期准确性。
## 8.如何通俗理解Word2vec
小编注：考虑到下文（https://zhuanlan.zhihu.com/p/26306795）没有对比清楚2013年Mikolov原始论文与2014年Rong.X文章的区别，故建议初学者先看July在CSDN上写的Word2vec笔记，《如何通俗理解word2vec》：https://blog.csdn.net/v_JULY_v/article/details/102708459，更清晰、更易懂

1. 引子
大家好
我叫数据挖掘机
皇家布鲁斯特大学肄业
我喝最烈的果粒橙，钻最深的牛角尖
——执着如我

今天我要揭开Word2vec的神秘面纱
直窥其本质

相信我，这绝对是你看到的
最浅白易懂的 Word2vec 中文总结
（蛤？你问我为啥有这个底气？
且看下面，我的踩坑血泪史。。。）

2. Word2vec参考资料总结
(以下都是我踩过的坑，建议先跳过本节，阅读正文部分，读完全文回头再来看)

先大概说下我深挖 word2vec 的过程：先是按照惯例，看了 Mikolov 关于 Word2vec 的两篇原始论文，然而发现看完依然是一头雾水，似懂非懂，主要原因是这两篇文章省略了太多理论背景和推导细节；
然后翻出 Bengio 03年那篇JMLR和 Ronan 11年那篇JMLR，看完对语言模型、用CNN处理NLP任务有所了解，但依然无法完全吃透 word2vec；
这时候我开始大量阅读中英文博客，其中 北漂浪子 的一篇阅读量很多的博客吸引了我的注意，里面非常系统地讲解了 Word2vec 的前因后果，最难得的是深入剖析了代码的实现细节，看完之后细节方面了解了很多，不过还是觉得有些迷雾；
终于，我在 quora 上看到有人推荐 Xin Rong 的那篇英文paper，看完之后只觉醍醐灌顶，酣畅淋漓，相见恨晚，成为我首推的 Word2vec 参考资料。

下面我将详细列出我阅读过的所有 Word2vec 相关的参考资料，并给出评价

Mikolov 两篇原论文：
『Distributed Representations of Sentences and Documents』
在前人基础上提出更精简的语言模型（language model）框架并用于生成词向量，这个框架就是 Word2vec

『Efficient estimation of word representations in vector space』
专门讲训练 Word2vec 中的两个trick：hierarchical softmax 和 negative sampling
优点：Word2vec 开山之作，两篇论文均值得一读
缺点：只见树木，不见森林和树叶，读完不得要义。这里『森林』指 word2vec 模型的理论基础——即 以神经网络形式表示的语言模型，『树叶』指具体的神经网络形式、理论推导、hierarchical softmax 的实现细节等等

北漂浪子的博客：『深度学习word2vec 笔记之基础篇』
优点：非常系统，结合源码剖析，语言平实易懂
缺点：太啰嗦，有点抓不住精髓

Yoav Goldberg 的论文：『word2vec Explained- Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method』
优点：对 negative-sampling 的公式推导非常完备
缺点：不够全面，而且都是公式，没有图示，略显干枯

Xin Rong 的论文：『word2vec Parameter Learning Explained』：
！重点推荐！
理论完备由浅入深非常好懂，且直击要害，既有 high-level 的 intuition 的解释，也有细节的推导过程
一定要看这篇paper！一定要看这篇paper！一定要看这篇paper！
评论区 @huichan 告知了一条沉重的信息，Rong Xin 于2017年驾驶飞机失事，永远离开了我们。缅怀，R.I.P，愿他能在天堂继续开心地科研

来斯惟的博士论文『基于神经网络的词和文档语义向量表示方法研究』以及他的博客（网名：licstar）
可以作为更深入全面的扩展阅读，这里不仅仅有 word2vec，而是把词嵌入的所有主流方法通通梳理了一遍

几位大牛在知乎的回答：『word2vec 相比之前的 Word Embedding 方法好在什么地方？』
刘知远、邱锡鹏、李韶华等知名学者从不同角度发表对 Word2vec 的看法，非常值得一看

Sebastian 的博客：『On word embeddings - Part 2: Approximating the Softmax』
详细讲解了 softmax 的近似方法，Word2vec 的 hierarchical softmax 只是其中一种

3. 正文
你会在本文看到：
提纲挈领地讲解 word2vec 的理论精髓
学会用gensim训练词向量，并寻找相似词
你不会在本文看到

神经网络训练过程的推导
hierarchical softmax/negative sampling 等 trick 的理论和实现细节

3.1. 什么是 Word2vec?
在聊 Word2vec 之前，先聊聊 NLP (自然语言处理)。NLP 里面，最细粒度的是 词语，词语组成句子，句子再组成段落、篇章、文档。所以处理 NLP 的问题，首先就要拿词语开刀。

举个简单例子，判断一个词的词性，是动词还是名词。用机器学习的思路，我们有一系列样本(x,y)，这里 x 是词语，y 是它们的词性，我们要构建 f(x)->y 的映射，但这里的数学模型 f（比如神经网络、SVM）只接受数值型输入，而 NLP 里的词语，是人类的抽象总结，是符号形式的（比如中文、英文、拉丁文等等），所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec，就是词嵌入（ word embedding) 的一种。

我在前作『都是套路: 从上帝视角看透时间序列和数据挖掘』提到，大部分的有监督机器学习模型，都可以归结为：
f(x)->y

在 NLP 中，把 x 看做一个句子里的一个词语，y 是这个词语的上下文词语，那么这里的 f，便是 NLP 中经常出现的『语言模型』（language model），这个模型的目的，就是判断 (x,y) 这个样本，是否符合自然语言的法则，更通俗点说就是：词语x和词语y放在一起，是不是人话。

Word2vec 正是来源于这个思想，但它的最终目的，不是要把 f 训练得多么完美，而是只关心模型训练完后的副产物——模型参数（这里特指神经网络的权重），并将这些参数，作为输入 x 的某种向量化的表示，这个向量便叫做——词向量（这里看不懂没关系，下一节我们详细剖析）。

我们来看个例子，如何用 Word2vec 寻找相似词：

对于一句话：『她们 夸 吴彦祖 帅 到 没朋友』，如果输入 x 是『吴彦祖』，那么 y 可以是『她们』、『夸』、『帅』、『没朋友』这些词
现有另一句话：『她们 夸 我 帅 到 没朋友』，如果输入 x 是『我』，那么不难发现，这里的上下文 y 跟上面一句话一样
从而 f(吴彦祖) = f(我) = y，所以大数据告诉我们：我 = 吴彦祖（完美的结论）

3.2. Skip-gram 和 CBOW 模型
上面我们提到了语言模型

如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』
而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』

3.2.1 Skip-gram 和 CBOW 的简单情形
我们先来看个最简单的例子。上面说到， y 是 x 的上下文，所以 y 只取上下文里一个词语的时候，语言模型就变成：
用当前词 x 预测它的下一个词 y

但如上面所说，一般的数学模型只接受数值型输入，这里的 x 该怎么表示呢？ 显然不能用 Word2vec，因为这是我们训练完模型的产物，现在我们想要的是 x 的一个原始输入形式。

答案是：one-hot encoder

所谓 one-hot encoder，其思想跟特征工程里处理类别变量的 one-hot 一样（参考我的前作『数据挖掘比赛通用框架』、『深挖One-hot和Dummy背后的玄机』）。本质上是用一个只含一个 1、其他都是 0 的向量来唯一表示词语。

我举个例子，假设全世界所有的词语总共有 V 个，这 V 个词语有自己的先后顺序，假设『吴彦祖』这个词是第1个词，『我』这个单词是第2个词，那么『吴彦祖』就可以表示为一个 V 维全零向量、把第1个位置的0变成1，而『我』同样表示为 V 维全零向量、把第2个位置的0变成1。这样，每个词语都可以找到属于自己的唯一表示。

OK，那我们接下来就可以看看 Skip-gram 的网络结构了，x 就是上面提到的 one-hot encoder 形式的输入，y 是在这 V 个词上输出的概率，我们希望跟真实的 y 的 one-hot encoder 一样。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154955627342704968.jpg'/>

首先说明一点：隐层的激活函数其实是线性的，相当于没做任何处理（这也是 Word2vec 简化之前语言模型的独到之处），我们要训练这个神经网络，用反向传播算法，本质上是链式求导，在此不展开说明了，

当模型训练完后，最后得到的其实是神经网络的权重，比如现在输入一个 x 的 one-hot encoder: [1,0,0,…,0]，对应刚说的那个词语『吴彦祖』，则在输入层到隐含层的权重里，只有对应 1 这个位置的权重被激活，这些权重的个数，跟隐含层节点数是一致的，从而这些权重组成一个向量 vx 来表示x，而因为每个词语的 one-hot encoder 里面 1 的位置是不同的，所以，这个向量 vx 就可以用来唯一表示 x。

注意：上面这段话说的就是 Word2vec 的精髓！！

此外，我们刚说了，输出 y 也是用 V 个节点表示的，对应V个词语，所以其实，我们把输出节点置成 [1,0,0,…,0]，它也能表示『吴彦祖』这个单词，但是激活的是隐含层到输出层的权重，这些权重的个数，跟隐含层一样，也可以组成一个向量 vy，跟上面提到的 vx 维度一样，并且可以看做是词语『吴彦祖』的另一种词向量。而这两种词向量 vx 和 vy，正是 Mikolov 在论文里所提到的，『输入向量』和『输出向量』，一般我们用『输入向量』。

需要提到一点的是，这个词向量的维度（与隐含层节点数一致）一般情况下要远远小于词语总数 V 的大小，所以 Word2vec 本质上是一种降维操作——把词语从 one-hot encoder 形式的表示降维到 Word2vec 形式的表示。

3.2.2. Skip-gram 更一般的情形
上面讨论的是最简单情形，即 y 只有一个词，当 y 有多个词时，网络结构如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154955634355639125.jpg'/>

可以看成是 单个x->单个y 模型的并联，cost function 是单个 cost function 的累加（取log之后）
如果你想深入探究这些模型是如何并联、 cost function 的形式怎样，不妨仔细阅读参考资料4. 在此我们不展开。

3.2.3 CBOW 更一般的情形
跟 Skip-gram 相似，只不过:
Skip-gram 是预测一个词的上下文，而 CBOW 是用上下文预测这个词
网络结构如下
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154955642339133552.jpg'/>

更 Skip-gram 的模型并联不同，这里是输入变成了多个单词，所以要对输入处理下（一般是求和然后平均），输出的 cost function 不变，在此依然不展开，建议你阅读参考资料4.

3.3. Word2vec 的训练trick
相信很多初次踩坑的同学，会跟我一样陷入 Mikolov 那篇论文（参考资料1.）里提到的 hierarchical softmax 和 negative sampling 里不能自拔，但其实，它们并不是 Word2vec 的精髓，只是它的训练技巧，但也不是它独有的训练技巧。 Hierarchical softmax 只是 softmax 的一种近似形式（详见参考资料7.），而 negative sampling 也是从其他方法借鉴而来。

为什么要用训练技巧呢？ 如我们刚提到的，Word2vec 本质上是一个语言模型，它的输出节点数是 V 个，对应了 V 个词语，本质上是一个多分类问题，但实际当中，词语的个数非常非常多，会给计算造成很大困难，所以需要用技巧来加速训练。

这里我总结了一下这两个 trick 的本质，有助于大家更好地理解，在此也不做过多展开，有兴趣的同学可以深入阅读参考资料1.~7.

hierarchical softmax
本质是把 N 分类问题变成 log(N)次二分类

negative sampling
本质是预测总体类别的一个子集

3.4. 扩展
很多时候，当我们面对林林总总的模型、方法时，我们总希望总结出一些本质的、共性的东西，以构建我们的知识体系，比如我在前作『分类和回归的本质』里，原创性地梳理了分类模型和回归模型的本质联系，比如在词嵌入领域，除了 Word2vec之外，还有基于共现矩阵分解的 GloVe 等等词嵌入方法。

深入进去我们会发现，神经网络形式表示的模型（如 Word2vec），跟共现矩阵分解模型（如 GloVe），有理论上的相通性，这里我推荐大家阅读参考资料5. ——来斯惟博士在它的博士论文附录部分，证明了 Skip-gram 模型和 GloVe 的 cost fucntion 本质上是一样的。是不是一个很有意思的结论？ 所以在实际应用当中，这两者的差别并不算很大，尤其在很多 high-level 的 NLP 任务（如句子表示、命名体识别、文档表示）当中，经常把词向量作为原始输入，而到了 high-level 层面，差别就更小了。

鉴于词语是 NLP 里最细粒度的表达，所以词向量的应用很广泛，既可以执行词语层面的任务，也可以作为很多模型的输入，执行 high-level 如句子、文档层面的任务，包括但不限于：

计算相似度
寻找相似词
信息检索

作为 SVM/LSTM 等模型的输入
中文分词
命名体识别

句子表示
情感分析

文档表示
文档主题判别

4. 实战
上面讲了这么多理论细节，其实在真正应用的时候，只需要调用 Gensim （一个 Python 第三方库）的接口就可以。但对理论的探究仍然有必要，你能更好地知道参数的意义、模型结果受哪些因素影响，以及举一反三地应用到其他问题当中，甚至更改源码以实现自己定制化的需求。

这里我们将使用 Gensim 和 NLTK 这两个库，来完成对生物领域的相似词挖掘，将涉及：

解读 Gensim 里 Word2vec 模型的参数含义
基于相应语料训练 Word2vec 模型，并评估结果
对模型结果调优
语料我已经放出来了，可以关注我的公众号『数据挖掘机养成记』，并回复 Sherlocked 获取语料，包含5000行生物医学领域相关文献的摘要(英文)


我将在下一篇文章里详细讲解实战步骤，敬请关注本人公众号。友情建议：请先自行安装 Gensim 和 NLTK 两个库，并建议使用 jupyter notebook 作为代码运行环境
## 9.什么是词嵌入word embedding?
Embedding在数学上表示一个maping, f: X -> Y， 也就是一个function，其中该函数是injective（就是我们所说的单射函数，每个Y只有唯一的X对应，反之亦然）和structure-preserving (结构保存，比如在X所属的空间上X1 < X2,那么映射后在Y所属空间上同理 Y1 < Y2)。那么对于word embedding，就是将单词word映射到另外一个空间，其中这个映射具有injective和structure-preserving的特点。

通俗的翻译可以认为是单词嵌入，就是把X所属空间的单词映射为到Y空间的多维向量，那么该多维向量相当于嵌入到Y所属空间中，一个萝卜一个坑。

word embedding，就是找到一个映射或者函数，生成在一个新的空间上的表达，该表达就是word representation。
推广开来，还有image embedding, video embedding, 都是一种将源数据映射到另外一个空间。

更多解释见：https://www.zhihu.com/question/32275069#130283448
## 10.了解NLP神经网络的发展历史么？
本文主要的内容如下：
2001 – 神经语言模型
2008 – 多任务学习
2013 – 词嵌入
2013 – NLP 神经网络
2014 – sequence-to-sequence 模型
2015 – 注意力机制
2015 – 基于记忆的网络
2018 – 预训练语言模型
其他的里程碑事件

传统算法里程碑事件
2001 – 神经语言模型
语言建模任务指的是给定前一个单词去预测文本中的下一个单词。它可能是比较简单的语言处理任务，具体的实际应用场景包括 智能键盘 、电子邮件回复建议（Kannan 等人， 2016）、拼写自动更正等。正如很多人所知，语言建模有着丰富的历史。其中比较经典的方法基于 n-grams ，并使用平滑处理不可见的 n-grams（Kneser & Ney, 1995）。

第一个神经语言模型是 Bengio 等人在 2001 年提出的前馈神经网络，如图 1 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728399726069727.png'/>图 1：前馈神经网络语言模型（Bengio 等，2001；2003）

这个模型将从表 C 中查找到的 n 个单词作为输入向量表征。这种向量被现在的学者们称做“词嵌入”。这些词嵌入级联后被输入到一个隐藏层中，该隐藏层的输出又被输入到 softmax 层。更多关于模型的信息，请看 这篇文章 。

最近，在语言建模技术方面，前馈神经网络被循环神经网络（RNNs；Mikolov 等人，2010）和长短时记忆网络（LSTMs；格雷夫斯，2013）所取代。尽管近年来提出了许多扩展经典 LSTM 的新语言模型（请参阅 本页 以获得概述），经典的 LSTM 仍然作为一个强大的基线存在着（Melis 等人， 2018）。甚至 Bengio 等人的经典前馈神经网络在某些情况下也可以与更复杂的模型一较高下，因为这些模型通常只会考虑距离较近的单词（Daniluk 等人， 2017）。因此，如何更好地理解这种语言模型所捕获的信息也是一个比较热门的研究领域（Kuncoro 等人， 2018；布莱文斯等人，2018 年）。

语言建模通常是应用 RNN 时的第一步，对于这一点大家已经形成了共识。许多人是通过 Andrej 的博客 文章第一次接触到语言建模的。语言建模是一种非监督学习形式，Yann LeCun 也将其称为预测性学习，并将其作为获得基础常识的先决条件（ 参见 NIPS 2016 年的幻灯片）。语言建模最引人关注的一点是，尽管它很简单，但却是本文后面讨论的许多技术发展的核心：
词嵌入：word2vec 的目标是简化语言建模。
sequence-to-sequence 模型：这种模型通过一次预测一个单词生成一个输出序列。
预训练语言模型：这些方法使用来自语言模型的表述进行迁移学习。

反过来讲，这意味着近年来 NLP 的许多重要进展都可以归结为某些形式的语言建模。为了“真正”理解自然语言，仅仅从文本的原始形式中学习是不够的。我们需要新的方法和模型。


2008 – 多任务学习
多任务学习是在多个任务上训练的模型之间共享参数的一种通用方法。在神经网络中，可以通过给不同层施以不同的权重，来很容易地实现多任务学习。多任务学习的概念最初由 Rich Caruana 在 1993 年提出，并被应用于道路跟踪和肺炎预测（Caruana, 1998）。直观地说，多任务学习鼓励模型学习对许多任务有用的表述。这对于学习一般的、低级的表述形式、集中模型的注意力或在训练数据有限的环境中特别有用。想要更全面地了解多任务学习，请看 这篇文章 。

在 2008 年，Collobert 和 Weston 将多任务学习首次应用于 NLP 的神经网络。在他们的模型中，查询表（或单词嵌入矩阵）在两个接受不同任务训练的模型之间共享，如下面的图 2 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728400924094710.png'/>图 2：单词嵌入矩阵的共享（Collobert & Weston, 2008 年；Collobert 等人，2011）

词嵌入的共享，使得模型能够在词嵌入矩阵中协作和共享一般的低级信息，而这些低级信息所占的参数量往往是模型中最大的一部分。2008 年，Collobert 和 Weston 共同撰写的论文对多任务学习之外的其他应用还产生了一定的影响。它率先提出了一些想法，如对文字嵌入进行预训练以及使用卷积神经网络（CNNs）来处理文本数据。它获得了 ICML 2018 年的经典论文奖 （ 参见 本文的经典论文奖演讲）。

多任务学习现在被广泛地用于 NLP 任务。充分利用现有的或“人造”的任务进行训练，可以更好的提高 NLP 效率。有关不同辅助任务的概述，请看 这篇文章 。虽然参数的共享通常是预定义的，但是在优化过程中也可以学习到不同的共享模式（Ruder 等人， 2017）。随着对多任务模型泛化能力的评估，多任务学习越来越重要，最近还提出了多任务学习的专用标准（Wang 等人， 2018；McCann 等人，2018 年）。


2013 – 词嵌入
用稀疏向量表示文本，即所谓的 词袋模型 在 NLP 有着悠久的历史。正如上文中介绍的，早在 2001 年就开始使用密集向量表示词或词嵌入。Mikolov 等人在 2013 年提出的创新技术是通过去除隐藏层，逼近目标，进而使这些单词嵌入的训练更加高效。虽然这些技术变更本质上很简单，但它们与高效的 word2vec 配合使用，便能使大规模的词嵌入训练成为可能。

Word2vec 有两种风格，如下面的图 3 所示：连续字袋（CBOW）和 skip-gram。不过他们的目标不同：一个是根据周围的单词预测中心单词，而另一个则相反。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728401749507875.png'/>图 3：连续字袋和 skip-gram 架构（Mikolov 等人， 2013a；2013 b）

虽然这些嵌入在概念上与使用前馈神经网络学习的嵌入在概念上没有区别，但是在一个非常大的语料库上训练之后，它们就能够捕获诸如性别、动词时态和国家 – 首都关系等单词之间的特定关系，如下图 4 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728402413484256.png'/>图 4：word2vec（Mikolov 等人， 2013a；2013 b）

这些词语间关系的获得及其背后的意义引发了人们对嵌入技术的兴趣——人们开始大量研究这些线性关系形成的原理（Arora 等人， 2016；Mimno & Thompson, 2017；Antoniak & Mimno，2018 年；Wendlandt 等人，2018 年）。然而，推动词嵌入技术成为当前 NLP 的主流技术的却不是这些原理，而是在初始化时使用预训练的嵌入，因为这样做可以提高下游任务的性能。

虽然 word2vec 捕捉到的词间关系很直观、质量高得甚至有些神奇，但后来的研究表明，word2vec 本身并没有什幺特别之处：词嵌入也可以通过矩阵分解来学习（Pennington 等人，2014）；通过适当的调优，SVD 和 LSA 等经典的矩阵分解方法也得到了类似的结果（Levy 等人， 2015）。

从那以后，人们开始投入大量的精力去探索词嵌入的各个方面（从 原文引用的惊人数量 可以看出）。通过 这篇文章 ，我们可以看出一些趋势和未来的方向。尽管有许多发展进步，但到现在为止，word2vec 仍然是大众的首选。对 Word2vec 的使用范围已经不限于单词级别了：基于局部上下文学习嵌入的简单目标——带负抽样的 skip-gram 已被用于学习句子表示（Mikolov & Le, 2014；Kiros 等人，2015）。Word2vec 甚至还在网络（Grover & Leskovec, 2016）和生物序列（Asgari & Mofrad, 2015）等其他应用场景中发挥了作用。

一个比较有研究价值的技术方向是将不同语言的词嵌入到同一个空间中，以实现（零样本）跨语言迁移。以一种完全不受监督的方式（至少对于类似的语言来说）学习数据以实现一个好的推测效果变得越来越有可能（Conneau 等人，2018 年；Artetxe 等人，2018 年；Søgaard 等人，2018）。这种学习方式可被应用于语言资源缺乏的无监督机器翻译系统中（Lample 等人，2018;；Artetxe 等人，2018）。查看（Ruder 等人， 2018）以获得概述。


2013 – NLP 神经网络
2013 年和 2014 年是 NLP 问题开始引入神经网络模型的时期。使用最广泛的三种主要的神经网络是：循环神经网络、卷积神经网络和递归神经网络。

循环神经网络（RNNs）循环神经网络是处理 NLP 中普遍存在的动态输入序列的一个最佳的技术方案。Vanilla RNNs（Elman, 1990）很快被经典的长 – 短期记忆网络（Hochreiter & Schmidhuber，1997）所取代，它被证明 对消失和爆炸梯度问题更有弹性 。在 2013 年之前，RNN 仍被认为很难训练； Ilya Sutskever 的博士论文 为改变这种现状提供了一个关键性的例子。下面的图 5 对 LSTM 单元进行了可视化显示。双向 LSTM（Graves 等人， 2013）通常用于处理左右两边的上下文。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728403223620244.png'/>图 5：LSTM 网络（来源：Chris Olah）

卷积神经网络（CNNs） 卷积神经网络本来是广泛应用于计算机视觉领域的技术，现在也开始应用于语言（Kalchbrenner 等人， 2014；Kim 等人，2014）。文本的卷积神经网络只在两个维度上工作，其中滤波器（卷积核）只需要沿着时间维度移动。下面的图 6 显示了 NLP 中使用的典型 CNN。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728404093234745.png'/>图 6：文本卷积神经网络（Kim, 2014）

卷积神经网络的一个优点是它们比 RNN 更可并行化，因为其在每个时间步长的状态只依赖于本地上下文（通过卷积运算），而不是像 RNN 那样依赖过去所有的状态。使用膨胀卷积，可以扩大 CNN 的感受野，使网络有能力捕获更长的上下文（Kalchbrenner 等人， 2016）。CNN 和 LSTM 可以组合和叠加（Wang 等人， 2016），卷积也可以用来加速 LSTM（Bradbury 等人， 2017）。

递归神经网络RNN 和 CNN 都将语言视为一个序列。然而，从语言学的角度来看，语言本质上是 层次化的 ：单词被组合成高阶短语和从句，这些短语和从句本身可以根据一组生产规则递归地组合。将句子视为树而不是序列的语言学启发思想产生了递归神经网络（Socher 等人， 2013），如下图 7 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728404734327836.png'/>图 7：递归神经网络（Socher 等人， 2013）

递归神经网络从下到上构建序列的表示，这一点不同于从左到右或从右到左处理句子的 RNN。在树的每个节点上，通过组合子节点的结果来计算新的结果。由于树也可以被视为在 RNN 上强加不同的处理顺序，所以 LSTM 自然地也被扩展到树上（Tai 等人， 2015）。

RNN 和 LSTM 可以扩展到使用层次结构。单词嵌入不仅可以在本地学习，还可以在语法语境中学习（Levy & Goldberg, 2014）；语言模型可以基于句法堆栈生成单词（Dyer 等人， 2016）；图卷积神经网络可以基于树结构运行（Bastings 等人， 2017）。


2014 – sequence-to-sequence 模型
2014 年，Sutskever 等人提出了 sequence-to-sequence 模型。这是一个使用神经网络将一个序列映射到另一个序列的通用框架。在该框架中，编码器神经网络逐符号处理一个句子，并将其压缩为一个向量表示；然后，一个解码器神经网络根据编码器状态逐符号输出预测值，并将之前预测的符号作为每一步的输入，如下图 8 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728405469075559.png'/>图 8：sequence-to-sequence 模型（Sutskever 等人， 2014）

机器翻译是对这个框架比较成功的应用。2016 年，谷歌宣布将开始用神经 MT 模型取代基于单片短语的 MT 模型（Wu 等人， 2016）。 根据 Jeff Dean 的说法 ，这意味着用 500 行神经网络模型替换 50 万行基于短语的 MT 代码。

由于其灵活性，这个框架现在是自然语言生成任务的首选框架，其中不同的模型承担了编码器和解码器的角色。重要的是，解码器模型不仅可以解码一个序列，而且可以解码任意表征。例如，可以基于图像生成标题（Vinyals 等人， 2015）（如下图 9 所示）、基于表生成文本（Lebret 等人， 2016）和基于应用程序中源代码更改描述（Loyola 等人， 2017）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728406157980794.png'/>图 9：基于图像生成标题（Vinyals 等人，2015）

sequence-to-sequence 学习甚至可以应用于 NLP 中输出具有特定结构的结构化预测任务。为了简单起见，输出被线性化，如下面的图 10 所示，用于进行选区解析。神经网络已经证明了在有足够数量的训练数据进行选区分析（Vinyals 等人，2015）和命名实体识别（Gillick 等人， 2016）的情况下，直接学习可以产生这种线性化输出的能力。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728407015071412.png'/>图 10：线性化选区解析树（Vinyals 等人，2015）

序列和解码器的编码器通常基于 RNN，但可以使用其他模型类型。新的体系结构主要来源于 MT 的贡献，它是 sequence-to-sequence 模型体系结构的主要开发者。最新的模型有 deep LSTMs（Wu 等人，2016；tional encoders 、Kalchbrenner 等人，2016；Gehring 等人， Transformer 、Vaswani 等人，2017）和 LSTM 与 Transformer 的结合体（Chen 等人， 2018）。


2015 – 注意力机制
注意力机制（Bahdanau 等人， 2015）是神经网络机器翻译（NMT）的核心创新之一，也是使 NMT 模型胜过经典的基于短语的 MT 系统的关键思想。sequence-to-sequence 模型的主要瓶颈是需要将源序列的全部内容压缩为一个固定大小的向量。注意力机制通过允许解码器回头查看源序列隐藏状态来缓解这一问题，然后将其加权平均作为额外输入提供给解码器，如下面的图 11 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728407872231977.png'/>
图 11：Attention（Bahdanau 等人， 2015）

注意力机制有很多不同的形式（Luong 等人，2015）。 这里  有一个简短的概述。注意力机制广泛适用于任何需要根据输入的特定部分做出决策的任务，并且效果不错。它已被应用于一致性解析（Vinyals 等人，2015）、阅读理解（Hermann 等人，2015）和一次性学习（Vinyals 等人，2016）等诸多领域。输入甚至不需要是一个序列，即可以包含其他表示，如图像字幕（Xu 等人， 2015），如下图 12 所示。注意力机制的一个额外的功能是，它提供了一种少见的功能，我们可以通过检查输入的哪些部分与基于注意力权重的特定输出相关来了解模型的内部工作方式。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728409185148843.png'/>图 12：图像字幕模型中的视觉注意力，预测模型在生成“飞盘”时所关注的内容。（Xu 等人， 2015）

注意力机制也不仅仅局限于观察输入序列；可以使用 self-attention 查看句子或文档中的周围单词，以获得更佳的上下文敏感的词表示。Transformer 架构的核心是多层次的自我关注（Vaswani 等人， 2017），这是目前 NMT 最先进的模型。


2015 – 基于记忆的网络
注意力机制可以看作是模糊记忆的一种形式。记忆由模型的隐藏状态组成，模型选择从记忆中检索内容。想要更详细地了解注意力及其与记忆的联系，请看 这篇文章 。研究者们提出了许多具有更明确记忆的模型。这些模型有不同的变体，如神经图灵机（Graves 等 ，2014）、记忆网络（Weston 等 ，2015）和端到端记忆网络（Sukhbaatar 等，2015）、动态记忆网络（Kumar 等 ，2015）、神经微分计算机（Graves 等，2016）和循环实体网络（Henaff 等，2017）。

记忆的访问通常基于与当前状态的相似度，类似于注意力，通常可以写入和读取。模型在如何实现和利用内存方面有所不同。例如，端到端记忆网络多次处理输入，并更新记忆以实现多个推理步骤。神经图灵机也有一个基于位置的寻址，这允许他们学习简单的计算机程序，如排序。基于记忆的模型通常应用于一些特定任务中，如语言建模和阅读理解。在这些任务中，长时间保存信息应该很有用。记忆的概念是非常通用的：知识库或表可以充当记忆，而记忆也可以根据整个输入或它的特定部分填充。


2018 – 预训练语言模
预训练的词嵌入与上下文无关，仅用于初始化模型中的第一层。最近几个月，一系列监督型任务被用于神经网络的预训练（Conneau 等人，2017；McCann 等人，2017；Subramanian 等人，2018 年）。相反，语言模型只需要无标签的文本；因此，训练可以扩展到数十亿个令牌、新域和新语言。预训练语言模型于 2015 年被首次提出（Dai & Le, 2015）；直到最近，它们才被证明在各种任务中效果还是不错的。语言模型嵌入可以作为目标模型中的特征（Peters 等人，2018 年），或者使用语言模型对目标任务数据进行微调（Ramachandran 等人，2017 年；霍华德 & 鲁德出版社，2018 年）。添加语言模型嵌入可以在许多不同的任务中提供比最先进的技术更大的改进，如下面的图 13 所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155728409864861426.png'/>图 13：嵌入到最先进的语言模型中的改进（Peters 等人，2018）

预训练的语言模型已经被证明可以用更少的数据进行学习。由于语言模型只需要无标记的数据，因此对于标记数据稀缺的低资源语言尤其有用。有关预训练语言模型潜力的更多信息， 请参阅本文 。

其他里程碑事件
其他一些技术发展没有上面提到的那样流行，但仍然有广泛的影响。

基于字符的表示在字符上使用 CNN 或 LSTM 以获得基于字符的词表示的做法现在相当普遍，特别是对于形态信息重要或有许多未知单词的丰富的语言和任务，效果更加明显。据我所知，序列标签使用基于字符的表示（Lample 等人，2016；普兰克等人，2016），可以减轻在计算成本增加的情况下必须处理固定词汇表的需要，并支持完全基于字符的 NMT（Ling 等人， 2016；Lee 等人，2017）。

对抗学习对抗学习方法已经在 ML 领域掀起了风暴，在 NLP 中也有不同形式的应用。对抗性的例子越来越被广泛使用，它不仅是作为一种工具来探究模型和理解它们的失败案例，而且也使自身更加鲁棒（Jia & Liang， 2017）。（虚拟）对抗性训练，即最坏情况扰动（Miyato 等人，2017）和领域对抗性损失（Ganin 等人， 2016；Kim 等人，2017），同样可以使模型更加鲁棒。生成对抗网络（GANs）对于自然语言生成还不是很有效（Semeniuta 等人， 2018），但在匹配分布时很有用（Conneau 等人， 2018）。

强化学习强化学习已被证明对具有时间依赖性的任务有效，例如在训练期间选择数据（Fang 等人， 2017；Wu 等人， 2018）和建模对话（Liu 等人， 2018）。RL 对于直接优化不可微的末端度量（如 ROUGE 或 BLEU）也有效，反而在汇总中优化替代损失（如交叉熵）（Paulus 等人， 2018；Celikyilmaz 等人，2018）和机器翻译场景效果就不明显了（Ranzato 等人，2016）。类似地，逆向强化学习在过于复杂而无法指定数据的情况下也很有用，比看图说话任务（Wang 等人， 2018）。

非神经网络算法的里程碑事件
在 1998 年和接下来的几年里， FrameNet 项目诞生了（Baker 等人， 1998），这指导了 语义角色 标注 的任务。这是一种浅语义解析的形式，至今仍在积极研究开发中。在本世纪初，与自然语言学习会议（CoNLL）一起组织的共享任务促进了核心 NLP 任务的研究，如组块（Tjong Kim Sang 等人， 2000）、命名实体识别（Tjong Kim Sang 等人， 2003）和依赖解析（Buchholz 等人， 2006）等。许多 CoNLL 共享任务数据集现在仍然被用作评估的标准。

2001 年，条件随机场（CRF；Lafferty 等人， 2001）成为了最具影响力的序列标注方法类别之一，获得了 ICML 2011 的最佳论文奖 。CRF 层是当前最先进的序列标注问题模型的核心部分，这些模型具有标签间的相互依赖性，如命名实体识别（Lample 等，2016）。

2002 年，双语互译质量评估辅助工具（BLEU；Papineni 等人，2002）给出了双语互译质量度量标准，这使得 MT 系统得以扩展。其现在仍然是 MT 评估的标准度量标准。同年，结构感知机（Collins，2002）问世，为结构化感知工作奠定了基础。在同一次会议上，情感分析也成了最受欢迎和广泛研究的 NLP 任务之一（Pang 等人， 2002）。这三篇论文都获得了 2018 年 NAACL 最佳论文奖 。

2003 年引入了潜在狄利克雷分配（LDA；Blei 等人，2003），这是机器学习中应用最广泛的技术之一，至今仍是主题建模的标准方法。在 2004 年，有学者提出了比 SVM 更适合于捕获结构化数据中的相关性的新最大边缘模型（Taskar 等人， 2004a；2004b）。

2006 年，OntoNotes（Hovy 等人， 2006）介绍了一个具有多个注释和高注释协议的大型多语言语料库。OntoNotes 已被用于训练和评估各种任务，如依赖解析和引用解析。Milne 和 Witten（2008）在 2008 年介绍了利用维基百科丰富机器学习方法的方案。到目前为止，Wikipedia 是训练 ML 方法最有用的资源之一，无论是用于实体链接和消除歧义、语言建模、知识库还是其他各种任务。

2009 年，提出了远程监督的概念（Mintz 等人， 2009）。远程监督利用启发式或现有知识库中的信息生成带有噪声的模式，可用于从大型语料库中自动提取示例。远程监督现已被广泛应用，并且已经是关系提取、信息提取、情感分析等领域的常用技术。

英文原文： A Review of the Neural History of Natural Language Processing
## 11.判别式（discriminative）模型和生成式(generative)模型的核心区别是什么？
本题解析来源：https://www.zhihu.com/question/35866596/answer/236886066

在监督学习下，模型可以分为判别式模型与生成式模型。

根据经验，A批模型（神经网络模型、SVM、perceptron、LR、DT……）与B批模型（NB、LDA……），有啥区别不？（这个问题需要一些模型使用经验）应该是这样的：
1. A批模型是这么工作的，他们直接将数据的Y（或者label），根据所提供的features，学习，最后画出了一个明显或者比较明显的边界（具体怎么做到的？通过复杂的函数映射，或者决策叠加等等mechanism），这一点线性LR、线性SVM应该很明显吧。 

2. B批模型是这么工作的，他们先从训练样本数据中，将所有的数据的分布情况摸透，然后最终确定一个分布，来作为我的所有的输入数据的分布，并且他是一个联合分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035990526332970.svg'/>   (注意<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035991554313137.svg'/>包含所有的特征<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035992531381823.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035993717483536.svg'/>包含所有的label)。然后我来了新的样本数据（inference），好，通过学习来的模型的联合分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035995664047739.svg'/>，再结合新样本给的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157035997142504104.svg'/>，通过条件概率就能出来<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036000361353863.svg'/>：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036001491062177.svg'/>

1. 判别式模型
那么A批模型对应了判别式模型。根据上面的两句话的区别，可以知道判别模型的特征了，所以有句话说：判别模型是直接对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036003810017449.svg'/>建模，就是说，直接根据X特征来对Y建模训练。

具体地，我的训练过程是确定构件  模型里面“复杂映射关系”中的参数，完了再去inference一批新的sample。

所以判别式模型的特征总结如下：
1) 对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036005749731553.svg'/>建模
2) 对所有的样本只构建一个模型，确认总体判别边界
3) 观测到输入什么特征，就预测最可能的label
4) 另外，判别式的优点是：对数据量要求没生成式的严格，速度也会快，小数据量下准确率也会好些。

2. 生成式模型
同样，B批模型对应了生成式模型。并且需要注意的是，在模型训练中，我学习到的是X与Y的联合模型<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036009086309819.svg'/>，也就是说，我在训练阶段是只对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036010615518794.svg'/>建模，我需要确定维护这个联合概率分布的所有的信息参数。完了之后在inference再对新的sample计算<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036015019525080.svg'/>，导出<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036016480767790.svg'/> ,但这已经不属于建模阶段了。

结合NB过一遍生成式模型的工作流程。学习阶段，建模：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415703601835560797.svg'/>（当然，NB具体流程去隔壁参考）,然后<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036022766630471.svg'/>。
另外，LDA也是这样，只是他更过分，需要确定很多个概率分布，而且建模抽样都蛮复杂的。

所以生成式总结下有如下特点：
① 对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036024830103287.svg'/>建模
② 这里我们主要讲分类问题，所以是要对每个label（<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036026167421345.svg'/>）都需要建模，最终选择最优概率的label为结果，所以没有什么判别边界。（对于序列标注问题，那只需要构件一个model）
③ 中间生成联合分布，并可生成采样数据。
④ 生成式模型的优点在于，所包含的信息非常齐全，我称之为“上帝信息”，所以不仅可以用来输入label，还可以干其他的事情。生成式模型关注结果是如何产生的。但是生成式模型需要非常充足的数据量以保证采样到了数据本来的面目，所以速度相比之下，慢。

这一点明白后，后面讲到的HMM与CRF的区别也会非常清晰。
最后identity the picture below：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157036028335909703.jpg'/>
## 12.如何通俗理解隐马尔可夫模型HMM？
作者：Yang Eninala​，杜克大学 生物化学博士
链接：https://www.zhihu.com/question/20962240

隐马尔可夫（HMM）好讲，简单易懂不好讲。我认为 @者也的回答没什么错误，不过我想说个更通俗易懂的例子。我希望我的读者不是专家，而是对这个问题感兴趣的入门者，所以我会多阐述数学思想，少写公式。霍金曾经说过，你多写一个公式，就会少一半的读者。所以时间简史这本关于物理的书和麦当娜关于性的书卖的一样好。我会效仿这一做法，写最通俗易懂的答案。

还是用最经典的例子，掷骰子。

假设我手里有三个不同的骰子。
第一个骰子是我们平常见的骰子（称这个骰子为D6），6个面，每个面（1，2，3，4，5，6）出现的概率是1/6。
第二个骰子是个四面体（称这个骰子为D4），每个面（1，2，3，4）出现的概率是1/4。
第三个骰子有八个面（称这个骰子为D8），每个面（1，2，3，4，5，6，7，8）出现的概率是1/8。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415496212405785727.jpg'/>

假设我们开始掷骰子，我们先从三个骰子里挑一个，挑到每一个骰子的概率都是1/3。然后我们掷骰子，得到一个数字，1，2，3，4，5，6，7，8中的一个。不停的重复上述过程，我们会得到一串数字，每个数字都是1，2，3，4，5，6，7，8中的一个。例如我们可能得到这么一串数字（掷骰子10次）：1 6 3 5 2 7 3 5 2 4

这串数字叫做可见状态链。但是在隐马尔可夫模型中，我们不仅仅有这么一串可见状态链，还有一串隐含状态链。在这个例子里，这串隐含状态链就是你用的骰子的序列。比如，隐含状态链有可能是：D6 D8 D8 D6 D4 D8 D6 D6 D4 D8

一般来说，HMM中说到的马尔可夫链其实是指隐含状态链，因为隐含状态（骰子）之间存在转换概率（transition probability）。在我们这个例子里，D6的下一个状态是D4，D6，D8的概率都是1/3。D4，D8的下一个状态是D4，D6，D8的转换概率也都一样是1/3。这样设定是为了最开始容易说清楚，但是我们其实是可以随意设定转换概率的。比如，我们可以这样定义，D6后面不能接D4，D6后面是D6的概率是0.9，是D8的概率是0.1。这样就是一个新的HMM。

同样的，尽管可见状态之间没有转换概率，但是隐含状态和可见状态之间有一个概率叫做输出概率（emission probability）。就我们的例子来说，六面骰（D6）产生1的输出概率是1/6。产生2，3，4，5，6的概率也都是1/6。我们同样可以对输出概率进行其他定义。比如，我有一个被赌场动过手脚的六面骰子，掷出来是1的概率更大，是1/2，掷出来是2，3，4，5，6的概率是1/10。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415496317172103652.jpeg'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963177265294242.jpeg'/>

其实对于HMM来说，如果提前知道所有隐含状态之间的转换概率和所有隐含状态到所有可见状态之间的输出概率，做模拟是相当容易的。但是应用HMM模型时候呢，往往是缺失了一部分信息的，有时候你知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；有时候你只是看到了很多次掷骰子的结果，剩下的什么都不知道。如果应用算法去估计这些缺失的信息，就成了一个很重要的问题。这些算法我会在下面详细讲。

回到正题，和HMM模型相关的算法主要分为三类，分别解决三种问题：
1）知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道每次掷出来的都是哪种骰子（隐含状态链）。这个问题呢，在语音识别领域呢，叫做解码问题。这个问题其实有两种解法，会给出两个不同的答案。每个答案都对，只不过这些答案的意义不一样。
第一种解法求最大似然状态路径，说通俗点呢，就是我求一串骰子序列，这串骰子序列产生观测结果的概率最大。
第二种解法呢，就不是求一组骰子序列了，而是求每次掷出的骰子分别是某种骰子的概率。比如说我看到结果后，我可以求得第一次掷骰子是D4的概率是0.5，D6的概率是0.3，D8的概率是0.2。
第一种解法我会在下面说到，但是第二种解法我就不写在这里了，如果大家有兴趣，我们另开一个问题继续写吧。

2）还是知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道掷出这个结果的概率。看似这个问题意义不大，因为你掷出来的结果很多时候都对应了一个比较大的概率。问这个问题的目的呢，其实是检测观察到的结果和已知的模型是否吻合。如果很多次结果都对应了比较小的概率，那么就说明我们已知的模型很有可能是错的，有人偷偷把我们的骰子給换了。

3）知道骰子有几种（隐含状态数量），不知道每种骰子是什么（转换概率），观测到很多次掷骰子的结果（可见状态链），我想反推出每种骰子是什么（转换概率）。这个问题很重要，因为这是最常见的情况。很多时候我们只有可见结果，不知道HMM模型里的参数，我们需要从可见结果估计出这些参数，这是建模的一个必要步骤。

问题阐述完了，下面就开始说解法。（0号问题在上面没有提，只是作为解决上述问题的一个辅助）

0.一个简单问题
其实这个问题实用价值不高。由于对下面较难的问题有帮助，所以先在这里提一下。

知道骰子有几种，每种骰子是什么，每次掷的都是什么骰子，根据掷骰子掷出的结果，求产生这个结果的概率。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963191446207141.jpeg'/>

解法无非就是概率相乘：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963195175430992.png'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963195950738756.png'/>

1.看见不可见的，破解骰子序列
这里我说的是第一种解法，解最大似然路径问题。

举例来说，我知道我有三个骰子，六面骰，四面骰，八面骰。我也知道我掷了十次的结果（1 6 3 5 2 7 3 5 2 4），我不知道每次用了那种骰子，我想知道最有可能的骰子序列。

其实最简单而暴力的方法就是穷举所有可能的骰子序列，然后依照第零个问题的解法把每个序列对应的概率算出来。然后我们从里面把对应最大概率的序列挑出来就行了。如果马尔可夫链不长，当然可行。如果长的话，穷举的数量太大，就很难完成了。

另外一种很有名的算法叫做Viterbi algorithm. 要理解这个算法，我们先看几个简单的列子。
首先，如果我们只掷一次骰子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963210314993492.jpeg'/>

看到结果为1.对应的最大概率骰子序列就是D4，因为D4产生1的概率是1/4，高于1/6和1/8.
把这个情况拓展，我们掷两次骰子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963215041149237.jpeg'/>

结果为1，6.这时问题变得复杂起来，我们要计算三个值，分别是第二个骰子是D6，D4，D8的最大概率。显然，要取到最大概率，第一个骰子必须为D4。这时，第二个骰子取到D6的最大概率是
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963245064702512.png'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963247286074053.png'/>

同上，我们可以计算第三个骰子是D6或D8时的最大概率。我们发现，第三个骰子取到D4的概率最大。而使这个概率最大时，第二个骰子为D6，第一个骰子为D4。所以最大概率骰子序列就是D4 D6 D4。

写到这里，大家应该看出点规律了。既然掷骰子一二三次可以算，掷多少次都可以以此类推。我们发现，我们要求最大概率骰子序列时要做这么几件事情。首先，不管序列多长，要从序列长度为1算起，算序列长度为1时取到每个骰子的最大概率。然后，逐渐增加长度，每增加一次长度，重新算一遍在这个长度下最后一个位置取到每个骰子的最大概率。因为上一个长度下的取到每个骰子的最大概率都算过了，重新计算的话其实不难。当我们算到最后一位时，就知道最后一位是哪个骰子的概率最大了。然后，我们要把对应这个最大概率的序列从后往前推出来。

2.谁动了我的骰子？
比如说你怀疑自己的六面骰被赌场动过手脚了，有可能被换成另一种六面骰，这种六面骰掷出来是1的概率更大，是1/2，掷出来是2，3，4，5，6的概率是1/10。你怎么办么？答案很简单，算一算正常的三个骰子掷出一段序列的概率，再算一算不正常的六面骰和另外两个正常骰子掷出这段序列的概率。如果前者比后者小，你就要小心了。

比如说掷骰子的结果是：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415496325727916489.jpeg'/>

要算用正常的三个骰子掷出这个结果的概率，其实就是将所有可能情况的概率进行加和计算。同样，简单而暴力的方法就是把穷举所有的骰子序列，还是计算每个骰子序列对应的概率，但是这回，我们不挑最大值了，而是把所有算出来的概率相加，得到的总概率就是我们要求的结果。这个方法依然不能应用于太长的骰子序列（马尔可夫链）。

我们会应用一个和前一个问题类似的解法，只不过前一个问题关心的是概率最大值，这个问题关心的是概率之和。解决这个问题的算法叫做前向算法（forward algorithm）。

首先，如果我们只掷一次骰子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963266094334650.jpeg'/>

看到结果为1.产生这个结果的总概率可以按照如下计算，总概率为0.18：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963269969259385.jpeg'/>

把这个情况拓展，我们掷两次骰子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963278953070319.jpeg'/>

看到结果为1，6.产生这个结果的总概率可以按照如下计算，总概率为0.05：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415496328485517326.jpeg'/>

继续拓展，我们掷三次骰子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963294735647584.jpeg'/>

看到结果为1，6，3.产生这个结果的总概率可以按照如下计算，总概率为0.03：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154963298847740924.jpeg'/>

同样的，我们一步一步的算，有多长算多长，再长的马尔可夫链总能算出来的。用同样的方法，也可以算出不正常的六面骰和另外两个正常骰子掷出这段序列的概率，然后我们比较一下这两个概率大小，就能知道你的骰子是不是被人换了。

3.掷一串骰子出来，让我猜猜你是谁
这个算法就是著名的EM算法，详见：https://www.julyedu.com/question/big/kp_id/23/ques_id/1007

上述算法呢，其实用到了递归，逆向推导，循环这些方法，我只不过用很直白的语言写出来了。如果你们去看专业书籍呢，会发现更加严谨和专业的描述。毕竟，我只做了会其意，要知其形，还是要看书的。
## 13.预训练方法 BERT和OpenAI GPT有什么区别？
1.GPT在BooksCorpus(800M单词)训练；BERT在BooksCorpus(800M单词)和维基百科(2,500M单词)训练
2.GPT使用一种句子分隔符([SEP])和分类符词块([CLS])，它们仅在微调时引入；BERT在预训练期间学习[SEP]，[CLS]和句子A/B嵌入
3.GPT用一个批量32,000单词训练1M步；BERT用一个批量128,000单词训练1M步
4.GPT对所有微调实验使用的5e-5相同学习率；BERT选择特定于任务的微调学习率，在开发集表现最佳
## 14.了解词嵌入的来龙去脉么？
本题解析来源：@Scofield_Phil，https://blog.csdn.net/Scotfield_msn/article/details/69075227 

Indexing:
〇、序
一、DeepNLP的核心关键：语言表示（Representation）
二、NLP词的表示方法类型
   1、词的独热表示one-hot representation
   2、词的分布式表示distributed representation
三、NLP语言模型

四、词的分布式表示
    1. 基于矩阵的分布表示
    2. 基于聚类的分布表示
    3. 基于神经网络的分布表示，词嵌入（ word embedding）
五、词嵌入（ word embedding）
    1、概念
    2、理解
六、神经网络语言模型与word2vec
    1、神经网络语言模型
    2.word2vec与CBOW、Skip-gram
    3.个人对word embedding的理解


〇、序
之前一段时间，在结合深度学习做NLP的时候一直有思考一些问题，其中有一个问题算是最核心一个：究竟深度网络是怎么做到让各种NLP任务解决地如何完美呢？到底我的数据在NN中发什么了什么呢？

并且，不少的terms like： 词向量、word embedding、分布式表示、word2vec、glove等等，这一锅粥的名词术语分别代表什么，他们具体的关系是什么，他们是否处于平级关系？

出于对知识结构追求完整梳理的强迫症的老毛病，于是不停地查资料、思考、keep revolving……

然后就感觉有一点小进展了。想到，不如将个人对其的理解，无论对错，先拿出来跟peer分享下，或许能交换出更有意义的东西呢？

整篇文章的构架是按照属于概念在逻辑上的先后大小顺序，一层一层一级一级地往下剖析、比较、说明。

另外说明下，here整篇文字内容相对是比较入门，甚至有的点可能描述的不太客观正确，限于当前的认知水平……还请您海涵，希望您在评论中指正！

一、DeepNLP的核心关键：语言表示（Representation）
最近有一个新名词：Deep Learning + NLP =  DeepNLP。当常规的机器学习Machine Learning升级发展到了一定的阶段后，慢慢的被后起的深度学习Deep Learning夺势而去，并如火如荼地引领了一波新高潮，因为Deep Learning有machine learning过而不及之处！那当Deep Learning进入NLP领域，自然是要横扫ACL一批paper才是。事实也是这样的。

先提下数据特征表示问题。数据表示是机器学习的核心问题，在过去的Machine Learning阶段，大量兴起特征工程，人工设计大量的特征解决数据的有效表示问题。而到了Deep Learning，想都别想，end-2-end，一步到位，hyper-parameter自动帮你选择寻找关键的特征参数。

那么，Deep Learning如何能在NLP中发挥出应有的real power呢？很明显，先不提如何设计出很强势的网络结构，不提如何在NLP中引入基于NN的解决例如情感分析、实体识别、机器翻译、文本生成这些高级任务，咱们首先得把语言表示这一关过了——如何让语言表示成为NN能够处理的数据类型。

我们看看图像和语音是怎么表示数据的：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211539066450273.jpg'/>

在语音中，用音频频谱序列向量所构成的matrix作为前端输入喂给NN进行处理，good；在图像中，用图片的像素构成的matrix展平成vector后组成的vector序列喂给NN进行处理，good；那在自然语言处理中呢？噢你可能知道或者不知道，将每一个词用一个向量表示出来！想法是挺简单的，对，事实上就是这么简单，然而真有这么简单吗？可能没这么简单。

有人提到，图像、语音属于比较自然地低级数据表示形式，在图像和语音领域，最基本的数据是信号数据，我们可以通过一些距离度量，判断信号是否相似，在判断两幅图片是否相似时，只需通过观察图片本身就能给出回答。而语言作为人类在进化了几百万年所产生的一种高层的抽象的思维信息表达的工具，其具有高度抽象的特征，文本是符号数据，两个词只要字面不同，就难以刻画它们之间的联系，即使是“麦克风”和“话筒”这样的同义词，从字面上也难以看出这两者意思相同（语义鸿沟现象），可能并不是简单地一加一那么简单就能表示出来，而判断两个词是否相似时，还需要更多的背景知识才能做出回答。

那么据上是不是可以自信地下一个结论呢：如何有效地表示出语言句子是决定NN能发挥出强大拟合计算能力的关键前提！


二、NLP词的表示方法类型
接下来将按照上面的思路，引出各种词的表示方法。按照现今目前的发展，词的表示分为独热表示one-hot、分布式表示distributed。

1、词的独热表示one-hot representation
NLP 中最直观，也是到目前为止最常用的词表示方法是 One-hot Representation，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。关于one-hot编码的资料很多，街货，这里简单举个栗子说明：

                “话筒”表示为 [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ...]
                “麦克”表示为 [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ...]

每个词都是茫茫 0 海中的一个 1。这种 One-hot Representation 如果采用稀疏方式存储，会是非常的简洁：也就是给每个词分配一个数字 ID。比如刚才的例子中，话筒记为 3，麦克记为 8（假设从 0 开始记）。如果要编程实现的话，用 Hash 表给每个词分配一个编号就可以了。这么简洁的表示方法配合上最大熵、SVM、CRF 等等算法已经很好地完成了 NLP 领域的各种主流任务。

现在我们分析他的不当处。1、向量的维度会随着句子的词的数量类型增大而增大；2、任意两个词之间都是孤立的，根本无法表示出在语义层面上词语词之间的相关信息，而这一点是致命的。

2、词的分布式表示distributed representation
传统的独热表示（ one-hot representation）仅仅将词符号化，不包含任何语义信息。如何将语义融入到词表示中？Harris 在 1954 年提出的分布假说（ distributional hypothesis）为这一设想提供了理论基础：上下文相似的词，其语义也相似。Firth 在 1957 年对分布假说进行了进一步阐述和明确：词的语义由其上下文决定（ a word is characterized by thecompany it keeps）。

到目前为止，基于分布假说的词表示方法，根据建模的不同，主要可以分为三类：基于矩阵的分布表示、基于聚类的分布表示和基于神经网络的分布表示。尽管这些不同的分布表示方法使用了不同的技术手段获取词表示，但由于这些方法均基于分布假说，它们的核心思想也都由两部分组成：一、选择一种方式描述上下文；二、选择一种模型刻画某个词（下文称“目标词”）与其上下文之间的关系。


三、NLP语言模型
在详细介绍词的分布式表示之前，需要将NLP中的一个关键概念描述清楚：语言模型。语言模型包括文法语言模型和统计语言模型。一般我们指的是统计语言模型。之所以要将语言模型摆在词表示方法之前，是因为后面的表示方法马上要用到这一概念。

统计语言模型： 统计语言模型把语言（词的序列）看作一个随机事件，并赋予相应的概率来描述其属于某种语言集合的可能性。给定一个词汇集合 V，对于一个由 V 中的词构成的序列S = ⟨w1, · · · , wT ⟩ ∈ Vn，统计语言模型赋予这个序列一个概率P(S)，来衡量S 符合自然语言的语法和语义规则的置信度。

用一句简单的话说，就语言模型就是计算一个句子的概率大小的这种模型。有什么意义呢？一个句子的打分概率越高，越说明他是更合乎人说出来的自然句子。

就是这么简单。常见的统计语言模型有N元文法模型（N-gram Model），最常见的是unigram model、bigram model、trigram model等等。形式化讲，统计语言模型的作用是为一个长度为 m 的字符串确定一个概率分布 P(w1; w2; :::; wm)，表示其存在的可能性，其中 w1 到 wm 依次表示这段文本中的各个词。一般在实际求解过程中，通常采用下式计算其概率值：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211544519075211.png'/>
 
同时通过这些方法均也可以保留住一定的词序信息，这样就能把一个词的上下文信息capture住。
具体的语言模型详情属于街货，详细请自行搜索。


四、词的分布式表示
1. 基于矩阵的分布表示
基于矩阵的分布表示通常又称为分布语义模型，在这种表示下，矩阵中的一行，就成为了对应词的表示，这种表示描述了该词的上下文的分布。由于分布假说认为上下文相似的词，其语义也相似，因此在这种表示下，两个词的语义相似度可以直接转化为两个向量的空间距离。

常见到的Global Vector 模型（ GloVe模型）是一种对“词-词”矩阵进行分解从而得到词表示的方法，属于基于矩阵的分布表示。

2. 基于聚类的分布表示
基于聚类的分布表示我也还不是太清楚，所以就不做具体描述。

3. 基于神经网络的分布表示，词嵌入（ word embedding）
基于神经网络的分布表示一般称为词向量、词嵌入（ word embedding）或分布式表示（ distributed representation）。这正是我们的主角today。

神经网络词向量表示技术通过神经网络技术对上下文，以及上下文与目标词之间的关系进行建模。由于神经网络较为灵活，这类方法的最大优势在于可以表示复杂的上下文。在前面基于矩阵的分布表示方法中，最常用的上下文是词。如果使用包含词序信息的 n-gram 作为上下文，当 n 增加时， n-gram 的总数会呈指数级增长，此时会遇到维数灾难问题。而神经网络在表示 n-gram 时，可以通过一些组合方式对 n 个词进行组合，参数个数仅以线性速度增长。有了这一优势，神经网络模型可以对更复杂的上下文进行建模，在词向量中包含更丰富的语义信息。


五、词嵌入（ word embedding）
1、概念
基于神经网络的分布表示又称为词向量、词嵌入，神经网络词向量模型与其它分布表示方法一样，均基于分布假说，核心依然是上下文的表示以及上下文与目标词之间的关系的建模。

前面提到过，为了选择一种模型刻画某个词（下文称“目标词”）与其上下文之间的关系，我们需要在词向量中capture到一个词的上下文信息。同时，上面我们恰巧提到了统计语言模型正好具有捕捉上下文信息的能力。那么构建上下文与目标词之间的关系，最自然的一种思路就是使用语言模型。从历史上看，早期的词向量只是神经网络语言模型的副产品。

2001年， Bengio 等人正式提出神经网络语言模型（ Neural Network Language Model ，NNLM），该模型在学习语言模型的同时，也得到了词向量。所以请注意一点：词向量可以认为是神经网络训练语言模型的副产品。

2、理解
前面提过，one-hot表示法具有维度过大的缺点，那么现在将vector做一些改进：
1、将vector每一个元素由整形改为浮点型，变为整个实数范围的表示；
2、将原来稀疏的巨大维度压缩嵌入到一个更小维度的空间。

如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211548737052650.png'/>
 
这也是词向量又名词嵌入的缘由了。


六、神经网络语言模型与word2vec
好了，到目前为止我们已经对的分布式表示以及词嵌入的概念的层级关系有了个理性的认识了，那这跟word2vec有什么联系？

1、神经网络语言模型
上面说，通过神经网络训练语言模型可以得到词向量，那么，究竟有哪些类型的神经网络语言模型呢？个人所知，大致有这么些个：

a) Neural Network Language Model ，NNLM
b) Log-Bilinear Language Model， LBL
c) Recurrent Neural Network based Language Model，RNNLM
d) Collobert 和 Weston 在2008 年提出的 C&W 模型
e) Mikolov 等人提出了 CBOW（ Continuous Bagof-Words）和 Skip-gram 模型

到这，估计有人看到了两个熟悉的term：CBOW、skip-gram，有看过word2vec的同学应该对此有所了解。我们继续。


2.word2vec与CBOW、Skip-gram
现在我们正式引出最火热的另一个term：word2vec。

上面提到的5个神经网络语言模型，只是个在逻辑概念上的东西，那么具体我们得通过设计将其实现出来，而实现CBOW（ Continuous Bagof-Words）和 Skip-gram 语言模型的工具正是well-known word2vec！另外，C&W 模型的实现工具是SENNA。

所以说，分布式词向量并不是word2vec的作者发明的，他只是提出了一种更快更好的方式来训练语言模型罢了。分别是：连续词袋模型Continous Bag of Words Model(CBOW)和Skip-Gram Model，这两种都是可以训练出词向量的方法，再具体代码操作中可以只选择其一，不过据论文说CBOW要更快一些。

顺便说说这两个语言模型。统计语言模型statistical language model就是给你几个词，在这几个词出现的前提下来计算某个词出现的（事后）概率。CBOW也是统计语言模型的一种，顾名思义就是根据某个词前面的C个词或者前后C个连续的词，来计算某个词出现的概率。Skip-Gram Model相反，是根据某个词，然后分别计算它前后出现某几个词的各个概率。

以“我爱北京天安门”这句话为例。假设我们现在关注的词是“爱”，C＝2时它的上下文分别是“我”，“北京天安门”。CBOW模型就是把“我” “北京天安门” 的one hot表示方式作为输入，也就是C个1xV的向量，分别跟同一个VxN的大小的系数矩阵W1相乘得到C个1xN的隐藏层hidden layer，然后C个取平均所以只算一个隐藏层。这个过程也被称为线性激活函数(这也算激活函数？分明就是没有激活函数了)。

然后再跟另一个NxV大小的系数矩阵W2相乘得到1xV的输出层，这个输出层每个元素代表的就是词库里每个词的事后概率。输出层需要跟ground truth也就是“爱”的one hot形式做比较计算loss。

这里需要注意的就是V通常是一个很大的数比如几百万，计算起来相当费时间，除了“爱”那个位置的元素肯定要算在loss里面，word2vec就用基于huffman编码的Hierarchical softmax筛选掉了一部分不可能的词，然后又用nagetive samping再去掉了一些负样本的词所以时间复杂度就从O(V)变成了O(logV)。Skip gram训练过程类似，只不过输入输出刚好相反。

补充下，Word embedding的训练方法大致可以分为两类：一类是无监督或弱监督的预训练；一类是端对端（end to end）的有监督训练。无监督或弱监督的预训练以word2vec和auto-encoder为代表。这一类模型的特点是，不需要大量的人工标记样本就可以得到质量还不错的embedding向量。不过因为缺少了任务导向，可能和我们要解决的问题还有一定的距离。因此，我们往往会在得到预训练的embedding向量后，用少量人工标注的样本去fine-tune整个模型。

相比之下，端对端的有监督模型在最近几年里越来越受到人们的关注。与无监督模型相比，端对端的模型在结构上往往更加复杂。同时，也因为有着明确的任务导向，端对端模型学习到的embedding向量也往往更加准确。例如，通过一个embedding层和若干个卷积层连接而成的深度神经网络以实现对句子的情感分类，可以学习到语义更丰富的词向量表达。

3.个人对word embedding的理解
现在，词向量既能够降低维度，又能够capture到当前词在本句子中上下文的信息（表现为前后距离关系），那么我们对其用来表示语言句子词语作为NN的输入是非常自信与满意的。

另外一点很实用的建议，在你做某一项具体的NLP任务时如你要用到词向量，那么我建议你：
要么1、选择使用别人训练好的词向量，注意，得使用相同语料内容领域的词向量；
要么2、自己训练自己的词向量。我建议是前者，因为……坑太多了。


References：
《How to Generate a Good Word Embedding?》,Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao
《基于神经网络的词和文档语义向量表示方法研究》，来斯惟
《面向自然语言处理的分布式表示学习》，邱锡鹏
《Deep Learning 实战之 word2vec》
http://www.cnblogs.com/iloveai/p/word2vec.html

http://www.hankcs.com/nlp/word2vec.html
http://licstar.net/archives/328
https://zhuanlan.zhihu.com/p/22477976
http://blog.csdn.net/itplus/article/details/37969519
http://www.tuicool.com/articles/fmuyamf
http://licstar.net/archives/620#comment-1542
http://blog.csdn.net/ycheng_sjtu/article/details/48520293
## 15.如何通俗理解深度学习中的注意力机制
本题解析来源：https://blog.csdn.net/qq_40027052/article/details/78421155

最近两年，注意力模型（Attention Model）被广泛使用在自然语言处理、图像识别及语音识别等各种不同类型的深度学习任务中，是深度学习技术中最值得关注与深入了解的核心技术之一。本文以机器翻译为例，深入浅出地介绍了深度学习中注意力机制的原理及关键计算机制，同时也抽象出其本质思想，并介绍了注意力模型在图像及语音等领域的典型应用场景。

注意力模型最近几年在深度学习各个领域被广泛使用，无论是图像处理、语音识别还是自然语言处理的各种不同类型的任务中，都很容易遇到注意力模型的身影。所以，了解注意力机制的工作原理对于关注深度学习技术发展的技术人员来说有很大的必要。

人类的视觉注意力
从注意力模型的命名方式看，很明显其借鉴了人类的注意力机制，因此，我们首先简单介绍人类视觉的选择性注意力机制。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211742747672925.1'/>
图1 人类的视觉注意力

视觉注意力机制是人类视觉所特有的大脑信号处理机制。人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段，是人类在长期进化中形成的一种生存机制，人类视觉注意力机制极大地提高了视觉信息处理的效率与准确性。

图1形象化展示了人类在看到一副图像时是如何高效分配有限的注意力资源的，其中红色区域表明视觉系统更关注的目标，很明显对于图1所示的场景，人们会把注意力更多投入到人的脸部，文本的标题以及文章首句等位置。

深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。

Encoder-Decoder框架
要了解深度学习中的注意力模型，就不得不先谈Encoder-Decoder框架，因为目前大多数注意力模型附着在Encoder-Decoder框架下，当然，其实注意力模型可以看作一种通用的思想，本身并不依赖于特定框架，这点需要注意。

Encoder-Decoder框架可以看作是一种深度学习领域的研究模式，应用场景异常广泛。图2是文本处理领域里常用的Encoder-Decoder框架最抽象的一种表示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211744751311155.2'/>
图2 抽象的文本处理领域的Encoder-Decoder框架

文本处理领域的Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对< Source,Target >，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。Source和Target可以是同一种语言，也可以是两种不同的语言。而Source和Target分别由各自的单词序列构成：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415521174572929437.3'/>

Encoder顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211747814699010.4'/>

对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息y1,y2……yi-1来生成i时刻要生成的单词yi：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211748555662960.5'/>

每个yi都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target。
如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题的Encoder-Decoder框架；
如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要的Encoder-Decoder框架；
如果Source是一句问句，Target是一句回答，那么这是问答系统或者对话机器人的Encoder-Decoder框架。
由此可见，在文本处理领域，Encoder-Decoder的应用领域相当广泛。

Encoder-Decoder框架不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用。
比如对于语音识别来说，图2所示的框架完全适用，区别无非是Encoder部分的输入是语音流，输出是对应的文本信息；
而对于“图像描述”任务来说，Encoder部分的输入是一副图片，Decoder的输出是能够描述图片语义内容的一句描述语。
一般而言，文本处理和语音识别的Encoder部分通常采用RNN模型，图像处理的Encoder一般采用CNN模型。

Attention模型
本节先以机器翻译作为例子讲解最常见的Soft Attention模型的基本原理，之后抛离Encoder-Decoder框架抽象出了注意力机制的本质思想，然后简单介绍最近广为使用的Self Attention的基本思路。

Soft Attention模型
图2中展示的Encoder-Decoder框架是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Target中每个单词的生成过程如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211749984814036.6'/>

其中f是Decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的，没有任何区别。而语义编码C是由句子Source的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词，y1，y2还是y3，其实句子Source中任意单词对生成某个目标单词yi来说影响力都是相同的，这是为何说这个模型没有体现出注意力的缘由。这类似于人类看到眼前的画面，但是眼中却没有注意焦点一样。

如果拿机器翻译来解释这个分心模型的Encoder-Decoder框架更好理解，比如输入的是英文句子：Tom chase Jerry，Encoder-Decoder框架逐步生成中文单词：“汤姆”，“追逐”，“杰瑞”。在翻译“杰瑞”这个中文单词的时候，分心模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要，但是分心模型是无法体现这一点的，这就是为何说它没有引入注意力的原因。没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。

上面的例子中，如果引入Attention模型的话，应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：

（Tom,0.3）(Chase,0.2) (Jerry,0.5)

每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词yi的时候，原先都是相同的中间语义表示C会被替换成根据当前生成单词而不断变化的Ci。

理解Attention模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。增加了注意力模型的Encoder-Decoder框架理解起来如图3所示。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415521175116518819.7'/>
图3 引入注意力模型的Encoder-Decoder框架

即生成目标句子单词的过程成了下面的形式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211752485511920.8'/>

而每个Ci可能对应着不同的源语句子单词的注意力分配概率分布，比如对于上面的英汉翻译来说，其对应的信息可能如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211753566542139.9'/>

其中，f2函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值；g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和，即下列公式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211754138153314.10'/>

其中，Lx代表输入句子Source的长度，aij代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数，而hj则是Source输入句子中第j个单词的语义编码。假设Ci下标i就是上面例子所说的“ 汤姆” ，那么Lx就是3，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)分别是输入句子每个单词的语义编码，对应的注意力模型权值则分别是0.6,0.2,0.2，所以g函数本质上就是个加权求和函数。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的中间语义表示Ci的形成过程类似图4。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415521175545482914.11'/>
图4 Attention的形成过程

这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，如何知道Attention模型所需要的输入句子单词注意力分配概率分布值呢？就是说“汤姆”对应的输入句子Source中各个单词的概率分布：(Tom,0.6)(Chase,0.2) (Jerry,0.2) 是如何得到的呢？

为了便于说明，我们假设对图2的非Attention模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图2的框架转换为图5。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211756860639756.12'/>
图5 RNN作为具体模型的Encoder-Decoder框架

那么用图6可以较为便捷地说明注意力分配概率分布值的通用计算过程。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211758070033907.13'/>
图6 注意力分配概率计算

对于采用RNN的Decoder来说，在时刻i，如果要生成yi单词，我们是可以知道Target在生成yi之前的时刻i-1时，隐层节点i-1时刻的输出值Hi-1的，而我们的目的是要计算生成yi时输入句子中的单词“Tom”、“Chase”、“Jerry”对yi来说的注意力分配概率分布，那么可以用Target输出句子i-1时刻的隐层节点状态Hi-1去一一和输入句子Source中每个单词对应的RNN隐层节点状态hj进行对比，即通过函数F(hj,Hi-1)来获得目标单词yi和每个输入单词对应的对齐可能性，这个F函数在不同论文里可能会采取不同的方法，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。绝大多数Attention模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在F的定义上可能有所不同。图7可视化地展示了在英语-德语翻译系统中加入Attention机制后，Source和Target两个句子每个单词对应的注意力分配概率分布。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415521175955491524.14'/>
图7 英语-德语翻译的注意力概率分布

上述内容就是经典的Soft Attention模型的基本思想，那么怎么理解Attention模型的物理含义呢？一般在自然语言处理应用里会把Attention模型看作是输出Target句子中某个单词和输入Source句子每个单词的对齐模型，这是非常有道理的。目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤，而注意力模型其实起的是相同的作用。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211760936424713.15'/>
图8 Google 神经网络机器翻译系统结构图

图8所示即为Google于2016年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了60%，其架构就是上文所述的加上Attention机制的Encoder-Decoder框架，主要区别无非是其Encoder和Decoder使用了8层叠加的LSTM模型。

Attention机制的本质思想
如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211762137451769.16'/>
图9 Attention机制的本质思想

我们可以这样来看待Attention机制（参考图9）：将Source中的构成元素想象成是由一系列的< Key,Value >数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415521176349162958.17'/>

其中， Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。

当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。从图9可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。

至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211765427968810.18'/>
图10 三阶段计算Attention过程

在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个Keyi，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211766341219640.19'/>

第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211766949656168.20'/>

第二阶段的计算结果ai即为Valuei对应的权重系数，然后进行加权求和即可得到Attention数值：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211768186014632.21'/>

通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。

Self Attention模型
通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。Self Attention也经常被称为intra Attention（内部Attention），最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型。在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。而Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。

如果是常规的Target不等于Source情形下的注意力计算，其物理含义正如上文所讲，比如对于机器翻译来说，本质上是目标语单词和源语单词之间的一种单词对齐机制。那么如果是Self Attention机制，一个很自然的问题是：通过Self Attention到底学到了哪些规律或者抽取出了哪些特征呢？或者说引入Self Attention有什么增益或者好处呢？我们仍然以机器翻译中的Self Attention来说明，图11和图12是可视化地表示Self Attention在同一个英语句子内单词间产生的联系。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211769243433697.22'/>
图11 可视化Self Attention实例

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211770048194399.23'/>
图12 可视化Self Attention实例

从两张图（图11、图12）可以看出，Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）。很明显，引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。

Attention机制的应用
前文有述，Attention机制在深度学习的各种应用领域都有广泛的使用场景。上文在介绍过程中我们主要以自然语言处理中的机器翻译任务作为例子，下面分别再从图像处理领域和语音识别选择典型应用实例来对其应用做简单说明。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211771159077828.24'/>
图13 图片-描述任务的Encoder-Decoder框架

图片描述（Image-Caption）是一种典型的图文结合的深度学习应用，输入一张图片，人工智能系统输出一句描述句子，语义等价地描述图片所示内容。很明显这种应用场景也可以使用Encoder-Decoder框架来解决任务目标，此时Encoder输入部分是一张图片，一般会用CNN来对图片进行特征抽取，Decoder部分使用RNN或者LSTM来输出自然语言句子（参考图13）。此时如果加入Attention机制能够明显改善系统输出效果，Attention模型在这里起到了类似人类视觉选择性注意的机制，在输出某个实体单词的时候会将注意力焦点聚焦在图片中相应的区域上。图14给出了根据给定图片生成句子“A person is standing on a beach with a surfboard.”过程时每个单词对应图片中的注意力聚焦区域。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211772324247639.25'/>
图14 图片生成句子中每个单词时的注意力聚焦区域

图15给出了另外四个例子形象地展示了这种过程，每个例子上方左侧是输入的原图，下方句子是人工智能系统自动产生的描述语句，上方右侧图展示了当AI系统产生语句中划横线单词的时候，对应图片中聚焦的位置区域。比如当输出单词dog的时候，AI系统会将注意力更多地分配给图片中小狗对应的位置。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211773028957634.26'/>
图15 图像描述任务中Attention机制的聚焦作用

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155211775024135448.27'/>
图16 语音识别中音频序列和输出字符之间的Attention

语音识别的任务目标是将语音流信号转换成文字，所以也是Encoder-Decoder的典型应用场景。Encoder部分的Source输入是语音流信号，Decoder部分输出语音对应的字符串流。图16可视化地展示了在Encoder-Decoder框架中加入Attention机制后，当用户用语音说句子 how much would a woodchuck chuck 时，输入部分的声音特征信号和输出字符之间的注意力分配概率分布情况，颜色越深代表分配到的注意力概率越高。从图中可以看出，在这个场景下，Attention机制起到了将输出字符和输入语音信号进行对齐的功能。

上述内容仅仅选取了不同AI领域的几个典型Attention机制应用实例，Encoder-Decoder加Attention架构由于其卓越的实际效果，目前在深度学习领域里得到了广泛的使用，了解并熟练使用这一架构对于解决实际问题会有极大帮助。
## 16.请详细说说Transformer （超详细图解，一图胜千言）
本文解析来源：https://blog.csdn.net/longxinchen_ml/article/details/86533005，原英文链接：https://jalammar.github.io/illustrated-transformer/

编者按：前一段时间谷歌推出的BERT模型在11项NLP任务中夺得SOTA结果，引爆了整个NLP界。而BERT取得成功的一个关键因素是Transformer的强大作用。谷歌的Transformer模型最早是用于机器翻译任务，当时达到了SOTA效果。Transformer改进了RNN最被人诟病的训练慢的缺点，利用self-attention机制实现快速并行。并且Transformer可以增加到非常深的深度，充分发掘DNN模型的特性，提升模型准确率。在本文中，我们将研究Transformer模型，把它掰开揉碎，理解它的工作原理。

正文：
Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。

在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。
Attention is All You Need：https://arxiv.org/abs/1706.03762

从宏观的视角开始
首先将这个模型看成是一个黑箱操作。在机器翻译中，就是输入一种语言，输出另一种语言。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845827584775818.png'/>

那么拆开这个黑箱，我们可以看到它是由编码组件、解码组件和它们之间的连接组成。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845828062714956.png'/>

编码组件部分由一堆编码器（encoder）构成（论文中是将6个编码器叠在一起——数字6没有什么神奇之处，你也可以尝试其他数字）。解码组件部分也是由相同数量（与编码器对应）的解码器（decoder）组成的。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845829057658997.png'/>

所有的编码器在结构上都是相同的，但它们没有共享参数。每个解码器都可以分解成两个子层。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845842850978652.jpg'/>

从编码器输入的句子首先会经过一个自注意力（self-attention）层，这层帮助编码器在对每个单词编码时关注输入句子的其他单词。我们将在稍后的文章中更深入地研究自注意力。

自注意力层的输出会传递到前馈（feed-forward）神经网络中。每个位置的单词对应的前馈神经网络都完全一样（译注：另一种解读就是一层窗口为一个单词的一维卷积神经网络）。

解码器中也有编码器的自注意力（self-attention）层和前馈（feed-forward）层。除此之外，这两个层之间还有一个注意力层，用来关注输入句子的相关部分（和seq2seq模型的注意力作用相似）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845844714514848.jpg'/>

将张量引入图景
我们已经了解了模型的主要部分，接下来我们看一下各种向量或张量（译注：张量概念是矢量概念的推广，可以简单理解矢量是一阶张量、矩阵是二阶张量。）是怎样在模型的不同部分中，将输入转化为输出的。

像大部分NLP应用一样，我们首先将每个输入单词通过词嵌入算法转换为词向量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845833046907804.png'/>

每个单词都被嵌入为512维的向量，我们用这些简单的方框来表示这些向量。

词嵌入过程只发生在最底层的编码器中。所有的编码器都有一个相同的特点，即它们接收一个向量列表，列表中的每个向量大小为512维。在底层（最开始）编码器中它就是词向量，但是在其他编码器中，它就是下一层编码器的输出（也是一个向量列表）。向量列表大小是我们可以设置的超参数——一般是我们训练集中最长句子的长度。

将输入序列进行词嵌入之后，每个单词都会流经编码器中的两个子层。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845847673986901.jpg'/>

接下来我们看看Transformer的一个核心特性，在这里输入序列中每个位置的单词都有自己独特的路径流入编码器。在自注意力层中，这些路径之间存在依赖关系。而前馈（feed-forward）层没有这些依赖关系。因此在前馈（feed-forward）层时可以并行执行各种路径。

然后我们将以一个更短的句子为例，看看编码器的每个子层中发生了什么。

现在我们开始“编码”
如上述已经提到的，一个编码器接收向量列表作为输入，接着将向量列表中的向量传递到自注意力层进行处理，然后传递到前馈神经网络层中，将输出结果传递到下一个编码器中。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684584927375001.jpg'/>

输入序列的每个单词都经过自编码过程。然后，他们各自通过前向传播神经网络——完全相同的网络，而每个向量都分别通过它。

从宏观视角看自注意力机制
不要被我用自注意力这个词弄迷糊了，好像每个人都应该熟悉这个概念。其实我之也没有见过这个概念，直到读到Attention is All You Need 这篇论文时才恍然大悟。让我们精炼一下它的工作原理。

例如，下列句子是我们想要翻译的输入句子：

The animal didn’t cross the street because it was too tired

这个“it”在这个句子是指什么呢？它指的是street还是这个animal呢？这对于人类来说是一个简单的问题，但是对于算法则不是。

当模型处理这个单词“it”的时候，自注意力机制会允许“it”与“animal”建立联系。

随着模型处理输入序列的每个单词，自注意力会关注整个输入序列的所有单词，帮助模型对本单词更好地进行编码。

如果你熟悉RNN（循环神经网络），回忆一下它是如何维持隐藏层的。RNN会将它已经处理过的前面的所有单词/向量的表示与它正在处理的当前单词/向量结合起来。而自注意力机制会将所有相关单词的理解融入到我们正在处理的单词中。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845850836579196.png'/>

当我们在编码器#5（栈中最上层编码器）中编码“it”这个单词的时，注意力机制的部分会去关注“The Animal”，将它的表示的一部分编入“it”的编码中。

请务必检查Tensor2Tensor notebook ，在里面你可以下载一个Transformer模型，并用交互式可视化的方式来检验。

从微观视角看自注意力机制
首先我们了解一下如何使用向量来计算自注意力，然后来看它实怎样用矩阵来实现。

计算自注意力的第一步就是从每个编码器的输入向量（每个单词的词向量）中生成三个向量。也就是说对于每个单词，我们创造一个查询向量、一个键向量和一个值向量。这三个向量是通过词嵌入与三个权重矩阵后相乘创建的。

可以发现这些新向量在维度上比词嵌入向量更低。他们的维度是64，而词嵌入和编码器的输入/输出向量的维度是512. 但实际上不强求维度更小，这只是一种基于架构上的选择，它可以使多头注意力（multiheaded attention）的大部分计算保持不变。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845853367578284.jpg'/>

X1与WQ权重矩阵相乘得到q1, 就是与这个单词相关的查询向量。最终使得输入序列的每个单词的创建一个查询向量、一个键向量和一个值向量。

什么是查询向量、键向量和值向量向量？

它们都是有助于计算和理解注意力机制的抽象概念。请继续阅读下文的内容，你就会知道每个向量在计算注意力机制中到底扮演什么样的角色。

计算自注意力的第二步是计算得分。假设我们在为这个例子中的第一个词“Thinking”计算自注意力向量，我们需要拿输入句子中的每个单词对“Thinking”打分。这些分数决定了在编码单词“Thinking”的过程中有多重视句子的其它部分。

这些分数是通过打分单词（所有输入句子的单词）的键向量与“Thinking”的查询向量相点积来计算的。所以如果我们是处理位置最靠前的词的自注意力的话，第一个分数是q1和k1的点积，第二个分数是q1和k2的点积。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845856798161316.jpg'/>

第三步和第四步是将分数除以8(8是论文中使用的键向量的维数64的平方根，这会让梯度更稳定。这里也可以使用其它值，8只是默认值)，然后通过softmax传递结果。softmax的作用是使所有单词的分数归一化，得到的分数都是正值且和为1。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845858096672999.jpg'/>

这个softmax分数决定了每个单词对编码当下位置（“Thinking”）的贡献。显然，已经在这个位置上的单词将获得最高的softmax分数，但有时关注另一个与当前单词相关的单词也会有帮助。

第五步是将每个值向量乘以softmax分数(这是为了准备之后将它们求和)。这里的直觉是希望关注语义上相关的单词，并弱化不相关的单词(例如，让它们乘以0.001这样的小数)。

第六步是对加权值向量求和（译注：自注意力的另一种解释就是在编码某个单词时，就是将所有单词的表示（值向量）进行加权求和，而权重是通过该词的表示（键向量）与被编码词表示（查询向量）的点积并通过softmax得到。），然后即得到自注意力层在该位置的输出(在我们的例子中是对于第一个单词)。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845866637300867.jpg'/>

这样自自注意力的计算就完成了。得到的向量就可以传给前馈神经网络。然而实际中，这些计算是以矩阵形式完成的，以便算得更快。那我们接下来就看看如何用矩阵实现的。

通过矩阵运算实现自注意力机制
第一步是计算查询矩阵、键矩阵和值矩阵。为此，我们将将输入句子的词嵌入装进矩阵X中，将其乘以我们训练的权重矩阵(WQ，WK，WV)。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641568458696369372.png'/>

x矩阵中的每一行对应于输入句子中的一个单词。我们再次看到词嵌入向量 (512，或图中的4个格子)和q/k/v向量(64，或图中的3个格子)的大小差异。

最后，由于我们处理的是矩阵，我们可以将步骤2到步骤6合并为一个公式来计算自注意力层的输出。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845871490011733.png'/>

自注意力的矩阵运算形式

“大战多头怪”
通过增加一种叫做“多头”注意力（“multi-headed” attention）的机制，论文进一步完善了自注意力层，并在两方面提高了注意力层的性能：

1.它扩展了模型专注于不同位置的能力。在上面的例子中，虽然每个编码都在z1中有或多或少的体现，但是它可能被实际的单词本身所支配。如果我们翻译一个句子，比如“The animal didn’t cross the street because it was too tired”，我们会想知道“it”指的是哪个词，这时模型的“多头”注意机制会起到作用。

2.它给出了注意力层的多个“表示子空间”（representation subspaces）。接下来我们将看到，对于“多头”注意机制，我们有多个查询/键/值权重矩阵集(Transformer使用八个注意力头，因此我们对于每个编码器/解码器有八个矩阵集合)。这些集合中的每一个都是随机初始化的，在训练之后，每个集合都被用来将输入词嵌入(或来自较低编码器/解码器的向量)投影到不同的表示子空间中。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684587581642812.jpg'/>

在“多头”注意机制下，我们为每个头保持独立的查询/键/值权重矩阵，从而产生不同的查询/键/值矩阵。和之前一样，我们拿X乘以WQ/WK/WV矩阵来产生查询/键/值矩阵。

如果我们做与上述相同的自注意力计算，只需八次不同的权重矩阵运算，我们就会得到八个不同的Z矩阵。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845876557715677.png'/>

这给我们带来了一点挑战。前馈层不需要8个矩阵，它只需要一个矩阵(由每一个单词的表示向量组成)。所以我们需要一种方法把这八个矩阵压缩成一个矩阵。那该怎么做？其实可以直接把这些矩阵拼接在一起，然后用一个附加的权重矩阵WO与它们相乘。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845878591825568.jpg'/>

这几乎就是多头自注意力的全部。这确实有好多矩阵，我们试着把它们集中在一个图片中，这样可以一眼看清。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845879611656792.jpg'/>

既然我们已经摸到了注意力机制的这么多“头”，那么让我们重温之前的例子，看看我们在例句中编码“it”一词时，不同的注意力“头”集中在哪里：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845880564049405.png'/>

当我们编码“it”一词时，一个注意力头集中在“animal”上，而另一个则集中在“tired”上，从某种意义上说，模型对“it”一词的表达在某种程度上是“animal”和“tired”的代表。

然而，如果我们把所有的attention都加到图示里，事情就更难解释了：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845881721069536.png'/>

使用位置编码表示序列的顺序
到目前为止，我们对模型的描述缺少了一种理解输入单词顺序的方法。

为了解决这个问题，Transformer为每个输入的词嵌入添加了一个向量。这些向量遵循模型学习到的特定模式，这有助于确定每个单词的位置，或序列中不同单词之间的距离。这里的直觉是，将位置向量添加到词嵌入中使得它们在接下来的运算中，能够更好地表达的词与词之间的距离。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845883192685943.jpg'/>

为了让模型理解单词的顺序，我们添加了位置编码向量，这些向量的值遵循特定的模式。

如果我们假设词嵌入的维数为4，则实际的位置编码如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845884574651402.jpg'/>

尺寸为4的迷你词嵌入位置编码实例

这个模式会是什么样子？

在下图中，每一行对应一个词向量的位置编码，所以第一行对应着输入序列的第一个词。每行包含512个值，每个值介于1和-1之间。我们已经对它们进行了颜色编码，所以图案是可见的。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845885532148322.png'/>

20字(行)的位置编码实例，词嵌入大小为512(列)。你可以看到它从中间分裂成两半。这是因为左半部分的值由一个函数(使用正弦)生成，而右半部分由另一个函数(使用余弦)生成。然后将它们拼在一起而得到每一个位置编码向量。

原始论文里描述了位置编码的公式(第3.5节)。你可以在 get_timing_signal_1d()中看到生成位置编码的代码。这不是唯一可能的位置编码方法。然而，它的优点是能够扩展到未知的序列长度(例如，当我们训练出的模型需要翻译远比训练集里的句子更长的句子时)。

残差模块
在继续进行下去之前，我们需要提到一个编码器架构中的细节：在每个编码器中的每个子层（自注意力、前馈网络）的周围都有一个残差连接，并且都跟随着一个“层-归一化”步骤。

层-归一化步骤：https://arxiv.org/abs/1607.06450 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845887369372446.jpg'/>

如果我们去可视化这些向量以及这个和自注意力相关联的层-归一化操作，那么看起来就像下面这张图描述一样：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845889917117177.jpg'/>

解码器的子层也是这样样的。如果我们想象一个2 层编码-解码结构的transformer，它看起来会像下面这张图一样：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845897927214300.jpg'/>

解码组件
既然我们已经谈到了大部分编码器的概念，那么我们基本上也就知道解码器是如何工作的了。但最好还是看看解码器的细节。

编码器通过处理输入序列开启工作。顶端编码器的输出之后会变转化为一个包含向量K（键向量）和V（值向量）的注意力向量集 。这些向量将被每个解码器用于自身的“编码-解码注意力层”，而这些层可以帮助解码器关注输入序列哪些位置合适：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156846894583861613.gif'/>
在完成编码阶段后，则开始解码阶段。解码阶段的每个步骤都会输出一个输出序列（在这个例子里，是英语翻译的句子）的元素。

接下来的步骤重复了这个过程，直到到达一个特殊的终止符号，它表示transformer的解码器已经完成了它的输出。每个步骤的输出在下一个时间步被提供给底端解码器，并且就像编码器之前做的那样，这些解码器会输出它们的解码结果 。另外，就像我们对编码器的输入所做的那样，我们会嵌入并添加位置编码给那些解码器，来表示每个单词的位置。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156846899939997439.gif'/>
而那些解码器中的自注意力层表现的模式与编码器不同：在解码器中，自注意力层只被允许处理输出序列中更靠前的那些位置。在softmax步骤前，它会把后面的位置给隐去（把它们设为-inf）。

这个“编码-解码注意力层”工作方式基本就像多头自注意力层一样，只不过它是通过在它下面的层来创造查询矩阵，并且从编码器的输出中取得键/值矩阵。

最终的线性变换和Softmax层
解码组件最后会输出一个实数向量。我们如何把浮点数变成一个单词？这便是线性变换层要做的工作，它之后就是Softmax层。

线性变换层是一个简单的全连接神经网络，它可以把解码组件产生的向量投射到一个比它大得多的、被称作对数几率（logits）的向量里。

不妨假设我们的模型从训练集中学习一万个不同的英语单词（我们模型的“输出词表”）。因此对数几率向量为一万个单元格长度的向量——每个单元格对应某一个单词的分数。

接下来的Softmax 层便会把那些分数变成概率（都为正数、上限1.0）。概率最高的单元格被选中，并且它对应的单词被作为这个时间步的输出。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845944079222297.jpg'/>

这张图片从底部以解码器组件产生的输出向量开始。之后它会转化出一个输出单词。

训练部分总结
既然我们已经过了一遍完整的transformer的前向传播过程，那我们就可以直观感受一下它的训练过程。

在训练过程中，一个未经训练的模型会通过一个完全一样的前向传播。但因为我们用有标记的训练集来训练它，所以我们可以用它的输出去与真实的输出做比较。

为了把这个流程可视化，不妨假设我们的输出词汇仅仅包含六个单词：“a”, “am”, “i”, “thanks”, “student”以及 “”（end of sentence的缩写形式）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845946021955055.png'/>

我们模型的输出词表在我们训练之前的预处理流程中就被设定好。

一旦我们定义了我们的输出词表，我们可以使用一个相同宽度的向量来表示我们词汇表中的每一个单词。这也被认为是一个one-hot 编码。所以，我们可以用下面这个向量来表示单词“am”：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684595265463151.jpg'/>

例子：对我们输出词表的one-hot 编码

接下来我们讨论模型的损失函数——这是我们用来在训练过程中优化的标准。通过它可以训练得到一个结果尽量准确的模型。

损失函数
比如说我们正在训练模型，现在是第一步，一个简单的例子——把“merci”翻译为“thanks”。

这意味着我们想要一个表示单词“thanks”概率分布的输出。但是因为这个模型还没被训练好，所以不太可能现在就出现这个结果。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845954355490127.jpg'/>

因为模型的参数（权重）都被随机的生成，（未经训练的）模型产生的概率分布在每个单元格/单词里都赋予了随机的数值。我们可以用真实的输出来比较它，然后用反向传播算法来略微调整所有模型的权重，生成更接近结果的输出。

你会如何比较两个概率分布呢？我们可以简单地用其中一个减去另一个。更多细节请参考交叉熵和KL散度。

交叉熵：https://colah.github.io/posts/2015-09-Visual-Information/
KL散度：https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained

但注意到这是一个过于简化的例子。更现实的情况是处理一个句子。例如，输入“je suis étudiant”并期望输出是“i am a student”。那我们就希望我们的模型能够成功地在这些情况下输出概率分布：

每个概率分布被一个以词表大小（我们的例子里是6，但现实情况通常是3000或10000）为宽度的向量所代表。

第一个概率分布在与“i”关联的单元格有最高的概率

第二个概率分布在与“am”关联的单元格有最高的概率

以此类推，第五个输出的分布表示“”关联的单元格有最高的概率
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845957766480817.jpg'/>

依据例子训练模型得到的目标概率分布

在一个足够大的数据集上充分训练后，我们希望模型输出的概率分布看起来像这个样子：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156845959033771441.jpg'/>

我们期望训练过后，模型会输出正确的翻译。当然如果这段话完全来自训练集，它并不是一个很好的评估指标（参考：交叉验证，链接https://www.youtube.com/watch?v=TIgfjmp-4BA）。注意到每个位置（词）都得到了一点概率，即使它不太可能成为那个时间步的输出——这是softmax的一个很有用的性质，它可以帮助模型训练。

因为这个模型一次只产生一个输出，不妨假设这个模型只选择概率最高的单词，并把剩下的词抛弃。这是其中一种方法（叫贪心解码）。另一个完成这个任务的方法是留住概率最靠高的两个单词（例如I和a），那么在下一步里，跑模型两次：其中一次假设第一个位置输出是单词“I”，而另一次假设第一个位置输出是单词“me”，并且无论哪个版本产生更少的误差，都保留概率最高的两个翻译结果。然后我们为第二和第三个位置重复这一步骤。这个方法被称作集束搜索（beam search）。在我们的例子中，集束宽度是2（因为保留了2个集束的结果，如第一和第二个位置），并且最终也返回两个集束的结果（top_beams也是2）。这些都是可以提前设定的参数。

再进一步
我希望通过上文已经让你们了解到Transformer的主要概念了。如果你想在这个领域深入，我建议可以走以下几步：阅读Attention Is All You Need，Transformer博客和Tensor2Tensor announcement，以及看看Łukasz Kaiser的介绍，了解模型和细节。

Attention Is All You Need：https://arxiv.org/abs/1706.03762
Transformer博客：https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html
Tensor2Tensor announcement：https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html
Łukasz Kaiser的介绍：https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb

接下来可以研究的工作：
Depthwise Separable Convolutions for Neural Machine Translation
https://arxiv.org/abs/1706.03059

One Model To Learn Them All
https://arxiv.org/abs/1706.05137

Discrete Autoencoders for Sequence Models
https://arxiv.org/abs/1801.09797

Generating Wikipedia by Summarizing Long Sequences
https://arxiv.org/abs/1801.10198

Image Transformer
https://arxiv.org/abs/1802.05751

Training Tips for the Transformer Model
https://arxiv.org/abs/1804.00247

Self-Attention with Relative Position Representations
https://arxiv.org/abs/1803.02155

Fast Decoding in Sequence Models using Discrete Latent Variables
https://arxiv.org/abs/1803.03382

Adafactor: Adaptive Learning Rates with Sublinear Memory Cost
https://arxiv.org/abs/1804.04235
## 17.如何通俗理解LDA主题模型
本题解析来源于July在CSDN上超过20万阅读量的LDA笔记《通俗理解LDA主题模型》，特原封不动的转至于此。


0 前言
    印象中，最开始听说“LDA”这个名词，是缘于rickjin在2013年3月写的一个LDA科普系列，叫LDA数学八卦，我当时一直想看来着，记得还打印过一次，但不知是因为这篇文档的前序铺垫太长（现在才意识到这些“铺垫”都是深刻理解LDA 的基础，但如果没有人帮助初学者提纲挈领、把握主次、理清思路，则很容易陷入LDA的细枝末节之中），还是因为其中的数学推导细节太多，导致一直没有完整看完过。

    2013年12月，在我组织的Machine Learning读书会第8期上，@夏粉_百度 讲机器学习中排序学习的理论和算法研究，@沈醉2011 则讲主题模型的理解。又一次碰到了主题模型，当时貌似只记得沈博讲了一个汪峰写歌词的例子，依然没有理解LDA到底是怎样一个东西（但理解了LDA之后，再看沈博主题模型的PPT会很赞）。

    直到昨日下午，机器学习班 第12次课上，邹讲完LDA之后，才真正明白LDA原来是那么一个东东！上完课后，趁热打铁，再次看LDA数学八卦，发现以前看不下去的文档再看时竟然一路都比较顺畅，一口气看完大部。看完大部后，思路清晰了，知道理解LDA，可以分为下述5个步骤：

一个函数：gamma函数
四个分布：二项分布、多项分布、beta分布、Dirichlet分布
一个概念和一个理念：共轭先验和贝叶斯框架
两个模型：pLSA、LDA（在本文第4 部分阐述）
一个采样：Gibbs采样
    本文便按照上述5个步骤来阐述，希望读者看完本文后，能对LDA有个尽量清晰完整的了解。同时，本文基于邹讲LDA的PPT、rickjin的LDA数学八卦及其它参考资料写就，可以定义为一篇学习笔记或课程笔记，当然，后续不断加入了很多自己的理解。若有任何问题，欢迎随时于本文评论下指出，thanks。


1 gamma函数
1.0 整体把握LDA
    关于LDA有两种含义，一种是线性判别分析（Linear Discriminant Analysis），一种是概率主题模型：隐含狄利克雷分布（Latent Dirichlet Allocation，简称LDA），本文讲后者。

    另外，我先简单说下LDA的整体思想，不然我怕你看了半天，铺了太长的前奏，却依然因没见到LDA的影子而显得“心浮气躁”，导致不想再继续看下去。所以，先给你吃一颗定心丸，明白整体框架后，咱们再一步步抽丝剥茧，展开来论述。

    按照wiki上的介绍，LDA由Blei, David M.、Ng, Andrew Y.、Jordan于2003年提出，是一种主题模型，它可以将文档集 中每篇文档的主题以概率分布的形式给出，从而通过分析一些文档抽取出它们的主题（分布）出来后，便可以根据主题（分布）进行主题聚类或文本分类。同时，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的关系。

    此外，一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。

    人类是怎么生成文档的呢？LDA的这三位作者在原始论文中给了一个简单的例子。比如假设事先给定了这几个主题：Arts、Budgets、Children、Education，然后通过学习训练，获取每个主题Topic对应的词语。如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223030724224382.1'/>

    然后以一定的概率选取上述某个主题，再以一定的概率选取那个主题下的某个单词，不断的重复这两步，最终生成如下图所示的一篇文章（其中不同颜色的词语分别对应上图中不同主题下的词）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223031682919494.2'/>

    而当我们看到一篇文章后，往往喜欢推测这篇文章是如何生成的，我们可能会认为作者先确定这篇文章的几个主题，然后围绕这几个主题遣词造句，表达成文。

    LDA就是要干这事：根据给定的一篇文档，反推其主题分布。

    通俗来说，可以假定认为人类是根据上述文档生成过程写成了各种各样的文章，现在某小撮人想让计算机利用LDA干一件事：你计算机给我推测分析网络上各篇文章分别都写了些啥主题，且各篇文章中各个主题出现的概率大小（主题分布）是啥。

    然，就是这么一个看似普通的LDA，一度吓退了不少想深入探究其内部原理的初学者。难在哪呢，难就难在LDA内部涉及到的数学知识点太多了。

    在LDA模型中，一篇文档生成的方式如下：

    ①从狄利克雷分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223041580429184.3'/>中取样生成文档 i 的主题分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223042353367220.4'/>
    ②从主题的多项式分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223043433138956.5'/>中取样生成文档i第 j 个词的主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223044046358704.6'/>
    ③从狄利克雷分布中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223047393316093.7'/>取样生成主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223048937470750.8'/>对应的词语分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223049586264826.9'/>
    ④从词语的多项式分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223050433074241.10'/>中采样最终生成词语<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223050950540711.11'/>

    其中，类似Beta分布是二项式分布的共轭先验概率分布，而狄利克雷分布（Dirichlet分布）是多项式分布的共轭先验概率分布。

    此外，LDA的图模型结构如下图所示（类似贝叶斯网络结构）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223057373957026.12'/>

    恩，不错，短短6句话整体概括了整个LDA的主体思想！但也就是上面短短6句话，却接连不断或重复出现了二项分布、多项式分布、beta分布、狄利克雷分布（Dirichlet分布）、共轭先验概率分布、取样，那么请问，这些都是啥呢？

    这里先简单解释下二项分布、多项分布、beta分布、Dirichlet 分布这4个分布。

 二项分布（Binomial distribution）。
    二项分布是从伯努利分布推进的。伯努利分布，又称两点分布或0-1分布，是一个离散型的随机分布，其中的随机变量只有两类取值，非正即负{+，-}。而二项分布即重复n次的伯努利试验，记为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415522306648555491.13'/>。简言之，只做一次实验，是伯努利分布，重复做了n次，是二项分布。二项分布的概率密度函数为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223067794562016.14'/>

    对于k = 0, 1, 2, ..., n，其中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223069542303939.15'/>是二项式系数（这就是二项分布的名称的由来），又记为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223070680695592.16'/>。回想起高中所学的那丁点概率知识了么：想必你当年一定死记过这个二项式系数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415522307199634074.17'/>就是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415522307255987551.18'/>。

多项分布，是二项分布扩展到多维的情况。
    多项分布是指单次试验中的随机变量的取值不再是0-1的，而是有多种离散值可能（1,2,3...,k）。比如投掷6个面的骰子实验，N次实验结果服从K=6的多项分布。其中
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223074876480664.19'/>

    多项分布的概率密度函数为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223075581464852.20'/>

Beta分布，二项分布的共轭先验分布。
    给定参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415522307904324346.21'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223080157054835.22'/>，取值范围为[0,1]的随机变量 x 的概率密度函数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415522308114994355.23'/>

    其中：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223082621142936.24'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223083618227062.25'/>

   注：便是所谓的gamma函数，下文会具体阐述。

Dirichlet分布，是beta分布在高维度上的推广。
    Dirichlet分布的的密度函数形式跟beta分布的密度函数如出一辙：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223112271512633.31'/>

    其中
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223112796786823.32'/>

    至此，我们可以看到二项分布和多项分布很相似，Beta分布和Dirichlet 分布很相似，而至于“Beta分布是二项式分布的共轭先验概率分布，而狄利克雷分布（Dirichlet分布）是多项式分布的共轭先验概率分布”这点在下文中说明。

    OK，接下来，咱们就按照本文开头所说的思路：“一个函数：gamma函数，四个分布：二项分布、多项分布、beta分布、Dirichlet分布，外加一个概念和一个理念：共轭先验和贝叶斯框架，两个模型：pLSA、LDA（文档-主题，主题-词语），一个采样：Gibbs采样”一步步详细阐述，争取给读者一个尽量清晰完整的LDA。

    （当然，如果你不想深究背后的细节原理，只想整体把握LDA的主体思想，可直接跳到本文第4 部分，看完第4部分后，若还是想深究背后的细节原理，可再回到此处开始看）

1.1 gamma函数
    咱们先来考虑一个问题（此问题1包括下文的问题2-问题4皆取材自LDA数学八卦）：

问题1 随机变量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223090075485218.26'/>
把这n 个随机变量排序后得到顺序统计量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223090967158461.27'/>
然后请问<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223091925303405.28'/>的分布是什么。

    为解决这个问题，可以尝试计算<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223093225086067.29'/>落在区间[x,x+Δx]的概率。即求下述式子的值：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223095916322370.30'/>

    首先，把 [0,1] 区间分成三段 [0,x)，[x,x+Δx]，(x+Δx,1]，然后考虑下简单的情形：即假设n 个数中只有1个落在了区间 [x,x+Δx]内，由于这个区间内的数X(k)是第k大的，所以[0,x)中应该有 k−1 个数，(x+Δx,1] 这个区间中应该有n−k 个数。如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223121355760865.33'/>

    从而问题转换为下述事件E：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223122063787678.34'/>

    对于上述事件E，有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223122755489443.35'/>

    其中，o(Δx)表示Δx的高阶无穷小。显然，由于不同的排列组合，即n个数中有一个落在 [x,x+Δx]区间的有n种取法，余下n−1个数中有k−1个落在[0,x)的有<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223127124703337.36'/>种组合，所以和事件E等价的事件一共有<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223127986924382.37'/>个。

    如果有2个数落在区间[x,x+Δx]呢？如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223128658913270.38'/>

    类似于事件E，对于2个数落在区间[x,x+Δx]的事件E’：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223130769986750.39'/>

    有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223131599874421.40'/>

   从上述的事件E、事件E‘中，可以看出，只要落在[x,x+Δx]内的数字超过一个，则对应的事件的概率就是 o(Δx)。于是乎有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223149499756040.41'/>

    从而得到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223153365411196.42'/>的概率密度函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223154232926834.43'/>为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641552231548865276.44'/>

    至此，本节开头提出的问题得到解决。然仔细观察<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223157626755370.45'/>的概率密度函数，发现式子的最终结果有阶乘，联想到阶乘在实数上的推广函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223159439752741.46'/>：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223160618449295.47'/>

    两者结合是否会产生奇妙的效果呢？考虑到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223163335912349.48'/>具有如下性质：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223164012353331.49'/>

    故将<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223166328509447.50'/>代入到的概率密度函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223167028426475.51'/>中，可得：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223168292466822.52'/>

    然后取<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223171484240359.53'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223172045156108.54'/>，转换<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223172857689698.55'/>得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155223173788644765.56'/>

    如果熟悉beta分布的朋友，可能会惊呼：哇，竟然推出了beta分布！


2 beta分布
2.1 beta分布
    在概率论中，beta是指一组定义在（0,1）区间的连续概率分布，有两个参数α和β，且α,β＞0。

    beta分布的概率密度函数是：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231207547653659.1'/>

  其中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231204010221212.3'/>便是函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231204560600459.4'/>：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231214890576308.1'/>

随机变量X服从参数为的beta分布通常写作：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229714554930785.png'/>。

2.2 Beta-Binomial 共轭
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229718746243542.png'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231219212539276.2'/>

熟悉贝叶斯方法（不熟悉的没事，参见此文第一部分）的朋友心里估计又犯“嘀咕”了，这不就是贝叶斯式的思考过程么？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229722237463745.png'/>

  回顾下贝叶斯派思考问题的固定模式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229725615314415.png'/>

上述思考模式意味着，新观察到的样本信息将修正人们以前对事物的认知。换言之，在得到新的样本信息之前，人们对的认知是先验分布π(θ)，在得到新的样本信息X后，人们对θ的认知为π(θ|X)。

    类比到现在这个问题上，我们也可以试着写下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415523122275489906.3'/>

   其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231227566978481.4'/>对应的是二项分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231228693653066.5'/>的计数。

    更一般的，对于非负实数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231229667562070.6'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231230168992630.7'/>，我们有如下关系
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231231826118016.8'/>

    针对于这种观测到的数据符合二项分布，参数的先验分布和后验分布都是Beta分布的情况，就是Beta-Binomial共轭。换言之，Beta分布是二项式分布的共轭先验概率分布。

    二项分布和Beta分布是共轭分布意味着，如果我们为二项分布的参数p选取的先验分布是Beta分布，那么以p为参数的二项分布用贝叶斯估计得到的后验分布仍然服从Beta分布。

    此外，如何理解参数α和β所表达的意义呢？α、β可以认为形状参数，通俗但不严格的理解是，α和β共同控制Beta分布的函数“长的样子”：形状千奇百怪，高低胖瘦，如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229756384798734.png'/>


2.3 共轭先验分布
    什么又是共轭呢？轭的意思是束缚、控制，共轭从字面上理解，则是共同约束，或互相约束。

    在贝叶斯概率理论中，如果后验概率P(θ|x)和先验概率p(θ)满足同样的分布律，那么，先验分布和后验分布被叫做共轭分布，同时，先验分布叫做似然函数的共轭先验分布。

    比如，某观测数据服从概率分布P(θ)时，当观测到新的X数据时，我们一般会遇到如下问题：

·可否根据新观测数据X，更新参数θ？
·根据新观测数据可以在多大程度上改变参数θ，即
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231235822904796.9'/>

·当重新估计θ的时候，给出新参数值θ的新概率分布，即P(θ|x)。
    事实上，根据根据贝叶斯公式可知：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231236453266717.10'/>

      其中，P(x|θ)表示以预估θ为参数的x概率分布，可以直接求得，P(θ)是已有原始的θ概率分布。
    所以，如果我们选取P(x|θ)的共轭先验作为P(θ)的分布，那么P(x|θ)乘以P(θ)，然后归一化的结果P(θ|x)跟和P(θ)的形式一样。换句话说，先验分布是P(θ)，后验分布是P(θ|x)，先验分布跟后验分布同属于一个分布族，故称该分布族是θ的共轭先验分布（族）。

    举个例子。投掷一个非均匀硬币，可以使用参数为θ的伯努利模型，θ为硬币为正面的概率，那么结果x的分布形式为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415523123964620658.11'/>

其共轭先验为beta分布，具有两个参数α和β，称为超参数（hyperparameters）。且这两个参数决定了θ参数，其Beta分布形式为
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231241271314136.12'/>

然后计算后验概率
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231242356335304.13'/>

归一化这个等式后会得到另一个Beta分布，从而证明了Beta分布确实是伯努利分布的共轭先验分布。

2.4 从beta分布推广到Dirichlet 分布
    接下来，咱们来考察beta分布的一个性质。

    如果<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155229779552176762.png'/>，则有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231263691991014.14'/>

注意到上式最后结果的右边积分
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231264996810441.15'/>

   其类似于概率分布，而对于这个分布有
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231267979918048.16'/>

    从而求得
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231299175015654.21'/>

    的结果为
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231299718546134.22'/>

 最后将此结果带入E（P）的计算式，得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231300527577258.23'/>

最后的这个结果意味着对于Beta 分布的随机变量，其均值（期望）可以用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231311169628707.24'/>来估计。此外，狄利克雷Dirichlet 分布也有类似的结论，即如果<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231312235227334.25'/>，同样可以证明有下述结论成立：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155231312887026070.26'/>

那什么是Dirichlet 分布呢？简单的理解Dirichlet 分布就是一组连续多变量概率分布，是多变量普遍化的beta分布。为了纪念德国数学家约翰·彼得·古斯塔夫·勒热纳·狄利克雷（Peter Gustav Lejeune Dirichlet）而命名。狄利克雷分布常作为贝叶斯统计的先验概率。


3 Dirichlet 分布
3.1 Dirichlet 分布
    根据wikipedia上的介绍，维度K ≥ 2（x1,x2…xK-1维，共K个）的狄利克雷分布在参数α1, ..., αK > 0上、基于欧几里得空间RK-1里的勒贝格测度有个概率密度函数，定义为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240414332330366.27'/>

其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240419717553514.30'/>相当于是多项beta函数
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641552404182645672.29'/>

    且<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240422584074653.31'/>

    此外，x1+x2+…+xK-1+xK=1，x1,x2…xK-1>0，且在(K-1)维的单纯形上，其他区域的概率密度为0。
    当然，也可以如下定义Dirichlet 分布
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240427272704404.33'/>

    其中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240429788762649.34'/>称为Dirichlet 分布的归一化系数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240431290405971.35'/>

   且根据Dirichlet分布的积分为1（概率的基本性质），可以得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155240433938538250.36'/>

3.2 Dirichlet-Multinomial 共轭
    下面，在2.2节问题2的基础上继续深入，引出问题3。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265255074788118.1'/>，
排序后对应的顺序统计量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265256651524124.2'/>，
问<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265257489886146.3'/>的联合分布是什么？

    为了简化计算，取x3满足x1+x2+x3=1,但只有x1,x2是变量，如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265261057547216.4'/>

    从而有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265262453871123.5'/>

    于是我们得到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265267979274685.6'/>的联合分布为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415526526886519885.7'/>

    观察上述式子的最终结果，可以看出上面这个分布其实就是3维形式的 Dirichlet 分布
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265272189207353.8'/>

    令<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265272917507727.9'/>，于是分布密度可以写为
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265277366683357.10'/>

    这个就是一般形式的3维 Dirichlet 分布，即便<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265290079110244.11'/>延拓到非负实数集合，以上概率分布也是良定义的。

    将Dirichlet分布的概率密度函数取对数，绘制对称Dirichlet分布的图像如下图所示（截取自wikipedia上）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265290955067777.gif'/>

    上图中，取K=3，也就是有两个独立参数x1,x2，分别对应图中的两个坐标轴，第三个参数始终满足x3=1-x1-x2且α1=α2=α3=α，图中反映的是参数α从α=(0.3, 0.3, 0.3)变化到(2.0, 2.0, 2.0)时的概率对数值的变化情况。

    为了论证Dirichlet分布是多项式分布的共轭先验概率分布，下面咱们继续在上述问题3的基础上再进一步，提出问题4。

① 问题4 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265317275584434.13'/>，排序后对应的顺序统计量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265318230535687.14'/>
② 令<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265319252084598.15'/>,<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265319879551800.16'/>,③<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265320814461814.17'/>（此处的p3非变量，只是为了表达方便），现在要猜测<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265321669607942.18'/>；
③<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265332219024969.19'/>，Yi中落到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265333267422880.20'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265333853453858.21'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265335340907058.22'/>三个区间的个数分别为 m1,m2,m3，m=m1+m2+m3；
④问后验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265338581905745.23'/>的分布是什么。

   为了方便讨论，记<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265346181281799.24'/>，及<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265348350761128.25'/>，根据已知条件“<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265350124385705.26'/>，Yi中落到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265351269767948.27'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265351725235046.28'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265352645486574.29'/>三个区间的个数分别为 m1,m2”，可得<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265358817258273.30'/>、<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415526535953785854.31'/>分别是这m+n个数中第<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265360948108935.32'/>大、第<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265361659953478.33'/>大的数。于是，后验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265362898411793.34'/>应该为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265363849875110.35'/>，即一般化的形式表示为：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265364434267239.36'/>。

    同样的，按照贝叶斯推理的逻辑，可将上述过程整理如下：

①我们要猜测参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265390663085151.1'/>，其先验分布为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265391457469294.2'/>；
②数据Yi落到三个区间<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265392382060271.3'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265393093165149.4'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265393692361971.5'/>的个数分别为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265400758288001.6'/>，所以<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415526540295154371.7'/>服从多项分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265405353911281.8'/>
③在给定了来自数据提供的知识<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415526540662937979.9'/>后，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265407534466969.10'/>的后验分布变为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265408067825039.11'/>

    上述贝叶斯分析过程的直观表述为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265412656626144.12'/>

    令<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265415634287599.13'/>，可把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265416576979635.14'/>从整数集合延拓到实数集合，从而得到更一般的表达式如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265418255923943.15'/>

    针对于这种观测到的数据符合多项分布，参数的先验分布和后验分布都是Dirichlet 分布的情况，就是Dirichlet-Multinomial 共轭。换言之，至此已经证明了Dirichlet分布的确就是多项式分布的共轭先验概率分布。

    意味着，如果我们为多项分布的参数p选取的先验分布是Dirichlet分布，那么以p为参数的多项分布用贝叶斯估计得到的后验分布仍然服从Dirichlet分布。

    进一步，一般形式的Dirichlet 分布定义如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265423696979493.16'/>

    而对于给定的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265424520230749.17'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265430564190441.18'/>，其多项分布为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265429665098453.19'/>

    结论是：Dirichlet分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265429077652257.20'/>和多项分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155265428574395056.21'/>是共轭关系。

4 主题模型LDA
 在开始下面的旅程之前，先来总结下我们目前所得到的最主要的几个收获：
通过上文的第2.2节，我们知道beta分布是二项式分布的共轭先验概率分布：
“对于非负实数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535172306510478.jpg'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351721212669411.png'/>，我们有如下关系
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351725130307553.png'/>
其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351776279339979.png'/>对应的是二项分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351777836965389.png'/>的计数。针对于这种观测到的数据符合二项分布，参数的先验分布和后验分布都是Beta分布的情况，就是Beta-Binomial 共轭。”

通过上文的3.2节，我们知道狄利克雷分布（Dirichlet分布）是多项式分布的共轭先验概率分布：
“ 把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351782973597967.png'/>从整数集合延拓到实数集合，从而得到更一般的表达式如下：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351786241796470.png'/>
 针对于这种观测到的数据符合多项分布，参数的先验分布和后验分布都是Dirichlet 分布的情况，就是 Dirichlet-Multinomial 共轭。 ”

以及贝叶斯派思考问题的固定模式：
先验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351796643995114.png'/>+ 样本信息<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351798095493072.png'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351798691125902.png'/>后验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351799854779787.png'/>

    上述思考模式意味着，新观察到的样本信息将修正人们以前对事物的认知。换言之，在得到新的样本信息之前，人们对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351804248040092.png'/>的认知是先验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351805677607437.png'/>，在得到新的样本信息<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351806970884361.png'/>后，人们对的认知为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351807977911931.png'/>。
顺便提下频率派与贝叶斯派各自不同的思考方式：
频率派把需要推断的参数θ看做是固定的未知常数，即概率虽然是未知的，但最起码是确定的一个值，同时，样本X 是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本X 的分布；
而贝叶斯派的观点则截然相反，他们认为待估计的参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351831867203184.png'/>是随机变量，服从一定的分布，而样本X 是固定的，由于样本是固定的，所以他们重点研究的是参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351834420951277.png'/>的分布。

  OK，在杀到终极boss——LDA模型之前，再循序渐进理解基础模型：Unigram model、mixture of unigrams model，以及跟LDA最为接近的pLSA模型。

   为了方便描述，首先定义一些变量：
1. <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535185486898741.png'/>表示词，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351858832767877.png'/>表示所有单词的个数（固定值）
2 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351862485749542.png'/>表示主题，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351865867996166.png'/>是主题的个数（预先给定，固定值）
3 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351869938370218.png'/>表示语料库，其中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535187439814166.png'/>是语料库中的文档数（固定值）
4 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351878044686506.png'/>表示文档，其中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351885884888045.png'/>表示一个文档中的词数（随机变量）

4.1 各个基础模型
4.1.1 Unigram model
对于文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415536018804693961.png'/>，用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351898332010957.png'/>表示词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351901293720939.png'/>的先验概率，生成文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351908620697437.png'/>的概率为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351912492396105.png'/>
 
其图模型为（图中被涂色的w表示可观测变量，N表示一篇文档中总共N个单词，M表示M篇文档）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351917291347195.png'/>
 
 或为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351920664655666.png'/>
 
unigram model假设文本中的词服从Multinomial分布，而我们已经知道Multinomial分布的先验分布为Dirichlet分布。
 上图中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351925688181359.png'/>表示在文本中观察到的第n个词，n∈[1,N]表示该文本中一共有N个单词。加上方框表示重复，即一共有N个这样的随机变量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351932891807587.png'/>。其中，p和α是隐含未知变量：

p是词服从的Multinomial分布的参数
α是Dirichlet分布（即Multinomial分布的先验分布）的参数。

一般α由经验事先给定，p由观察到的文本中出现的词学习得到，表示文本中出现每个词的概率。

4.1.2 Mixture of unigrams model
    该模型的生成过程是：给某个文档先选择一个主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351952988658030.png'/>,再根据该主题生成文档，该文档中的所有词都来自一个主题。假设主题有<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351960621703324.png'/>，生成文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351965060615535.png'/>的概率为：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351970337977230.png'/>

其图模型为（图中被涂色的w表示可观测变量，未被涂色的z表示未知的隐变量，N表示一篇文档中总共N个单词，M表示M篇文档）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351974151443443.png'/>

4.2 PLSA模型
    啊哈，长征两万五，经过前面这么长的铺垫，终于快要接近LDA模型了！因为跟LDA模型最为接近的便是下面要阐述的这个pLSA模型，理解了pLSA模型后，到LDA模型也就一步之遥——给pLSA加上贝叶斯框架，便是LDA。

4.2.1 pLSA模型下生成文档
    OK，在上面的Mixture of unigrams model中，我们假定一篇文档只有一个主题生成，可实际中，一篇文章往往有多个主题，只是这多个主题各自在文档中出现的概率大小不一样。比如介绍一个国家的文档中，往往会分别从教育、经济、交通等多个主题进行介绍。那么在pLSA中，文档是怎样被生成的呢？

    假设你要写M篇文档，由于一篇文档由各个不同的词组成，所以你需要确定每篇文档里每个位置上的词。

    再假定你一共有K个可选的主题，有V个可选的词，咱们来玩一个扔骰子的游戏。

1. 假设你每写一篇文档会制作一颗K面的“文档-主题”骰子（扔此骰子能得到K个主题中的任意一个），和K个V面的“主题-词项” 骰子（每个骰子对应一个主题，K个骰子对应之前的K个主题，且骰子的每一面对应要选择的词项，V个面对应着V个可选的词）。

比如可令K=3，即制作1个含有3个主题的“文档-主题”骰子，这3个主题可以是：教育、经济、交通。然后令V = 3，制作3个有着3面的“主题-词项”骰子，其中，教育主题骰子的3个面上的词可以是：大学、老师、课程，经济主题骰子的3个面上的词可以是：市场、企业、金融，交通主题骰子的3个面上的词可以是：高铁、汽车、飞机。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351982937840324.png'/>

2. 每写一个词，先扔该“文档-主题”骰子选择主题，得到主题的结果后，使用和主题结果对应的那颗“主题-词项”骰子，扔该骰子选择要写的词。

 先扔“文档-主题”的骰子，假设（以一定的概率）得到的主题是教育，所以下一步便是扔教育主题筛子，（以一定的概率）得到教育主题筛子对应的某个词：大学。

 上面这个投骰子产生词的过程简化下便是：“先以一定的概率选取主题，再以一定的概率选取词”。事实上，一开始可供选择的主题有3个：教育、经济、交通，那为何偏偏选取教育这个主题呢？其实是随机选取的，只是这个随机遵循一定的概率分布。比如可能选取教育主题的概率是0.5，选取经济主题的概率是0.3，选取交通主题的概率是0.2，那么这3个主题的概率分布便是{教育：0.5，经济：0.3，交通：0.2}，我们把各个主题z在文档d中出现的概率分布称之为主题分布，且是一个多项分布。

同样的，从主题分布中随机抽取出教育主题后，依然面对着3个词：大学、老师、课程，这3个词都可能被选中，但它们被选中的概率也是不一样的。比如大学这个词被选中的概率是0.5，老师这个词被选中的概率是0.3，课程被选中的概率是0.2，那么这3个词的概率分布便是{大学：0.5，老师：0.3，课程：0.2}，我们把各个词语w在主题z下出现的概率分布称之为词分布，这个词分布也是一个多项分布。

所以，选主题和选词都是两个随机的过程，先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后从该教育主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。

3. 最后，你不停的重复扔“文档-主题”骰子和”主题-词项“骰子，重复N次（产生N个词），完成一篇文档，重复这产生一篇文档的方法M次，则完成M篇文档。
    
上述过程抽象出来即是PLSA的文档生成模型。在这个过程中，我们并未关注词和词之间的出现顺序，所以pLSA是一种词袋方法。具体说来，该模型假设一组共现(co-occurrence)词项关联着一个隐含的主题类别<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351998213382297.png'/>。同时定义：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352002150071997.png'/>表示海量文档中某篇文档被选中的概率。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352006183724999.png'/>表示词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352009676765455.png'/>在给定文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352013258884194.png'/>中出现的概率。

怎么计算得到呢？针对海量文档，对所有文档进行分词后，得到一个词汇列表，这样每篇文档就是一个词语的集合。对于每个词语，用它在文档中出现的次数除以文档中词语总的数目便是它在文档中出现的概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352019545721319.png'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352024270338597.png'/>表示具体某个主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352029443891821.png'/>在给定文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352033453911092.png'/>下出现的概率。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352039853393421.png'/>表示具体某个词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352043684085659.png'/>在给定主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352047799253493.png'/>下出现的概率，与主题关系越密切的词，其条件概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535205203254211.png'/>越大。
 
利用上述的第1、3、4个概率，我们便可以按照如下的步骤得到“文档-词项”的生成模型：

1.按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352062899758278.png'/>选择一篇文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352064564521102.png'/>
2.选定文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535206908447580.png'/>后，从主题分布中按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155360862879631619.png'/>选择一个隐含的主题类别<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352074435875593.png'/>
3.选定<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352077286470568.png'/>后，从词分布中按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352080024544288.png'/>
选择一个词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352082182451662.png'/>
  
所以pLSA中生成文档的整个过程便是选定文档生成主题，确定主题生成词。
  

4.2.1 根据文档反推其主题分布
反过来，既然文档已经产生，那么如何根据已经产生好的文档反推其主题呢？这个利用看到的文档推断其隐藏的主题（分布）的过程（其实也就是产生文档的逆过程），便是主题建模的目的：自动地发现文档集中的主题（分布）。

    换言之，人类根据文档生成模型写成了各类文章，然后丢给了计算机，相当于计算机看到的是一篇篇已经写好的文章。现在计算机需要根据一篇篇文章中看到的一系列词归纳出当篇文章的主题，进而得出各个主题各自不同的出现概率：主题分布。即文档d和单词w是可被观察到的，但主题z却是隐藏的。

    如下图所示（图中被涂色的d、w表示可观测变量，未被涂色的z表示未知的隐变量，N表示一篇文档中总共N个单词，M表示M篇文档）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352089953160215.png'/>

上图中，文档d和词w是我们得到的样本（样本随机，参数虽未知但固定，所以pLSA属于频率派思想。区别于下文要介绍的LDA中：样本固定，参数未知但不固定，是个随机变量，服从一定的分布，所以LDA属于贝叶斯派思想），可观测得到，所以对于任意一篇文档，其<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352102141399349.png'/>是已知的。
 
从而可以根据大量已知的文档-词项信息<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352105911187250.png'/>，训练出文档-主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352109183646967.png'/>和主题-词项<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352112551770813.png'/>，如下公式所示：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352116356129118.png'/>

 故得到文档中每个词的生成概率为：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352119519512651.png'/>

由于<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352126947332565.png'/>可事先计算求出，而<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352131773110152.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535213557938318.png'/>未知，所以<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352138829704316.png'/>就是我们要估计的参数（值），通俗点说，就是要最大化这个θ。

  用什么方法进行估计呢，常用的参数估计方法有极大似然估计MLE、最大后验证估计MAP、贝叶斯估计等等。因为该待估计的参数中含有隐变量z，所以我们可以考虑EM算法。

4.2.1.1 EM算法的简单介绍
    EM算法，全称为Expectation-maximization algorithm，为期望最大算法，其基本思想是：首先随机选取一个值去初始化待估计的值<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352147339279118.png'/>，然后不断迭代寻找更优的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352150834369608.png'/>使得其似然函数likelihood<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352154639134153.png'/>比原来的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352157815713092.png'/>要大。换言之，假定现在得到了<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352161771937404.png'/>，想求<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155360898816678576.png'/>，使得
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352173565268236.png'/>
 
EM的关键便是要找到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352178841278499.png'/>的一个下界<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352185797295694.png'/>（注：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352189266615936.png'/>，其中，X表示已经观察到的随机变量），然后不断最大化这个下界，通过不断求解下界<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535219387766440.png'/>的极大化，从而逼近要求解的似然函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352197891781163.png'/>。
 
所以EM算法的一般步骤为：
1. 随机选取或者根据先验知识初始化<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352204413956527.png'/>；
2. 不断迭代下述两步
①给出当前的参数估计<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352209568676129.png'/>，计算似然函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352211731064424.png'/>的下界<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535221606467251.png'/>
②重新估计参数θ，即求<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535222196165359.png'/>，使得<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535222814050175.png'/>
3. 上述第二步后，如果<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352230788002509.png'/>收敛（即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352233865245342.png'/>收敛）则退出算法，否则继续回到第二步。

上述过程好比在二维平面上，有两条不相交的曲线，一条曲线在上（简称上曲线<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352239311710575.png'/>），一条曲线在下（简称下曲线<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352243013947692.png'/>），下曲线为上曲线的下界。现在对上曲线未知，只已知下曲线，为了求解上曲线的最高点，我们试着不断增大下曲线，使得下曲线不断逼近上曲线，下曲线在某一个点达到局部最大值并与上曲线在这点的值相等，记录下这个值，然后继续增大下曲线，寻找下曲线上与上曲线上相等的值，迭代到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352246021459246.png'/>收敛（即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352250118457515.png'/>收敛）停止，从而利用当前下曲线上的局部最大值当作上曲线的全局最大值（换言之，EM算法不保证一定能找到全局最优值）。如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352254171513644.png'/>

  以下是详细介绍。

  假定有训练集<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352259269706536.png'/>，包含m个独立样本，希望从中找到该组数据的模型p(x,z)的参数。   

  然后通过极大似然估计建立目标函数--对数似然函数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352263149431888.png'/>

  这里，z是隐随机变量，直接找到参数的估计是很困难的。我们的策略是建立<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352267759665504.png'/>的下界，并且求该下界的最大值；重复这个过程，直到收敛到局部最大值。

    令Qi是z的某一个分布，Qi≥0，且结合Jensen不等式，有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352271697349975.png'/>

  为了寻找尽量紧的下界，我们可以让使上述等号成立，而若要让等号成立的条件则是：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352274895011896.png'/>
 
 换言之，有以下式子成立：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535227756859447.png'/>，且由于有：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535228017333416.png'/>

  所以可得：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352283565659113.png'/>
 最终得到EM算法的整体框架如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352286271375942.png'/>

OK，EM算法还会在本博客后面的博文中具体阐述。接下来，回到pLSA参数的估计问题上。

4.2.1.2 EM算法估计pLSA的两未知参数
    首先尝试从矩阵的角度来描述待估计的两个未知变量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352291631971718.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352292434633816.png'/>。
假定用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352297030254289.png'/>表示词表<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352301189888408.png'/>在主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352302913692625.png'/>上的一个多项分布，则<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352304964410693.png'/>可以表示成一个向量，每个元素<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352308442000767.png'/>表示词项<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352311344378316.png'/>出现在主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352313382645402.png'/>中的概率，即
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352317021943030.png'/>

用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352320939984026.png'/>表示所有主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352324316569651.png'/>在文档上的一个多项分布，则<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352333393092979.png'/>可以表示成一个向量，每个元素<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352336563166032.png'/>表示主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352338527941490.png'/>出现在文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352340930640951.png'/>中的概率，即
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535234352433311.png'/>
   
  这样，巧妙的把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352348486881632.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352350645765621.png'/>转换成了两个矩阵。换言之，最终我们要求解的参数是这两个矩阵：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352353073972994.png'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352355184296622.png'/>

 由于词和词之间是相互独立的，所以整篇文档N个词的分布为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352357788773508.png'/>
  
  再由于文档和文档之间也是相互独立的，所以整个语料库中词的分布为（整个语料库M篇文档，每篇文档N个词）：
  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352361858829873.png'/>

  其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352364935525345.png'/>表示词项<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535236667245054.png'/>在文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352368213803366.png'/>中的词频，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352371760112539.png'/>表示文档di中词的总数，显然有<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352374898103137.png'/>。
从而得到整个语料库的词分布的对数似然函数（下述公式中有个小错误，正确的应该是：N为M，M为
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352379054242048.png'/>

 现在，我们需要最大化上述这个对数似然函数来求解参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352381489436443.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352385566987101.png'/>。对于这种含有隐变量的最大似然估计，可以使用EM算法。EM算法，分为两个步骤：先E-step，后M-step。

E-step：假定参数已知，计算此时隐变量的后验概率。
    利用贝叶斯法则，可以得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352389044058754.png'/>

M-step：带入隐变量的后验概率，最大化样本分布的对数似然函数，求解相应的参数。

 观察之前得到的对数似然函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352394396615182.png'/>的结果，由于文档长度<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352396955817109.png'/>可以单独计算，所以去掉它不影响最大化似然函数。此外，根据E-step的计算结果，把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352399941382410.png'/>，于是我们只要最大化下面这个函数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535240327201289.png'/>即可（下述公式中有个小错误，正确的应该是：N为M，M为N）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352406233499612.png'/>
 
这是一个多元函数求极值问题，并且已知有如下约束条件（下述公式中有个小错误，正确的应该是：M为N）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352409455110165.png'/>

熟悉凸优化的朋友应该知道，一般处理这种带有约束条件的极值问题，常用的方法便是拉格朗日乘数法，即通过引入拉格朗日乘子将约束条件和多元（目标）函数融合到一起，转化为无约束条件的极值问题。

 这里我们引入两个拉格朗日乘子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352413738698728.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352416366512507.png'/>，从而写出拉格朗日函数（下述公式中有个小错误，正确的应该是：N为M，M为N）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352420359093828.png'/>

因为我们要求解的参数是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352422618590365.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352424574090968.png'/>，所以分别对<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352426669008186.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352427787047989.png'/>求偏导，然后令偏导结果等于0，得到（下述公式中有个小错误，正确的应该是：N为M，M为N）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535243064050167.png'/>

  消去拉格朗日乘子，最终可估计出参数和（下述公式中有个小错误，正确的应该是：N为M，M为N）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352434487426261.png'/>

综上，在pLSA中：
1.由于<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352440312429474.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352442333506579.png'/>未知，所以我们用EM算法去估计<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352445692171967.png'/>这个参数的值。
2.而后，用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352449631508144.png'/>表示词项<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535245186332266.png'/>出现在主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352453435224270.png'/>中的概率，即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352456134811955.png'/>，用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352457840200732.png'/>表示主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352459424484384.png'/>出现在文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352460946637132.png'/>中的概率，即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352463463822127.png'/>，从而把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535246568171006.png'/>转换成了“主题-词项”矩阵Φ（主题生成词），把<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352467848466871.png'/>转换成了“文档-主题”矩阵Θ（文档生成主题）。
3.最终求解出<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352471759175481.png'/>、<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155352473630066744.png'/>。

4.3 LDA模型
    
事实上，理解了pLSA模型，也就差不多快理解了LDA模型，因为LDA就是在pLSA的基础上加层贝叶斯框架，即LDA就是pLSA的贝叶斯版本（正因为LDA被贝叶斯化了，所以才需要考虑历史先验知识，才加的两个先验参数）。

4.3.1 pLSA跟LDA的对比：生成文档与参数估计

    在pLSA模型中，我们按照如下的步骤得到“文档-词项”的生成模型：
1.按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359322481186453.png'/>选择一篇文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359323551097493.png'/>
2.选定文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359325085150489.png'/>后，确定文章的主题分布
3.从主题分布中按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359328613609280.png'/>选择一个隐含的主题类别<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359329788656813.png'/>
4.选定<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359331176906780.png'/>后，确定主题下的词分布
5.从词分布中按照概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535933335424985.png'/>选择一个词 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359334640009226.png'/>”

下面，咱们对比下本文开头所述的LDA模型中一篇文档生成的方式是怎样的：

1.按照先验概率<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359337712851481.png'/>选择一篇文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359338815076728.png'/>
2.从狄利克雷分布（即Dirichlet分布）<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359341151542134.png'/>中取样生成文档 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359342523720499.png'/>的主题分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359344162706964.png'/>，换言之，主题分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359345336716218.png'/>由超参数为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359347046693480.png'/>的Dirichlet分布生成
3.从主题的多项式分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359349648689986.png'/>中取样生成文档<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359351216238589.png'/>第 j 个词的主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359354911479692.png'/>
4.从狄利克雷分布（即Dirichlet分布）<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359359147094570.png'/>中取样生成主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359360763058204.png'/>对应的词语分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359363613691817.png'/>，换言之，词语分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359364720601037.png'/>由参数为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359366134741704.png'/>的Dirichlet分布生成
5.从词语的多项式分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359368732723273.png'/>中采样最终生成词语<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359371844169016.png'/>
 
从上面两个过程可以看出，LDA在PLSA的基础上，为主题分布和词分布分别加了两个Dirichlet先验。

    继续拿之前讲解PLSA的例子进行具体说明。如前所述，在PLSA中，选主题和选词都是两个随机的过程，先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后从该主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359376162963529.png'/>

而在LDA中，选主题和选词依然都是两个随机的过程，依然可能是先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后再从该主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。

    那PLSA跟LDA的区别在于什么地方呢？区别就在于：

PLSA中，主题分布和词分布是唯一确定的，能明确的指出主题分布可能就是{教育：0.5，经济：0.3，交通：0.2}，词分布可能就是{大学：0.5，老师：0.3，课程：0.2}。
但在LDA中，主题分布和词分布不再唯一确定不变，即无法确切给出。例如主题分布可能是{教育：0.5，经济：0.3，交通：0.2}，也可能是{教育：0.6，经济：0.2，交通：0.2}，到底是哪个我们不再确定（即不知道），因为它是随机的可变化的。但再怎么变化，也依然服从一定的分布，即主题分布跟词分布由Dirichlet先验随机确定。
   
看到这，你可能凌乱了，你说面对多个主题或词，各个主题或词被抽中的概率不一样，所以抽取主题或词是随机抽取，还好理解。但现在你说主题分布和词分布本身也都是不确定的，这是怎么回事？没办法，谁叫Blei等人“强行”给PLSA安了个贝叶斯框架呢，正因为LDA是PLSA的贝叶斯版本，所以主题分布跟词分布本身由先验知识随机给定。

    进一步，你会发现：
pLSA中，主题分布和词分布确定后，以一定的概率（<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359387526978616.png'/>、<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359390838572878.png'/>）分别选取具体的主题和词项，生成好文档。而后根据生成好的文档反推其主题分布、词分布时，最终用EM算法（极大似然估计思想）求解出了两个未知但固定的参数的值：<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641553593940883943.png'/>（由<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359396128718352.png'/>转换而来）和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359399361339549.png'/>（由<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359400856737205.png'/>转换而来）。

文档d产生主题z的概率，主题z产生单词w的概率都是两个固定的值。

举个文档d产生主题z的例子。给定一篇文档d，主题分布是一定的，比如{ P(zi|d), i = 1,2,3 }可能就是{0.4,0.5,0.1}，表示z1、z2、z3，这3个主题被文档d选中的概率都是个固定的值：P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1，如下图所示（图截取自沈博PPT上）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359406378585932.png'/>

但在贝叶斯框架下的LDA中，我们不再认为主题分布（各个主题在文档中出现的概率分布）和词分布（各个词语在某个主题下出现的概率分布）是唯一确定的（而是随机变量），而是有很多种可能。但一篇文档总得对应一个主题分布和一个词分布吧，怎么办呢？LDA为它们弄了两个Dirichlet先验参数，这个Dirichlet先验为某篇文档随机抽取出某个主题分布和词分布。
文档d产生主题z（准确的说，其实是Dirichlet先验为文档d生成主题分布Θ，然后根据主题分布Θ产生主题z）的概率，主题z产生单词w的概率都不再是某两个确定的值，而是随机变量。
还是再次举下文档d具体产生主题z的例子。给定一篇文档d，现在有多个主题z1、z2、z3，它们的主题分布{ P(zi|d), i = 1,2,3 }可能是{0.4,0.5,0.1}，也可能是{0.2,0.2,0.6}，即这些主题被d选中的概率都不再认为是确定的值，可能是P(z1|d) = 0.4、P(z2|d) = 0.5、P(z3|d) = 0.1，也有可能是P(z1|d) = 0.2、P(z2|d) = 0.2、P(z3|d) = 0.6等等，而主题分布到底是哪个取值集合我们不确定（为什么？这就是贝叶斯派的核心思想，把未知参数当作是随机变量，不再认为是某一个确定的值），但其先验分布是dirichlet 分布，所以可以从无穷多个主题分布中按照dirichlet 先验随机抽取出某个主题分布出来。如下图所示（图截取自沈博PPT上）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359413518476943.png'/>

换言之，LDA在pLSA的基础上给这两参数（<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359415874805605.png'/>、<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359417519361279.png'/>加了两个先验分布的参数（贝叶斯化）：一个主题分布的先验分布Dirichlet分布
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359419613790061.png'/>，和一个词语分布的先验分布Dirichlet分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359421690163061.png'/>。

    综上，LDA真的只是pLSA的贝叶斯版本，文档生成后，两者都要根据文档去推断其主题分布和词语分布（即两者本质都是为了估计给定文档生成主题，给定主题生成词语的概率），只是用的参数推断方法不同，在pLSA中用极大似然估计的思想去推断两未知的固定参数，而LDA则把这两参数弄成随机变量，且加入dirichlet先验。

    所以，pLSA跟LDA的本质区别就在于它们去估计未知参数所采用的思想不同，前者用的是频率派思想，后者用的是贝叶斯派思想。
    好比，我去一朋友家：

按照频率派的思想，我估计他在家的概率是1/2，不在家的概率也是1/2，是个定值。
而按照贝叶斯派的思想，他在家不在家的概率不再认为是个定值1/2，而是随机变量。比如按照我们的经验（比如当天周末），猜测他在家的概率是0.6，但这个0.6不是说就是完全确定的，也有可能是0.7。如此，贝叶斯派没法确切给出参数的确定值（0.3,0.4，0.6,0.7，0.8,0.9都有可能），但至少明白在哪个范围或哪些取值（0.6,0.7，0.8,0.9）更有可能，哪个范围或哪些取值（0.3,0.4） 不太可能。进一步，贝叶斯估计中，参数的多个估计值服从一定的先验分布，而后根据实践获得的数据（例如周末不断跑他家），不断修正之前的参数估计，从先验分布慢慢过渡到后验分布。
    
OK，相信已经解释清楚了。如果是在机器学习班上face-to-face，更好解释和沟通。

4.3.2 LDA生成文档过程的进一步理解

    上面说，LDA中，主题分布 —— 比如{ P(zi), i =1,2,3 }等于{0.4,0.5,0.1}或{0.2,0.2,0.6} —— 是由dirichlet先验给定的，不是根据文档产生的。所以，LDA生成文档的过程中，先从dirichlet先验中“随机”抽取出主题分布，然后从主题分布中“随机”抽取出主题，最后从确定后的主题对应的词分布中“随机”抽取出词。

    那么，dirichlet先验到底是如何“随机”抽取主题分布的呢？

    事实上，从dirichlet分布中随机抽取主题分布，这个过程不是完全随机的。为了说清楚这个问题，咱们得回顾下dirichlet分布。事实上，如果我们取3个事件的话，可以建立一个三维坐标系，类似xyz三维坐标系，这里，我们把3个坐标轴弄为p1、p2、p3，如下图所示：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359431634073119.png'/>

在这个三维坐标轴所划分的空间里，每一个坐标点(p1,p2,p3)就对应着一个主题分布，且某一个点(p1,p2,p3)的大小表示3个主题z1、z2、z3出现的概率大小（因为各个主题出现的概率和为1，所以p1+p2+p3 = 1，且p1、p2、p3这3个点最大取值为1）。比如(p1,p2,p3) = (0.4,0.5,0.1)便对应着主题分布{ P(zi), i =1,2,3 } = {0.4,0.5,0.1}。

    可以想象到，空间里有很多这样的点(p1,p2,p3)，意味着有很多的主题分布可供选择，那dirichlet分布如何选择主题分布呢？把上面的斜三角形放倒，映射到底面的平面上，便得到如下所示的一些彩图（3个彩图中，每一个点对应一个主题分布，高度代表某个主题分布被dirichlet分布选中的概率，且选不同的，dirichlet 分布会偏向不同的主题分布）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359435897199172.png'/>

我们来看上图中左边这个图，高度就是代表dirichlet分布选取某个坐标点(p1,p2,p3)（这个点就是一个主题分布）的概率大小。如下图所示，平面投影三角形上的三个顶点上的点：A=(0.9,0.05,0.05)、B=(0.05,0.9,0.05)、C=(0.05,0.05,0.9)各自对应的主题分布被dirichlet分布选中的概率值很大，而平面三角形内部的两个点：D、E对应的主题分布被dirichlet分布选中的概率值很小。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359441826947541.png'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359442527123381.png'/>
 
所以虽然说dirichlet分布是随机选取任意一个主题分布的，但依然存在着P(A) = P(B) = P(C) >> P(D) = P(E)，即dirichlet分布还是“偏爱”某些主题分布的。至于dirichlet分布的参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359444870976857.png'/>是如何决定dirichlet分布的形状的，可以从dirichlet分布的定义和公式思考。

此外，就算说“随机”选主题也是根据主题分布来“随机”选取，这里的随机不是完全随机的意思，而是根据各个主题出现的概率值大小来抽取。比如当dirichlet先验为文档d生成的主题分布{ P(zi), i =1,2,3 }是{0.4,0.5,0.1}时，那么主题z2在文档d中出现的概率便是0.5。所以，从主题分布中抽取主题，这个过程也不是完全随机的，而是按照各个主题出现的概率值大小进行抽取。

4.3.3 pLSA跟LDA的概率图对比

    接下来，对比下LDA跟pLSA的概率模型图模型，左图是pLSA，右图是LDA（右图不太规范，z跟w都得是小写， 其中，阴影圆圈表示可观测的变量，非阴影圆圈表示隐变量，箭头表示两变量间的条件依赖性conditional dependency，方框表示重复抽样，方框右下角的数字代表重复抽样的次数）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359454253285997.png'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359454968917172.png'/>

对应到上面右图的LDA，只有W / w是观察到的变量，其他都是隐变量或者参数，其中，Φ表示词分布，Θ表示主题分布，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359458595491102.png'/>是主题分布Θ的先验分布（即Dirichlet 分布）的参数，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535946061014763.png'/>是词分布Φ的先验分布（即Dirichlet 分布）的参数，N表示文档的单词总数，M表示文档的总数。

    所以，对于一篇文档d中的每一个单词，LDA根据先验知识<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359463185900462.png'/>确定某篇文档的主题分布θ，然后从该文档所对应的多项分布（主题分布）θ中抽取一个主题z，接着根据先验知识<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535946517204555.png'/>确定当前主题的词语分布ϕ，然后从主题z所对应的多项分布（词分布）ϕ中抽取一个单词w。然后将这个过程重复N次，就产生了文档d。

    换言之：
1.假定语料库中共有M篇文章，每篇文章下的Topic的主题分布是一个从参数为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359468293917039.png'/>的Dirichlet先验分布中采样得到的Multinomial分布，每个Topic下的词分布是一个从参数为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359470361659360.png'/>的Dirichlet先验分布中采样得到的Multinomial分布。
2.对于某篇文章中的第n个词，首先从该文章中出现的每个主题的Multinomial分布（主题分布）中选择或采样一个主题，然后再在这个主题对应的词的Multinomial分布（词分布）中选择或采样一个词。不断重复这个随机生成过程，直到M篇文章全部生成完成。

    综上，M 篇文档会对应于 M 个独立的 Dirichlet-Multinomial 共轭结构，K 个 topic 会对应于 K 个独立的 Dirichlet-Multinomial 共轭结构。

其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359475913343454.png'/>→θ→z 表示生成文档中的所有词对应的主题，显然<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535947715124992.png'/>→θ 对应的是Dirichlet 分布，θ→z 对应的是 Multinomial 分布，所以整体是一个 Dirichlet-Multinomial 共轭结构，如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359480762738464.png'/>

类似的，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359482582107994.png'/>→φ→w，容易看出， 此时β→φ对应的是 Dirichlet 分布， φ→w 对应的是 Multinomial 分布， 所以整体也是一个Dirichlet-Multinomial 共轭结构，如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359486495256002.png'/>

4.3.4 pLSA跟LDA参数估计方法的对比

    上面对比了pLSA跟LDA生成文档的不同过程，下面，咱们反过来，假定文档已经产生，反推其主题分布。那么，它们估计未知参数所采用的方法又有什么不同呢？

在pLSA中，我们使用EM算法去估计“主题-词项”矩阵Φ（由<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359492797010796.png'/>转换得到）和“文档-主题”矩阵Θ（由<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359494649312679.png'/>转换得到）这两个参数，而且这两参数都是个固定的值，只是未知，使用的思想其实就是极大似然估计MLE。

而在LDA中，估计Φ、Θ这两未知参数可以用变分(Variational inference)-EM算法，也可以用gibbs采样，前者的思想是最大后验估计MAP（MAP与MLE类似，都把未知参数当作固定的值），后者的思想是贝叶斯估计。贝叶斯估计是对MAP的扩展，但它与MAP有着本质的不同，即贝叶斯估计把待估计的参数看作是服从某种先验分布的随机变量。
关于贝叶斯估计再举个例子。假设中国的大学只有两种：理工科和文科，这两种学校数量的比例是1:1，其中，理工科男女比例7:1，文科男女比例1:7。某天你被外星人随机扔到一个校园，问你该学校可能的男女比例是多少？然后，你实际到该校园里逛了一圈，看到的5个人全是男的，这时候再次问你这个校园的男女比例是多少？

1.因为刚开始时，有先验知识，所以该学校的男女比例要么是7:1，要么是1:7，即P(比例为7:1) = 1/2，P(比例为1:7) = 1/2。
2.然后看到5个男生后重新估计男女比例，其实就是求P(比例7:1|5个男生）= ？，P(比例1:7|5个男生) = ？
3.用贝叶斯公式<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359514141108834.png'/>可得：P(比例7:1|5个男生) = P(比例7:1)*P(5个男生|比例7:1) / P(5个男生)，P(5个男生)是5个男生的先验概率，与学校无关，所以是个常数；类似的，P(比例1:7|5个男生) = P((比例1:7)*P(5个男生|比例1:7)/P(5个男生)。
4.最后将上述两个等式比一下，可得：P(比例7:1|5个男生)/P(比例1:7|5个男生) = {P((比例7:1)*P(5个男生|比例7:1)} / { P(比例1:7)*P(5个男生|比例1:7)}。

由于LDA把要估计的主题分布和词分布看作是其先验分布是Dirichlet分布的随机变量，所以，在LDA这个估计主题分布、词分布的过程中，它们的先验分布（即Dirichlet分布）事先由人为给定，那么LDA就是要去求它们的后验分布（LDA中可用gibbs采样去求解它们的后验分布，得到期望<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359522940867967.png'/>、<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359527491936344.png'/>）！

  此外，不厌其烦的再插一句，在LDA中，主题分布和词分布本身都是多项分布，而由上文3.2节可知“Dirichlet分布是多项式分布的共轭先验概率分布”，因此选择Dirichlet 分布作为它们的共轭先验分布。意味着为多项分布的参数p选取的先验分布是Dirichlet分布，那么以p为参数的多项分布用贝叶斯估计得到的后验分布仍然是Dirichlet分布。

4.3.5 LDA参数估计：Gibbs采样

    理清了LDA中的物理过程，下面咱们来看下如何学习估计。

    类似于pLSA，LDA的原始论文中是用的变分-EM算法估计未知参数，后来发现另一种估计LDA未知参数的方法更好，这种方法就是：Gibbs Sampling，有时叫Gibbs采样或Gibbs抽样，都一个意思。Gibbs抽样是马尔可夫链蒙特卡尔理论（MCMC）中用来获取一系列近似等于指定多维概率分布（比如2个或者多个随机变量的联合概率分布）观察样本的算法。

    OK，给定一个文档集合，w是可以观察到的已知变量，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359532295718453.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359533165831980.png'/>是根据经验给定的先验参数，其他的变量z，θ和φ都是未知的隐含变量，需要根据观察到的变量来学习估计的。根据LDA的图模型，可以写出所有变量的联合分布：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359536591356059.png'/>

注：上述公式中及下文中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359542072756136.png'/>等价上文中定义的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359543426315732.png'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359546885021084.png'/>等价于上文中定义的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359548691118426.png'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359551783463011.png'/>等价于上文中定义的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359553751938575.png'/>，等价于上文中定义的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359555428029823.png'/>。

因为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359557999852117.png'/>产生主题分布θ，主题分布θ确定具体主题，且<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359559851337359.png'/>产生词分布φ、词分布φ确定具体词，所以上述式子等价于下述式子所表达的联合概率分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359563427448219.png'/>：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359565643480339.png'/>

其中，第一项因子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359570837415527.png'/>表示的是根据确定的主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359573958414593.png'/>和词分布的先验分布参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359576639474534.png'/>
采样词的过程，第二项因子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359580311650615.png'/>是根据主题分布的先验分布参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359582066245634.png'/>采样主题的过程，这两项因子是需要计算的两个未知参数。

    由于这两个过程是独立的，所以下面可以分别处理，各个击破。

第一个因子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359585480164890.png'/>，可以根据确定的主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359586943966272.png'/>和从先验分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359588773349861.png'/>取样得到的词分布Φ产生：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359590962076577.png'/>

   由于样本中的词服从参数为主题<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359618289654499.png'/>的独立多项分布，这意味着可以把上面对词的乘积分解成分别对主题和对词的两层乘积：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359593565890753.png'/>

其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359596066791425.png'/>是词 t 在主题 k 中出现的次数。

    回到第一个因子上来。目标分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359597966127928.png'/>需要对词分布Φ积分，且结合我们之前在3.1节定义的Dirichlet 分布的归一化系数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359611537715401.png'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359613776670830.png'/>

  可得：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359622645015940.png'/>

这个结果可以看作K个Dirichlet-Multinomial模型的乘积。
现在开始求第二个因子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359625338257227.png'/>。类似于<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359627036094269.png'/>的步骤，先写出条件分布，然后分解成两部分的乘积：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535962945646534.png'/>

其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359631142393470.png'/>表示的单词 i 所属的文档，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359633983813018.png'/>是主题 k 在文章 m 中出现的次数。

    对主题分布Θ积分可得：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359636767049375.png'/>

 综合第一个因子和第二个因子的结果，得到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359639459843092.png'/>的联合分布结果为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359644266955465.png'/>

接下来，有了联合分布<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359647047163864.png'/>，咱们便可以通过联合分布来计算在给定可观测变量 w 下的隐变量 z 的条件分布（后验分布）<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155361082156196393.png'/>来进行贝叶斯分析。

换言之，有了这个联合分布后，要求解第m篇文档中的第n个词（下标为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359654142524587.png'/>的词）的全部条件概率就好求了。

 先定义几个变量。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535965893140882.png'/>表示除去<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415535966138089608.png'/>的词，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359664740690253.png'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359666565248711.png'/>

然后，排除当前词的主题分配，即根据其他词的主题分配和观察到的单词来计算当前词主题的概率公式为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359668959930366.png'/>

勘误：考虑到<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359671922591402.png'/>，所以上述公式的第二行的分子，非p(w,z) *p(z)，而是p(w|z)*p(z)。

    且有：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359674945226332.png'/>

    最后一步，便是根据Markov链的状态获取主题分布的参数Θ和词分布的参数Φ。

    换言之根据贝叶斯法则和Dirichlet先验，以及上文中得到的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359681388661477.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359683243973640.png'/>自被分解成两部分乘积的结果，可以计算得到每个文档上Topic的后验分布和每个Topic下的词的后验分布分别如下（据上文可知：其后验分布跟它们的先验分布一样，也都是Dirichlet 分布）：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359685989285980.png'/>

其中，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359691132878833.png'/>是构成文档m的主题数向量，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359693536235563.png'/>是构成主题k的词项数向量。

  此外，别忘了上文中2.4节所述的Dirichlet的一个性质，如下：
“ 如果<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359696734950792.png'/>，同样可以证明有下述结论成立：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359699468234589.png'/>

即：如果<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359702552311065.png'/>，则<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359706273738100.png'/>中的任一元素<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359707051302300.png'/>的期望是：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359709377025673.png'/>

可以看出，超参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359713397744124.png'/>的直观意义就是事件先验的伪计数(prior pseudo-count)。 ”
    所以，最终求解的Dirichlet 分布期望为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359715726876809.png'/>

然后将<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359717666161966.png'/>和<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359724431736088.png'/>的结果代入之前得到的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359727179740137.png'/>的结果中，可得：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359729581510899.png'/>

    仔细观察上述结果，可以发现，式子的右半部分便是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359731996365266.png'/>，这个概率的值对应着<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359734227139103.png'/>的路径概率。如此，K 个topic 对应着K条路径，Gibbs Sampling 便在这K 条路径中进行采样，如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155359739411606863.png'/>

何等奇妙，就这样，Gibbs Sampling通过求解出主题分布和词分布的后验分布，从而成功解决主题分布和词分布这两参数未知的问题。

5 读者微评
 

    本文发表后，部分热心的读者在微博上分享了他们自己理解LDA的心得，也欢迎更多朋友分享你的理解心得（比如评论在本文下，或评论在微博上），从而在分享、讨论的过程中让更多人可以更好的理解：

@SiNZeRo：lda 如果用em就是 map估计了. lda本意是要去找后验分布 然后拿后验分布做bayesian分析. 比如theta的期望 . 而不是把先验作为正则化引入。最后一点gibbs sampling其实不是求解的过程 是去explore后验分布 去采样 用于求期望.
@研究者July：好问题好建议，这几天我陆续完善下！//@帅广应s：LDA这个东西该怎么用？可以用在哪些地方？还有就是Gibbs抽样的原理是什么？代码怎么实现？如果用EM来做，代码怎么实现？ LDA模型的变形和优化有哪些？LDA不适用于解决哪类的问题？总之，不明白怎么用，参数怎么调优？ 

@xiangnanhe：写的很好，4.1.3节中的那两个图很赞，非常直观的理解了LDA模型加了先验之后在学参数的时候要比PLSI更灵活；PLSI在学参数的过程中比较容易陷入local minimum然后overfitting。

@asker2：无论是pLSA中，还是LDA中，主题分布和词分布本身是固定的存在，但都未知。pLSA跟LDA的区别在于，去探索这两个未知参数的方法或思想不一样。pLSA是求到一个能拟合文本最好的参数（分布），这个值就认为是真实的参数。但LDA认为，其实我们没法去完全求解出主题分布、词分布到底是什么参数，我们只能把它们当成随机变量，通过缩小其方差（变化度）来尽量让这个随机变量变得更“确切”。换言之，我们不再求主题分布、词分布的具体值，而是通过这些分布生成的观测值（即实际文本）来反推分布的参数的范围，即在什么范围比较可能，在什么范围不太可能。所以，其实这就是一种贝叶斯分析的思想，虽然无法给出真实值具体是多少，但可以按照经验给一个相对合理的真实值服从的先验分布，然后从先验出发求解其后验分布。
..
 

6 参考文献与推荐阅读
1.Blei, David M.; Ng, Andrew Y.; Jordan, Michael I. Latent Dirichlet allocation（LDA原始论文）：http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf。
2 Blei. Probabilistic Topic Models：http://www.cs.princeton.edu/~blei/papers/Blei2012.pdf，一网友的翻译：http://www.cnblogs.com/siegfang/archive/2013/01/30/2882391.html；
3 一堆wikipedia，比如隐含狄利克雷分布LDA的wiki：http://zh.wikipedia.org/wiki/éå«çå©åé·åå¸，狄利克雷分布的wiki：http://zh.wikipedia.org/wiki/çå©åé·åå¸；
4.从贝叶斯方法谈到贝叶斯网络 ；
5.rickjin的LDA数学八卦（力荐，本文部分图片和公式来自于此文档）网页版：http://www.flickering.cn/tag/lda/，PDF版：http://emma.memect.com/t/9756da9a47744de993d8df13a26e04e38286c9bc1c5a0d2b259c4564c6613298/LDA；

6.Thomas Hofmann.Probabilistic Latent Semantic Indexing（pLSA原始论文）：http://cs.brown.edu/~th/papers/Hofmann-SIGIR99.pdf；
7.Gregor Heinrich.Parameter estimation for text analysis（关于Gibbs 采样最精准细致的论述）：http://www.arbylon.net/publications/text-est.pdf；
8.Probabilistic latent semantic analysis (pLSA)：http://blog.tomtung.com/2011/10/plsa/http://blog.tomtung.com/2011/10/plsa/。
9.《概率论与数理统计教程第二版 茆诗松等人著》，如果忘了相关统计分布，建议复习此书或此文第二部分；
10.《支持向量机通俗导论：理解SVM的三层境界》，第二部分关于拉格朗日函数的讨论；

11.机器学习班第11次课上，邹博讲EM & GMM的PPT：http://pan.baidu.com/s/1i3zgmzF；
12.机器学习班第12次课上，邹博讲主题模型LDA的PPT：http://pan.baidu.com/s/1jGghtQm；
13.主题模型之pLSA：http://blog.jqian.net/post/plsa.html；
14主题模型之LDA：http://blog.jqian.net/post/lda.html；
15.搜索背后的奥秘——浅谈语义主题计算：http://www.semgle.com/search-engine-algorithms-mystery-behind-
search-on-the-calculation-of-semantic-topic；

16.LDA的EM推导：http://www.cnblogs.com/hebin/archive/2013/04/25/3043575.html；
17.Machine Learning读书会第8期上，沈博讲主题模型的PPT：http://vdisk.weibo.com/s/zrFL6OXKgKMAf；
18.Latent Dirichlet Allocation （LDA）- David M.Blei：http://www.xperseverance.net/blogs/2012/03/17/；
19.用GibbsLDA做Topic Modeling：http://weblab.com.cityu.edu.hk/blog/luheng/2011/06/24/ç¨gibbsldaåtopic-modeling/#comment-87；
20.主题模型在文本挖掘中的应用：http://net.pku.edu.cn/~zhaoxin/Topic-model-xin-zhao-wayne.pdf；

21.二项分布和多项分布，beta分布的对比：http://www.cnblogs.com/wybang/p/3206719.html；
22.LDA简介：http://cos.name/2010/10/lda_topic_model/；
23.LDA的相关论文、工具库：http://site.douban.com/204776/widget/notes/12599608/note/287085506/；
24.一个网友学习LDA的心得：http://www.xuwenhao.com/2011/03/20/suggestions-for-programmers-to-learn-lda/；
25.http://blog.csdn.net/hxxiaopei/article/details/7617838；

26.主题模型LDA及其在微博推荐&广告算法中的应用：http://www.wbrecom.com/?p=136；
27.LDA发明人之一Blei 写的毕业论文：http://www.cs.princeton.edu/~blei/papers/Blei2004.pdf；
28.LDA的一个C实现：http://www.cs.princeton.edu/~blei/lda-c/index.html；
29.LDA的一些其它资料：http://www.xperseverance.net/blogs/2012/03/657/。
## 18.Word2Vec中为什么使用负采样（negtive sample）？
解析一
如七月在线推荐就业班的专家讲师李老师所言
负采样这个点引入word2vec非常巧妙，两个作用：
1.加速了模型计算
2.保证了模型训练的效果，其一 模型每次只需要更新采样的词的权重，不用更新所有的权重，那样会很慢，其二 中心词其实只跟它周围的词有关系，位置离着很远的词没有关系，也没必要同时训练更新，作者这点非常聪明。

解析二
下述解析来源于：https://zhuanlan.zhihu.com/p/29488930
1. 随机梯度下降法有什么问题？
通过对代价函数求权重的梯度，我们可以一次性对所有的参数<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415527417807244433.svg'/>进行优化，但是如果每次等全部计算完成再优化升级，我们将等待很长时间（对于很大的语料库来说）。

所以我们采用随机梯度下降（ Stochastic Gradient Descent），也就是说每次完成一次计算就进行升级。

但是，还有两个问题导致目前的模型效率低下！
第一个问题，我们每次只对窗口中出现的几个单词进行升级，但是在计算梯度的过程中，我们是对整个参数矩阵进行运算，这样参数矩阵中的大部分值都是0。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274196765063064.jpg'/>

计算效率低下！

第二个问题：我们使用的目标函数是softmax函数   
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274198388176499.svg'/>

我们观察分母，分母需要把窗口中所有单词的“得分”都算出来再求和，效率低下！

2. 使用负采样
负采样的核心思想是：计算目标单词和窗口中的单词的真实单词对“得分”，再加一些“噪声”，即词表中的随机单词和目标单词的“得分”。

真实单词对“得分”和“噪声”作为代价函数。
每次优化参数，只关注代价函数中涉及的词向量。

下面给出公式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274230878559348.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274232155160337.svg'/>

采用上述公式解决了之前说的两个问题：

我们仅对K个参数进行采样
我们放弃softmax函数，采用sigmoid函数，这样就不存在先求一遍窗口中所有单词的‘“得分”的情况了。

3. 计算梯度
既然代价函数已经更新了，那么我们需要对梯度进行更新。

首先考虑一下，我们想要求导的目标，也就是对谁求导？

答案是，对我们想要优化的参数求导，前面说了，负采样的目的是不需要对整个向量矩阵 U或 V 进行优化，而是仅对求代价过程中涉及的词向量进行优化，因此，求导对象是目标向量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274269955206683.svg'/>,窗口中的其他词向量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641552742710818869.svg'/>和负采样时随机选取的词向量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274272164152465.svg'/> 。

此篇文章关注的问题不是求导的过程，因此下面直接给出梯度：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274273898566277.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274275076208119.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155274276112212624.svg'/>
## 19.什么是TF-IDF算法？
TF-IDF(term frequency–inverse document frequency)是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。

TF-IDF有两层意思，一层是"词频"（Term Frequency，缩写为TF），另一层是"逆文档频率"（Inverse Document Frequency，缩写为IDF）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156630878613221494.jpg'/><img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156630878961416696.jpg'/>

假设我们现在有一片长文叫做《量化系统架构设计》词频高在文章中往往是停用词，“的”，“是”，“了”等，这些在文档中最常见但对结果毫无帮助、需要过滤掉的词，用TF可以统计到这些停用词并把它们过滤。当高频词过滤后就只需考虑剩下的有实际意义的词。

但这样又会遇到了另一个问题，我们可能发现"量化"、"系统"、"架构"这三个词的出现次数一样多。这是不是意味着，作为关键词，它们的重要性是一样的？事实上系统应该在其他文章比较常见，所以在关键词排序上，“量化”和“架构”应该排在“系统”前面，这个时候就需要IDF，IDF会给常见的词较小的权重，它的大小与一个词的常见程度成反比。

当有TF(词频)和IDF(逆文档频率)后，将这两个词相乘，就能得到一个词的TF-IDF的值。某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由大到小排序，排在最前面的几个词，就是该文章的关键词。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156630881464111037.jpg'/>

TF-IDF算法步骤

第一步，计算词频：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156630882773571227.jpg'/>
考虑到文章有长短之分，为了便于不同文章的比较，进行"词频"标准化。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415718241246797631.jpg'/>
第二步，计算逆文档频率：

这时，需要一个语料库（corpus），用来模拟语言的使用环境。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157182414255715037.jpg'/>
如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数。

第三步，计算TF-IDF：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157182415370411571.jpg'/>
可以看到，TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。所以，自动提取关键词的算法就很清楚了，就是计算出文档的每个词的TF-IDF值，然后按降序排列，取排在最前面的几个词。

优缺点
TF-IDF的优点是简单快速，而且容易理解。缺点是有时候用词频来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。如果要体现词的上下文结构，那么你可能需要使用word2vec算法来支持。

示例代码
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156630890881435667.jpg'/>

本题解析来源：https://zhuanlan.zhihu.com/p/31197209
## 20.请说说word2vec的简要理解
本题解析来源：https://blog.csdn.net/lilong117194/article/details/81979522?tdsourcetag=s_pcqq_aiomsg

在自然语言处理领域中，本文向量化是文本表示的一种重要方式。在当前阶段，对文本的大部分研究都是通过词向量化实现的，但同时也有一部分研究将句子作为文本处理的基本单元，也就是doc2vec和str2vec技术。

1. word2vec简介
大家很熟悉的词袋(bag of words)模型是最早的以词语为基本处理单元的文本向量化算法，所谓的词袋模型就是借助于词典把文本转化为一组向量，下面是两个简单的文本示例：

john likes to watch movies, mary likes too.
john also likes to watch football games.

现假设词典如下： 
{"john":1,"likes":2,"to":3,"watch":4, "movies":5,"also":6,"football":7,"games":8,"mary":9 "too":10} 
在这个自己构建的词典中，每个单词都有一个唯一的索引，那么上述的两个文本就可以基于这个暂时的词典来构建其文本的向量表示，如下： 
[1,2,1,1,1,0,0,0,1,1] 
[1,1,1,1,0,1,1,1,0,0] 

由此可以看出此向量的构建是根据该词在词典出现的次数而构成的，比如第一条文本中的”likes”,这个词在文本中出现了2次，所以基于词袋的文本向量是根据词出现的次数构建的。但是此向量与文本中单词出现的顺序没有关系，只是一种频率的表示，该方法容易实现，但是有很大的问题：
a)维数灾难：假如词典包含10000个单词，那么每个文本需要使用10000维的向量表示，那么向量的很多位置必定是0，如此稀疏的高维向量会严重影响计算速度。
b)这样构成的向量无法保存词序信息，而词序对于自然语言处理又是那么的重要。
c)存在语义鸿沟

例如：关于数据稀疏的问题 
自然语言处理经常把字词转为离散的单独的符号，也就是One-Hot Encoder。
杭州 [0,0,0,0,0,0,0,1,0,……，0,0,0,0,0,0,0]
上海 [0,0,0,0,1,0,0,0,0,……，0,0,0,0,0,0,0]
宁波 [0,0,0,1,0,0,0,0,0,……，0,0,0,0,0,0,0]
北京 [0,0,0,0,0,0,0,0,0,……，1,0,0,0,0,0,0]

比如上面的这个例子，在语料库中，杭州、上海、宁波、北京各对应一个向量，向量中只有一个值为1，其余都为0。但是使用One-Hot Encoder有以下问题。一方面，城市编码是随机的，向量之间相互独立，看不出城市之间可能存在的关联关系。其次，向量维度的大小取决于语料库中字词的多少。如果将世界所有城市名称对应的向量合为一个矩阵的话，那这个矩阵过于稀疏，并且会造成维度灾难。

现在随着互联网的发展，大量的无标注数据产生，此时的word2vec技术即是利用神经网络从大量的无标注的文本中提取有用的信息而产生的。

为什么说word2vec能提取有用的信息呢？ 
我们知道词语是表达语义的基本单元，而词袋模型只是简单的将词语符号化，举个不太恰当的比喻就是：现在有”一麻袋”的词语，而我们要处理的文本就像是从一个麻袋中无序得（不分先后顺序）抽出麻袋中所有的词，再查看文本中出现的个数，注意这里的从麻袋中抽取词的过程是无序的，也就是只是简单的统计文本中有没有出现该词和该词出现了几次，所以对于词袋模型，文本的语序特征就丧失了，也就丧失了语义的信息。

此时我们需要一个模型就是能在使文本向量化的同时也保留了词序的信息。分布式假说的提出就是解决了语义信息的问题。该方法的思想是：上下文相似的词，其语义也相似，随后就有了基于上下文分布表示词义的方法，这就是“词空间模型“。Word2Vec可以将One-Hot Encoder转化为低维度的连续值，也就是稠密向量，并且其中意思相近的词将被映射到向量空间中相近的位置。而使用神经网络可以灵活的对上下文进行建模，也因此成为用的比较多的方法。

2. 模型简介
one-hot向量作为word2vec的输入，通过word2vec训练低维词向量（word embedding） 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156820904924499143.png'/>
输入层：One-Hot Vector 
隐藏层：没有激活函数，也就是线性的单元。 
输出层：维度跟输入层的维度一样，用的是Softmax回归。 
我们要获取的dense vector其实就是Hidden Layer的输出单元。有的地方定为Input Layer和Hidden Layer之间的权重，其实说的是一回事。

下面用具体的例子看下： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156820906914753774.png'/>
可以看出： 
输入层：5个神经元 
隐藏层：3个神经元 
所以权重矩阵是5x3的大小，可以看出权重矩阵中的[10,12,19]和前向传播后[10,12,19]是一样的。

3. CBOW模式
word2vec主要分为CBOW（Continuous Bag of Words）和Skip-Gram两种模式。CBOW是从原始语句推测目标字词；而Skip-Gram正好相反，是从目标字词推测出原始语句。CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。

CBOW模型的理解： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156820922755035768.png'/>
CBOW模型结构图
1 输入层：上下文单词的onehot. {假设单词向量空间dim为V，也就是词典的大小。上下文单词个数为C}。
2 所有onehot分别乘以共享的输入权重矩阵W(V*N矩阵，N为自己设定的数，N也是隐藏层的神经元个数，初始化权重矩阵W）。
3 所得的向量 {因为是onehot所以为向量} 相加求平均作为隐层向量, size为1*N。
4 乘以输出权重矩阵W′W′(N*V)。
5 得到向量 (1*V) ，激活函数处理得到V-dim概率分布，概率最大的index所指示的单词为预测出的中间词(target word)。
6 与true label的onehot做比较，误差越小越好。

所以，需要定义loss function（一般为交叉熵代价函数），采用梯度下降算法更新W和W′W′。训练完毕后，输入层的每个单词与矩阵W相乘得到的向量的就是我们想要的词向量（word embedding），这个矩阵（所有单词的word embedding）也叫做look up table（其实这个look up table就是矩阵W自身），也就是说，任何一个单词的onehot乘以这个矩阵都将得到自己的词向量。有了look up table就可以免去训练过程直接查表得到单词的词向量了。

案例： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156820926741308823.png'/>

5. Skip-Gram模式
从直观上理解，Skip-Gram是给定input word来预测上下文。

接下来我们来看看如何训练我们的神经网络。假如我们有一个句子“The dog barked at the mailman”。

首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；

有了input word以后，我们再定义一个叫做skip_window的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设置skip_window=2，那么我们最终获得窗口中的词（包括input word在内）就是[‘The’, ‘dog’，’barked’, ‘at’]。skip_window=2代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小span=2x2=4。

另一个参数叫num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当skip_window=2，num_skips=2时，我们将会得到两组 (input word, output word) 形式的训练数据，即 (‘dog’, ‘barked’)，(‘dog’, ‘the’)。

神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词是output word的可能性。这句话有点绕，我们来看个栗子。第二步中我们在设置skip_window和num_skips=2的情况下获得了两组训练数据。假如我们先拿一组数据 (‘dog’, ‘barked’) 来训练神经网络，那么模型通过学习这个训练样本，会告诉我们词汇表中每个单词是“barked”的概率大小。

模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。举个栗子，如果我们向神经网络模型中输入一个单词“中国“，那么最终模型的输出概率中，像“英国”， ”俄罗斯“这种相关词的概率将远高于像”苹果“，”蝈蝈“非相关词的概率。因为”英国“，”俄罗斯“在文本中更大可能在”中国“的窗口中出现。我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。

面的图中给出了一些我们的训练样本的例子。我们选定句子“The quick brown fox jumps over lazy dog”，设定我们的窗口大小为2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。Training Samples（输入， 输出） 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156820979228136457.png'/>
我们的模型将会从每对单词出现的次数中习得统计结果。例如，我们的神经网络可能会得到更多类似（“中国“，”英国“）这样的训练样本对，而对于（”英国“，”蝈蝈“）这样的组合却看到的很少。因此，当我们的模型完成训练后，给定一个单词”中国“作为输入，输出的结果中”英国“或者”俄罗斯“要比”蝈蝈“被赋予更高的概率。

再次提醒，最终我们需要的是训练出来的权重矩阵。

5. 训练优化
此时注意到，这个训练过程的参数规模非常巨大。 
假设语料库中有30000个不同的单词，hidden layer取128，word2vec两个权值矩阵维度都是[30000,128]，在使用SGD对庞大的神经网络进行学习时，将是十分缓慢的。而且，你需要大量的训练数据来调整许多权重，避免过度拟合。数以百万计的重量数十亿倍的训练样本意味着训练这个模型将是一个野兽。 
一般来说，有两种加速算法：Hierarchical Softmax、Negative Sampling等方式来解决。

参考： 
https://blog.csdn.net/mylove0414/article/details/61616617 
https://blog.csdn.net/free356/article/details/79445895
## 21.如何理解Word2vec 之 Skip-Gram 模型
本题解析来源：https://zhuanlan.zhihu.com/p/27234078

什么是Word2Vec和Embeddings？
Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，它被大量地用在自然语言处理（NLP）中。那么它是如何帮助我们做自然语言处理呢？

Word2Vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。
本质上，Embedding其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。

我们从直观角度上来理解一下，cat这个单词和kitten属于语义上很相近的词，而dog和kitten则不是那么相近，iphone这个单词和kitten的语义就差的更远了。通过对词汇表中单词进行这种数值表示方式的学习（也就是将单词转换为词向量），能够让我们基于这样的数值进行向量化的操作从而得到一些有趣的结论。

比如说，如果我们对词向量kitten、cat以及dog执行这样的操作：kitten - cat + dog，那么最终得到的嵌入向量（embedded vector）将与puppy这个词向量十分相近。

第一部分
模型
Word2Vec模型中，主要有CBOW和Skip-Gram两种模型，从直观上理解，CBOW是给定上下文来预测input word，而Skip-Gram是给定input word来预测上下文。本篇文章仅讲解Skip-Gram模型。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821561640625665.png'/>
Skip-Gram模型的基础形式非常简单，为了更清楚地解释模型，我们先从最一般的基础模型来看Word2Vec（下文中所有的Word2Vec都是指Skip-Gram模型）。

Word2Vec模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。基于训练数据建模的过程，我们给它一个名字叫“Fake Task”，意味着建模并不是我们最终的目的。

上面提到的这种方法实际上会在无监督特征学习（unsupervised feature learning）中见到，最常见的就是自编码器（auto-encoder）：通过在隐层将输入进行编码压缩，继而在输出层将数据解码恢复初始状态，训练完成后，我们会将输出层“砍掉”，仅保留隐层。

The Fake Task
我们在上面提到，训练模型的真正目的是获得模型基于训练数据学得的隐层权重。为了得到这些权重，我们首先要构建一个完整的神经网络作为我们的“Fake Task”，后面再返回来看通过“Fake Task”我们如何间接地得到这些词向量。

接下来我们来看看如何训练我们的神经网络。假如我们有一个句子“The dog barked at the mailman（狗对邮递员吠叫）”。

首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；
有了input word以后，我们再定义一个叫做skip_window的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设置<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821596247104635.svg'/>，那么我们最终获得窗口中的词（包括input word在内）就是[&#39;The&#39;, &#39;dog&#39;，&#39;barked&#39;, &#39;at&#39;]。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821612511990114.svg'/>代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821615820640129.svg'/>。

另一个参数叫num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821618970912257.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821619542372541.svg'/>时，我们将会得到两组 (input word, output word) 形式的训练数据，即 (&#39;dog&#39;, &#39;barked&#39;)，(&#39;dog&#39;, &#39;the&#39;)。

神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词是output word的可能性。这句话有点绕，我们来看个栗子。

第二步中我们在设置skip_window和num_skips=2的情况下获得了两组训练数据。假如我们先拿一组数据 (&#39;dog&#39;, &#39;barked&#39;) 来训练神经网络，那么模型通过学习这个训练样本，会告诉我们词汇表中每个单词是“barked”的概率大小。模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。

举个栗子，如果我们向神经网络模型中输入一个单词“Soviet“，那么最终模型的输出概率中，像“Union”， ”Russia“这种相关词的概率将远高于像”watermelon“，”kangaroo“非相关词的概率。因为”Union“，”Russia“在文本中更大可能在”Soviet“的窗口中出现。

我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。下面的图中给出了一些我们的训练样本的例子。我们选定句子“The quick brown fox jumps over lazy dog”，设定我们的窗口大小为2（<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821622585857030.svg'/>），也就是说我们仅选输入词前后各两个词和输入词进行组合。

下图中，蓝色代表input word，方框内代表位于窗口内的单词。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821625740244978.png'/>
我们的模型将会从每对单词出现的次数中习得统计结果。例如，我们的神经网络可能会得到更多类似（“Soviet“，”Union“）这样的训练样本对，而对于（”Soviet“，”Sasquatch“）这样的组合却看到的很少。

因此，当我们的模型完成训练后，给定一个单词”Soviet“作为输入，输出的结果中”Union“或者”Russia“要比”Sasquatch“被赋予更高的概率。

模型细节
我们如何来表示这些单词呢？

首先，我们都知道神经网络只能接受数值输入，我们不可能把一个单词字符串作为输入，因此我们得想个办法来表示这些单词。最常用的办法就是基于训练文档来构建我们自己的词汇表（vocabulary）再对单词进行one-hot编码。

假设从我们的训练文档中抽取出10000个唯一不重复的单词组成词汇表。我们对这10000个单词进行one-hot编码，得到的每个单词都是一个10000维的向量，向量每个维度的值只有0或者1，假如单词ants在词汇表中的出现位置为第3个，那么ants的向量就是一个第三维度取值为1，其他维都为0的10000维的向量（<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821632794376147.svg'/>）。

还是上面的例子，“The dog barked at the mailman”，那么我们基于这个句子，可以构建一个大小为5的词汇表（忽略大小写和标点符号）：("the", "dog", "barked", "at", "mailman")，我们对这个词汇表的单词进行编号0-4。那么”dog“就可以被表示为一个5维向量[0, 1, 0, 0, 0]。

模型的输入如果为一个10000维的向量，那么输出也是一个10000维度（词汇表的大小）的向量，它包含了10000个概率，每一个概率代表着当前词是输入样本中output word的概率大小。

下图是我们神经网络的结构：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821635134131103.png'/>
隐层没有使用任何激活函数，但是输出层使用了sotfmax。

我们基于成对的单词来对神经网络进行训练，训练样本是 ( input word, output word ) 这样的单词对，input word和output word都是one-hot编码的向量。最终模型的输出是一个概率分布。

隐层

说完单词的编码和训练样本的选取，我们来看下我们的隐层。如果我们现在想用300个特征来表示一个单词（即每个词可以被表示为300维的向量）。那么隐层的权重矩阵应该为10000行，300列（隐层有300个结点）。
Google在最新发布的基于Google news数据集训练的模型中使用的就是300个特征的词向量。词向量的维度是一个可以调节的超参数（在Python的gensim包中封装的Word2Vec接口默认的词向量大小为100， window_size为5）。

看下面的图片，左右两张图分别从不同角度代表了输入层-隐层的权重矩阵。左图中每一列代表一个10000维的词向量和隐层单个神经元连接的权重向量。从右边的图来看，每一行实际上代表了每个单词的词向量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821637292037709.png'/>
所以我们最终的目标就是学习这个隐层的权重矩阵。
我们现在回来接着通过模型的定义来训练我们的这个模型。上面我们提到，input word和output word都会被我们进行one-hot编码。仔细想一下，我们的输入被one-hot编码以后大多数维度上都是0（实际上仅有一个位置为1），所以这个向量相当稀疏，那么会造成什么结果呢。如果我们将一个1 x 10000的向量和10000 x 300的矩阵相乘，它会消耗相当大的计算资源，为了高效计算，它仅仅会选择矩阵中对应的向量中维度值为1的索引行（这句话很绕），看图就明白。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821638986334887.png'/>
我们来看一下上图中的矩阵运算，左边分别是1 x 5和5 x 3的矩阵，结果应该是1 x 3的矩阵，按照矩阵乘法的规则，结果的第一行第一列元素为<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821640674382966.svg'/>，同理可得其余两个元素为12，19。如果10000个维度的矩阵采用这样的计算方式是十分低效的。

为了有效地进行计算，这种稀疏状态下不会进行矩阵乘法计算，可以看到矩阵的计算的结果实际上是矩阵对应的向量中值为1的索引，上面的例子中，左边向量中取值为1的对应维度为3（下标从0开始），那么计算结果就是矩阵的第3行（下标从0开始）—— [10, 12, 19]，这样模型中的隐层权重矩阵便成了一个”查找表“（lookup table），进行矩阵计算时，直接去查输入向量中取值为1的维度下对应的那些权重值。隐层的输出就是每个输入单词的“嵌入词向量”。

输出层

经过神经网络隐层的计算，ants这个词会从一个1 x 10000的向量变成1 x 300的向量，再被输入到输出层。输出层是一个softmax回归分类器，它的每个结点将会输出一个0-1之间的值（概率），这些所有输出层神经元结点的概率之和为1。

下面是一个例子，训练样本为 (input word: “ants”， output word: “car”) 的计算示意图。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821642423711066.png'/>
直觉上的理解

下面我们将通过直觉来进行一些思考。

如果两个不同的单词有着非常相似的“上下文”（也就是窗口单词很相似，比如“Kitty climbed the tree”和“Cat climbed the tree”），那么通过我们的模型训练，这两个单词的嵌入向量将非常相似。

那么两个单词拥有相似的“上下文”到底是什么含义呢？比如对于同义词“intelligent”和“smart”，我们觉得这两个单词应该拥有相同的“上下文”。而例如”engine“和”transmission“这样相关的词语，可能也拥有着相似的上下文。

实际上，这种方法实际上也可以帮助你进行词干化（stemming），例如，神经网络对”ant“和”ants”两个单词会习得相似的词向量。词干化（stemming）就是去除词缀得到词根的过程。

第二部分
第一部分我们了解skip-gram的输入层、隐层、输出层。在第二部分，会继续深入讲如何在skip-gram模型上进行高效的训练。

在第一部分讲解完成后，我们会发现Word2Vec模型是一个超级大的神经网络（权重矩阵规模非常大）。

举个栗子，我们拥有10000个单词的词汇表，我们如果想嵌入300维的词向量，那么我们的输入-隐层权重矩阵和隐层-输出层的权重矩阵都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。百万数量级的权重矩阵和亿万数量级的训练样本意味着训练这个模型将会是个灾难（太凶残了）。

Word2Vec的作者在它的第二篇论文中强调了这些问题，下面是作者在第二篇论文中的三个创新：
1 将常见的单词组合（word pairs）或者词组作为单个“words”来处理。
2 对高频次单词进行抽样来减少训练样本的个数。
3 对优化目标采用“negative sampling”方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担。事实证明，对常用词抽样并且对优化目标采用“negative sampling”不仅降低了训练过程中的计算负担，还提高了训练的词向量的质量。

Word pairs and "phases"
论文的作者指出，一些单词组合（或者词组）的含义和拆开以后具有完全不同的意义。比如“Boston Globe”是一种报刊的名字，而单独的“Boston”和“Globe”这样单个的单词却表达不出这样的含义。因此，在文章中只要出现“Boston Globe”，我们就应该把它作为一个单独的词来生成其词向量，而不是将其拆开。同样的例子还有“New York”，“United Stated”等。

在Google发布的模型中，它本身的训练样本中有来自Google News数据集中的1000亿的单词，但是除了单个单词以外，单词组合（或词组）又有3百万之多。

如果你对模型的词汇表感兴趣，可以点击这里，你还可以直接浏览这个词汇表。

如果想了解这个模型如何进行文档中的词组抽取，可以看论文中“Learning Phrases”这一章，对应的代码word2phrase.c被发布在这里。

对高频词抽样

在第一部分的讲解中，我们展示了训练样本是如何从原始文档中生成出来的，这里我再重复一次。我们的原始文本为“The quick brown fox jumps over the laze dog”，如果我使用大小为2的窗口，那么我们可以得到图中展示的那些训练样本。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821649048207731.png'/>
但是对于“the”这种常用高频单词，这样的处理方式会存在下面两个问题：
①当我们得到成对的单词训练样本时，("fox", "the") 这样的训练样本并不会给我们提供关于“fox”更多的语义信息，因为“the”在每个单词的上下文中几乎都会出现。
②由于在文本中“the”这样的常用词出现概率很大，因此我们将会有大量的（”the“，...）这样的训练样本，而这些样本数量远远超过了我们学习“the”这个词向量所需的训练样本数。

Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。

如果我们设置窗口大小<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821654393333842.svg'/>（即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821654950932640.svg'/>），并且从我们的文本中删除所有的“the”，那么会有下面的结果：
i)由于我们删除了文本中所有的“the”，那么在我们的训练样本中，“the”这个词永远也不会出现在我们的上下文窗口中。
ii)当“the”作为input word时，我们的训练样本数至少会减少10个。

这句话应该这么理解，假如我们的文本中仅出现了一个“the”，那么当这个“the”作为input word时，我们设置span=10，此时会得到10个训练样本 ("the", ...) ，如果删掉这个“the”，我们就会减少10个训练样本。实际中我们的文本中不止一个“the”，因此当“the”作为input word的时候，至少会减少10个训练样本。上面提到的这两个影响结果实际上就帮助我们解决了高频词带来的问题。

抽样率

word2vec的C语言代码实现了一个计算在词汇表中保留某个词概率的公式。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821660035913093.svg'/>是一个单词，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821660960057105.svg'/>是<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415682166216556831.svg'/>这个单词在所有语料中出现的频次。举个栗子，如果单词“peanut”在10亿规模大小的语料中出现了1000次，那么<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821664260389439.svg'/>。

在代码中还有一个参数叫“sample”，这个参数代表一个阈值，默认值为0.001（在gensim包中的Word2Vec类说明中，这个参数默认为0.001，文档中对这个参数的解释为“ threshold for configuring which higher-frequency words are randomly downsampled”）。这个值越小意味着这个单词被保留下来的概率越小（即有越大的概率被我们删除）。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821667218406991.svg'/>代表着保留某个单词的概率：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821667767554195.svg'/>
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821669356397987.png'/>
图中x轴代表着<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415682167169523557.svg'/>，即单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821672898807262.svg'/>在语料中出现频率，y轴代表某个单词被保留的概率。对于一个庞大的语料来说，单个单词的出现频率不会很大，即使是常用词，也不可能特别大。

从这个图中，我们可以看到，随着单词出现频率的增高，它被采样保留的概率越来越小，我们还可以看到一些有趣的结论：
当时<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821677797780491.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821678357190252.svg'/>。当单词在语料中出现的频率小于0.0026时，它是100%被保留的，这意味着只有那些在语料中出现频率超过0.26%的单词才会被采样。
当时<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821679039763958.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821679552009052.svg'/>，意味着这一部分的单词有50%的概率被保留。
当时<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821680087665003.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821680494439768.svg'/>，意味着这部分单词以3.3%的概率被保留。如果你去看那篇论文的话，你会发现作者在论文中对函数公式的定义和在C语言代码的实现上有一些差别，但我认为C语言代码的公式实现是更权威的一个版本。

负采样（negative sampling）

训练一个神经网络意味着要输入训练样本并且不断调整神经元的权重，从而不断提高对目标的准确预测。每当神经网络经过一个训练样本的训练，它的权重就会进行一次调整。

正如我们上面所讨论的，vocabulary的大小决定了我们的Skip-Gram神经网络将会拥有大规模的权重矩阵，所有的这些权重需要通过我们数以亿计的训练样本来进行调整，这是非常消耗计算资源的，并且实际中训练起来会非常慢。

负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。

当我们用训练样本 ( input word: "fox"，output word: "quick") 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的vocabulary大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为“negative” word。

当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。
在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。

回忆一下我们的隐层-输出层拥有300 x 10000的权重矩阵。如果使用了负采样的方法我们仅仅去更新我们的positive word-“quick”的和我们选择的其他5个negative words的结点对应的权重，共计6个输出神经元，相当于每次只更新<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821683822681342.svg'/>个权重。对于3百万的权重来说，相当于只计算了0.06%的权重，这样计算效率就大幅度提高。

如何选择negative words

我们使用“一元模型分布（unigram distribution）”来选择“negative words”。

要注意的一点是，一个单词被选作negative sample的概率跟它出现的频次有关，出现频次越高的单词越容易被选作negative words。

在word2vec的C语言实现中，你可以看到对于这个概率的实现公式。每个单词被选为“negative words”的概率计算公式与其出现的频次有关。

代码中的公式实现如下：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821686197707259.svg'/>
每个单词被赋予一个权重，即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821687325457026.svg'/>， 它代表着单词出现的频次。

公式中开3/4的根号完全是基于经验的，论文中提到这个公式的效果要比其它公式更加出色。你可以在google的搜索栏中输入“plot y = x^(3/4) and y = x”，然后看到这两幅图（如下图），仔细观察x在[0,1]区间内时y的取值，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821691465707304.svg'/>有一小段弧形，取值在<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821692590826333.svg'/>函数之上。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156821694160224328.png'/>
负采样的C语言实现非常的有趣。unigram table有一个包含了一亿个元素的数组，这个数组是由词汇表中每个单词的索引号填充的，并且这个数组中有重复，也就是说有些单词会出现多次。那么每个单词的索引在这个数组中出现的次数该如何决定呢，有公式<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415682169593130473.svg'/>，也就是说计算出的负采样概率*1亿=单词在表中出现的次数。

有了这张表以后，每次去我们进行负采样时，只需要在0-1亿范围内生成一个随机数，然后选择表中索引号为这个随机数的那个单词作为我们的negative word即可。一个单词的负采样概率越大，那么它在这个表中出现的次数就越多，它被选中的概率就越大。

到目前为止，Word2Vec中的Skip-Gram模型就讲完了，对于里面具体的数学公式推导细节这里并没有深入。这篇文章只是对于实现细节上的一些思想进行了阐述。
## 22.请用图形象的解释word2vec（一图胜千言）
本题解析来源：https://blog.csdn.net/longxinchen_ml/article/details/89077048#commentBox，英文原文：https://jalammar.github.io/illustrated-word2vec/

嵌入（embedding）是机器学习中最迷人的想法之一。 如果你曾经使用Siri、Google Assistant、Alexa、Google翻译，甚至智能手机键盘进行下一词预测，那么你很有可能从这个已经成为自然语言处理模型核心的想法中受益。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844468755940872.png'/>
在过去的几十年中，嵌入技术用于神经网络模型已有相当大的发展。尤其是最近，其发展包括导致BERT和GPT2等尖端模型的语境化嵌入。

BERT：
https://jalammar.github.io/illustrated-bert/

Word2vec是一种有效创建词嵌入的方法，它自2013年以来就一直存在。但除了作为词嵌入的方法之外，它的一些概念已经被证明可以有效地创建推荐引擎和理解时序数据。在商业的、非语言的任务中。像Airbnb、阿里巴巴、Spotify这样的公司都从NLP领域中提取灵感并用于产品中，从而为新型推荐引擎提供支持。

在这篇文章中，我们将讨论嵌入的概念，以及使用word2vec生成嵌入的机制。让我们从一个例子开始，熟悉使用向量来表示事物。你是否知道你的个性可以仅被五个数字的列表（向量）表示？

个性嵌入：你是什么样的人？
如何用0到100的范围来表示你是多么内向/外向（其中0是最内向的，100是最外向的）？ 你有没有做过像MBTI那样的人格测试，或者五大人格特质测试？ 如果你还没有，这些测试会问你一系列的问题，然后在很多维度给你打分，内向/外向就是其中之一。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684451535247366.png'/>

五大人格特质测试测试结果示例。它可以真正告诉你很多关于你自己的事情，并且在学术、人格和职业成功方面都具有预测能力。此处可以找到测试结果。

假设我的内向/外向得分为38/100。 我们可以用这种方式绘图：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844516154531694.png'/>

让我们把范围收缩到-1到1:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844520232134004.png'/>
当你只知道这一条信息的时候，你觉得你有多了解这个人？了解不多。人很复杂，让我们添加另一测试的得分作为新维度。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844520941566848.png'/>

我们可以将两个维度表示为图形上的一个点，或者作为从原点到该点的向量。我们拥有很棒的工具来处理即将上场的向量们。

我已经隐藏了我们正在绘制的人格特征，这样你会渐渐习惯于在不知道每个维度代表什么的情况下，从一个人格的向量表示中获得价值信息。

我们现在可以说这个向量部分地代表了我的人格。当你想要将另外两个人与我进行比较时，这种表示法就有用了。假设我被公共汽车撞了，我需要被性格相似的人替换，那在下图中，两个人中哪一个更像我？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844521531016577.png'/>

处理向量时，计算相似度得分的常用方法是余弦相似度：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844522551918807.png'/>

1号替身在性格上与我更相似。指向相同方向的向量（长度也起作用）具有更高的余弦相似度。

再一次，两个维度还不足以捕获有关不同人群的足够信息。心理学已经研究出了五个主要人格特征（以及大量的子特征），所以让我们使用所有五个维度进行比较：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844523345738278.png'/>

使用五个维度的问题是我们不能在二维平面绘制整齐小箭头了。这是机器学习中的常见问题，我们经常需要在更高维度的空间中思考。 但好在余弦相似度仍然有效，它适用于任意维度：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844524467774046.png'/>

余弦相似度适用于任意数量的维度。这些得分比上次的得分要更好，因为它们是根据被比较事物的更高维度算出的。

在本节的最后，我希望提出两个中心思想：
1.我们可以将人和事物表示为代数向量（这对机器来说很棒！）。
2.我们可以很容易地计算出相似的向量之间的相互关系。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844525211426089.png'/>

词嵌入
通过上文的理解，我们继续看看训练好的词向量实例（也被称为词嵌入）并探索它们的一些有趣属性。

这是一个单词“king”的词嵌入（在维基百科上训练的GloVe向量）：
[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]

这是一个包含50个数字的列表。通过观察数值我们看不出什么，但是让我们稍微给它可视化，以便比较其它词向量。我们把所有这些数字放在一行：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844527173787486.png'/>

让我们根据它们的值对单元格进行颜色编码（如果它们接近2则为红色，接近0则为白色，接近-2则为蓝色）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844527990576253.png'/>

我们将忽略数字并仅查看颜色以指示单元格的值。现在让我们将“king”与其它单词进行比较：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844529089103826.png'/>

看看“Man”和“Woman”彼此之间是如何比它们任一一个单词与“King”相比更相似的？ 这暗示你一些事情。这些向量图示很好的展现了这些单词的信息/含义/关联。

这是另一个示例列表（通过垂直扫描列来查找具有相似颜色的列）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844530092331232.png'/>

有几个要点需要指出：
1.所有这些不同的单词都有一条直的红色列。 它们在这个维度上是相似的（虽然我们不知道每个维度是什么）
2.你可以看到“woman”和“girl”在很多地方是相似的，“man”和“boy”也是一样
3.“boy”和“girl”也有彼此相似的地方，但这些地方却与“woman”或“man”不同。这些是否可以总结出一个模糊的“youth”概念？可能吧。
4.除了最后一个单词，所有单词都是代表人。 我添加了一个对象“water”来显示类别之间的差异。你可以看到蓝色列一直向下并在 “water”的词嵌入之前停下了。
5.“king”和“queen”彼此之间相似，但它们与其它单词都不同。这些是否可以总结出一个模糊的“royalty”概念？

类比
展现嵌入奇妙属性的著名例子是类比。我们可以添加、减去词嵌入并得到有趣的结果。一个著名例子是公式：“king”-“man”+“woman”：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844530810251399.png'/>

在python中使用Gensim库，我们可以添加和减去词向量，它会找到与结果向量最相似的单词。该图像显示了最相似的单词列表，每个单词都具有余弦相似性。

我们可以像之前一样可视化这个类比：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844531824302089.png'/>

由“king-man + woman”生成的向量并不完全等同于“queen”，但“queen”是我们在此集合中包含的400,000个字嵌入中最接近它的单词。

现在我们已经看过训练好的词嵌入，接下来让我们更多地了解训练过程。 但在我们开始使用word2vec之前，我们需要看一下词嵌入的父概念：神经语言模型。

语言模型
如果要举自然语言处理最典型的例子，那应该就是智能手机输入法中的下一单词预测功能。这是个被数十亿人每天使用上百次的功能。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684453231639228.png'/>

下一单词预测是一个可以通过语言模型实现的任务。语言模型会通过单词列表(比如说两个词)去尝试预测可能紧随其后的单词。

在上面这个手机截屏中，我们可以认为该模型接收到两个绿色单词(thou shalt)并推荐了一组单词(“not” 就是其中最有可能被选用的一个)：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844533326231028.png'/>

我们可以把这个模型想象为这个黑盒:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844534289421319.png'/>

但事实上，该模型不会只输出一个单词。实际上，它对所有它知道的单词(模型的词库，可能有几千到几百万个单词)的按可能性打分，输入法程序会选出其中分数最高的推荐给用户。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844535045920372.png'/>

自然语言模型的输出就是模型所知单词的概率评分，我们通常把概率按百分比表示，但是实际上，40%这样的分数在输出向量组是表示为0.4

自然语言模型(请参考Bengio 2003)在完成训练后，会按如下中所示法人三步完成预测：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844536035278081.png'/>

第一步与我们最相关，因为我们讨论的就是Embedding。模型在经过训练之后会生成一个映射单词表所有单词的矩阵。在进行预测的时候，我们的算法就是在这个映射矩阵中查询输入的单词，然后计算出预测值:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844536687698599.png'/>

现在让我们将重点放到模型训练上，来学习一下如何构建这个映射矩阵。

语言模型训练
相较于大多数其他机器学习模型，语言模型有一个很大有优势，那就是我们有丰富的文本来训练语言模型。所有我们的书籍、文章、维基百科、及各种类型的文本内容都可用。相比之下，许多其他机器学习的模型开发就需要手工设计数据或者专门采集数据。

我们通过找常出现在每个单词附近的词，就能获得它们的映射关系。机制如下：

1.先是获取大量文本数据(例如所有维基百科内容)
2. 然后我们建立一个可以沿文本滑动的窗(例如一个窗里包含三个单词)
3. 利用这样的滑动窗就能为训练模型生成大量样本数据。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684453784736340.png'/>

当这个窗口沿着文本滑动时，我们就能(真实地)生成一套用于模型训练的数据集。为了明确理解这个过程，我们看下滑动窗是如何处理这个短语的:

“Thou shalt not make a machine in the likeness of a human mind” ~Dune

在一开始的时候，窗口锁定在句子的前三个单词上:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844538544645411.png'/>

我们把前两个单词单做特征，第三个单词单做标签:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844539597558241.png'/>

这时我们就生产了数据集中的第一个样本，它会被用在我们后续的语言模型训练中。

接着，我们将窗口滑动到下一个位置并生产第二个样本:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844540444667243.png'/>

这时第二个样本也生成了。

不用多久，我们就能得到一个较大的数据集，从数据集中我们能看到在不同的单词组后面会出现的单词:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844541131034754.png'/>

在实际应用中，模型往往在我们滑动窗口时就被训练的。但是我觉得将生成数据集和训练模型分为两个阶段会显得更清晰易懂一些。除了使用神经网络建模之外，大家还常用一项名为N-gams的技术进行模型训练。

如果想了解现实产品从使用N-gams模型到使用神经模型的转变，可以看一下Swiftkey (我最喜欢的安卓输入法)在2015年的发表一篇博客，文中介绍了他们的自然语言模型及该模型与早期N-gams模型的对比。我很喜这个例子，因为这个它能告诉你如何在营销宣讲中把Embedding的算法属性解释清楚。

顾及两头
根据前面的信息进行填空:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844542392507445.png'/>

在空白前面，我提供的背景是五个单词(如果事先提及到‘bus’)，可以肯定，大多数人都会把bus填入空白中。但是如果我再给你一条信息——比如空白后的一个单词，那答案会有变吗？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844542995294732.png'/>

这下空白处改填的内容完全变了。这时’red’这个词最有可能适合这个位置。从这个例子中我们能学到，一个单词的前后词语都带信息价值。事实证明，我们需要考虑两个方向的单词(目标单词的左侧单词与右侧单词)。那我们该如何调整训练方式以满足这个要求呢，继续往下看。

Skipgram模型
我们不仅要考虑目标单词的前两个单词，还要考虑其后两个单词。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844544844737440.png'/>

如果这么做，我们实际上构建并训练的模型就如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844545454377802.png'/>

上述的这种架构被称为连续词袋(CBOW)，在一篇关于word2vec的论文中有阐述。

还有另一种架构，它不根据前后文(前后单词)来猜测目标单词，而是推测当前单词可能的前后单词。我们设想一下滑动窗在训练数据时如下图所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844551639257097.png'/>

绿框中的词语是输入词，粉框则是可能的输出结果

这里粉框颜色深度呈现不同，是因为滑动窗给训练集产生了4个独立的样本:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844552333293804.png'/>

这种方式称为Skipgram架构。我们可以像下图这样将展示滑动窗的内容。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844553075616690.png'/>
这样就为数据集提供了4个样本:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844556060192397.png'/>
然后我们移动滑动窗到下一个位置:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844557654635876.png'/>

这样我们又产生了接下来4个样本:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844560054138247.png'/>

在移动几组位置之后，我们就能得到一批样本:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844576031016522.png'/>

重新审视训练过程
现在我们已经从现有的文本中获得了Skipgram模型的训练数据集，接下来让我们看看如何使用它来训练一个能预测相邻词汇的自然语言模型。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684457694057923.png'/>

从数据集中的第一个样本开始。我们将特征输入到未经训练的模型，让它预测一个可能的相邻单词。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844578099644532.png'/>

该模型会执行三个步骤并输入预测向量(对应于单词表中每个单词的概率)。因为模型未经训练，该阶段的预测肯定是错误的。但是没关系，我们知道应该猜出的是哪个单词——这个词就是我训练集数据中的输出标签:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844579274876441.png'/>

目标单词概率为1，其他所有单词概率为0，这样数值组成的向量就是“目标向量”。

模型的偏差有多少？将两个向量相减，就能得到偏差向量:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684457997052686.png'/>

现在这一误差向量可以被用于更新模型了，所以在下一轮预测中，如果用not作为输入，我们更有可能得到thou作为输出了。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844580832335579.png'/>

这其实就是训练的第一步了。我们接下来继续对数据集内下一份样本进行同样的操作，直到我们遍历所有的样本。这就是一轮（epoch）了。我们再多做几轮（epoch），得到训练过的模型，于是就可以从中提取嵌入矩阵来用于其他应用了。

以上确实有助于我们理解整个流程，但这依然不是word2vec真正训练的方法。我们错过了一些关键的想法。

负例采样
回想一下这个神经语言模型计算预测值的三个步骤：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844581688372943.png'/>

从计算的角度来看，第三步非常昂贵 - 尤其是当我们将需要在数据集中为每个训练样本都做一遍（很容易就多达数千万次）。我们需要寻找一些提高表现的方法。

一种方法是将目标分为两个步骤：
1.生成高质量的词嵌入（不要担心下一个单词预测）。
2.使用这些高质量的嵌入来训练语言模型（进行下一个单词预测）。

在本文中我们将专注于第1步（因为这篇文章专注于嵌入）。要使用高性能模型生成高质量嵌入，我们可以改变一下预测相邻单词这一任务：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844582543625632.png'/>

将其切换到一个提取输入与输出单词的模型，并输出一个表明它们是否是邻居的分数（0表示“不是邻居”，1表示“邻居”）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844585866655467.png'/>

这个简单的变换将我们需要的模型从神经网络改为逻辑回归模型——因此它变得更简单，计算速度更快。

这个开关要求我们切换数据集的结构——标签值现在是一个值为0或1的新列。它们将全部为1，因为我们添加的所有单词都是邻居。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684458643093615.png'/>

现在的计算速度可谓是神速啦——在几分钟内就能处理数百万个例子。但是我们还需要解决一个漏洞。如果所有的例子都是邻居（目标：1），我们这个”天才模型“可能会被训练得永远返回1——准确性是百分百了，但它什么东西都学不到，只会产生垃圾嵌入结果。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844594770863315.png'/>

为了解决这个问题，我们需要在数据集中引入负样本 - 不是邻居的单词样本。我们的模型需要为这些样本返回0。模型必须努力解决这个挑战——而且依然必须保持高速。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844595838706243.png'/>

对于我们数据集中的每个样本，我们添加了负面示例。它们具有相同的输入字词，标签为0。

但是我们作为输出词填写什么呢？我们从词汇表中随机抽取单词
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844596819454389.png'/>

这个想法的灵感来自噪声对比估计。我们将实际信号（相邻单词的正例）与噪声（随机选择的不是邻居的单词）进行对比。这导致了计算和统计效率的巨大折衷。

噪声对比估计
http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf

基于负例采样的Skipgram（SGNS）
我们现在已经介绍了word2vec中的两个（一对）核心思想：负例采样，以及skipgram。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684459775546353.png'/>

Word2vec训练流程
现在我们已经了解了skipgram和负例采样的两个中心思想，可以继续仔细研究实际的word2vec训练过程了。

在训练过程开始之前，我们预先处理我们正在训练模型的文本。在这一步中，我们确定一下词典的大小（我们称之为vocab_size，比如说10,000）以及哪些词被它包含在内。

在训练阶段的开始，我们创建两个矩阵——Embedding矩阵和Context矩阵。这两个矩阵在我们的词汇表中嵌入了每个单词（所以vocab_size是他们的维度之一）。第二个维度是我们希望每次嵌入的长度（embedding_size——300是一个常见值，但我们在前文也看过50的例子）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844599257221521.png'/>

在训练过程开始时，我们用随机值初始化这些矩阵。然后我们开始训练过程。在每个训练步骤中，我们采取一个相邻的例子及其相关的非相邻例子。我们来看看我们的第一组：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844599990371684.png'/>

现在我们有四个单词：输入单词not和输出/上下文单词: thou（实际邻居词），aaron和taco（负面例子）。我们继续查找它们的嵌入——对于输入词，我们查看Embedding矩阵。对于上下文单词，我们查看Context矩阵（即使两个矩阵都在我们的词汇表中嵌入了每个单词）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844600598212911.png'/>

然后，我们计算输入嵌入与每个上下文嵌入的点积。在每种情况下，结果都将是表示输入和上下文嵌入的相似性的数字。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684460147082011.png'/>

现在我们需要一种方法将这些分数转化为看起来像概率的东西——我们需要它们都是正值，并且 处于0到1之间。sigmoid这一逻辑函数转换正适合用来做这样的事情啦。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844602121676776.png'/>

现在我们可以将sigmoid操作的输出视为这些示例的模型输出。您可以看到taco得分最高，aaron最低，无论是sigmoid操作之前还是之后。

既然未经训练的模型已做出预测，而且我们确实拥有真实目标标签来作对比，那么让我们计算模型预测中的误差吧。为此我们只需从目标标签中减去sigmoid分数。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844602716214910.png'/>

error = target - sigmoid_scores

这是“机器学习”的“学习”部分。现在，我们可以利用这个错误分数来调整not、thou、aaron和taco的嵌入，使我们下一次做出这一计算时，结果会更接近目标分数。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844603139960024.png'/>

训练步骤到此结束。我们从中得到了这一步所使用词语更好一些的嵌入（not，thou，aaron和taco）。我们现在进行下一步（下一个相邻样本及其相关的非相邻样本），并再次执行相同的过程。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415684460369423321.png'/>

当我们循环遍历整个数据集多次时，嵌入会继续得到改进。然后我们就可以停止训练过程，丢弃Context矩阵，并使用Embeddings矩阵作为下一项任务的已被训练好的嵌入。

窗口大小和负样本数量
word2vec训练过程中的两个关键超参数是窗口大小和负样本的数量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844605320678823.png'/>

不同的任务适合不同的窗口大小。一种启发式方法是，使用较小的窗口大小（2-15）会得到这样的嵌入：两个嵌入之间的高相似性得分表明这些单词是可互换的（注意，如果我们只查看附近距离很近的单词，反义词通常可以互换——例如，好的和坏的经常出现在类似的语境中）。使用较大的窗口大小（15-50，甚至更多）会得到相似性更能指示单词相关性的嵌入。在实际操作中，你通常需要对嵌入过程提供指导以帮助读者得到相似的”语感“。Gensim默认窗口大小为5（除了输入字本身以外还包括输入字之前与之后的两个字）。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156844605973404618.png'/>

负样本的数量是训练训练过程的另一个因素。原始论文认为5-20个负样本是比较理想的数量。它还指出，当你拥有足够大的数据集时，2-5个似乎就已经足够了。Gensim默认为5个负样本。

结论
我希望您现在对词嵌入和word2vec算法有所了解。我也希望现在当你读到一篇提到“带有负例采样的skipgram”（SGNS）的论文（如顶部的推荐系统论文）时，你已经对这些概念有了更好的认识。
## 23.请简要说说word2vec的来龙去脉/前世今生？
本题解析来源：https://www.cnblogs.com/iloveai/p/word2vec.html，July和助教远根特给此文补充了相关例子和表格，以为更通俗形象、一目了然。

2013年，Google开源了一款用于词向量计算的工具——word2vec，引起了工业界和学术界的关注。首先，word2vec可以在百万数量级的词典和上亿的数据集上进行高效地训练；其次，该工具得到的训练结果——词向量（word embedding），可以很好地度量词与词之间的相似性。

随着深度学习（Deep Learning）在自然语言处理中应用的普及，很多人误以为word2vec是一种深度学习算法。其实word2vec算法的背后是一个浅层神经网络，是一个计算word vector的开源工具。所以，当我们在说word2vec算法或模型的时候，其实指的是其背后用于计算word vector的CBoW模型和Skip-gram模型。

接下来，本文将从统计语言模型出发，尽可能详细地介绍word2vec工具背后的算法模型的来龙去脉。

Statistical Language Model
在深入word2vec算法的细节之前，我们首先回顾一下自然语言处理中的一个基本问题：如何计算一段文本序列在某种语言下出现的概率？之所为称其为一个基本问题，是因为它在很多NLP任务中都扮演着重要的角色。

例如，在机器翻译的问题中，如果我们知道了目标语言中每句话的概率，就可以从候选集合中挑选出最合理的句子做为翻译结果返回。统计语言模型给出了这一类问题的一个基本解决框架。对于一段文本序列S=w1,w2,...,wT，它的概率可以表示为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904223641520182.png'/>
即将序列的联合概率转化为一系列条件概率的乘积。

问题变成了如何去预测这些给定previous words下的条件概率：p(wt|w1,w2,...,wt−1)

由于其巨大的参数空间，这样一个原始的模型在实际中并没有什么卵用。我们更多的是采用其简化版本——Ngram模型：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904231571943173.png'/>

常见的如bigram模型（N=2N=2）和trigram模型（N=3N=3）。事实上，由于模型复杂度和预测精度的限制，我们很少会考虑N>3N>3的模型。

我们可以用最大似然法去求解Ngram模型的参数——等价于去统计每个Ngram的条件词频。

为了避免统计中出现的零概率问题（一段从未在训练集中出现过的Ngram片段会使得整个序列的概率为0），人们基于原始的Ngram模型进一步发展出了back-off trigram模型（用低阶的bigram和unigram代替零概率的trigram），和interpolated trigram模型（将条件概率表示为unigram、bigram、trigram三者的线性函数）。此处不再赘述。感兴趣者可进一步阅读相关的文献[3]。

Distributed Representation
不过，Ngram模型仍有其局限性。首先，由于参数空间的爆炸式增长，它无法处理更长程的context（N>3N>3）。

其次，它没有考虑词与词之间内在的联系性。例如，考虑"the cat is walking in the bedroom"这句话。如果我们在训练语料中看到了很多类似“the dog is walking in the bedroom”或是“the cat is running in the bedroom”这样的句子，那么，即使我们没有见过这句话，也可以从“cat”和“dog”（“walking”和“running”）之间的相似性，推测出这句话的概率[3]。
然而， Ngram模型做不到。这是因为，Ngram本质上是将词当做一个个孤立的原子单元（atomic unit）去处理的。这种处理方式对应到数学上的形式是一个个离散的one-hot向量（除了一个词典索引的下标对应的方向上是1，其余方向上都是0）。

例如，对于一个大小为5的词典：
{"I", "love", "nature", "luaguage", "processing"}，
“nature”对应的one-hot向量为：
[0,0,1,0,0]。

显然，one-hot向量的维度等于词典的大小。这在动辄上万甚至百万词典的实际应用中，面临着巨大的维度灾难问题（the curse of dimensionality）。

于是，人们就自然而然地想到，能否用一个连续的稠密向量去刻画一个word的特征呢？这样，我们不仅可以直接刻画词与词之间的相似度，还可以建立一个从向量到概率的平滑函数模型，使得相似的词向量可以映射到相近的概率空间上。这个稠密连续向量也被称为word的distributed representation[3]。

事实上，这个概念在信息检索（Information Retrieval）领域早就已经被广泛地使用了。只不过，在IR领域里，这个概念被称为向量空间模型（Vector Space Model，以下简称VSM）。

VSM是基于一种Statistical Semantics Hypothesis[4]：语言的统计特征隐藏着语义的信息（Statistical pattern of human word usage can be used to figure out what people mean）。例如，两篇具有相似词分布的文档可以被认为是有着相近的主题。

这个Hypothesis有很多衍生版本。其中，比较广为人知的两个版本是Bag of Words Hypothesis和Distributional Hypothesis。前者是说，一篇文档的词频（而不是词序）代表了文档的主题；后者是说，上下文环境相似的两个词有着相近的语义。后面我们会看到，word2vec算法也是基于Distributional的假设。

那么，VSM是如何将稀疏离散的one-hot词向量映射为稠密连续的distributional representation的呢？

简单来说，基于Bag of Words Hypothesis，我们可以构造一个term-document矩阵A：
矩阵的行Ai对应着词典里的一个word；
矩阵的列Aj对应着训练语料里的一篇文档；
矩阵里的元素Aij代表着word wi在文档Dj中出现的次数（或频率）。
那么，我们就可以提取行向量做为word的语义向量（不过，在实际应用中，我们更多的是用列向量做为文档的主题向量）。

恩，屏幕前的你现在是不是想问，有没例子呀？当然有的，我们基于这两个相似语句，例如
语句A：the dog is walking in the bedrom
语句B：the cat is running in the bedroom
构造term-document矩阵A，用于表示每一个词分别语句A和语句B中出现的次数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156905711095687650.png'/>
则提取行向量作为word的语义向量，思考：这是基于词频的，也可以基于此得到主题分布，但是缺少语义的考虑即上下文context。

类似地，我们可以基于Distributional Hypothesis构造一个word-context的矩阵。此时，矩阵的列变成了context里的word，矩阵的元素也变成了一个context窗口里word的共现次数。

举个例子：通过分析如下三个相似语句
语句A：I like deep learning.
语句B：I like NLP.
语句C：I enjoy flying.
构造word-content矩阵（设置窗口的大小为1），表示每个词与这三个语句中其他词相邻的次数（这个时候，必须上表格，便可瞬间清晰、一目了然）：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156905680876441470.png'/>

注意，这两类矩阵的行向量所计算的相似度有着细微的差异：
term-document矩阵会给经常出现在同一篇document里的两个word赋予更高的相似度；
而word-context矩阵会给那些有着相同context的两个word赋予更高的相似度。

后者相对于前者是一种更高阶的相似度，因此在传统的信息检索领域中得到了更加广泛的应用。不过，这种co-occurrence矩阵仍然存在着数据稀疏性和维度灾难的问题。为此，人们提出了一系列对矩阵进行降维的方法（如LSI／LSA等）。这些方法大都是基于SVD的思想，将原始的稀疏矩阵分解为两个低秩矩阵乘积的形式。

关于VSM更多的介绍，可以进一步阅读文末的参考文献[4]。

Neural Network Language Model
接下来，让我们回到对统计语言模型的讨论。鉴于Ngram等模型的不足，2003年，Bengio等人发表了一篇开创性的文章：A neural probabilistic language model[3]。在这篇文章里，他们总结出了一套用神经网络建立统计语言模型的框架（Neural Network Language Model，以下简称NNLM），并首次提出了word embedding的概念（虽然没有叫这个名字），从而奠定了包括word2vec在内后续研究word representation learning的基础。

NNLM模型的基本思想可以概括如下：
①假定词表中的每一个word都对应着一个连续的特征向量；
②假定一个连续平滑的概率模型，输入一段词向量的序列，可以输出这段序列的联合概率；
③同时学习词向量的权重和概率模型里的参数。
值得注意的一点是，这里的词向量也是要学习的参数。

在03年的论文里，Bengio等人采用了一个简单的前向反馈神经网络f(wt−n+1,...,wt)来拟合一个词序列的条件概率p(wt|w1,w2,...,wt−1)。
整个模型的网络结构见下图：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904095373575974.png'/>

我们可以将整个模型拆分成两部分加以理解：
    i)首先是一个线性的embedding层。它将输入的N−1个one-hot词向量，通过一个共享的D×V的矩阵C，映射为N−1个分布式的词向量（distributed vector）。其中，V是词典的大小，D是embedding向量的维度（一个先验参数）。C矩阵里存储了要学习的word vector。
    ii)其次是一个简单的前向反馈神经网络g。它由一个tanh隐层和一个softmax输出层组成。

通过将embedding层输出的N−1N−1个词向量映射为一个长度为VV的概率分布向量，从而对词典中的word在输入context下的条件概率做出预估：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904307614253337.png'/>

我们可以通过最小化一个cross-entropy的正则化损失函数来调整模型的参数θ：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904311795942433.png'/>

其中，模型的参数θ包括了embedding层矩阵C的元素，和前向反馈神经网络模型g里的权重。这是一个巨大的参数空间。不过，在用SGD学习更新模型的参数时，并不是所有的参数都需要调整（例如未在输入的context中出现的词对应的词向量）。计算的瓶颈主要是在softmax层的归一化函数上（需要对词典中所有的word计算一遍条件概率）。

然而，抛却复杂的参数空间，我们不禁要问，为什么这样一个简单的模型会取得巨大的成功呢？仔细观察这个模型就会发现，它其实在同时解决两个问题：一个是统计语言模型里关注的条件概率p(wt|context)的计算；一个是向量空间模型里关注的词向量的表达。而这两个问题本质上并不独立。通过引入连续的词向量和平滑的概率模型，我们就可以在一个连续空间里对序列概率进行建模，从而从根本上缓解数据稀疏性和维度灾难的问题。

另一方面，以条件概率p(wt|context)为学习目标去更新词向量的权重，具有更强的导向性，同时也与VSM里的Distributional Hypothesis不谋而合。CBoW & Skip-gram Model铺垫了这么多，终于要轮到主角出场了。

不过在主角正式登场前，我们先看一下NNLM存在的几个问题。一个问题是，同Ngram模型一样，NNLM模型只能处理定长的序列。在03年的论文里，Bengio等人将模型能够一次处理的序列长度N提高到了5，虽然相比bigram和trigram已经是很大的提升，但依然缺少灵活性。因此，Mikolov等人在2010年提出了一种RNNLM模型[7]，用递归神经网络代替原始模型里的前向反馈神经网络，并将embedding层与RNN里的隐藏层合并，从而解决了变长序列的问题。

另一个问题就比较严重了。NNLM的训练太慢了。即便是在百万量级的数据集上，即便是借助了40个CPU进行训练，NNLM也需要耗时数周才能给出一个稍微靠谱的解来。显然，对于现在动辄上千万甚至上亿的真实语料库，训练一个NNLM模型几乎是一个impossible mission。这时候，还是那个Mikolov站了出来。他注意到，原始的NNLM模型的训练其实可以拆分成两个步骤：
用一个简单模型训练出连续的词向量；
基于词向量的表达，训练一个连续的Ngram神经网络模型。
而NNLM模型的计算瓶颈主要是在第二步。

如果我们只是想得到word的连续特征向量，是不是可以对第二步里的神经网络模型进行简化呢？Mikolov是这么想的，也是这么做的。他在2013年一口气推出了两篇paper，并开源了一款计算词向量的工具——至此，word2vec横空出世，主角闪亮登场。

下面，我将带领大家简单剖析下word2vec算法的原理。有了前文的基础，理解word2vec算法就变得很简单了。首先，我们对原始的NNLM模型做如下改造：
1 移除前向反馈神经网络中非线性的hidden layer，直接将中间层的embedding layer与输出层的softmax layer连接；
2 忽略上下文环境的序列信息：输入的所有词向量均汇总到同一个embedding layer；
3 将future words纳入上下文环境

得到的模型称之为CBoW模型（Continuous Bag-of-Words Model），也是word2vec算法的第一个模型：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904097679110071.png'/>

从数学上看，CBoW模型等价于一个词袋模型的向量乘以一个embedding矩阵，从而得到一个连续的embedding向量。这也是CBoW模型名称的由来。CBoW模型依然是从context对target word的预测中学习到词向量的表达。反过来，我们能否从target word对context的预测中学习到word vector呢？答案显然是可以的：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904099658168993.png'/>
这个模型被称为Skip-gram模型（名称源于该模型在训练时会对上下文环境里的word进行采样）。如果将Skip-gram模型的前向计算过程写成数学形式，我们得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904107834080692.png'/>
其中，Vi是embedding层矩阵里的列向量，也被称为wi的input vector。Uj是softmax层矩阵里的行向量，也被称为wj的output vector。

因此，Skip-gram模型的本质是计算输入word的input vector与目标word的output vector之间的余弦相似度，并进行softmax归一化。我们要学习的模型参数正是这两类词向量。

然而，直接对词典里的V个词计算相似度并归一化，显然是一件极其耗时的impossible mission。为此，Mikolov引入了两种优化算法：层次Softmax（Hierarchical Softmax）和负采样（Negative Sampling）。

Hierarchical Softmax[5]
层次Softmax的方法最早由Bengio在05年引入到语言模型中。它的基本思想是将复杂的归一化概率分解为一系列条件概率乘积的形式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904345491578560.png'/>

其中，每一层条件概率对应一个二分类问题，可以通过一个简单的逻辑回归函数去拟合。这样，我们将对V个词的概率归一化问题，转化成了对logV个词的概率拟合问题。

我们可以通过构造一颗分类二叉树来直观地理解这个过程。首先，我们将原始字典D划分为两个子集D1、D2，并假设在给定context下，target word属于子集D1的概率p(wt∈D1|context)服从logistical function的形式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904122895000025.png'/>
其中，UDroot和Vwt都是模型的参数。

接下来，我们可以对子集D1和D2进一步划分。重复这一过程，直到集合里只剩下一个word。这样，我们就将原始大小为V的字典DD转换成了一颗深度为logV的二叉树。树的叶子节点与原始字典里的word一一对应；非叶节点则对应着某一类word的集合。显然，从根节点出发到任意一个叶子节点都只有一条唯一路径——这条路径也编码了这个叶子节点所属的类别。

同时，从根节点出发到叶子节点也是一个随机游走的过程。因此，我们可以基于这颗二叉树对叶子节点出现的似然概率进行计算。例如，对于训练样本里的一个target word wt，假设其对应的二叉树编码为{1,0,1,...,1}，则我们构造的似然函数为：
p(wt|context)=p(D1=1|context)p(D2=0|D1=1)…p(wt|Dk=1)

乘积中的每一项都是一个逻辑回归的函数。

我们可以通过最大化这个似然函数来求解二叉树上的参数——非叶节点上的向量，用来计算游走到某一个子节点的概率。层次Softmax是一个很巧妙的模型。它通过构造一颗二叉树，将目标概率的计算复杂度从最初的V降低到了logV的量级。不过付出的代价是人为增强了词与词之间的耦合性。例如，一个word出现的条件概率的变化，会影响到其路径上所有非叶节点的概率变化，间接地对其他word出现的条件概率带来不同程度的影响。因此，构造一颗有意义的二叉树就显得十分重要。实践证明，在实际的应用中，基于Huffman编码的二叉树可以满足大部分应用场景的需求。

Negative Sampling[6]
负采样的思想最初来源于一种叫做Noise-Contrastive Estimation的算法[6]，原本是为了解决那些无法归一化的概率模型的参数预估问题。与改造模型输出概率的层次Softmax算法不同，NCE算法改造的是模型的似然函数。以Skip-gram模型为例，其原始的似然函数对应着一个Multinomial的分布。在用最大似然法求解这个似然函数时，我们得到一个cross-entropy的损失函数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904138341967126.png'/>

式中的p(wt+j|wt)是一个在整个字典上归一化了的概率。而在NCE算法中，我们构造了这样一个问题：对于一组训练样本，我们想知道，target word的出现，是来自于context的驱动，还是一个事先假定的背景噪声的驱动？显然，我们可以用一个逻辑回归的函数来回答这个问题：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904147178192128.png'/>
这个式子给出了一个target word w来自于context驱动的概率。其中，k是一个先验参数，表明噪声的采样频率。p(w|context)是一个非归一化的概率分布，这里采用softmax归一化函数中的分子部分。pn(w)pn(w)则是背景噪声的词分布。通常采用word的unigram分布。

通过对噪声分布的kk采样，我们得到一个新的数据集：。其中，label标记了数据的来源（真实数据分布还是背景噪声分布？）。在这个新的数据集上，我们就可以用最大化上式中逻辑回归的似然函数来求解模型的参数。而Mikolov在2013年的论文里提出的负采样算法， 是NCE的一个简化版本。在这个算法里，Mikolov抛弃了NCE似然函数中对噪声分布的依赖，直接用原始softmax函数里的分子定义了逻辑回归的函数，进一步简化了计算：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904153361440918.png'/>
此时，模型相应的目标函数变为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641569041599475384.png'/>
J(θ)=logσ(Uo⋅Vi)+∑j=1kEwj∼pn(w)[logσ(−Uj⋅Vi)]J(θ)=log⁡σ(Uo⋅Vi)+∑j=1kEwj∼pn(w)[log⁡σ(−Uj⋅Vi)]除了这里介绍的层次Softmax和负采样的优化算法，Mikolov在13年的论文里还介绍了另一个trick：下采样（subsampling）。其基本思想是在训练时依概率随机丢弃掉那些高频的词：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904166546122186.png'/>
其中，tt=是一个先验参数，一般取为10^5。f(w)是w在语料中出现的频率。实验证明，这种下采样技术可以显著提高低频词的词向量的准确度。

Beyond the Word Vector
介绍完word2vec模型的算法和原理，我们来讨论一些轻松点的话题——模型的应用。13年word2vec模型横空出世后，人们最津津乐道的是它学到的向量在语义和语法相似性上的应用——尤其是这种相似性居然对数学上的加减操作有意义[8]！

最经典的一个例子是，v("King")−v("Man")+v("Woman")=v("Queen")。然而，这种例子似乎并没有太多实际的用途。

除此之外，word2vec模型还被应用于机器翻译和推荐系统领域。Machine Translation[9]与后来提出的在sentence level上进行机器翻译的RNN模型不同，word2vec模型主要是用于词粒度上的机器翻译。具体来说，我们首先从大量的单语种语料中学习到每种语言的word2vec表达，再借助一个小的双语语料库学习到两种语言word2vec表达的线性映射关系WW。构造的损失函数为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156904177816033839.png'/>
在翻译的过程中，我们首先将源语言的word2vec向量通过矩阵WW映射到目标语言的向量空间上；再在目标语言的向量空间中找出与投影向量距离最近的word做为翻译的结果返回。

其原理是，不同语言学习到的word2vec向量空间在几何上具有一定的同构性。映射矩阵W本质上是一种空间对齐的线性变换。

Item2Vec[11]
本质上，word2vec模型是在word-context的co-occurrence矩阵基础上建立起来的。因此，任何基于co-occurrence矩阵的算法模型，都可以套用word2vec算法的思路加以改进。比如，推荐系统领域的协同过滤算法。协同过滤算法是建立在一个user-item的co-occurrence矩阵的基础上，通过行向量或列向量的相似性进行推荐。如果我们将同一个user购买的item视为一个context，就可以建立一个item-context的矩阵。进一步的，可以在这个矩阵上借鉴CBoW模型或Skip-gram模型计算出item的向量表达，在更高阶上计算item间的相似度。关于word2vec更多应用的介绍，可以进一步参考这篇文献[10]。

Word Embedding
最后，我想简单阐述下我对word embedding的几点思考。不一定正确，也欢迎大家提出不同的意见。

Word embedding最早出现于Bengio在03年发表的开创性文章中[3]。通过嵌入一个线性的投影矩阵（projection matrix），将原始的one-hot向量映射为一个稠密的连续向量，并通过一个语言模型的任务去学习这个向量的权重。这一思想后来被广泛应用于包括word2vec在内的各种NLP模型中。

Word embedding的训练方法大致可以分为两类：
一类是无监督或弱监督的预训练；
一类是端对端（end to end）的有监督训练。

无监督或弱监督的预训练以word2vec和auto-encoder为代表。这一类模型的特点是，不需要大量的人工标记样本就可以得到质量还不错的embedding向量。不过因为缺少了任务导向，可能和我们要解决的问题还有一定的距离。因此，我们往往会在得到预训练的embedding向量后，用少量人工标注的样本去fine-tune整个模型。

相比之下，端对端的有监督模型在最近几年里越来越受到人们的关注。与无监督模型相比，端对端的模型在结构上往往更加复杂。同时，也因为有着明确的任务导向，端对端模型学习到的embedding向量也往往更加准确。例如，通过一个embedding层和若干个卷积层连接而成的深度神经网络以实现对句子的情感分类，可以学习到语义更丰富的词向量表达。

Word embedding的另一个研究方向是在更高层次上对sentence的embedding向量进行建模。我们知道，word是sentence的基本组成单位。一个最简单也是最直接得到sentence embedding的方法是将组成sentence的所有word的embedding向量全部加起来——类似于CBoW模型。显然，这种简单粗暴的方法会丢失很多信息。
另一种方法借鉴了word2vec的思想——将sentence或是paragraph视为一个特殊的word，然后用CBoW模型或是Skip-gram进行训练[12]。

这种方法的问题在于，对于一篇新文章，总是需要重新训练一个新的sentence2vec。此外，同word2vec一样，这个模型缺少有监督的训练导向。个人感觉比较靠谱的是第三种方法——基于word embedding的端对端的训练。Sentence本质上是word的序列。因此，在word embedding的基础上，我们可以连接多个RNN模型或是卷积神经网络，对word embedding序列进行编码，从而得到sentence embedding。这方面的工作已有很多。有机会，我会再写一篇关于sentence embedding的综述。

References
[1]: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013, January 17). Efficient Estimation of Word Representations in Vector Space. arXiv.org.
[2]: Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013, October 17). Distributed Representations of Words and Phrases and their Compositionality. arXiv.org.
[3]: Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). A neural probabilistic language model. The Journal of Machine Learning Research, 3, 1137–1155.
[4]: Turney, P. D., & Pantel, P. (2010). From frequency to meaning: vector space models of semantics. Journal of Artificial Intelligence Research, 37(1).

[5]: Morin, F., & Bengio, Y. (2005). Hierarchical Probabilistic Neural Network Language Model. Aistats.
[6]: Mnih, A., & Kavukcuoglu, K. (2013). Learning word embeddings efficiently with noise-contrastive estimation, 2265–2273.
[7]: Mikolov, T., Karafiát, M., Burget, L., & Cernocký, J. (2010). Recurrent neural network based language model. Interspeech.
[8]: Mikolov, T., Yih, W., & Zweig, G. (2013). Linguistic Regularities in Continuous Space Word Representations. Hlt-Naacl.

[9]: Mikolov, T., Le, Q. V., & Sutskever, I. (2013, September 17). Exploiting Similarities among Languages for Machine Translation. arXiv.org.
[10]: Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011). Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12(Aug), 2493–2537.
[11]: Barkan, O., & Koenigstein, N. (2016, March 14). Item2Vec: Neural Item Embedding for Collaborative Filtering. arXiv.org.
[12]: Le, Q. V., & Mikolov, T. (2014, May 16). Distributed Representations of Sentences and Documents. arXiv.org
.

## 24.了解什么是正则表达式么？
本题解析来源：https://www.liaoxuefeng.com/wiki/897692888725344/923056128128864，配图则来自七月在线自然语言处理课程。

字符串是编程时涉及到的最多的一种数据结构，对字符串进行操作的需求几乎无处不在。比如判断一个字符串是否是合法的Email地址，虽然可以编程提取@前后的子串，再分别判断是否是单词和域名，但这样做不但麻烦，而且代码难以复用。

正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。

所以我们判断一个字符串是否是合法的Email的方法是：
1 创建一个匹配Email的正则表达式；
2 用该正则表达式去匹配用户的输入来判断是否合法。

因为正则表达式也是用字符串表示的，所以，我们要首先了解如何用字符来描述字符。在正则表达式中，如果直接给出字符，就是精确匹配。用\d可以匹配一个数字，\w可以匹配一个字母或数字，所以：
  &#39;00\d&#39;可以匹配&#39;007&#39;，但无法匹配&#39;00A&#39;；
  &#39;\d\d\d&#39;可以匹配&#39;010&#39;；
  &#39;\w\w\d&#39;可以匹配&#39;py3&#39;；
.可以匹配任意字符，
所以：
&#39;py.&#39;可以匹配&#39;pyc&#39;、&#39;pyo&#39;、&#39;py!&#39;等等。

要匹配变长的字符，在正则表达式中，用*表示任意个字符（包括0个），用+表示至少一个字符，用?表示0个或1个字符，用{n}表示n个字符，用{n,m}表示n-m个字符：

来看一个复杂的例子：\d{3}\s+\d{3,8}。我们来从左到右解读一下：
  \d{3}表示匹配3个数字，例如&#39;010&#39;；
  \s可以匹配一个空格（也包括Tab等空白符），所以\s+表示至少有一个空格，例如匹配&#39; &#39;，&#39; &#39;等；
  \d{3,8}表示3-8个数字，例如&#39;1234567&#39;。
综合起来，上面的正则表达式可以匹配以任意个空格隔开的带区号的电话号码。

如果要匹配&#39;010-12345&#39;这样的号码呢？由于&#39;-&#39;是特殊字符，在正则表达式中，要用&#39;\&#39;转义，所以，上面的正则是\d{3}\-\d{3,8}。

但是，仍然无法匹配&#39;010 - 12345&#39;，因为带有空格。所以我们需要更复杂的匹配方式。

进阶
要做更精确地匹配，可以用[]表示范围，比如：
  [0-9a-zA-Z\_]可以匹配一个数字、字母或者下划线；
  [0-9a-zA-Z\_]+可以匹配至少由一个数字、字母或者下划线组成的字符串，比如&#39;a100&#39;，&#39;0_Z&#39;，&#39;Py3000&#39;等等；
  [a-zA-Z\_][0-9a-zA-Z\_]*可以匹配由字母或下划线开头，后接任意个由一个数字、字母或者下划线组成的字符串，也就是Python合法的变量；
  [a-zA-Z\_][0-9a-zA-Z\_]{0, 19}更精确地限制了变量的长度是1-20个字符（前面1个字符+后面最多19个字符）。

A|B可以匹配A或B，所以(P|p)ython可以匹配&#39;Python&#39;或者&#39;python&#39;。
  ^表示行的开头，^\d表示必须以数字开头。
  $表示行的结束，\d$表示必须以数字结束。
你可能注意到了，py也可以匹配&#39;python&#39;，但是加上^py$就变成了整行匹配，就只能匹配&#39;py&#39;了。

re模块
有了准备知识，我们就可以在Python中使用正则表达式了。Python提供re模块，包含所有正则表达式的功能。由于Python的字符串本身也用\\u8f6c义，所以要特别注意：
s = &#39;ABC\\-001&#39; # Python的字符串
# 对应的正则表达式字符串变成：
# &#39;ABC\-001&#39;
因此我们强烈建议使用Python的r前缀，就不用考虑转义的问题了：
s = r&#39;ABC\-001&#39; 
# Python的字符串
# 对应的正则表达式字符串不变：
# &#39;ABC\-001&#39;

先看看如何判断正则表达式是否匹配：
>>> import re
>>> re.match(r&#39;^\d{3}\-\d{3,8}$&#39;, &#39;010-12345&#39;)
>>> re.match(r&#39;^\d{3}\-\d{3,8}$&#39;, &#39;010 12345&#39;)
>>>

match()方法判断是否匹配，如果匹配成功，返回一个Match对象，否则返回None。常见的判断方法就是：
test = &#39;用户输入的字符串
&#39;if re.match(r&#39;正则表达式&#39;, test):
    print &#39;ok&#39;
else:
    print &#39;failed&#39;

切分字符串用
正则表达式切分字符串比用固定的字符更灵活，请看正常的切分代码：
>>> &#39;a b   c&#39;.split(&#39; &#39;)
[&#39;a&#39;, &#39;b&#39;, &#39;&#39;, &#39;&#39;, &#39;c&#39;]

嗯，无法识别连续的空格，用正则表达式试试：
>>> re.split(r&#39;\s+&#39;, &#39;a b   c&#39;)
[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]

无论多少个空格都可以正常分割。加入,试试：
>>> re.split(r&#39;[\s\,]+&#39;, &#39;a,b, c  d&#39;)
[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]

再加入;试试：
>>> re.split(r&#39;[\s\,\;]+&#39;, &#39;a,b;; c  d&#39;)
[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]

如果用户输入了一组标签，下次记得用正则表达式来把不规范的输入转化成正确的数组。

分组
除了简单地判断是否匹配之外，正则表达式还有提取子串的强大功能。用()表示的就是要提取的分组（Group）。比如：
^(\d{3})-(\d{3,8})$分别定义了两个组，可以直接从匹配的字符串中提取出区号和本地号码：
>>> m = re.match(r&#39;^(\d{3})-(\d{3,8})$&#39;, &#39;010-12345&#39;)
>>> m

>>> m.group(0)
&#39;010-12345&#39;
>>> m.group(1)
&#39;010&#39;
>>> m.group(2)
&#39;12345&#39;

如果正则表达式中定义了组，就可以在Match对象上用group()方法提取出子串来。

注意到group(0)永远是原始字符串，group(1)、group(2)……表示第1、2、……个子串。提取子串非常有用。

来看一个更凶残的例子：
>>> t = &#39;19:05:30&#39;
>>> m = re.match(r&#39;^(0[0-9]|1[0-9]|2[0-3]|[0-9])\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])$&#39;, t)>>> m.groups()
(&#39;19&#39;, &#39;05&#39;, &#39;30&#39;)

这个正则表达式可以直接识别合法的时间。但是有些时候，用正则表达式也无法做到完全验证，比如识别日期：
&#39;^(0[1-9]|1[0-2]|[0-9])-(0[1-9]|1[0-9]|2[0-9]|3[0-1]|[0-9])$&#39;

对于&#39;2-30&#39;，&#39;4-31&#39;这样的非法日期，用正则还是识别不了，或者说写出来非常困难，这时就需要程序配合识别了。

贪婪匹配
最后需要特别指出的是，正则匹配默认是贪婪匹配，也就是匹配尽可能多的字符。举例如下，匹配出数字后面的0：
>>> re.match(r&#39;^(\d+)(0*)$&#39;, &#39;102300&#39;).groups()
(&#39;102300&#39;, &#39;&#39;)

由于\d+采用贪婪匹配，直接把后面的0全部匹配了，结果0*只能匹配空字符串了。

必须让\d+采用非贪婪匹配（也就是尽可能少匹配），才能把后面的0匹配出来，加个?就可以让\d+采用非贪婪匹配：
>>> re.match(r&#39;^(\d+?)(0*)$&#39;, &#39;102300&#39;).groups()
(&#39;1023&#39;, &#39;00&#39;)

编译
当我们在Python中使用正则表达式时，re模块内部会干两件事情：
  i)编译正则表达式，如果正则表达式的字符串本身不合法，会报错；
  ii)用编译后的正则表达式去匹配字符串。

如果一个正则表达式要重复使用几千次，出于效率的考虑，我们可以预编译该正则表达式，接下来重复使用时就不需要编译这个步骤了，直接匹配：
>>> import re
# 编译:
>>> re_telephone = re.compile(r&#39;^(\d{3})-(\d{3,8})$&#39;)
# 使用：
>>> re_telephone.match(&#39;010-12345&#39;).groups()
(&#39;010&#39;, &#39;12345&#39;)
>>> re_telephone.match(&#39;010-8086&#39;).groups()
(&#39;010&#39;, &#39;8086&#39;)

编译后生成Regular Expression对象，由于该对象自己包含了正则表达式，所以调用对应的方法时不用给出正则字符串。

练习与总结
请尝试写一个验证Email地址的正则表达式。
版本一应该可以验证出类似的Email：
someone@gmail.com
bill.gates@microsoft.com

版本二可以验证并提取出带名字的Email地址： tom@voyager.org

最后为方便快速查阅，附一张正则表达式的快速查阅图
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156912802151684336.png'/>
## 25.如何理解NNLM（Neural Network Language Model）模型？
本题解析来源：https://blog.csdn.net/qq_39422642/article/details/78658309?tdsourcetag=s_pcqq_aiomsg，参考：https://shomy.top/2017/07/28/word2vec-all/

1 基本概念
传统的机器翻译，自然语言处理多是基于规则的，现在更多的是基于模型，规则隐含的参数里。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415692507657682586.png'/>

词编码 
每个词都用不同的含义，而要被机器所识别，就必须要把词进行编码，同时词编码时要保证词的相似性。图像识别的时候，对图像在RGB三个颜色通道中看他们的相似性就可以了，但是，无论中文还是英文，词都太多了，他是人造的，很难保持像图片这样的信息，所以我们希望能对词进行编码，保持它所包含的信息量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925077029995836.png'/>
因此，我们希望能有一个对应关系，如图，这些数字在空间中的表示能有一个对应关系。这不就是和机器学习差不多吗？很多机器学习的预测都是寻找一个对应关系，也就是数据（X）和预测的东西（Y）的对应。机器翻译其实原理也差不多。 


向量空间子结构 
我们希望找到这样一个关系，可以作为机器学习/深度学习的输入.
VKing−VQueen+VWomen=VMan
VKing−VQueen+VWomen=VMan


这个有没有感觉呢？其实换成这样，你可能更好理解:
VKing−VQueen=VMan−VWomen
VKing−VQueen=VMan−VWomen
我们就是希望找到King,QueenKing,Queen之间的差异，隐含的关系，然后通过一个dense vector表示。

One-Hot 
最简单的一种想法，就是对一句话用one-hot编码:比如对于这句话：
John likes to watch movies,Mary likes too.
John also likes to watch football games.

"John":1,"likes":2,"to":3,"watch":4,"movies":5,"also":6,"football":7,"games":8,"Mary":9,"too":10

用one-hot可以表示为：
John:[1,0,0,0,0,0,0,0,0,0]
likes:[0,1,0,0,0,0,0,0,0,0]
...
too:[0,0,0,0,0,0,0,0,0,1]

但事实是，这样做耗费的资源太多，而且不能很好的表征每句话的特性。

Bag of words(词袋模型) 
另一种方法则是词袋模型，它相当于一个词袋，不考虑词/句之间的相关性，只要出现了该词，就会记为1，再次出现就会+1。比如前面的那句话：
John likes to watch movies,Mary likes too.

可以表示为
[1,2,1,1,1,0,0,0,1,1]

与其相似的是binary weighting,它就是看一下每个词是否出现，出现记为1，没出现记为0
[1,1,1,1,1,0,0,0,1,1]

但他们都有一个缺点，不能将每个单词所代表的中心表示出来，比如John 和to,watch他们都是1，怎么直到那个词更重要呢？

TF-IDF 
因此，就有tf-idf解决这个问题，它的主要思路就是有两方面： 
A—第一就是如果这个词在我们当前文档出现的频率非常高，说明它在当前文档应该是比较重要的。 
B-但如果它在所有的文档中出现的频次都非常好，大家可能就会觉得这个词应该不是那么重要的。

比如中文的“的“，或者我们上面那两个句子中的to. 
因此，tf-idf就是一个在当前文档和所有文档中权衡他们的重要性，然后计算出每个词的重要度的方法。

语言模型 作为Word Embedding的背景, 语言模型(Language Model)也是很有必要简要介绍一下。
统计语言模型就是用来计算一个句子的概率分布简单来说，就是计算一个句子的概率, 语言模型用处很广泛,比如机器翻译中, 如何挑选一个概率尽可能大的句子也就是尽量靠谱的句子返回. 假设一个长度为m的句子,包含词:[(w1,w2,w3,..,wm), 
那么这个句子的概率,也就是这m个词共现的概率:<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925419251452802.png'/>

一般情况, 语言模型都是为了使得条件概率: P(wt|w1,w2,..,wt−1)最大化, 不过考虑到近因效应, 当前词与距离它比较近的n个词更加相关，而非前面所有的词都有关, 因此上述公式可以近似为:<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925443879348816.png'/>
上述便是经典的n-gram模型的近似表示方式。

语言模型可以考虑到词之间的前后关系，缺点就是n-gram随着预料的增多，离散化越严重，最终导致数据稀疏的问题。

分布式表示 
如果数据的维度过高，经常需要用到分布式来表示，它是个什么东西呢？ 
比如有这么一个例子：
红色的大型卡车，黄色的中型轿车，蓝色的小型电动车

可以发现，这几个词之间都有一定的模式存在，我们可以通过这些模式，把表达的空间压缩的三个维度：颜色，车型，那个品牌的车。然后他们做一个笛卡尔积就可以有:
需要记忆的单元数=颜色 X 型号 X 车型

其中，在现代统计自然语言处理中，有一个非常有洞见的想法，就是：
能否用一个词附近的其他词来表示该词？

就像你认识一个新认识的朋友，想直到他的收入，你可以看一下他周围10个朋友的收入大致是多少，来推断他的收入是多少一样。

共现矩阵 
基于前面那个想法，就有了这样一个局域窗口，这个窗口的主要作用就是看一下周围的几个词才好，窗口的长度一般设为5-10。就像你看你朋友的收入一样，你是看周边的五个人还是10个人呢？

那他具体怎么表示呢？ 
假设为三句话： 
I like deep learning,
I like NLP,
I enjoy flying

假设局域窗口为1，可以得到这样的对称矩阵 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925104327717944.png'/>

可以发现I 的前后一共出现了2次like，所以第一行第二列是2。
其实共现矩阵的本质代表着词与词之间的连接，且不考虑顺序。因为他是对称的，所以一般会取行向量或者列向量来表示词向量。

其中面临的问题就是向量的维数随着词典的大小呈先行增长，且对于模型有着严重的稀疏性问题。

2.NNLM（Neural Network Language Model）神经网络语言模型
NNLM的基本思想
在一开始的时候，做自然语言处理可以发现很多问题，比如很多情况下，要做平滑处理等，因此深度学习慢慢开始火了之后，就有人说，不然咱来试一下神经网络来做这个吧。


他的本质就是：直接从语言模型出发，将模型最优化的过程转换为求词向量表示的过程。 
最优的方向是这个目标函数：用“我爱自然语言处理“为例,窗长度为n-1：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925110098567176.png'/>
这个是n=3的通俗表达。 

概率p满足归一化条件，因为词典中的每个词都有一个概率:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925108395493047.png'/>

NNLM原理 
神经语言模型(NNLM)最初由Bengio提出的A Neural Probabilistic Language Mode最为经典， word2vec便是从其中简化训练而来. Bengio通过下面的一个三层神经网络来计算P(wt|wt−1,wt−2...wt−n(+1))。
其原理图如下所示：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925110832083434.png'/>
首先第一层输入就是前n−1个词wt−(n+1),...,wt−1 去预测第t个词是wt 的概率. 这里面的矩阵C∈|V|×d维护着词汇表中所有词的词向量, 其中|V|是词汇表中词的个数, d是词向量的维度. 然后根据输入的前n−1个词, 在C中找到它们对应的词向量, 然后直接串联起来成为一个维度为(n−1)d的向量x 作为接下来三层神经网络的输入，后面就是普通神经网络了。

需要说明的是,因为我们那要预测概率最大的wt, 因此最后输出层的神经元应该与词汇表大小同样为|V|, 这里使用使用softmax函数归一化输出层的值到[0,1], 代表可能的每个词的概率. 此外在原文中, 存在一些直连边, 也就是上图中的虚线, 从输入层直接到输出层，是一个线性变换。Bingo在文中表示, 直连边的存在大幅降低迭代次数, 但对语言模型效果无提升, 随着计算能力的提高, 后续的工作基本都去掉了直连边。

整个模型可以解决类似这样的问题：假设我们有一分文本，我们可以通过他的前N−1个词预测他的第N个词应该是什么？

projection layer 
举个例子，有这样一个句子：“我爱自然语言处理“，拆开就是‘我’，’爱’，’自然’，’语言’，一共四个词，预测一下，下一个词是什么？ 
如果每个词都给一个索引表示，’我’为0，’爱’为1。

C矩阵为投影矩阵，其中词典的维数为v，假设v=10000。 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925113218237384.png'/>
那么‘我‘和‘爱‘的one-hot向量表示为：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415692511387257335.png'/>

每个列向量都有D行，那么一个词对应的列向量乘以这个矩阵C就可以得到
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925115965306031.png'/>

这样的乘法相当于把一个句子中的一个词取出来了，将每个词的结果进行拼接，就可以得到一句话，这句话的向量表示为
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156925115378871189.png'/>的列向量。

总结一下这一层做的事：主要是把一句话用one-hot向量表示，通过一个权重矩阵，得到表示这一句话的词向量。

hidden layer 
这一部分主要做的就是将上一层的输出作为输入，进行全连接，然后一般会有个tanh，来处理这些数据。

SoftMax层 
隐层出来之后，接一个SoftMax分类器，预测一下，在这10000个词的词表中，出现每个单词出现概率有多大。因此拿到的是一个10000X1 的概率向量。 

因为我们有标准答案，就是”我爱自然语言”的第五个词应该是“处理“，如果预测出来的不准确，就可以通过定义一个交叉熵损失函数来计算损失，通过BP算法来调整参数C。
## 26.请详细推导下word2vec(Xin Rong牛论文的解读)
本题解析来源：https://shomy.top/2017/07/28/word2vec-all/ 

目前需要做Network Embedding方面的课题，而复杂网络本身就经常借鉴NLP的一些算法模型, Embedding也不例外. 因此先从Word Embedding入手。之前对Word Embedding(暂且翻译为词嵌入或者词向量)的理解就是将单词根据某种特征转为数值向量，再来做其他工作比如文本分类的工作。而word2vec则是word embedding的一种模型，也是目前使用最广的词向量模型, 由Google的Mikolov团队2013年提出。之前仅仅能够使用第三方库来训练直接使用, 对其中原理并没有多少理解, 这篇博客则比较完整的从背景知识到原理，参数训练等方面整理一下word2Vec。

Mikolov的两篇文章中涉及word2vec的细节甚少. 有不少人都对此模型作出了更加详细的解释, 本文主要沿着Rong, X.word2vec Parameter Learning Explained这篇文章的思路来整理一下，很多公式参考于这篇文章。

参考:
Mikolov, T.(2013). Distributed Representations of Words and Phrases and their Compositionality.
Mikolov, T.(2013). Efficient Estimation of Word Representations in Vector Space.
Rong, X. (2014). word2vec Parameter Learning Explained.

背景
神经网络
引入word2vec之前需要先对神经网络的知识有一定了解, 这里只贴一张图说明一个简单三层神经网络，如下图, x=[x1,x2,...xK]是模型的输入, 中间经过与权重矩阵<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932500382113185.png'/>运算, 矩阵运算结果再经过非线性激活函数得到隐层的结果h, 从隐层到输出层同理. 这样从输入层到输出层既有线性变换,又有非线性变换, 因此可以更好刻画出输入变量的特征。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932503956394300.png'/>
神经语言模型
作为Word Embedding的背景, 语言模型(Language Model)也是很有必要简要介绍一下.

统计语言模型就是用来计算一个句子的概率分布

简单来说，就是计算一个句子的概率, 语言模型用处很广泛,比如机器翻译中, 如何挑选一个概率尽可能大的句子也就是尽量靠谱的句子返回. 假设一个长度为m的句子,包含词:[(w1,w2,w3,..,wm)，那么这个句子的概率,也就是这m个词共现的概率:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932513663330365.png'/>

一般情况, 语言模型都是为了使得条件概率: P(wt|w1,w2,..,wt−1)最大化, 不过考虑到近因效应, 当前词与距离它比较近的n个词更加相关(一般n不超过5),而非前面所有的词都有关, 因此上述公式可以近似为:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415693252738649414.png'/>

上述便是经典的n-gram模型的近似表示方式. 下面需要介绍一下神经语言模型(NNLM), 最初由Bengio提出的A Neural Probabilistic Language Mode最为经典, word2vec便是从其中简化训练而来. Bengio通过下面的一个三层神经网络来计算P(wt|wt−1,wt−2...wt−n(+1))：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932530668515851.png'/>
首先第一层输入就是前n−1个词wt−(n+1),...,wt−1wt−(n+1),...,wt−1 去预测第t个词是wt的概率. 这里面的矩阵C∈|V|×d维护着词汇表中所有词的词向量, 其中|V|是词汇表中词的个数, d是词向量的维度. 然后根据输入的前n−1个词, 在C中找到它们对应的词向量, 然后直接串联起来成为一个维度为(n−1)d的向量x 作为接下来三层神经网络的输入, 后面就是普通神经网络了。

需要说明的是,因为我们那要预测概率最大的wt, 因此最后输出层的神经元应该与词汇表大小同样为|V|, 这里使用使用softmax函数归一化输出层的值到[0,1], 代表可能的每个词的概率. 此外在原文中, 存在一些直连边, 也就是上图中的虚线, 从输入层直接到输出层, 是一个线性变换, Bingo在文中表示, 直连边的存在大幅降低迭代次数, 但对语言模型效果无提升, 随着计算能力的提高, 后续的工作基本都去掉了直连边.

神经语言模型构建完成之后,就是训练参数了. 这里的参数包括词向量矩阵C, 以及三层神经网络的权重, 偏置等参数. 训练数据就是大堆大堆的预料库. 训练结束之后, 语言模型得到了, 词向量也得到了. 换言之, 词向量是这个语言模型的副产品. 但是这个模型的缺点就是速度问题, 因为词汇表往往很大,几十万几百王, 训练起来就很耗时, Bengo仅仅训练5个epoch就花了3周, 这还是40个CPU并行训练的结果. 因此才会有了后续好多的优化工作, word2vec便是其中一个.

Word2Vec
简介
背景介绍完毕, 终于到主角了. word2vec是google于2013年的Distributed Representations ofWords and Phrases and their Compositionality 以及后续的Distributed Representations ofWords and Phrases and their Compositionality 两篇文章中提出的一种高效训练词向量的模型, 基本出发点是上下文相似的两个词,它们的词向量也应该相似, 比如香蕉和梨在句子中可能经常出现在相同的上下文中，因此这两个词的表示向量应该就比较相似.

word2vec模型中比较重要的概念是词汇的上下文, 说白了就是一个词周围的词, 比如wt的范围为1的上下文就是wt−1和wt+1. 在word2vec中提出两个模型(假设上下文窗口为3)

CBOW(Continuous Bag-of-Word): 以上下文词汇预测当前词: wt−1,wt+1去预测 wt
SkipGram: 以当前词预测其上下文词汇: wt 去预测wt−1,wt+1

两个模型图示如下
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932542888568931.png'/>
下面将会从最简单的上下文只有一个词的情形入手, 然后扩展到CBOW以及Skip-gram, 介绍原理以及参数训练过程. 关于word2vec的训练这里将会从完全的BP神经网络的过程来介绍。

One-Word Model
首先先看简化版入手: 输入输出都只有一个词, 如下图示:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933399243251430.png'/>
首先说明符号:
V: 词汇表长度; N: 隐层神经元个数, 同时也是词向量维度
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933416211290600.png'/>：输入层到隐层的权重矩阵, 其实就是词向量矩阵,其中每一行代表一个词的词向量
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933423931286960.png'/>：隐层到输出层的权重矩阵, 其中每一列也可以看作额外的一种词向量

下面从神经网络的前向过程开始介绍:
我们需要做的是用输入的词去预测输出的词. 其中 输入层的单词wI使用one-hot来表示的, 即在上图中x1,x2,x3,...,xV只有xk为1, 其余为0, 其中k可以是输入的词在词汇表中的索引下标。之后就是经过词向量矩阵W连接输入层和隐层. 其中由于X中只有一个1, 因此经过与W相乘, 相当于取出W中的第k行，实际也就是输入单词的wI的N维的词向量，使用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933436784247204.png'/>表示，来作为隐层的值，注意word2vec的隐层并没有激活函数:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933441348742956.png'/>

然后考虑从隐层的h到输出层Y, 同样h经过矩阵W′相乘，得到一个V×1的向量u:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415693345038428885.png'/>
其中u中的每个元素<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933461833468244.png'/>就是W′的第j列：用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933466565509846.png'/>表示，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933466565509846.png'/>与h做内积得到: <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933470292881474.png'/>，含义就是词汇表中第j个词的分数，我们的目的就是要根据输入词wI去预测输出的词，因此预测的词就取分数最高的即可。

这里为了方便概率表示，使用softmax将u归一化到[0,1]之间, 从而作为输出词的概率, 其实是一个多项分布, 也就是上图中的y:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933478828014034.png'/>

其中<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933486213632671.png'/>与<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933486591104120.png'/>都称为词w的词向量，一般使用前者作为词向量，而非后者，原因后续会解释。至此前向过程完成，就是给定一个词作为输入，来预测它的上下文词，还是比较简单的，属于简化版的神经语言模型。这个过程中需要用到的参数有两个词向量矩阵W,W′，下面就是重点了，介绍如何根据语料库来训练模型，更新参数，得到最终的词向量。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933532510144847.png'/>
到此为止， 一个训练样本的反向传播训练过程就为止了。 我们可以看到，对于输入层到隐层的矩阵W，我们每次训练只需要更新一行向量即可，而对于隐层到输出层的矩阵W′的所有N×V个元素都需要更新一遍，这里的计算量还是很大的。这一节主要比较细致的介绍了最简单的输入输出只有一个单词的情况的推理和训练的过程，后面的CBOW(上下文预测单词)以及SG(单词预测上下文)均基于这一节扩展开来。

CBOW Model
这一部分讲word2vec的第一个形式: Continurous Bag-Of-Word，模型图示如下:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933536021583652.png'/>
跟上一个模型唯一的不同就是输入不再是一个词wI, 而是多个词，上图中一共有C个单词: x1k,x2k,...,xCk，每个x都是one-hot表示。 这样隐层的h的计算就会不同了: 之前一个单词的模型是直接取出W的一行<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933544723147001.png'/>作为h的值，在CBOW中则是取出W中输入的所有C个单词的词向量，然后直接取平均，如下:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933540692252654.png'/>

后面隐层到输出层的过程与One-Word Model 一模一样，包括目标函数定义， 反向传播训练等。将W′的更新公式照抄下来如下,依旧是每次都需要更新所有的行:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933556266255795.png'/>

隐层神经元的梯度也相同:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933561236111035.png'/>

下面考虑输入层到隐层稍微有些不同，在One-Word Model里面因为输入只有一个词，因此每次训练只更新这个词对应到W的那一行，但是在CBOW里面有多个词，这里采取的策略是将hh的梯度均摊到每个词上，因此每次训练会更新W中的C行，如下:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933564744121022.png'/>

到此为止 CBOW 的推理和训练过程也介绍完毕，基本跟One-Word Model 一样。

SkipGram Model
现在开始介绍word2vec的第二种形式: SkipGram(根据单词预测上下文)，这个模型与One-Word Model不同的地方在于，SG的输出有多个词，而非One-Word 中输出只有一个词，这样输出层就不是一个多项分布了，而是C个多项分布了，

模型图示如下：

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933585910070681.png'/>

因此从输入层到隐层部分与One-Word Model 相同，隐层神经元的计算方式如下:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933591963138753.png'/>

因为输出层是有C个单词， 因此有C个多项分布: y1,y2...yC, 因此前向计算的过程也需要分开计算，如下公式，用来计算第c个输出单词的预测的多项分布中第j项，相比One-Word Model 多了一个c参数:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933595936280024.png'/>

需要主要的是这C个输出向量是相互独立的，可以当做是独立的C个One-Word Model 中的输出向量，相互之间没有影响，并且从图中也可以看出，连接隐层与C个输出层的参数矩阵W′是共享的，于是便有: <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933602574149387.png'/>

这里的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415693360812860055.png'/>的含义与One Word Model 中相同，都代表W′的第j列，同时也是词汇表中第j个单词的一种词向量(虽然实际中不用)。从前向后 根据上述公式计算出C个输出向量之后，在每个V维向量中选取概率最大的作为输出的单词，这样根据输出单词wI就得到了C个输出单词，也就达到了根据单词预测上下文的目的。

下面开始介绍SG的反向传播训练的过程，这个跟前面的有些许的不同, 首先是损失函数:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933612843278322.png'/>

前面说过输出的C个词是相互独立，因此<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933616694598396.png'/>, 此外<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933621342815638.png'/>的含义同One-Word Model 中的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933624099921471.png'/>一样，都代表训练的真实的输出单词在词汇表的下标。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156933629761306020.png'/>

优化
复杂度
前面的CBOW与SG模型是标准的word2vec模型，或者说是神经语言模型的简化版，去掉了隐层的激活函数，其余的变化不大，因此训练效率还是很低的。

我们分析下训练的复杂度。首先明确需要学习的两个词向量矩阵W,W′，从前面的推导中知道对于每一个训练样本，CBOW更新W的C行，SG更新W其中一行，也就是每次更新有限个词的词向量。但是对于W′则不同了，正如前面一直提到的，无论是CBOW还是SG，对每个训练样本(或者Mini Batch)从梯度更新中需要对W′的所有V×N个元素，也就是词汇表中所有V个单词都需要更新词向量，考虑现实任务词汇表一般是几十万，上百万千万级别的， 这个计算成本是巨大的。

关于计算成本大的原因，除了上面提到的训练部分，还有就是在每次前向计算的时候，隐层到输出层的softmax函数计算输出层V个元素，计算量也是很大，这样整个模型现实意义不大。

考虑到计算量大的部分都是在隐层到输出层上，尤其是W′的更新。因此word2vec使用了两种优化策略: Hierarchical Softmax 和 Negative Sampling。二者的出发点一致，就是在每个训练样本中，不再完全计算或者更新W′这个矩阵。二者都不再显示使用W′这个矩阵。

因此这也就解释了前面说的为什么不用W′作为最终词向量。在多一句，其实上述训练和推理的复杂度很大的根本原因是softmax的分母上的∑，因此在求梯度的时候，就会有V次的计算。因此下面的两种方法其实是对softmax的优化，不仅仅限制在word2vec.两种优化方式使得word2vec的训练速度大大提升，并且词向量的质量几乎没有下降，这也是word2vec在NLP领域如此流行的原因。

这里只介绍其中一种优化算法：Hierarchical SoftMax。

Hierarchical SoftMax
首先Hierarchical SoftMax(HS)并不是word2vec提出来的, 而是之前Bengio在2005年最早提出来专门为了加速计算神经语言模型中的softmax的一种方式, 这里介绍如何在word2vec中使用. 

HS主要基于哈夫曼树(一种二叉数)将计算量大的部分变为了一种二分类的问题. 先看下面的图，原来的模型在隐层之后通过W′连接输出层, 现在HS则去掉了W′, 隐层h直接与下面的二叉树的root节点相连：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932079953438540.png'/>
其中图中白色的叶子节点表示词汇表中所有的|V|个词, 黑色节点表示非叶子节点, 每一个叶子节点也就是每一个单词, 都对应唯一的一条从root节点出发的路径。而我们的目的是使的w=wO这条路径的概率最大，即: P(w=wO|wI)最大, 此时每一个分支都代表一个选择, 向左转还是向右转. 所以如何判断向左还是向右呢? 

我们用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932096590847829.png'/>表示从root到叶子节点w的路径上的第j个非叶子节点, 并且每个非叶子节点都对应一个向量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932102811228403.png'/>，维度与h相同, 然后使用一个sigmod函数: <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932131541126578.png'/>, 结合向量的内积, 来判断该向左还是向右, 如下, 第n个节点向左 以及向右的概率定义：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415693213649981981.png'/>

有了上述的概率, 我们可以重新定义P(wO|wi)了：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932160782034408.png'/>

其中I()是指示函数, 条件成立值为1, 反之为-1. 而L(w)表示整条路径的长度, 这样整个概率就是从root节点到叶子节点这条路径的概率, 这样我们在训练的时候, 通过训练样本来更新非叶子节点的参数v′w.

举个例子, 比如上图中的加粗的黑色路径: (n(w2,1),n(w2,2),n(w2,3),w2(n(w2,1),n(w2,2),n(w2,3),w2 , 就是说假设有一个训练样本是(wI,w2), 我们需要使得P(wO=w2|wI)概率最大:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932197471243141.png'/>

并且在一个非叶子节点处, 向左向右的概率和为1, 因此一直分裂下去,最后的和肯定还是1. 因此可以很容易得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932202684423110.png'/>

这一点的证明是有必要的, 因为在原始的softmax本身保证了所有单词的概率和是1, 而通过上式也知道了通过HS得到的输出层仍然是一个概率多项分布, 输出所有的单词概率和为1.

讲完了从前向后的如何得到输出单词的概率的过程, 下面开始后向传播的训练过程.。

首先需要明确的是训练的参数: 输入层与隐层的词向量矩阵W, 以及二叉树的非叶子节点对应的向量<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932216066412765.png'/>。

为了书写方便,下面简化一部分的符号: 用[I]表示前面的指示函数I(n(w,j+1)==left), 使用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932228316836686.png'/>表示<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase641569322289906486.png'/>。

对于一组训练数据, 损失函数的定义与前面相同, 最大似然(注意这里以One-Word Model为例，CBOW与Skip-Gram按照上述的思路完全一样)：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932238064727751.png'/>

之后便可以逐项求梯度了, 先考虑<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415693224974527821.png'/>, 注意<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932250552539245.png'/>：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932251433225928.png'/>

之后对[I]分情况讨论, 可得:<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932256676898368.png'/>

这里如果[I]=1, 那么tj=1, 否则 tj=0, 这个公式与前面的yj−tj很类似, 可以理解为预测值与实际值的差别。

有了上述的梯度,就可以很简单的求出<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932281131851034.png'/>的梯度了：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932278177987766.png'/>

有了梯度,便可以更新了, 具体公式还是梯度下降：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932286990541555.png'/>

也就是说对于一个训练样本， 我们只需要更新L(w)−1个向量就好了， 而未优化的版本需要更新V个， 相当于时间复杂度从O(V)O(V)降到了O(logV), 这个提升还是非常大的。
虽然在考察空间复杂度方面，HS的二叉树的非叶子节点有V−1个，也就是我们需要V−1存储<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932298814615118.png'/>,优化之前则是V个， 空间复杂度相同， 但总体而言，时间复杂度大大降低了。

然后考虑隐层h的梯度，因为我们的优化目标都是在隐层到输出层，因此前面的几乎不变， 跟One-Word Model 一样，路径上的非叶子节点的表达式都含有hh，因此需要对梯度求和：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64156932307786980718.png'/>
其实跟前面很一样了， 只需要替代下导数就好了， 后面就不再赘述了。整个Hierarchical Softmax的优化算法介绍完了，隐层到输出层的计算量从O(V), 利用二叉树(更加具体来说是哈夫曼树）降为了O(logV)。

## 27.word2vec 相比之前的 Word Embedding 方法好在什么地方？
本题解析来源：https://www.zhihu.com/question/53011711

解析一
@邱锡鹏：Word2vec训练方面采用的HSoftmax以及负采样确实可以认为是创新不大。但Word2vec流行的主要原因也不在于此。主要原因在于以下3点：
1. 极快的训练速度。以前的语言模型优化的目标是MLE，只能说词向量是其副产品。Mikolov应该是第一个提出抛弃MLE（和困惑度）指标，就是要学习一个好的词嵌入。如果不追求MLE，模型就可以大幅简化，去除隐藏层。再利用HSoftmax以及负采样的加速方法，可以使得训练在小时级别完成。而原来的语言模型可能需要几周时间。
2. 一个很酷炫的man-woman=king-queen的示例。这个示例使得人们发现词嵌入还可以这么玩，并促使词嵌入学习成为了一个研究方向，而不再仅仅是神经网络中的一些参数。
3. word2vec里有大量的tricks，比如噪声分布如何选？如何采样？如何负采样？等等。这些tricks虽然摆不上台面，但是对于得到一个好的词向量至关重要。

举一个生活中的例子，语言模型和word2vec的关系可以类比于单反相机和美颜手机，它们的受众不一样。就照片质量（MLE）而言，单反肯定好。但如果更关心美颜（词嵌入）和便携性（训练速度），美颜手机就更受欢迎。

更多的资料可以参考：
https://nndl.github.io/ch12.pdf 

解析二
@吴海波：最近正好写了篇相关的文章，原文如下：

引子
此时再谈Word2Vec，特别是ELMo、Bert相继大火后，有点炒冷饭的意味。但有一个点，很多同学可能好奇过却没有深究：自然语言处理里面应用Distribution representation做Embedding，word2vec不是第一个，对比Matrix Fatorication的方法，比如SVD、LSA，为何Word2Vec出来后，Emebedding变的这么火？

面试的时候，如果聊得时间有多，会顺便问下这个问题，大多数同学都没有想过这个问题，能提到Word2Vec比如SVD之类的更容易训练更多的数据，已经寥寥无几。

其实这个问题学术界相关的研究很多，但不熟悉的同学往往不知道该用什么关键词，而Word2Vec的相关的文献不计其数，导致真正有价值的信息被埋没了，比如上面这个问题，你如果搜why word2vec so good，出来的大多是描述其原理的（知乎上就有很好的讨论帖[2][3]），需要搜的key是word2vec matrix fatorication，这就需要你了解一些相关的背景。准确的定义问题的描述，往往比问题的答案更重要。

本文参考了一些相关论文，尽量少引入公式（主要是我懒，公式编辑太麻烦了），尝试解答上述问题。

背景知识
Word2Vec
详尽的原理，网上有很多非常好的资料，这里不再赘述。Word2Vec和Deep Learing的关系并不深，至少一点也不Deep。13、14年那段时间，很多学者都在尝试解释它到底学到了什么。无论是CBOW还是Skip-Gram，本质还是要基于word和context做文章，即可以理解为模型在学习word和context的co-occurrence。

参考[1]本文重点关注Skip-Gram with Negative Sample，简称为SGNS，让我们来回顾下它的目标函数：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157034184239331650.jpg'/>

再介绍论文[1]如何将它一步步变成和Matrix Factorization等价前，我们先来了解下MF的背景。

Matrix Factorization
MF即矩阵分解，典型的算法有SVD、LSA，是一种常见的技术，在推荐、NLP都有应用。据资料显示，百度在2010年之前，就做过大规模分布式的plsa算法。简单来讲，MF就是将一个矩阵分解成多个矩阵的乘积。让我们回顾下word2vec，最终每一个word都会有两个向量：V_word和V_context，假设存在一个矩阵W = V_word * V_context，word2vec也可以理解成是对矩阵W的矩阵分解。那么，问题就变成这个matrix W是什么？

Pointwise Mutual Information（PMI）
论文[1]中有将SGNS的目标函数，一步步的变换，变成下面的形式（具体见论文）
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157034185955190239.jpg'/>

上述公式中，左边的部分就是大名鼎鼎的PMI，即w和c同时出现的次数/(w出现的次数 + c出现的次数)。和MF对比，就多了个常量logk。PMI是dense matrix，对模型优化带来很多困难，所以一般会用sparse matrix PPMI（positive PMI）来替代它。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157034187556666755.jpg'/>

到此，可以说明word2vec和MF并没有本质的区别，是一种implicit的MF，正如论文[1]的标题。既然其本质差别不大，是否MF的测试效果也能和word2vec一致呢。首先，在word similar上，SVD和Word2vec差距并不大，但在word analogies上，word2vec要明显好于SVD。

Why
训练大规模语料的能力
这个当然是核心优点，虽然SVD有SMF的版本去处理大数据，LSA也有大数据的方案，但谁都没有word2vec的简单易实现。google一开始的开源代码，根本不需要分布式就能跑以前不能跑的数据规模。

Simple is Power！

举个不恰当的例子：很多人怀疑牛顿是先发明了微积分，后面才做出了很多重要的工作，为了装逼在他的著作里面用传统的数学方法重写了一遍，各种奇技淫巧。当然这个逼装的有点过，导致后面和莱布尼茨关于微积分的发明权吵了一辈子。

很多论文，因为word2vec的成果，有了方向后再去回溯传统，其价值自然远不如开创者。数据量的问题，在之前可能都不是核心问题，再加上LSA模型稳定，在小数据集上效果挺好，数据规模增长一点不一定能看出收益，而不像word2vec，在数据集小的时候，可能还不如CountVectorizer，必须往大数据去。

再一次，定义问题是什么，比找到方法难。

MF suffer from unobserved value
在word-context matrix中，这是个常见的问题。另外，对matrix中的value做不同的weight也不是件容易的事情。最重要的是，MF类的算法在analogies task上有明显劣势。即当初那个经典的word2vec展示case：king - queen = man - woman，即词向量可以相互做加减，具有实际意义。

这个case有点特殊，并不是所有的词向量都成立，但确实比MF类的模型总体上好。比较酷炫，其原理我还没有找到特别好的解释，如果有童鞋知道，求告知。

融合二者
Glove是其中代表，即结合了global matrix factorization and local context window methods。论文[5]中给出了不错的实验数据，但也有不少人质疑它，比如论文[6]。另外，word2vec比glove简单多了，也更容易被大家接受。我们自己实践来看，glove也没有明显增量。

最后
这篇是好奇心驱动的产物，实用价值并不高，只为答疑解惑。前期在搜索过程中看了不少无效的资料，浪费了不少时间，希望本文能给感兴趣的同学一些帮助。

参考文献
[1] Neural Word Embedding as Implicit Matrix Factorization
[2] word2vec 相比之前的 Word Embedding 方法好在什么地方？
[3] 词向量，LDA，word2vec三者的关系是什么
[4] What is the connection between PCA and Word2Vec in terms of word embedding? Is there an empiric superiority between the two?
[5]GloVe: Global Vectors for Word Representation 

[6]Improving Distributional Similarity with Lessons Learned from Word Embeddings
[7]Linguistic Regularities in Sparse and Explicit Word Representations
[8]ALL-BUT-THE-TOP: SIMPLE AND EFFECTIVE POSTPROCESSING FOR WORD REPRESENTATIONS
[9]Enriching Word Vectors with Subword Information

解析三
@李韶华：词嵌入模型效果好不好的关键之一，是用上下文词预测当前词的formulation，即采用的回归函数。

Hinton等07年和08年的log bilinear language model之前的工作都采用的是 softmax([上下文词向量,当前词向量]的线性变换) 的形式，softmax里边可以简化认为是一些向量的线性和。但几个向量的线性和不能很好的抓住这几个向量在隐空间一些维度上取值接近的特点，所以效果并不好。

07年的Three New Graphical Models for Statistical Language Modelling里，三个模型之一是log bilinear language model (LBL), 题目中08年的论文扩展了这个方法，得到Hierarchical Log Bilinear Language model。

为了叙述简单，下面把这两种方法统称为LBL。LBL使用了 softmax(上下文词向量的线性变换 * 当前词向量) 的形式，点乘在抓两个向量在一些维度上取值接近方面，比相加要好得多，这是词向量模型发展的一个重大突破。

word2vec使用的也是LBL。那么和之前的方法有什么区别呢？08年的Hierarchical LBL里，用的是这样的回归函数:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157034315393488470.svg'/>

这里的Ci都是矩阵，不同位置对应不同的矩阵。
word2vec的CBOW用的是（skip-gram我觉得和CBOW基本是等价的，效果也类似，但CBOW的概率解释好些，所以拿它来比较）:
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157034316295567998.svg'/>

可见它移除了变换矩阵Ci和偏移量bw. 实际上我们事后诸葛来看，变换矩阵Ci也的确是多余的，因为 两个词经常一块出现两个词在某方面有相似语义两个向量在某些维度取值类似，那么直接点乘就可以了，用Ci变换一下，反而有可能把本来相似的维度变得不同，从而让学出来的向量不能很好满足“相似词在有些维度上取值相近”的训练效果。

显而易见，移除Ci会极大的提高运算速度，使word2vec在大语料上训练非常可行。两个LBL模型训练语料都是1000w单词左右，而word2vec即使用wikipedia这样>20亿词规模的语料也只需几小时，大语料下得出的词向量当然会抓住更多的语法语义规律，从而更准确。

所以我觉得，word2vec的成功，印证了一句话：Less is more.
## 28.说说NLP中的预训练技术发展史：从Word Embedding到Bert模型
本题解析的作者：张俊林，链接：https://zhuanlan.zhihu.com/p/49271699

July注：本文是了解bert历史的最佳文章，把整个来龙去脉介绍的高屋建瓴、通俗细致，全文主要有这些内容：预训练在图像领域的应用、从语言模型到Word Embedding、从Word Embedding到ELMO、从Word Embedding到GPT、Bert的诞生。

当然，在此之前，建议先通过此文了解word2vec：https://blog.csdn.net/v_JULY_v/article/details/102708459，这是理解bert的关键，其次则是Transformer，关于Transformer，推荐此文 https://www.julyedu.com/question/big/kp_id/30/ques_id/2912。

因为我也是这么过来的，之前bert刚火起来的时候，就看到俊林老师这篇文章，当时看的不甚了解，及至后来先学习word2vec和Transformer之后，再看此文，你会觉得真是高屋建瓴，甚至醍醐灌顶，是关于bert中文介绍少有的好文章。
话休絮烦，以下即为张俊林老师所写的正文。

Bert最近很火，应该是最近最火爆的AI进展，网上的评价很高，那么Bert值得这么高的评价吗？我个人判断是值得。那为什么会有这么高的评价呢？是因为它有重大的理论或者模型创新吗？

其实并没有，从模型创新角度看一般，创新不算大。但是架不住效果太好了，基本刷新了很多NLP的任务的最好性能，有些任务还被刷爆了，这个才是关键。另外一点是Bert具备广泛的通用性，就是说绝大部分NLP任务都可以采用类似的两阶段模式直接去提升效果，这个第二关键。客观的说，把Bert当做最近两年NLP重大进展的集大成者更符合事实。

本文的主题是自然语言处理中的预训练过程，会大致说下NLP中的预训练技术是一步一步如何发展到Bert模型的，从中可以很自然地看到Bert的思路是如何逐渐形成的，Bert的历史沿革是什么，继承了什么，创新了什么，为什么效果那么好，主要原因是什么，以及为何说模型创新不算太大，为何说Bert是近年来NLP重大进展的集大成者。

我们一步一步来讲，而串起来这个故事的脉络就是自然语言的预训练过程，但是落脚点还是在Bert身上。要讲自然语言的预训练，得先从图像领域的预训练说起。

图像领域的预训练自从深度学习火起来后，预训练过程就是做图像或者视频领域的一种比较常规的做法，有比较长的历史了，而且这种做法很有效，能明显促进应用的效果。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156040081288495.png'/>

那么图像领域怎么做预训练呢，上图展示了这个过程，我们设计好网络结构以后，对于图像来说一般是CNN的多层叠加网络结构，可以先用某个训练集合比如训练集合A或者训练集合B对这个网络进行预先训练，在A任务上或者B任务上学会网络参数，然后存起来以备后用。假设我们面临第三个任务C，网络结构采取相同的网络结构，在比较浅的几层CNN结构，网络参数初始化的时候可以加载A任务或者B任务学习好的参数，其它CNN高层参数仍然随机初始化。

之后我们用C任务的训练数据来训练网络，此时有两种做法，
一种是浅层加载的参数在训练C任务过程中不动，这种方法被称为“Frozen”;
另外一种是底层网络参数尽管被初始化了，在C任务训练过程中仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”，顾名思义，就是更好地把参数进行调整使得更适应当前的C任务。

一般图像或者视频领域要做预训练一般都这么做。这么做有几个好处，首先，如果手头任务C的训练集合数据量较少的话，现阶段的好用的CNN比如Resnet/Densenet/Inception等网络结构层数很深，几百万上千万参数量算起步价，上亿参数的也很常见，训练数据少很难很好地训练这么复杂的网络，但是如果其中大量参数通过大的训练集合比如ImageNet预先训练好直接拿来初始化大部分网络结构参数，然后再用C任务手头比较可怜的数据量上Fine-tuning过程去调整参数让它们更适合解决C任务，那事情就好办多了。

这样原先训练不了的任务就能解决了，即使手头任务训练数据也不少，加个预训练过程也能极大加快任务训练的收敛速度，所以这种预训练方式是老少皆宜的解决方案，另外疗效又好，所以在做图像处理领域很快就流行开来。

那么新的问题来了，为什么这种预训练的思路是可行的？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154534918309172.jpg'/>

目前我们已经知道，对于层级的CNN结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构，如上图所示，如果我们手头是个人脸识别任务，训练好网络后，把每层神经元学习到的特征可视化肉眼看一看每层学到了啥特征，你会看到最底层的神经元学到的是线段等特征，图示的第二个隐层学到的是人脸五官的轮廓，第三层学到的是人脸的轮廓，通过三步形成了特征的层级结构，越是底层的特征越是所有不论什么领域的图像都会具备的比如边角线弧线等底层基础特征，越往上抽取出的特征越与手头任务相关。

正因为此，所以预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务网络参数的原因。而高层特征跟任务关联较大，实际可以不用使用，或者采用Fine-tuning用新数据集合清洗掉高层无关的特征抽取器。

一般我们喜欢用ImageNet来做网络的预训练，主要有两点，一方面ImageNet是图像领域里有超多事先标注好训练数据的数据集合，分量足是个很大的优势，量越大训练出的参数越靠谱；另外一方面因为ImageNet有1000类，类别多，算是通用的图像数据，跟领域没太大关系，所以通用性好，预训练完后哪哪都能用，是个万金油。分量足的万金油当然老少通吃，人人喜爱。

听完上述话，如果你是具备研究素质的人，也就是说具备好奇心，你一定会问下面这个问题：”既然图像领域预训练这么好用，那干嘛自然语言处理不做这个事情呢？是不是搞NLP的人比搞CV的傻啊？就算你傻，你看见人家这么做，有样学样不就行了吗？这不就是创新吗，也许能成，万一成了，你看，你的成功来得就是这么突然!”

嗯，好问题，其实搞NLP的人一点都不比你傻，早就有人尝试过了，不过总体而言不太成功而已。听说过word embedding吗？2003年出品，陈年技术，馥郁芳香。word embedding其实就是NLP里的早期预训练技术。当然也不能说word embedding不成功，一般加到下游任务里，都能有1到2个点的性能提升，只是没有那么耀眼的成功而已。没听过？那下面就把这段陈年老账讲给你听听。

Word Embedding考古史这块大致讲讲Word Embedding的故事，很粗略，因为网上关于这个技术讲的文章太多了，汗牛冲动，我不属牛，此刻更没有流汗，所以其实丝毫没有想讲Word Embedding的冲动和激情，但是要说预训练又得从这开始，那就粗略地讲讲，主要是引出后面更精彩的部分。在说Word Embedding之前，先更粗略地说下语言模型，因为一般NLP里面做预训练一般的选择是用语言模型任务来做。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157155009414062228.jpg'/>

什么是语言模型？其实看上面这张PPT上扣下来的图就明白了，为了能够量化地衡量哪个句子更像一句人话，可以设计如上图所示函数，核心函数P的思想是根据句子里面前面的一系列前导单词预测后面跟哪个单词的概率大小（理论上除了上文之外，也可以引入单词的下文联合起来预测单词出现概率）。句子里面每个单词都有个根据上文预测自己的过程，把所有这些单词的产生概率乘起来，数值越大代表这越像一句人话。

语言模型压下暂且不表，我隐约预感到我这么讲你可能还是不太会明白，但是大概这个意思，不懂的可以去网上找，资料多得一样地汗牛冲动（July注：关于语言模型还可以看看这篇 https://www.julyedu.com/question/big/kp_id/30/ques_id/2984）。

假设现在让你设计一个神经网络结构，去做这个语言模型的任务，就是说给你很多语料做这个事情，训练好一个神经网络，训练好之后，以后输入一句话的前面几个单词，要求这个网络输出后面紧跟的单词应该是哪个，你会怎么做？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154560273799310.jpg'/>

你可以像上图这么设计这个网络结构，这其实就是大名鼎鼎的中文人称“神经网络语言模型”，英文小名NNLM的网络结构，用来做语言模型。这个工作有年头了，是个陈年老工作，是Bengio 在2003年发表在JMLR上的论文。它生于2003，火于2013，以后是否会不朽暂且不知，但是不幸的是出生后应该没有引起太大反响，沉寂十年终于时来运转沉冤得雪，在2013年又被NLP考古工作者从海底湿淋淋地捞出来了祭入神殿。

为什么会发生这种技术奇遇记？你要想想2013年是什么年头，是深度学习开始渗透NLP领域的光辉时刻，万里长征第一步，而NNLM可以算是南昌起义第一枪。在深度学习火起来之前，极少有人用神经网络做NLP问题，如果你10年前坚持用神经网络做NLP，估计别人会认为你这人神经有问题。所谓红尘滚滚，谁也挡不住历史发展趋势的车轮，这就是个很好的例子。

上面是闲话，闲言碎语不要讲，我们回来讲一讲NNLM的思路。先说训练过程，现在看其实很简单，见过RNN、LSTM、CNN后的你们回头再看这个网络甚至显得有些简陋。学习任务是输入某个句中单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154570412977637.svg'/>前面句子的t-1个单词，要求网络正确预测单词Bert，即最大化：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154572430609079.svg'/>

前面任意单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154574554305681.svg'/>用Onehot编码（比如：0001000）作为原始单词输入，之后乘以矩阵Q后获得向量 <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154576598939623.svg'/>，每个单词的<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715457745240528.svg'/>拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。

这个<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154578489621011.svg'/>是什么？这其实就是单词对应的Word Embedding值，那个矩阵Q包含V行，V代表词典大小，每一行内容代表对应单词的Word embedding值。只不过Q的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵Q，当这个网络训练好之后，矩阵Q的内容被正确赋值，每一行代表一个单词对应的Word embedding值。

所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵Q，这就是单词的Word Embedding是被如何学会的。2013年最火的用语言模型做Word Embedding的工具是Word2Vec，后来又出了Glove。

Word2Vec是怎么工作的呢？看下图。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154582635057288.jpg'/>

Word2Vec的网络结构其实和NNLM是基本类似的，只是这个图长得清晰度差了点，看上去不像，其实它们是亲兄弟。不过这里需要指出：尽管网络结构相近，而且也是做语言模型任务，但是其训练方法不太一样。

Word2Vec有两种训练方法，一种叫CBOW，核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词；
第二种叫做Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。

而你回头看看，NNLM是怎么训练的？是输入一个单词的上文，去预测这个单词。这是有显著差异的。为什么Word2Vec这么处理？原因很简单，因为Word2Vec和NNLM不一样，NNLM的主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而word embedding只是无心插柳的一个副产品。

但是Word2Vec目标不一样，它单纯就是要word embedding的，这是主产品，所以它完全可以随性地这么去训练网络。为什么要讲Word2Vec呢？这里主要是要引出CBOW的训练方法，BERT其实跟它有关系，后面会讲它们之间是如何的关系，当然它们的关系BERT作者没说，是我猜的，至于我猜的对不对，后面你看后自己判断。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154605888272540.jpg'/>

使用Word2Vec或者Glove，通过做语言模型任务，就可以获得每个单词的Word Embedding，那么这种方法的效果如何呢？上图给了网上找的几个例子，可以看出有些例子效果还是很不错的，一个单词表达成Word Embedding后，很容易找出语义相近的其它词汇。

我们的主题是预训练，那么问题是Word Embedding这种做法能算是预训练吗？这其实就是标准的预训练过程。要理解这一点要看看学会Word Embedding后下游任务是怎么用它的。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156048711181137.png'/>

假设如上图所示，我们有个NLP的下游任务，比如QA，就是问答问题，所谓问答问题，指的是给定一个问题X，给定另外一个句子Y,要判断句子Y是否是问题X的正确答案。问答问题假设设计的网络结构如上图所示，这里不展开讲了，懂得自然懂，不懂的也没关系，因为这点对于本文主旨来说不关键，关键是网络如何使用训练好的Word Embedding的。

它的使用方法其实和前面讲的NNLM是一样的，句子中每个单词以Onehot形式作为输入，然后乘以学好的Word Embedding矩阵Q，就直接取出单词对应的Word Embedding了。

这乍看上去好像是个查表操作，不像是预训练的做法是吧？其实不然，那个Word Embedding矩阵Q其实就是网络Onehot层到embedding层映射的网络参数矩阵。所以你看到了，使用Word Embedding等价于什么？等价于把Onehot层到embedding层的网络用预训练好的参数矩阵Q初始化了。这跟前面讲的图像领域的低层预训练过程其实是一样的，区别无非Word Embedding只能初始化第一层网络参数，再高层的参数就无能为力了。

下游NLP任务在使用Word Embedding的时候也类似图像有两种做法，一种是Frozen，就是Word Embedding那层网络参数固定不动；另外一种是Fine-Tuning，就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新掉。

上面这种做法就是18年之前NLP领域里面采用预训练的典型做法，之前说过，Word Embedding其实对于很多下游NLP任务是有帮助的，只是帮助没有大到闪瞎忘记戴墨镜的围观群众的双眼而已。

那么新问题来了，为什么这样训练及使用Word Embedding的效果没有期待中那么好呢？答案很简单，因为Word Embedding有问题呗。这貌似是个比较弱智的答案，关键是Word Embedding存在什么问题？这其实是个好问题。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715605432899758.png'/>

这片在Word Embedding头上笼罩了好几年的乌云是什么？是多义词问题。我们知道，多义词是自然语言中经常出现的现象，也是语言灵活性和高效性的一种体现。多义词对Word Embedding来说有什么负面影响？

如上图所示，比如多义词Bank，有两个常用含义，但是Word Embedding在对bank这个单词进行编码的时候，是区分不开这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。

所以word embedding无法区分多义词的不同语义，这就是它的一个比较严重的问题。你可能觉得自己很聪明，说这可以解决啊，确实也有很多研究人员提出很多方法试图解决这个问题，但是从今天往回看，这些方法看上去都成本太高或者太繁琐了，有没有简单优美的解决方案呢？ELMO提供了一种简洁优雅的解决方案。

从Word Embedding到ELMO
ELMO是“Embedding from Language Models”的简称，其实这个名字并没有反应它的本质思想，提出ELMO的论文题目：“Deep contextualized word representation”更能体现其精髓，而精髓在哪里？在deep contextualized这个短语，一个是deep，一个是context，其中context更关键。

在此之前的Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变，所以对于比如Bank这个词，它事先学好的Word Embedding中混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含money等词）明显可以看出它代表的是“银行”的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。这是为何说它是静态的，这也是问题所在。

ELMO的本质思想是：我事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。

所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715606044100672.png'/>

ELMO采用了典型的两阶段过程，第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。

上图展示的是其预训练过程，它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715463197090044.svg'/>的上下文去正确预测单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154632618450065.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154633882353118.svg'/>之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。

图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154634360481822.svg'/>的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM叠加。

这个网络结构其实在NLP中是很常用的。使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154642594810124.svg'/>，句子中每个单词都能得到对应的三个Embedding：
最底层是单词的Word Embedding；
往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；
再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。

也就是说，ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156022749639923.png'/>

上面介绍的是ELMO的第一阶段：预训练阶段。那么预训练好网络结构后，如何给下游任务使用呢？上图展示了下游任务的使用过程，比如我们的下游任务仍然是QA问题，此时对于问句X，我们可以先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding，之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。

然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。对于上图所示下游任务QA中的回答句子Y来说也是如此处理。因为ELMO给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。至于为何这么做能够达到区分多义词的效果，你可以想一想，其实比较容易想明白原因。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156065876593589.png'/>

上面这个图是TagLM采用类似ELMO的思路做命名实体识别任务的过程，其步骤基本如上述ELMO的思路，所以此处不展开说了。TagLM的论文发表在2017年的ACL会议上，作者就是AllenAI里做ELMO的那些人，所以可以将TagLM看做ELMO的一个前导工作。

前几天这个PPT发出去后有人质疑说FastAI的在18年4月提出的ULMFiT才是抛弃传统Word Embedding引入新模式的开山之作，我深不以为然。
首先TagLM出现的更早而且模式基本就是ELMO的思路；
另外ULMFiT使用的是三阶段模式，在通用语言模型训练之后，加入了一个领域语言模型预训练过程，而且论文重点工作在这块，方法还相对比较繁杂，这并不是一个特别好的主意，因为领域语言模型的限制是它的规模往往不可能特别大，精力放在这里不太合适，放在通用语言模型上感觉更合理；
再者，尽管ULFMiT实验做了6个任务，但是都集中在分类问题相对比较窄，不如ELMO验证的问题领域广，我觉得这就是因为第二步那个领域语言模型带来的限制。
所以综合看，尽管ULFMiT也是个不错的工作，但是重要性跟ELMO比至少还是要差一档，当然这是我个人看法。

每个人的学术审美口味不同，我个人一直比较赞赏要么简洁有效体现问题本质，要么思想特别游离现有框架脑洞开得异常大的工作，所以ULFMiT我看论文的时候就感觉看着有点难受，觉得这工作没抓住重点而且特别麻烦，但是看ELMO论文感觉就赏心悦目，觉得思路特别清晰顺畅，看完暗暗点赞，心里说这样的文章获得NAACL2018最佳论文当之无愧，比ACL很多最佳论文也好得不是一点半点，这就是好工作带给一个有经验人士的一种在读论文时候就能产生的本能的感觉，也就是所谓的这道菜对上了食客的审美口味。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154658139852177.jpg'/>

前面我们提到静态Word Embedding无法解决多义词的问题，那么ELMO引入上下文动态调整单词的embedding后多义词问题解决了吗？解决了，而且比我们期待的解决得还要好。

上图给了个例子，对于Glove训练出的Word Embedding来说，多义词比如play，根据它的embedding找出的最接近的其它单词大多数集中在体育领域，这很明显是因为训练数据中包含play的句子中体育领域的数量明显占优导致；而使用ELMO，根据上下文动态调整后的embedding不仅能够找出对应的“演出”的相同语义的句子，而且还可以保证找出的句子中的play对应的词性也是相同的，这是超出期待之处。之所以会这样，是因为我们上面提到过，第一层LSTM编码了很多句法信息，这在这里起到了重要作用。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154672595377892.jpg'/>

ELMO经过这般操作，效果如何呢？实验效果见上图，6个NLP任务中性能都有幅度不同的提升，最高的提升达到25%左右，而且这6个任务的覆盖范围比较广，包含句子语义关系判断，分类任务，阅读理解等多个领域，这说明其适用范围是非常广的，普适性强，这是一个非常好的优点。

那么站在现在这个时间节点看，ELMO有什么值得改进的缺点呢？
首先，一个非常明显的缺点在特征抽取器选择方面，ELMO使用了LSTM而不是新贵Transformer，Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响，很多研究已经证明了Transformer提取特征的能力是要远强于LSTM的。如果ELMO采取Transformer作为特征提取器，那么估计Bert的反响远不如现在的这种火爆场面。
另外一点，ELMO采取双向拼接这种融合特征的能力可能比Bert一体化的融合特征方式弱，但是，这只是一种从道理推断产生的怀疑，目前并没有具体实验说明这一点。我们如果把ELMO这种预训练方法和图像领域的预训练方法对比，发现两者模式看上去还是有很大差异的。

除了以ELMO为代表的这种基于特征融合的预训练方法外，NLP里还有一种典型做法，这种做法和图像领域的方式就是看上去一致的了，一般将这种方法称为“基于Fine-tuning的模式”，而GPT就是这一模式的典型开创者。

从Word Embedding到GPT
GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156074526382168.png'/>

GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：
首先，特征抽取器不是用的RNN，而是用的Transformer，上面提到过它的特征抽取能力要强于RNN，这个选择很明显是很明智的；
其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，所谓“单向”的含义是指：语言模型训练的任务目标是根据<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154689157643717.svg'/>单词的上下文去正确预测单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154689157643717.svg'/>，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154689157643717.svg'/>之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。

ELMO在做语言模型预训练的时候，预测单词<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154689157643717.svg'/>同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文。这个选择现在看不是个太好的选择，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到Word Embedding中，是很吃亏的，白白丢掉了很多信息。

这里强行插入一段简单提下Transformer，尽管上面提到了，但是说的还不完整，补充两句。首先，Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器，注意力这个机制在此被发扬光大，从任务的配角不断抢戏，直到Transformer一跃成为踢开RNN和CNN传统特征提取器，荣升头牌，大红大紫。

你问了：什么是注意力机制？这里再插个广告，对注意力不了解的可以参考鄙人16年出品17年修正的文章：“深度学习中的注意力模型”：https://www.julyedu.com/question/big/kp_id/30/ques_id/2911，补充下相关基础知识，如果不了解注意力机制你肯定会落后时代的发展。

而介绍Transformer比较好的文章可以参考以下两篇文章：一个是Jay Alammar可视化地介绍Transformer的博客文章The Illustrated Transformer（July注，其中文题库版见：https://www.julyedu.com/question/big/kp_id/30/ques_id/2912），非常容易理解整个机制，建议先从这篇看起；然后可以参考哈佛大学NLP研究组写的“The Annotated Transformer. ”：http://nlp.seas.harvard.edu/2018/04/03/attention.html，代码原理双管齐下，讲得非常清楚。我相信上面两个文章足以让你了解Transformer了，所以这里不展开介绍。

其次，我的判断是Transformer在未来会逐渐替代掉RNN成为主流的NLP工具，RNN一直受困于其并行计算能力，这是因为它本身结构的序列性依赖导致的，尽管很多人在试图通过修正RNN结构来修正这一点，但是我不看好这种模式，因为给马车换轮胎不如把它升级到汽车，这个道理很好懂，更何况目前汽车的雏形已经出现了，干嘛还要执着在换轮胎这个事情呢？是吧？

再说CNN，CNN在NLP里一直没有形成主流，CNN的最大优点是易于做并行计算，所以速度快，但是在捕获NLP的序列关系尤其是长距离特征方面天然有缺陷，不是做不到而是做不好，目前也有很多改进模型，但是特别成功的不多。综合各方面情况，很明显Transformer同时具备并行性好，又适合捕获长距离特征，没有理由不在赛跑比赛中跑不过RNN和CNN。

好了，题外话结束，我们再回到主题，接着说GPT。上面讲的是GPT如何进行第一阶段的预训练，那么假设预训练好了网络模型，后面下游任务怎么用？它有自己的个性，和ELMO的方式大有不同。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156078381376885.png'/>

上图展示了GPT在第二阶段如何使用。

首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的。
然后，在做下游任务的时候，利用第一步预训练好的参数初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了，这是个非常好的事情。
再次，你可以用手头的任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决手头的问题。就是这样。

看到了么？这有没有让你想起最开始提到的图像领域如何做预训练的过程（请参考上图那句非常容易暴露年龄的歌词）？对，这跟那个模式是一模一样的。这里引入了一个新问题：对于NLP各种花样的不同任务，怎么改造才能靠近GPT的网络结构呢？
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715472108350789.jpg'/>

GPT论文给了一个改造施工图如上，其实也很简单：对于分类问题，不用怎么动，加上一个起始和终结符号即可；对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可；对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154730583116276.jpg'/>

GPT的效果是非常令人惊艳的，在12个任务里，9个达到了最好的效果，有些任务性能提升非常明显。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154732491204502.jpg'/>

那么站在现在的时间节点看，GPT有什么值得改进的地方呢？其实最主要的就是那个单向语言模型，如果改造成双向的语言模型任务估计也没有Bert太多事了。当然，即使如此GPT也是非常非常好的一个工作，跟Bert比，其作者炒作能力亟待提升。

Bert的诞生
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154737146796255.jpg'/>

我们经过跋山涉水，终于到了目的地Bert模型了。Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型，当然另外一点是语言模型的数据规模要比GPT大。所以这里Bert的预训练过程不必多讲了。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156085031331286.png'/>

第二阶段，Fine-Tuning阶段，这个阶段的做法和GPT是一样的。当然，它也面临着下游任务网络结构改造的问题，在改造任务方面Bert和GPT有些不同，下面简单介绍一下。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415715474627832825.jpg'/>

在介绍Bert如何改造下游任务之前，先大致说下NLP的几类问题，说这个是为了强调Bert的普适性有多强。通常而言，绝大部分NLP问题可以归入上图所示的四类任务中：
一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。
第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。
第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；
第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156088092600404.png'/>

对于种类如此繁多而且各具特点的下游NLP任务，Bert如何改造输入输出部分使得大部分NLP任务都可以使用Bert预训练好的模型参数呢？
上图给出示例，对于句子关系类任务，很简单，和GPT类似，加上一个起始和终结符号，句子之间加个分隔符即可。
对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。
对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；
对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。

从这里可以看出，上面列出的NLP四大任务里面，除了生成类任务外，Bert其它都覆盖到了，而且改造起来很简单直观。尽管Bert论文没有提，但是稍微动动脑子就可以想到，其实对于机器翻译或者文本摘要，聊天机器人这种生成式任务，同样可以稍作改造即可引入Bert的预训练成果。只需要附着在S2S结构上，encoder部分是个深度Transformer结构，decoder部分也是个深度Transformer结构。根据任务选择不同的预训练数据初始化encoder和decoder即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个Transformer结构上加装隐层产生输出也是可以的。

不论如何，从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154759012126361.jpg'/>

Bert采用这种两阶段方式解决各种NLP任务效果如何？在11个各种类型的NLP任务中达到目前最好的效果，某些任务性能有极大的提升。一个新模型好不好，效果才是王道。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156091744887247.png'/>

到这里我们可以再梳理下几个模型之间的演进关系。从上图可见，Bert其实和ELMO及GPT存在千丝万缕的关系，
比如如果我们把GPT预训练阶段换成双向语言模型，那么就得到了Bert；
而如果我们把ELMO的特征抽取器换成Transformer，那么我们也会得到Bert。

所以你可以看出：Bert最关键两点，一点是特征抽取器采用Transformer；第二点是预训练的时候采用双向语言模型。那么新问题来了：对于Transformer来说，怎么才能在这个结构上做双向语言模型任务呢？

乍一看上去好像不太好搞。我觉得吧，其实有一种很直观的思路，怎么办？看看ELMO的网络结构图，只需要把两个LSTM替换成两个Transformer，一个负责正向，一个负责反向特征提取，其实应该就可以。当然这是我自己的改造，Bert没这么做。

那么Bert是怎么做的呢？我们前面不是提过Word2Vec吗？我前面肯定不是漫无目的地提到它，提它是为了在这里引出那个CBOW训练方法，所谓写作时候埋伏笔的“草蛇灰线，伏脉千里”，大概就是这个意思吧？

前面提到了CBOW方法，它的核心思想是：在做语言模型任务的时候，我把要预测的单词抠掉，然后根据它的上文Context-Before和下文Context-after去预测单词。其实Bert怎么做的？Bert就是这么做的。从这里可以看到方法间的继承关系。当然Bert作者没提Word2Vec及CBOW方法，这是我的判断，Bert作者说是受到完形填空任务的启发，这也很可能，但是我觉得他们要是没想到过CBOW估计是不太可能的。

从这里可以看出，在文章开始我说过Bert在模型方面其实没有太大创新，更像一个最近几年NLP重要技术的集大成者，原因在于此，当然我不确定你怎么看，是否认同这种看法，而且我也不关心你怎么看。其实Bert本身的效果好和普适性强才是最大的亮点。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157156095587160714.png'/>

那么Bert本身在模型和方法角度有什么创新呢？就是论文中指出的Masked 语言模型和Next Sentence Prediction。而Masked语言模型上面讲了，本质思想其实是CBOW，但是细节方面有改进。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154773536828846.jpg'/>

Masked双向语言模型向上图展示这么做：随机选择语料中15%的单词，把它抠掉，也就是用[Mask]掩码代替原始单词，然后要求模型去正确预测被抠掉的单词。但是这里有个问题：训练过程大量看到[mask]标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对[mask]这个标记的，但是实际使用又见不到这个标记，这自然会有问题。

为了避免这个问题，Bert改造了一下，15%的被上天选中要执行[mask]替身这项光荣任务的单词中，只有80%真正被替换成[mask]标记，10%被狸猫换太子随机替换成另外一个单词，10%情况这个单词还待在原地不做改动。这就是Masked双向语音模型的具体做法。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154778357827261.jpg'/>

至于说“Next Sentence Prediction”，指的是做语言模型预训练的时候，分两种情况选择两个句子，一种是选择语料中真正顺序相连的两个句子；另外一种是第二个句子从语料库中抛色子，随机选择一个拼到第一个句子后面。我们要求模型除了做上述的Masked语言模型任务外，附带再做个句子关系预测，判断第二个句子是不是真的是第一个句子的后续句子。

之所以这么做，是考虑到很多NLP任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。所以可以看到，它的预训练是个多任务过程。这也是Bert的一个创新。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154781152753624.jpg'/>

上面这个图给出了一个我们此前利用微博数据和开源的Bert做预训练时随机抽出的一个中文训练实例，从中可以体会下上面讲的masked语言模型和下句预测任务。训练数据就长这种样子。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154783471696873.jpg'/>

顺带讲解下Bert的输入部分，也算是有些特色。它的输入部分是个线性序列，两个句子通过分隔符分割，最前面和最后增加两个标识符号。每个单词有三个embedding：
位置信息embedding，这是因为NLP中单词顺序是很重要的特征，需要在这里对位置信息进行编码；
单词embedding,这个就是我们之前一直提到的单词embedding；
第三个是句子embedding，因为前面提到训练数据都是由两个句子构成的，那么每个句子有个句子整体的embedding项对应给每个单词。

把单词对应的三个embedding叠加，就形成了Bert的输入。<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157157778492670494.png'/>

至于Bert在预训练的输出部分如何组织，可以参考上图的注释。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154788099931636.jpg'/>

我们说过Bert效果特别好，那么到底是什么因素起作用呢？如上图所示，对比试验可以证明，跟GPT相比，双向语言模型起到了最主要的作用，对于那些需要看到下文的任务来说尤其如此。而预测下个句子来说对整体性能来说影响不算太大，跟具体任务关联度比较高。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157154789784307560.jpg'/>

最后，我讲讲我对Bert的评价和看法，我觉得Bert是NLP里里程碑式的工作，对于后面NLP的研究和工业应用会产生长久的影响，这点毫无疑问。但是从上文介绍也可以看出，从模型或者方法角度看，Bert借鉴了ELMO，GPT及CBOW，主要提出了Masked 语言模型及Next Sentence Prediction，但是这里Next Sentence Prediction基本不影响大局，而Masked LM明显借鉴了CBOW的思想。所以说Bert的模型没什么大的创新，更像最近几年NLP重要进展的集大成者，这点如果你看懂了上文估计也没有太大异议，如果你有大的异议，杠精这个大帽子我随时准备戴给你。

如果归纳一下这些进展就是：首先是两阶段模型，第一阶段双向语言模型预训练，这里注意要用双向而不是单向，第二阶段采用具体任务Fine-tuning或者做特征集成；第二是特征抽取要用Transformer作为特征提取器而不是RNN或者CNN；第三，双向语言模型可以采取CBOW的方法去做（当然我觉得这个是个细节问题，不算太关键，前两个因素比较关键）。

Bert最大的亮点在于效果好及普适性强，几乎所有NLP任务都可以套用Bert这种两阶段解决思路，而且效果应该会有明显提升。可以预见的是，未来一段时间在NLP应用领域，Transformer将占据主导地位，而且这种两阶段预训练方法也会主导各种应用。

另外，我们应该弄清楚预训练这个过程本质上是在做什么事情，本质上预训练是通过设计好一个网络结构来做语言模型任务，然后把大量甚至是无穷尽的无标注的自然语言文本利用起来，预训练任务把大量语言学知识抽取出来编码到网络结构中，当手头任务带有标注信息的数据有限时，这些先验的语言学特征当然会对手头任务有极大的特征补充作用，因为当数据有限的时候，很多语言学现象是覆盖不到的，泛化能力就弱，集成尽量通用的语言学知识自然会加强模型的泛化能力。

如何引入先验的语言学知识其实一直是NLP尤其是深度学习场景下的NLP的主要目标之一，不过一直没有太好的解决办法，而ELMO/GPT/Bert的这种两阶段模式看起来无疑是解决这个问题自然又简洁的方法，这也是这些方法的主要价值所在。

对于当前NLP的发展方向，我个人觉得有两点非常重要，一个是需要更强的特征抽取器，目前看Transformer会逐渐担当大任，但是肯定还是不够强的，需要发展更强的特征抽取器；
第二个就是如何优雅地引入大量无监督数据中包含的语言学知识，注意我这里强调地是优雅，而不是引入，此前相当多的工作试图做各种语言学知识的嫁接或者引入，但是很多方法看着让人牙疼，就是我说的不优雅。目前看预训练这种两阶段方法还是很有效的，也非常简洁，当然后面肯定还会有更好的模型出现。

完了，这就是自然语言模型预训练的发展史。

PS，July注：上文PPT下载地址：http://ccl.pku.edu.cn/doubtfire/NLP/Deep_Learning/%E4%BB%8EWord%20Embedding%E5%88%B0Bert%20ppt%20%E5%BC%A0%E4%BF%8A%E6%9E%97.pdf

## 29.如何理解Seq2Seq Attention模型：图解Seq2Seq Attention
本题解析来源：https://zhuanlan.zhihu.com/p/40920384 ，和https://blog.csdn.net/Irving_zhang/article/details/78889364

seq2seq是一个Encoder–Decoder结构的网络，它的输入是一个序列，输出也是一个序列，Encoder中将一个可变长度的信号序列变为固定长度的向量表达，Decoder将这个固定长度的向量变成可变长度的目标的信号序列。

首先，我们使用x={x1，x2，…，xn}代表输入的语句，y={y1,y2,…,yn}代表输出的语句，yt代表当前输出词。在理解seq2seq的过程中，我们要牢记我们的目标是：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820189856707261.jpg'/>
即输出的yt不仅依赖之前的输出{y1,y2,…,yt−1}，还依赖输入语句x，模型再怎么变化都是在上述公式的约束之下。

seq2seq最初模型
最早由bengio等人发表在computerscience上的论文：LearningPhraseRepresentationsusingRNNEncoder–Decoder 
forStatisticalMachineTranslation。

对于RNN来说，x={x1，x2，…，xt}代表输入，在每个时间步t，RNN的隐藏状态ht由下述公式更新：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820210449024097.png'/>
其中，f代表一个非线性函数。这时ht就是一个rnn_size的隐含状态。然后需要通过一个矩阵W将其转成一个symbol_size的输出，并通过softmax函数将其转化为概率，然后筛选出概率最大的symbol为输出symbol。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820217792527635.jpg'/>

以上是rnn的基本原理，接下来介绍论文中的seq2seq模型： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820221847717693.jpg'/>
模型包括encoder和decoder两个部分。
首先在encoder部分，将输入传到encoder部分，得到最后一个时间步长t的隐藏状态C，这就是RNNcell的基本功能。

其次是decoder部分，从上述模型的箭头中可以看出，decoder的隐藏状态ht就由ht−1，yt−1和C三部分构成。即： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820259193663983.jpg'/>
由此我们的到了decoder的隐藏状态，那么最后的输出yt从图中也可以看得出来由三部分得到，yt从图中也可以看得出来由三部分得到，ht−1，yt−1和C，即：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820266236752971.jpg'/>
到现在为止，我们就实现了上文最开始的目标。

seq2seq的改进模型
改进模型介绍2014年发表的论文SequencetoSequenceLearningwithNeuralNetworks。模型图： 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820278511576601.jpg'/>
可以看到，该模型和第一个模型主要的区别在于从输入到输出有一条完整的流：ABC为encoder的输入，WXYZ为decoder的输入。将encoder最后得到的隐藏层的状态ht输入到decoder的第一个cell里，就不用像第一个模型一样，每一个decoder的cell都需要ht，因此从整体上看，从输入到输出像是一条“线性的数据流”。

本文的论文也提出来，ABC翻译为XYZ，将encoder的input变为“CBA”效果更好。即A和X的距离更近了，更有利于seq2seq模型的交流。

具体来说，encoder的过程如下图。这和我们之前的encoder都一样。 
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820293919476479.jpg'/>
不同的是decoder的阶段：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820297935765869.jpg'/>

得到了encoderrepresention，即encoder的最后一个时间步长的隐层ht以后，输入到decoder的第一个cell里，然后通过一个激活函数和softmax层，得到候选的symbols，筛选出概率最大的symbol，然后作为下一个时间步长的输入，传到cell中。这样，我们就得到了我们最开始的目标。

seq2seq with attention
我们前面提到，距离decoder的第一个cell越近的输入单词，对decoder的影响越大。但这并不符合常理，这时就提出了attention机制，对于输出的每一个cell，都检测输入的sequence里每个单词的重要性，即论文Neural Machine Translation by Jointly Learning to Align and Translate。attention在NMT基于seq2seq的改进模型再进行改进。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820423226415848.jpg'/>
上图中，encoder和decoder都发生了变化。首先说encoder，使用了双向RNN，因为希望不仅能得到前向的词的顺序，还希望能够得到反向的词的顺序。使用<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820447266351394.png'/>代表hj前向的隐层状态，<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820448261022537.png'/>代表hj的反向隐层状态，hj的最终状态为将两者连接(concat)起来，即<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820449073286618.png'/>。

再说decoder。我们再来回顾一下我们最开始的目标公式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820189856707261.jpg'/>
对于加入attention机制的seq2seq，每一个输出为公式如下。即对于时间步i的输出yi，由时间步i的隐藏状态si，由attention计算得到的输入内容ci和上一个输出yi-1得到。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820454744511338.jpg'/>

其中si是对于时间步i的隐藏状态，由下述公式计算。即对于时间步i的隐藏状态，由时间步i-1的隐藏状态si-1，由attention计算得到的输入内容ci和上一个输出yi-1得到。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820459313937216.jpg'/>
通过以上公式可以看出，加入attention的seq2seq比之前的seq2seq多了一个输入内容向量ci，那么这个ci是怎么得来的呢？和输入内容以及attention有什么关系呢？我们接着看下述公式：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820464935278728.jpg'/>
即，对于decoder的时间步长i的隐藏状态si，ci等于Tx个输入向量[1,Tx]与其权重<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820483584567741.png'/>相乘求和。这个权重<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820483584567741.png'/>由这个公式得到：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820476762319690.jpg'/>
其中，eij由下面公式得到
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820479476665219.jpg'/>
总结一下，对于时间步i的隐藏状态si，可以通过求时间步i-1的隐藏状态si-1、输入内容的编码向量ci和上一个输出yi-1得到。输入内容编码ci是新加入的内容，可以通过计算输入句子中每个单词的权重，然后加权求和得到ci。
直观解释这个权重：对于decoder的si和encoder的hj的权重<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157820483584567741.png'/>，就是上一个时间步长的隐藏状态si-1与encoder的hj通过非线性函数得到的。这样就把输入内容加入到解码的过程中，这和我们人类翻译的过程也是类似的，即对于当前输出的词，每一个输入给与的注意力是不一样的。

上面一堆公式是不是看懵了。好了别管了，接下来开始刷图吧。
大框架
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819613655535774.jpg'/>
想象一下翻译任务，input是一段英文，output是一段中文。
公式（直接跳过看图最佳）
输入： <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819666441379406.svg'/>
输出： <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415781966719389889.svg'/>
(1)  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819667877685337.svg'/>,Encoder方面接受的是每一个单词wordembedding，和上一个时间点的hiddenstate。输出的是这个时间点的hiddenstate。
(2)  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819668613974666.svg'/>，Decoder方面接受的是目标句子里单词的wordembedding，和上一个时间点的hiddenstate。
(3)  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819669118649057.svg'/>,contextvector是一个对于encoder输出的hiddenstates的一个加权平均。
(4) <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819670368675621.svg'/>,每一个encoder的hiddenstates对应的权重。
(5)  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819671335923896.svg'/>,通过decoder的hiddenstates加上encoder的hiddenstates来计算一个分数，用于计算权重(4)
(6)<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819672177541328.svg'/> ,将contextvector和decoder的hiddenstates串起来。
(7) <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819672968590209.svg'/>，计算最后的输出概率。

详细图
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819617176492580.jpg'/>
左侧为Encoder+输入，右侧为Decoder+输出。中间为Attention。

(1)<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819677645031312.svg'/> ,Encoder方面接受的是每一个单词wordembedding，和上一个时间点的hiddenstate。输出的是这个时间点的hiddenstate。
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819621681817468.jpg'/>
从左边Encoder开始，输入转换为wordembedding,进入LSTM。LSTM会在每一个时间点上输出hiddenstates。如图中的h1,h2,...,h8。

(2)<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819678688484058.svg'/>，Decoder方面接受的是目标句子里单词的wordembedding，和上一个时间点的hiddenstate。
接下来进入右侧Decoder，输入为(1)句首符号，原始contextvector(为0)，以及从encoder最后一个hiddenstate:h8。LSTM的是输出是一个hiddenstate。（当然还有cellstate，这里没用到，不提。）
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819623777338747.jpg'/>

(3)  <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819679648091265.svg'/>,contextvector是一个对于encoder输出的hiddenstates的一个加权平均。
(4) <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819680210877215.svg'/> ,每一个encoder的hiddenstates对应的权重。
(5) <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819680637877193.svg'/> ,通过decoder的hiddenstates加上encoder的hiddenstates来计算一个分数，用于计算权重(4)
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819627541167161.jpg'/>
Decoder的hiddenstate与Encoder所有的hiddenstates作为输入，放入Attention模块开始计算一个contextvector。之后会介绍attention的计算方法。

下一个时间点
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819628860823770.jpg'/>
来到时间点2，之前的contextvector可以作为输入和目标的单词串起来作为lstm的输入。之后又回到一个hiddnstate。以此循环。

(6)<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819683848695318.svg'/> ,将contextvector和decoder的hiddenstates串起来。
(7) <img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819684211239337.svg'/>，计算最后的输出概率。

<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819631334788367.jpg'/>
另一方面，contextvector和decoder的hiddenstate合起来通过一系列非线性转换以及softmax最后计算出概率。

在luong中提到了三种score的计算方法。这里图解前两种：
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819633397841742.jpg'/>
Attentionscorefunction: dot

输入是encoder的所有hiddenstatesH:大小为(hiddim,sequencelength)。decoder在一个时间点上的hiddenstate，s：大小为（hiddim,1）。
第一步：旋转H为（sequencelength,hiddim)与s做点乘得到一个大小为(sequencelength,1)的分数。
第二步：对分数做softmax得到一个合为1的权重。
第三步：将H与第二步得到的权重做点乘得到一个大小为(hiddim,1)的contextvector。
Attentionscorefunction: general
<img  src='https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64157819635740854076.jpg'/>
输入是encoder的所有hiddenstatesH:大小为(hiddim1,sequencelength)。decoder在一个时间点上的hiddenstate，s：大小为（hiddim2,1）。此处两个hiddenstate的纬度并不一样。
第一步：旋转H为（sequencelength,hiddim1)与 Wa[大小为hiddim1,hiddim2)] 做点乘，再和s做点乘得到一个大小为(sequencelength,1)的分数。
第二步：对分数做softmax得到一个合为1的权重。
第三步：将H与第二步得到的权重做点乘得到一个大小为(hiddim,1)的contextvector。

完结
看懂一个模型的最好办法就是在心里想一遍从输入到模型到输出每一个步骤里，tensor是如何流动的。希望对大家有帮助～点个赞吧

后记
写完这篇看图解说快有一年了，看到评论里的一些关于细节的问题我想在这里统一回复：本文主要介绍Attention机制的一个思路，其实就是一个对于hiddenstates的weightedaverage。至于怎么去用c，s，怎么去算，方法五花八门。如果你大概念不理解，你可以看看这篇文章。如果你对细节有疑问，正确的方法还是去看开源的代码。毕竟最后还是全靠实战。

